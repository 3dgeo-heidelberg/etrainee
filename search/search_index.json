{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"E-TRAINEE","text":"<p>E-learning course on Time Series Analysis in Remote Sensing for Understanding Human-Environment Interactions (E-TRAINEE) was developed by collaboration of research groups from four partner universities \u2013 Charles University, Heidelberg University, University of Innsbruck, and University of Warsaw within the ERASMUS+ Strategic partnership project (ID 2020-1-CZ01-KA203-078308).</p> <p> </p> <p>The course provides a theoretical background to methods used for information extraction from time series of remote sensing data. It consists of the following Modules:</p> <ul> <li>Module 1: Methods of Time Series Analysis in Remote Sensing</li> <li>Module 2: Satellite Multispectral Images Time Series Analysis</li> <li>Module 3: 3D/4D Geographic Point Cloud Time Series Analysis</li> <li>Module 4: Airborne Imaging and Laboratory Spectroscopy Time Series Analysis</li> </ul> <p> </p> <p>Based on several multimodal data sets - satellite and airborne multi- and hyperspectral imagery and point clouds, and applications ranging from landcover monitoring on various spatial scales to 3D change analysis, the course demonstrates the whole processing chain, from data acquisition, pre-processing,  time series analysis to validation and accuracy assessment of the final product. </p> <p>The self-evaluation quizzes and practical exercises follow the theory. Either open data archives, such as Landsat or Sentinel, or datasets acquired within research projects of participating institutions are used for exercises. Moreover, application case studies are provided in Modules 2 to 4. All exercises and case studies are processed in open or free software packages; codes in Python, R, or Google Earth Engine are provided.</p> <p>The course is primarily developed for MSc students of geoinformatics and geography who specialize in remote sensing for monitoring Earth surface dynamics and changes.  It may also be of interest and use to MSc and PhD students in fields related to\u00a0environmental  studies, ecology, geology, and other potential users dealing with remote sensing applications,  such as\u00a0practitioners of national environmental and conservation agencies.</p> <p>Project partners</p> <p>Project news</p> <p> </p>"},{"location":"assets/python_envs/index.html","title":"Environment Yaml files","text":"<p>This directory contains the Yaml files for Modules 1, 3 and 4. In addition, there is the env_config.yaml, which defines the environment name in the yaml file. The goal is to create a merged environment yaml file which can be used for all three modules. This can be done with running <code>conda-merge etrainee_m1.yml etrainee_m3.yml etrainee_m4.yml env_config.yml &gt; etrainee.yml</code> in the conda prompt. This has to be redone manually each time the single modules are modified. The resulting <code>etrainee.yml</code> has to be uploaded in this folder, so it is always up-to-date. The etrainee.yml can then be downloaded with a link on the course site.</p>"},{"location":"assets/python_functions/pointcloud_functions.html","title":"Pointcloud functions","text":"In\u00a0[\u00a0]: Copied! <pre># import required modules\nimport laspy # import the laspy package for handling point cloud files\nimport numpy as np # import numpy for array handling\n</pre> # import required modules import laspy # import the laspy package for handling point cloud files import numpy as np # import numpy for array handling In\u00a0[\u00a0]: Copied! <pre>def read_las(infile,get_attributes=False,use_every=1):\n    '''\n    Function to read coordinates and optionally attribute information of point cloud data from las/laz file.\n\n    :param infile: specification of input file (format: las or laz)\n    :param get_attributes: if True, will return all attributes in file, otherwise will only return coordinates (default is False)\n    :param use_every: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)\n    :return: Array of point coordinates of shape (N,3) with N number of points in input file (or subsampled by 'use_every')\n    '''\n\n    # read the file using the laspy read function\n    indata = laspy.read(infile)\n\n    # get the coordinates (XYZ) and stack them in a 3D array\n    coords = np.vstack((indata.x, indata.y, indata.z)).transpose()\n\n    # subsample the point cloud, if use_every = 1 will remain the full point cloud data\n    coords = coords[::use_every, :]\n\n    # read attributes if get_attributes is set to True\n    if get_attributes == True:\n        # get all attribute names in the las file as list\n        las_fields= list(indata.points.point_format.dimension_names)\n\n        # create a dictionary to store attributes\n        attributes = {}\n\n        # loop over all available fields in the las point cloud data\n        for las_field in las_fields[3:]: # skip the first three fields, which contain coordinate information (X,Y,Z)\n            attribute = np.array(indata.points[las_field]) # transpose shape to (N,1) to fit coordinates array\n            if np.sum(attribute)==0: # if field contains only 0, it is empty\n                continue\n            # add the attribute to the dictionary with the name (las_field) as key\n            attributes[las_field] = attribute[::use_every] # subsample by use_every, corresponding to point coordinates\n\n        # return coordinates and attribute data\n        return (coords, attributes)\n\n    else: # get_attributes == False\n        return (coords) # return coordinates only\n</pre> def read_las(infile,get_attributes=False,use_every=1):     '''     Function to read coordinates and optionally attribute information of point cloud data from las/laz file.      :param infile: specification of input file (format: las or laz)     :param get_attributes: if True, will return all attributes in file, otherwise will only return coordinates (default is False)     :param use_every: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)     :return: Array of point coordinates of shape (N,3) with N number of points in input file (or subsampled by 'use_every')     '''      # read the file using the laspy read function     indata = laspy.read(infile)      # get the coordinates (XYZ) and stack them in a 3D array     coords = np.vstack((indata.x, indata.y, indata.z)).transpose()      # subsample the point cloud, if use_every = 1 will remain the full point cloud data     coords = coords[::use_every, :]      # read attributes if get_attributes is set to True     if get_attributes == True:         # get all attribute names in the las file as list         las_fields= list(indata.points.point_format.dimension_names)          # create a dictionary to store attributes         attributes = {}          # loop over all available fields in the las point cloud data         for las_field in las_fields[3:]: # skip the first three fields, which contain coordinate information (X,Y,Z)             attribute = np.array(indata.points[las_field]) # transpose shape to (N,1) to fit coordinates array             if np.sum(attribute)==0: # if field contains only 0, it is empty                 continue             # add the attribute to the dictionary with the name (las_field) as key             attributes[las_field] = attribute[::use_every] # subsample by use_every, corresponding to point coordinates          # return coordinates and attribute data         return (coords, attributes)      else: # get_attributes == False         return (coords) # return coordinates only In\u00a0[\u00a0]: Copied! <pre>def write_las(outpoints,outfilepath,attribute_dict={},correct_wkt_entry=True):\n\n    '''\n    :param outpoints: 3D array of points to be written to output file\n    :param outfilepath: specification of output file (format: las or laz)\n    :param attribute_dict: dictionary of attributes (key: name of attribute; value: 1D array of attribute values in order of points in 'outpoints'); if not specified, dictionary is empty and nothing is added\n    :return: None\n    '''\n\t\n    # create a header for new las file\n    hdr = laspy.LasHeader(version=\"1.4\", point_format=6)\n\n    # set the coordinate resolutions and offset in the header\n    hdr.x_scale = 0.00025\n    hdr.y_scale = 0.00025\n    hdr.z_scale = 0.00025\n    mean_extent = np.mean(outpoints, axis=0)\n    hdr.x_offset = int(mean_extent[0])\n    hdr.y_offset = int(mean_extent[1])\n    hdr.z_offset = int(mean_extent[2])\n\n    # create the las data\n    las = laspy.LasData(hdr)\n\n    # write coordinates into las data\n    las.x = outpoints[:, 0]\n    las.y = outpoints[:, 1]\n    las.z = outpoints[:, 2]\n    \n    # add all dictionary entries to las data (if available)\n    for key,vals in attribute_dict.items():\n        if not key in las:\n            las.add_extra_dim(laspy.ExtraBytesParams(\n            name=key,\n            type=type(vals[0])\n            ))\n        las[key] = vals\n\n    # write las file\n    las.write(outfilepath)\n    \n    # this is required because alobal encoding WKT flag must be set for point format 6 - 10 since las 1.4\n    # otherwise programs such as pdal will not be able to read the file\n    if correct_wkt_entry:\n        filename = outfilepath\n        f = open(filename, \"rb+\")\n        f.seek(6)\n        f.write(bytes([17, 0, 0, 0]));\n        f.close()\n\n    return\n</pre> def write_las(outpoints,outfilepath,attribute_dict={},correct_wkt_entry=True):      '''     :param outpoints: 3D array of points to be written to output file     :param outfilepath: specification of output file (format: las or laz)     :param attribute_dict: dictionary of attributes (key: name of attribute; value: 1D array of attribute values in order of points in 'outpoints'); if not specified, dictionary is empty and nothing is added     :return: None     ''' \t     # create a header for new las file     hdr = laspy.LasHeader(version=\"1.4\", point_format=6)      # set the coordinate resolutions and offset in the header     hdr.x_scale = 0.00025     hdr.y_scale = 0.00025     hdr.z_scale = 0.00025     mean_extent = np.mean(outpoints, axis=0)     hdr.x_offset = int(mean_extent[0])     hdr.y_offset = int(mean_extent[1])     hdr.z_offset = int(mean_extent[2])      # create the las data     las = laspy.LasData(hdr)      # write coordinates into las data     las.x = outpoints[:, 0]     las.y = outpoints[:, 1]     las.z = outpoints[:, 2]          # add all dictionary entries to las data (if available)     for key,vals in attribute_dict.items():         if not key in las:             las.add_extra_dim(laspy.ExtraBytesParams(             name=key,             type=type(vals[0])             ))         las[key] = vals      # write las file     las.write(outfilepath)          # this is required because alobal encoding WKT flag must be set for point format 6 - 10 since las 1.4     # otherwise programs such as pdal will not be able to read the file     if correct_wkt_entry:         filename = outfilepath         f = open(filename, \"rb+\")         f.seek(6)         f.write(bytes([17, 0, 0, 0]));         f.close()      return In\u00a0[\u00a0]: Copied! <pre>def transform_points(points, trafomat, reduction_point = [.0,.0,.0]):\n    '''\n    Applies a rigid transformation, i.e. rotation and translation, to 3D point data.\n    :param points: 2D array of 3D points with shape (N,3)\n    :param trafomat: 2D array of rigid transformation matrix with 3x3 rotation and 1x3 translation parameters\n    :return: transformed points in 2D array of shape (N,3)\n    '''\n    \n    rotation = np.array(trafomat)[:, :3]\n    translation = np.array(trafomat)[:, 3].flatten()\n\n    points -= centroid\n    pts_rot = points.dot(rotation.T)\n    pts_trafo = pts_rot + translation\n    points_out = pts_trafo[:,:3] + centroid\n    \n    return points_out\n</pre> def transform_points(points, trafomat, reduction_point = [.0,.0,.0]):     '''     Applies a rigid transformation, i.e. rotation and translation, to 3D point data.     :param points: 2D array of 3D points with shape (N,3)     :param trafomat: 2D array of rigid transformation matrix with 3x3 rotation and 1x3 translation parameters     :return: transformed points in 2D array of shape (N,3)     '''          rotation = np.array(trafomat)[:, :3]     translation = np.array(trafomat)[:, 3].flatten()      points -= centroid     pts_rot = points.dot(rotation.T)     pts_trafo = pts_rot + translation     points_out = pts_trafo[:,:3] + centroid          return points_out"},{"location":"assets/python_functions/vector_functions.html","title":"Vector functions","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def getAngle(v1, v2):\n    '''\n    Calculate the angle between two vectors.\n\n    Parameters:\n        v1 (numpy.array): The first vector.\n        v2 (numpy.array): The second vector.\n\n    Returns:\n        float: The calculated angle.\n    '''\n    rad=np.arccos(np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)))\n    angle=rad*(180/np.pi)\n    return angle\n</pre> def getAngle(v1, v2):     '''     Calculate the angle between two vectors.      Parameters:         v1 (numpy.array): The first vector.         v2 (numpy.array): The second vector.      Returns:         float: The calculated angle.     '''     rad=np.arccos(np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)))     angle=rad*(180/np.pi)     return angle In\u00a0[\u00a0]: Copied! <pre>def rotate(v1, v2, norm, angl):\n    '''\n    Take two vectors and rotate the first one around a given line, the second one gets rotated in the same manner, so that both vectors keep their angle to each other.\n\n    Parameters:\n        v1 (numpy.array): The first vector.\n        v2 (numpy.array): The second vector.\n        norm (numpy.array): A vector that represents the direction of a line. vec1/vec2 will be rotated around this line.\n        angl (numpy.array): A given angle that defines about how many degrees both vectors gets rotated\n\n    Returns:\n        tuple:\n            - numpy.array -- The rotated first vector.\n            - numpy.array -- The rotated second vector.\n    '''\n    nx,ny,nz = norm\n    r1 = [nx*nx*(1-np.cos(angl)) + np.cos(angl),\n        nx*ny*(1-np.cos(angl)) - nz * np.sin(angl),\n        nx*nz*(1-np.cos(angl)) + ny * np.sin(angl)]\n\n    r2 = [ny*nx*(1-np.cos(angl)) + nz * np.sin(angl),\n        ny*ny*(1-np.cos(angl)) + np.cos(angl),\n        ny*nz*(1-np.cos(angl)) - nx * np.sin(angl)]\n\n    r3 = [nz*nx*(1-np.cos(angl)) - ny * np.sin(angl),\n        nz*ny*(1-np.cos(angl)) + nx * np.sin(angl),\n        nz*nz*(1-np.cos(angl)) + np.cos(angl)]\n\n    mat = (r1,r2,r3)\n\n    return np.matmul(v1, mat), np.matmul(v2, mat)\n</pre> def rotate(v1, v2, norm, angl):     '''     Take two vectors and rotate the first one around a given line, the second one gets rotated in the same manner, so that both vectors keep their angle to each other.      Parameters:         v1 (numpy.array): The first vector.         v2 (numpy.array): The second vector.         norm (numpy.array): A vector that represents the direction of a line. vec1/vec2 will be rotated around this line.         angl (numpy.array): A given angle that defines about how many degrees both vectors gets rotated      Returns:         tuple:             - numpy.array -- The rotated first vector.             - numpy.array -- The rotated second vector.     '''     nx,ny,nz = norm     r1 = [nx*nx*(1-np.cos(angl)) + np.cos(angl),         nx*ny*(1-np.cos(angl)) - nz * np.sin(angl),         nx*nz*(1-np.cos(angl)) + ny * np.sin(angl)]      r2 = [ny*nx*(1-np.cos(angl)) + nz * np.sin(angl),         ny*ny*(1-np.cos(angl)) + np.cos(angl),         ny*nz*(1-np.cos(angl)) - nx * np.sin(angl)]      r3 = [nz*nx*(1-np.cos(angl)) - ny * np.sin(angl),         nz*ny*(1-np.cos(angl)) + nx * np.sin(angl),         nz*nz*(1-np.cos(angl)) + np.cos(angl)]      mat = (r1,r2,r3)      return np.matmul(v1, mat), np.matmul(v2, mat) In\u00a0[\u00a0]: Copied! <pre>def transform(v1, v2):\n    '''\n    Take two vectors as input and rotate the second vector, so that it has the same angle to the z-unitvector as the first vector to the second one.\n\n    Parameters: \n        v1 (numpy.array): The first vector.\n        v2 (numpy.array): The second vector.\n\n    Returns: \n        numpy.array: The rotated second vector.\n    '''\n    # create normal vector of v1 and z-unitvector\n    n = np.cross(v1, (0,0,1))\n    n1,n2,n3 = n\n    n = n / np.sqrt(n1*n1 + n2*n2 + n3*n3) # normalize normal vector\n\n    a = getAngle((0,0,1),v1)\n\n    r1,r2 = rotate(v1,v2,n,-np.radians(a))\n\n    return r2\n</pre> def transform(v1, v2):     '''     Take two vectors as input and rotate the second vector, so that it has the same angle to the z-unitvector as the first vector to the second one.      Parameters:          v1 (numpy.array): The first vector.         v2 (numpy.array): The second vector.      Returns:          numpy.array: The rotated second vector.     '''     # create normal vector of v1 and z-unitvector     n = np.cross(v1, (0,0,1))     n1,n2,n3 = n     n = n / np.sqrt(n1*n1 + n2*n2 + n3*n3) # normalize normal vector      a = getAngle((0,0,1),v1)      r1,r2 = rotate(v1,v2,n,-np.radians(a))      return r2 In\u00a0[\u00a0]: Copied! <pre>def getAspect(normal):\n    '''\n    Calculate the aspect of a given vector.\n\n    Parameters: \n        normal: The vector whose aspect should be returned.\n\n    Returns:\n        float: The calculated aspect.\n    '''\n    nx,ny,nz=normal\n\n    a = 0.0\n    b = 0.0\n    a = float(0.0 * nx + 1.0 * ny)\n\n    b = float(np.sqrt(nx * nx + ny * ny) * 1)\n    cn = 0.0\n\n    if b == 0.0:\n        cn=0.0\n    else:\n        cn=((np.arccos(a/b)*180.0)/np.pi)\n        if nx &lt; 0.0:\n            cn = 360.0 - cn\n\n    return cn\n</pre> def getAspect(normal):     '''     Calculate the aspect of a given vector.      Parameters:          normal: The vector whose aspect should be returned.      Returns:         float: The calculated aspect.     '''     nx,ny,nz=normal      a = 0.0     b = 0.0     a = float(0.0 * nx + 1.0 * ny)      b = float(np.sqrt(nx * nx + ny * ny) * 1)     cn = 0.0      if b == 0.0:         cn=0.0     else:         cn=((np.arccos(a/b)*180.0)/np.pi)         if nx &lt; 0.0:             cn = 360.0 - cn      return cn In\u00a0[\u00a0]: Copied! <pre>def getSlope(normal):\n    '''\n    Calculate the slope of a given vector.\n\n    Parameters: \n        normal: The vector whose slope should be returned.\n\n    Returns:\n        float: The calculated slope.\n    '''\n    nx,ny,nz=normal\n\n    dx = float(np.sqrt(nx * nx + ny * ny))\n    dy = float(nz)\n    c = 0.0\n\n    if dx == 0.0:\n        c=0.0\n    else:\n        c=abs(np.arctan(dy/dx))*180.0/np.pi\n        c=90.0-c\n\n    return c\n</pre> def getSlope(normal):     '''     Calculate the slope of a given vector.      Parameters:          normal: The vector whose slope should be returned.      Returns:         float: The calculated slope.     '''     nx,ny,nz=normal      dx = float(np.sqrt(nx * nx + ny * ny))     dy = float(nz)     c = 0.0      if dx == 0.0:         c=0.0     else:         c=abs(np.arctan(dy/dx))*180.0/np.pi         c=90.0-c      return c"},{"location":"assets/r_envs/index.html","title":"R environment for Module 2","text":"<p>This directory contains the Lockfile for Module 2 R environment. It is used by <code>renv</code> package to capture the state of packages library. The file contains the R version used in the source project, as well as package records with versions and installation sources.</p> <p>The <code>renv.lock</code> file will always contain the tested up-to-date environment which was used to develop exercises.</p>"},{"location":"data_usecases/usecase_beach_kijkduin.html","title":"Use case: Permanent laser scanning observation of the sandy beach at Kijkduin (The Netherlands)","text":"<p>This use case is part of the CoastScan project, lead by researchers at TU Delft.</p>"},{"location":"data_usecases/usecase_beach_kijkduin.html#background-and-motivation","title":"Background and motivation","text":"<p>Coastal environments are highly dynamic and their morphology is continuously changing. With changing climate, high-impact events like storms are becoming more frequent. But the complex interplay of processes that shape the coast is not yet well understood. Coastal monitoring is therefore important for coastal management, as well as for research. Observations allow modeling and predicting coastal dynamics and processes such as sand transport on a beach during storms or recovery of the beach morphology after such events (Vos et al., 2022).</p> <p>Current coastal management uses observation strategies that are either low in spatial resolution (e.g., satellite remote sensing) or low in temporal resolution (e.g. annual airborne LiDAR surveys). To close this gap, near-continuous terrestrial laser scanning is used for spatiotemporally detailed observation (see CoastScan project). This observation strategy generates time series of 3D point clouds, which are introduced in module 3.</p> <p>You can find a detailed overview on all data acquired in the CoastScan project on the website: https://coastscan.citg.tudelft.nl/index.php/important-dates-for-the-coastscan-project/</p>"},{"location":"data_usecases/usecase_beach_kijkduin.html#study-site-and-data","title":"Study site and data","text":"<p>In this course, we use the laser scanning time series acquired at Kijkduin beach (52\u00b004\u201914\u201d N 4\u00b013\u201910\u201d E) in The Netherlands from November 2016 to May 2017 (Vos et al., 2022), which is openly available on the PANGAEA data repository.</p> <p>During this period, a terrestrial laser scanner (Riegl VZ-2000) was mounted on top of a hotel building adjacent to the beach area (see map below) at a height of around 30 m and 100 to 600 m distance to the target area on the beach. A Riegl VZ-2000 was used as instrument, mounted on an iron pole attached which was fixed to the building roof terrace (Vos et al., 2022).</p> <p> <p>Study area and data of terrestrial laser scanning observation at Kijkduin beach, The Netherlands. The point cloud (left) is colored by RGB for visualization, showing the dune area with a path way down to the beach. The overlay shows possible changes due to sand transport when comparing to epochs. The aerial image (right) shows the observed beach area and the laser scanner position (triangle) on the building roof (see photo). Figure by K. Anders. Data source: S. Vos (point cloud, cf. Vos et al., 2017) and Het Waterschapshuis (2017) (aerial photo, provided by pdok.nl) / [CC BY 4.0]. </p> <p>The dataset consists of 4,082 point clouds hourly between 11 November 2016 and 26 May 26th 2017. Therein, regular scans were acquired at 0.05\u00b0 resolution, resulting in a point spacing of 2 to 20 points/m\u00b2 (depending on the range). Additionally, a high-resolution scan was acquired at 0.013\u00b0 resolution around noon starting end of January 2017 (Vos et al., 2022).</p> <p> <p>View of scans acquired at standard (left part of map) and high (right part of map) resolution, and time series of height change on the beach at one location extracted from the point cloud time series. Figure by K. Anders, modified from Vos et al. (2022) / CC BY 4.0 and Anders et al. (2019) / CC BY 4.0. </p>"},{"location":"data_usecases/usecase_beach_kijkduin.html#use-case-in-the-course","title":"Use case in the course","text":"<p>The use case is featured in the following parts of the course:</p> <ul> <li>Exercise: Time series analysis of 3D point clouds (Module 3, Theme 4)</li> <li>Case study: Time series-based change analysis of sandy beach dynamics (Module 3, Theme 6)</li> </ul>"},{"location":"data_usecases/usecase_beach_kijkduin.html#references","title":"References","text":"<ul> <li>Vos, S., Anders, K., Kuschnerus, M., Lindenbergh, R., H\u00f6fle, B., Aarninkhof, S., &amp; de Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands. Scientific Data, 9 (1), pp. 191. doi: 10.1038/s41597-022-01291-9.</li> <li>Vos, S., Anders, K., Kuschnerus, M., Lindenbergh, R., H\u00f6fle, B., Aarninkhof, S., &amp; de Vries, S. (2021). A six month high resolution 4D geospatial stationiary laser scan dataset of the Kijkduin beach dune system, The Netherlands: PANGAEA.</li> <li>Vos, S., Lindenbergh, R., &amp; de Vries, S. (2017). CoastScan: Continuous Monitoring of Coastal Change using Terrestrial Laser Scanning. Proceedings of Coastal Dynamics 2017, 233, pp. 1518-1528.</li> </ul>"},{"location":"data_usecases/usecase_forests_tatras.html","title":"Use case: Vegetation disturbance detection in Polish-Slovak Tatra Mountains","text":"<p>This use case is part of the AveReS project, lead by researchers at University of Warsaw and part of Adrian Ochtyra PhD thesis.</p>"},{"location":"data_usecases/usecase_forests_tatras.html#background-and-motivation","title":"Background and motivation","text":"<p>Constant monitoring of the environment allows the detection of disturbances and recovery of vegetation. Satellite image data acquired for decades and developed newer and newer processing methods and algorithms ensures excellent support for traditional monitoring and preservation of vegetation. Temporal-scale changes in vegetation can be divided into two categories: abrupt (caused by e.g.\u00a0windstorm) and gradual (e.g.\u00a0pest infestation). Quantitative estimation of losses after such disasters can be technically difficult with only field inventory (Nikolov et al., 2014). It can be more efficiently provided with e.g.\u00a0annual time series data from various sensors, where a great example is Landsat, whose mission began as early as 1972 (Kennedy et al., 2010). Change detection algorithms have evolved from simple ones that indicate changes to more sophisticated ones that enable the analysis of continuous processes, trends or phenological changes in dense time series data (Zhu et al., 2014).</p>"},{"location":"data_usecases/usecase_forests_tatras.html#study-site-and-data","title":"Study site and data","text":"<p>In this course we will focus on forest disturbance detection in Tatras, which is the highest mountain range in the Carpathian Mountains. It is an important area for biodiversity conservation, protected by two national parks: the Slovak Tatransk\u00fd N\u00e1rodn\u00fd Park (TANAP) and the Polish Tatrza\u0144ski Park Narodowy (TPN). Forests cover about 60% of the mountains, where 14% is dwarf pine shrubs (Ochtyra et al., 2020). Spruce dominates the majority of the tree stands, while in the lower forest zone, spruce and fir together form tree stands.</p> <p>Strong winds and bark beetle (Ips typographus [L.]) outbreaks are the primary causes of vegetation disturbances in Polish-Slovak Tatra Mountains, in particular of spruce (Picea abies [L.] Karst.) ecosystems (Mezei et al., 2014).</p> <p> <p></p> <p>Tatra Mountains (red squares in the thumbnail on the left indicate Landsat scenes location, figure by course authors, Landsat 8 courtesy of the U.S. Geological Survey/ Terms of use) and the effects of: (A) bark beetle outbreak (photo by A. Ochtyra) and (B) bora wind (photo by Fal\u0165an et al., 2020/ CC BY 4.0). </p>"},{"location":"data_usecases/usecase_forests_tatras.html#use-case-in-the-course","title":"Use case in the course","text":"<p>The use case is featured in the following parts of the course:</p> <ul> <li>Exercise: Principles of multispectral imaging (Module 2, Theme 1)</li> <li>Exercise: Temporal information in satellite data (Module 2, Theme 2)</li> <li>Exercise: Image processing (Module 2, Theme 3)</li> <li>Exercise: Vegetation change and disturbance detection (Module 2, Theme 5)</li> <li>Case study: Forest disturbance detection (Tatra Mountains) (Module 2, Case Study 3)</li> </ul>"},{"location":"data_usecases/usecase_forests_tatras.html#references","title":"References","text":"<p>Fal\u0165an, V., Katina, S., Min\u00e1r, J., Pol\u010d\u00e1k, N., B\u00e1novsk\u00fd, M., Maretta, M., \u2026 &amp; Petrovi\u010d, F. (2020). Evaluation of abiotic controls on windthrow disturbance using a generalized additive model: A case study of the Tatra National Park, Slovakia. Forests, 11(12), 1259. https://doi.org/10.3390/f11121259</p> <p>Kennedy, R. E., Cohen, W. B., &amp; Schroeder, T. A. (2007). Trajectory-based change detection for automated characterization of forest disturbance dynamics. Remote Sensing of Environment, 110(3), 370-386. https://doi.org/10.1016/j.rse.2010.07.008</p> <p>Mezei, P., Grodzki, W., Bla\u017eenec, M., &amp; Jaku\u0161, R. (2014). Factors influencing the wind\u2013bark beetles\u2019 disturbance system in the course of an Ips typographus outbreak in the Tatra Mountains. Forest Ecology and Management, 312, 67-77. https://doi.org/10.1016/j.foreco.2013.10.020</p> <p>Nikolov, C., Kon\u00f4pka, B., Kajba, M., Galko, J., Kunca, A., &amp; Jansk\u00fd, L. (2014). Post-disaster forest management and bark beetle outbreak in Tatra National Park, Slovakia. Mountain Research and Development, 34(4), 326-335. https://doi.org/10.1659/MRD-JOURNAL-D-13-00017.1</p> <p>Ochtyra, A., Marcinkowska-Ochtyra, A., &amp; Raczko, E. (2020). Threshold-and trend-based vegetation change monitoring algorithm based on the inter-annual multi-temporal normalized difference moisture index series: A case study of the Tatra Mountains. Remote Sensing of Environment, 249, 112026. https://doi.org/10.1016/j.rse.2020.112026</p> <p>Zhu, Z., &amp; Woodcock, C. E. (2014). Continuous change detection and classification of land cover using all available Landsat data. Remote sensing of Environment, 144, 152-171. https://doi.org/10.1016/j.rse.2014.01.011</p>"},{"location":"data_usecases/usecase_grasses_krkonose.html","title":"Use case: Tundra vegetation monitoring in Krkono\u0161e Mountains","text":"<p>All data was collected by researchers from Charles University, Krkono\u0161e Mts. National Park Administration, and Institute of Botany of the Czech Academy of Sciences as part of the project CZ.05.4.27/0.0/0.0/17_078/0009044.</p>"},{"location":"data_usecases/usecase_grasses_krkonose.html#background-and-motivation","title":"Background and motivation","text":"<p>The relict arctic-alpine tundra located in the Krkono\u0161e Mountains National Park (KRNAP) above the treeline (1300 m a. s. l., 50.73N, 15.69E) combines arctic, alpine, and middle European flora and fauna with many endangered and endemic species. Different management schemes have been applied in the tundra grasslands over the last 100 years \u2013 from grazing and cutting to no human interference. Due to that, as well as nitrogen deposition from acid rains in the 1970s, 80s, and 90s and ongoing climate change, some changes in vegetation composition have been observed: closed alpine grasslands dominated by Nardus stricta are threatened by the spread of three native but competitive grass species: Calamagrostis villosa, Molinia caerulea (Hejcman et al., 2010), and Deschampsia cespitosa (Hejcman et al., 2009). Also, the expansion of Pinus mugo shrub (\u0160tursa and Wild, 2014) and Norway spruce (Treml et al., 2012) was observed. Systematic monitoring of the changes is a key element of tundra conservation. Since 2019, teams from Charles University, in cooperation with the National Park Administration and the Institute of Botany, have been monitoring the changes using remote sensing methods. For the first results, see Kupkov\u00e1 et al. (2023)</p>"},{"location":"data_usecases/usecase_grasses_krkonose.html#study-site-and-data","title":"Study site and data","text":"<p>Three main types of tundra are present in KRNAP: cryo-eolian / lichen tundra (mosses, lichens, and alpine heathlands) on the tops of the highest mountains; vegetated-cryogenic / grassy tundra (closed alpine grasslands dominated by Nardus stricta and subalpine tall grasslands, Pinus mugo shrubs, and peat bogs) on the plateaus of the huts Lu\u010dn\u00ed bouda and Labsk\u00e1 bouda; and niveo-glacigenic / flower-rich tundra in the glacier cirques (Koci\u00e1nov\u00e1 et al., 2015; Soukupov\u00e1 et al., 1995). During the above-mentioned project, data was acquired repeatedly for sample areas of each tundra type for four years, four times in a vegetation season. In Module 4, we will use data from lichen tundra (plot called Lu\u010dn\u00ed hora mountain) and grassy tundra (plot called B\u00edl\u00e1 louka meadow), where most changes in vegetation composition are being observed.</p>"},{"location":"data_usecases/usecase_grasses_krkonose.html#data","title":"Data","text":"<p>Hyperspectral images were acquired with the Headwall Nano-Hyperspec\u00ae fastened to the DJI Matrice 600 Pro drone. The flight height was 68.9 m above the terrain to ensure a ground sampling distance of 3 cm. The scanned areas were 100 m x 100 m in size. The radiometric corrections were performed in the Headwall SpectralView - Hyperspec v3.1.0 software using a portable 3 m x 3 m fabric target placed in the scanned area. The data cube consists of 269 bands, with wavelengths ranging from 400 to 1,000 nm. The geometric corrections were performed in Headwall SpectralView - Hyperspec v3.1.0 and ArcGIS Desktop. The manual addition of tie points and the development of scripts to mosaic the data in ArcGIS Desktop were necessary to reach an acceptable root mean square error (RMSE) on ground control points (RMSE of about 0.1 m).</p> <p>The mosaics used in this course are resampled to 54 bands and 9 cm to reduce data volume and noise. For both study areas, data acquired in 2020 (or 2019) is provided, approximately in the middle of the months of June (the beginning of the season in tundra), July, August, and September (the end of the season).</p> <p>The data is provided in the Czech national coordinate system S-JTSK / Krovak East North (EPSG: 5514). </p>"},{"location":"data_usecases/usecase_grasses_krkonose.html#grasses","title":"Grasses","text":"<p>All four studied grass species (Figures 1-3) belong to the Poaceae family. Nardus stricta (nard), also known as matgrass, is a strongly tufted grass that usually grows to maximum heights of 40 cm. The leaves are gray-green, thin, and rough, and the stems are stiff and hard. The old biomass is present during the whole growing season. Calamagrostis villosa (cv), or reedgrass, is up to 1.2 m high, the knees are often hairy, and the grass-green leaves are smooth and usually 4\u201310 mm wide. In the KRNAP tundra, the leaves turn a characteristic violet color in August and September (Figure 3). Molinia caerulea (mol), also called purple moor-grass, is a very resistant grass, also up to 1.2 m high, with light-green leaves. It prefers sunny and moist locations. In the KRNAP tundra, it is characterized by a large amount of old biomass in June, which can be clearly seen in aerial images (Figure 1). Deschampsia cespitosa (desch), known as tufted hairgrass or tussock grass, can grow to 1.4 m and flowers distinctively, mainly in July and August in the KRNAP tundra. The dark green leaf blade's upper surface has a rough texture and can cut in one direction, but is smooth in the other. (Kub\u00e1t and B\u011blohl\u00e1vkov\u00e1, 2002) </p> <p> </p> <p>Figure 1. Studied grass species in June visualized, on the hyperspectral image acquired on June 16th, 2020.</p> <p> </p> <p>Figure 2. Studied grass species in July, visualized on the hyperspectral image acquired on July 13th, 2020.</p> <p> </p> <p>Figure 3. Studied grass species in August, visualized on the hyperspectral image acquired on August  11th, 2020.</p>"},{"location":"data_usecases/usecase_grasses_krkonose.html#use-case-in-the-course","title":"Use case in the course","text":"<p>The use case is featured in the following parts of the course:</p> <ul> <li> <p>Exercise: Exploration of hyperspectral data using EnMAP-Box (Module 4, Theme 1)</p> </li> <li> <p>Exercise: Geometric correction (Module 4, Theme 2) </p> </li> <li> <p>Tutorial: Spectra smoothening and denoising (Module 4, Theme 2) </p> </li> <li> <p>Exercise: Classification of hyperspectral data (Module 4, Theme 4)</p> </li> <li> <p>Case study: Seasonal spectral separability of selected grass species of the Krkono\u0161e Mts. tundra ecosystem (Module 4, Theme 8) </p> </li> <li> <p>Case study: Discrimination of selected grass species from time series of RPAS hyperspectral imagery (Module 4, Theme 6)</p> </li> </ul>"},{"location":"data_usecases/usecase_grasses_krkonose.html#references","title":"References","text":"<p>Hejcman, M., \u010ce\u0161kov\u00e1, M., Pavl\u016f, V., (2010). Control of Molinia caerulea by cutting management on sub-alpine grassland. Flora - Morphol. Distrib. Funct. Ecol. Plants 205, 577\u2013582. 10.1016/j.flora.2010.04.019. </p> <p>Hejcman, M., Klaudisov\u00e1, M., Hejcmanov\u00e1, P., Pavl\u016f, V., Jones, M., (2009). Expansion of Calamagrostis villosa in sub-alpine Nardus stricta grassland: Cessation of cutting management or high nitrogen deposition? Agric. Ecosyst. Environ. 129, 91\u201396. 10.1016/j.agee.2008.07.007.  </p> <p>Koci\u00e1nov\u00e1, M., \u0160tursa, J., Van\u011bk, J., (2015). Krkono\u0161sk\u00e1 tundra. Spr\u00e1va Krkono\u0161sk\u00e9ho n\u00e1rodn\u00edho parku, Vrchlab\u00ed.</p> <p>Kub\u00e1t, K., B\u011blohl\u00e1vkov\u00e1, R. (Eds.), (2002). Kl\u00ed\u010d ke kv\u011bten\u011b \u010cesk\u00e9 republiky. 1. edition. Academia, Praha.</p> <p>Kupkov\u00e1, L., \u010cerven\u00e1,L., Pot\u016f\u010dkov\u00e1, M., Lys\u00e1k, J., Roubalov\u00e1, M., Hr\u00e1zsk\u00fd, Z., B\u0159ezina, S., Epstein, H.E., M\u00fcllerov\u00e1, J. 2023. Towards reliable monitoring of grass species in nature conservation: Evaluation of the potential of UAV and PlanetScope multi-temporal data in the Central European tundra. Remote Sensing of Environment, 294, 113645. ISSN 0034-4257. 10.1016/j.rse.2023.113645.</p> <p>Soukupov\u00e1, L., Koci\u00e1nov\u00e1, M., Jen\u00edk, J., Sekyra, J., (1995). Arctic alpine tundra in the Krkono\u0161e, the Sudetes. Opera Corcon. 32, 5\u201388.</p> <p>\u0160tursa, J., Wild, J., (201). Kle\u010d a smilka \u2013 kl\u00ed\u010dov\u00ed hr\u00e1\u010di v\u00fdvoje alp\u00ednsk\u00e9ho bezles\u00ed Krkono\u0161 (Vysok\u00e9 Sudety, \u010cesk\u00e1 republika). Opera Corcon. 51, 5\u201336.</p> <p>Treml, V., Ponocn\u00e1, T., B\u00fcntgen, U., (2012). Growth trends and temperature responses of treeline Norway spruce in the Czech-Polish Sudetes Mountains. Clim. Res. 55, 91\u2013103. 10.3354/cr01122.</p>"},{"location":"data_usecases/usecase_ore_mts_disturbance.html","title":"Use case: Forest disturbances in Ore Mountains (Czechia)","text":""},{"location":"data_usecases/usecase_ore_mts_disturbance.html#background-and-motivation","title":"Background and motivation","text":"<p>The forests in Ore Mountains with majority of spruce monocultures were heavily damaged by acid pollution in the second half of the 20<sup>th</sup> century. The pollution was caused mainly by the industry and power plants using the lignite deposits mined from nearby open pit mines. The lignite has a high sulphur content. Until 1990s the sulphur filters were not used in former Czechoslovakia. Therefore, SO<sub>2</sub> was the main air pollution agent. The forest decline started already in late 1940s and despite several management actions (planting more resistant and pioneering species, airborne liming) continued till 1980s. The area belonged to the most polluted areas in Europe, often called as \u201cthe black triangle\u201d (border area of Germany, Czechia, and Poland). After new regulations in 1990s, the situation stared to become better, and the forest recovery is still in progress. The use case follows the methodology applied in the publication Kupkov\u00e1 et al., 2018 which compared forest changes between the western part of the mountains that was less affected by the pollution and the eastern part where the damages were severe.</p>"},{"location":"data_usecases/usecase_ore_mts_disturbance.html#study-site-and-data","title":"Study site and data","text":"<p>The Ore Mountains (Kru\u0161n\u00e9 hory, Das Erzgebirge, 50.40N, 12.97E) is about 130 km long mountain range creating a natural border North-Western Bohemia and Saxony. The study area covers the western part of the Ore Mountains (about 90 km long) with the highest elevation Kl\u00ednovec (1244 meters a. s. l.). The mountain separates the study area into two geologically distinct localities (in the study Kupkov\u00e1 et al., 2018 called west and east).</p> <p> <p>Location of the Ore mountains study area. (figure by Kupkov\u00e1 et al., 2018, CC BY 3.0 DEED) </p> <p>The Case study will use Landsat Collection 2, Level 2 images from 1984 till 2023. The open archive of Landsat images provided by the US Geological Survey will be accessed vie Google Earth Engine tools.</p>"},{"location":"data_usecases/usecase_ore_mts_disturbance.html#use-case-in-the-course","title":"Use case in the course","text":"<p>The use case is featured in a case study of Module 2: Forest disturbances in Ore Mountains (Czechia).</p>"},{"location":"data_usecases/usecase_ore_mts_disturbance.html#references","title":"References","text":"<p>Key reference</p> <p>Kupkov\u00e1, L., Pot\u016f\u010dkov\u00e1, M., Lhot\u00e1kov\u00e1, Z., Albrechtov\u00e1, J. (2018). Forest cover and disturbance changes, and their driving forces: A case study in the Ore Mountains, Czechia, heavily affected by anthropogenic acidic pollution in the second half of the 20th century. Environmental Research Letters, 13(9), 095008. https://iopscience.iop.org/article/10.1088/1748-9326/aadd2c</p> <p>Other references (optional)</p> <p>The following list provides selected research studies on spruce forest decline, recovery, and resistance to air pollution in the Ore Mountains using both multispectral satellite data and airborne hyperspectral data (that is the topic of Module 4 of this course).</p> <p>Campbell, P. E., Rock, B. N., Martin, M. E., Neefus, C. D., Irons, J. R., Middleton, E. M., Albrechtova, J. (2004). Detection of initial damage in Norway spruce canopies using hyperspectral airborne data. International Journal of Remote Sensing, 25(24), 5557-5584. https://doi.org/10.1080/01431160410001726058</p> <p>Cudl\u00edn, P., Sej\u00e1k, J., Pokorn\u00fd, J., Albrechtov\u00e1, J., Bastian, O., Marek, M. (2013). Forest ecosystem services under climate change and air pollution. Developments in Environmental Science, 13, 521-546. https://doi.org/10.1016/B978-0-08-098349-3.00024-4</p> <p>Mi\u0161urec, J., Kopa\u010dkov\u00e1, V., Lhot\u00e1kov\u00e1, Z., Hanu\u0161, J., Weyermann, J., Entcheva-Campbell, P., Albrechtov\u00e1, J. (2012). Utilization of hyperspectral image optical indices to assess the Norway spruce forest health status. Journal of Applied Remote Sensing, 6(1), 063545-063545. https://doi.org/10.1117/1.JRS.6.063545</p> <p>Mi\u0161urec, J., Kopa\u010dkov\u00e1, V., Lhot\u00e1kov\u00e1, Z., Campbell, P., Albrechtov\u00e1, J. (2016). Detection of spatio-temporal changes of Norway spruce forest stands in Ore Mountains using Landsat time series and airborne hyperspectral imagery. Remote Sensing, 8(2), 92. https://doi.org/10.3390/rs8020092</p> <p>Pol\u00e1k, T., Rock, B. N., Campbell, P. E., Soukupov\u00e1, J., Solcov\u00e1, B., Zv\u00e1ra, K., Albrechtov\u00e1, J. (2006). Shoot growth processes, assessed by bud development types, reflect Norway spruce vitality and sink prioritization. Forest Ecology and Management, 225(1-3), 337-348. https://doi.org/10.1016/j.foreco.2006.01.027</p> <p>Soukupov\u00e1, J., Rock, B. N., Albrechtov\u00e1, J. (2001). Comparative study of two spruce species in a polluted mountainous region. New Phytologist, 150(1), 133-145. https://doi.org/10.1046/j.1469-8137.2001.00066.x</p>"},{"location":"data_usecases/usecase_rockglacier_ahk.html","title":"Use case: Multitemporal and multi-sensor observation of the active rock glacier \u00c4u\u00dferes Hochebenkar (Austria)","text":"<p>This use case is part of the AHK-4D project, lead by researchers at Heidelberg University.</p>"},{"location":"data_usecases/usecase_rockglacier_ahk.html#background-and-motivation","title":"Background and motivation","text":"<p>Rock glaciers are active creep phenomena in high-mountain environments. They occur under permafrost conditions where unconsolidated and ice-supersaturated debris causes topographic deformation (Barsch et al., 1992). Changes to rock glacier surfaces reflect the interaction of various geomorphic deformation processes. These include permafrost creep, permafrost slide, zonal thinning or thickening, advection of surface microtopography, 3D straining, general mass changes (heaving or settlement), and horizontal shearing and rotation. The surface changes induced by these processes feature different spatial characteristics, magnitudes and timescales of occurrence, which are not yet fully understood (Moore 2014). Monitoring 3D surface change at high spatial resolution (centimeter point spacing) and high temporal resolution (sub-monthly) allows characterizing different geomorphic processes by their spatiotemporal dynamics and to reveal their contribution to surface change on rock glaciers. This information can enhance our general understanding of the spatial and temporal variability of rock glacier deformation and the interaction of rock glaciers with connected environmental systems.</p>"},{"location":"data_usecases/usecase_rockglacier_ahk.html#study-site-and-data","title":"Study site and data","text":"<p>In this course, we use multitemporal point clouds of the rock glacier \u00c4u\u00dferes Hochebenkar (46\u00b050\u203229\u2033 N 11\u00b00\u203215\u2033 E), located in the southern \u00d6tztal Alps in Austria (Zahs et al., 2022). The active tongue\u2010shaped, bouldery talus rock glacier is being extensively monitored with different survey strategies and sensors since the 1930s in order to decode the complex dynamics of rock glacier surface and subsurface activities and their driving forces. Multisensor monitoring and data integration revealed cyclical destabilization of the rock glacier with a recent sharp increase of movement rates at the rock glacier front, which is presented in recent research by Hartl et al. (2023).</p> <p> <p>Study area rock glacier \u00c4u\u00dferes Hochebenkar (\u00d6tztal Alps, Austria) seen from the opposite side of the valley in 2021. The rock glacier is situated in a NW\u2010oriented glacial cirque, surrounded by the near\u2010vertical slopes of Hangerer (left) and Hochebenkamm (right).  (a) - (c) Cross-sections of different types of point clouds acquired at the lower tongue area of the rock glacier. Figure by V. Zahs. </p> <p>In this course, we will use point clouds acquired in the summers of 2020 and 2021 by UAV-borne laser scanning and UAV-borne photogrammetry in the lower tongue area of the rock glacier. The dataset consists of one point cloud per acquisition method and epoch, i.e. one ULS point cloud from 2020, and one ULS and UAV photogrammetry point cloud, respectively, from 2021.</p> <p>ULS point clouds were collected using a RIEGL VUX-1LR laser scanner mounted on a RIEGL RiCOPTER. The survey was carried out with a strip spacing of 90 m and a mean flight altitude of 105 m above ground level (AGL). This resulted in point clouds with an average point spacing of 0.05 m.</p> <p>Images for the photogrammetric point clouds were acquired with a DJI Phantom 4 RTK equipped with a 1\u201d CMOS 20 MP camera mounted on a gimbal. The survey was carried out with a strip spacing of  40 m at a constant flight altitude of 85 m AGL with 80% front and side overlap of the acquired nadir images. Additionally, a smaller number of  oblique images were captured to strengthen the bundle block. The point cloud was reconstructed by dense image matching and has an average point spacing of 0.05 m.</p> <p>All details on the datasets can be found in the related publication (Zahs et al. 2022).</p> <p>The point clouds used in this course are subsampled to a point spacing of 10 cm, to reduce the data volume and enable processing on standard computers.</p> <p>Coordinates of point clouds are in ETRS89 / UTM 32N (EPSG: 32632).</p>"},{"location":"data_usecases/usecase_rockglacier_ahk.html#use-case-in-the-course","title":"Use case in the course","text":"<p>The use case is featured in the following parts of the course:</p> <ul> <li>Exercise: Principles of 3D/4D geographic point clouds (Module 3, Theme 1)</li> <li>Exercise: Virtual Laser Scanning in HELIOS++ for Point Cloud Change Analysis (Module 3, Theme 2)</li> <li>Exercise: Exercise: 3D Change Analysis at an Active Rock Glacier using Multitemporal Point Clouds</li> <li>Case study: Multitemporal 3D change analysis at an active rock glacier (Module 3, Theme 6)</li> </ul>"},{"location":"data_usecases/usecase_rockglacier_ahk.html#references","title":"References","text":"<ul> <li>Barsch, D. (1992). Permafrost creep and rockglaciers. Permafrost and Periglacial Processes, 3 (3), pp. 175-188. doi: 10.1002/ppp.3430030303.</li> <li>Hartl, L., Zieher, T., Bremer, M., Stocker-Waldhuber, M., Zahs, V., H\u00f6fle, B., Klug, C. &amp; Cicoira, A. (2023): Multisensor monitoring and data integration reveal cyclical destabilization of \u00c4u\u00dferes Hochebenkar Rock Glacier. Earth Surface Dynamics. Vol. 11, pp. 117-147. doi: 10.5194/esurf-11-117-2023</li> <li>Moore, P. L. (2014): Deformation of debris-ice mixtures. Review of Geophysics. Vol. 52(3), pp. 435-467. doi: 10.1002/2014RG000453.</li> <li>Zahs, V., Winiwarter, L., Anders, K., Bremer, M. Rutzinger, M. Pot\u016f\u010dkov\u00e1, M. &amp; H\u00f6fle, B. (2022): Evaluation of UAV-borne photogrammetry and UAV-borne laser scanning for 3D topographic change analysis of an active rock glacier. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences. Vol. XLIII-B2-2022, pp. 1109-1116. doi: https://doi.org/10.5194/isprs-archives-XLIII-B2-2022-1109-2022.</li> <li>Zahs, V., Winiwarter, L., Anders, K., Williams, J.G., Rutzinger, M. &amp; H\u00f6fle, B. (2022): Correspondence-driven plane-based M3C2 for lower uncertainty in 3D topographic change quantification. ISPRS Journal of Photogrammetry and Remote Sensing. Vol. 183, pp. 541-559. doi: https://doi.org/10.1016/j.isprsjprs.2021.11.018. </li> </ul>"},{"location":"data_usecases/usecase_tundra_karkonosze.html","title":"Use case: Land cover monitoring in Karkonosze/Krkono\u0161e Mountains (Poland/Czechia)","text":"<p>Field data for this use case were collected as a part of the HyMountEcos project, lead by researchers at University of Warsaw and Charles University in Prague, and part of Adriana Marcinkowska-Ochtyra PhD thesis.</p>"},{"location":"data_usecases/usecase_tundra_karkonosze.html#background-and-motivation","title":"Background and motivation","text":"<p>The impact of climate change is visible in many environments around the globe. One of them are mountain areas, where changes of the treeline or shrubs encroachment into grassland areas can be observed (Beniston, 2016). Mountain vegetation, including endangered species, rare endemic species or glacial relics is unique and systematic monitoring and identification is a key element of its protection. Commonly used field mapping is laborious and time consuming, which is especially visible in case of high-mountain areas, where limited availability affects the possibilities of research. Hence, satellite remote sensing data, characterized by great objectivity and spatial coverage, as well as high temporal resolution are increasingly used in monitoring works (Wakuli\u0144ska and Marcinkowska-Ochtyra, 2020). Due to the presence of difficulties peculiar to this type of terrain many steps required to properly prepare the images from such areas for further analysis.</p>"},{"location":"data_usecases/usecase_tundra_karkonosze.html#study-site-and-data","title":"Study site and data","text":"<p>The area is located in Polish and Czech Karkonosze/Krkono\u0161e Mountains within the borders of national parks. The highest parts of the mountains cover two plant floors: subalpine (1250\u20131450 m a.s.l.) and alpine (above 1450 m a.s.l.). Research area is considered unique in this region of arctic-alpine tundra. Examples of vegetation classes occurring there, are presented below.</p> <p> <p></p> <p>Subalpine and alpine vegetation in Karkonosze/Krkono\u0161e Mts. (figure by Wakuli\u0144ska and Marcinkowska-Ochtyra 2020, modified/ CC BY 4.0). </p> <p>In this course, we will use multitemporal Sentinel-2 data and reference data in the following configurations:</p> <ul> <li> <p>Sentinel-2 data from six terms of data acquisition (19, 24 and 27 June, 19 and 24 July, 20 October 2022) as reference data for land cover classification from CORINE Land Cover 2018 database.</p> </li> <li> <p>Sentinel-2 data from seven terms of data acquisition - 14 May, 31 May, 7 August, 27 August, 18 September (five dates from 2018), 25 June and 30 June (two dates from 2019) - and field data collected in 2013-2014 (updated to 2018-2019 based on high-resolution data interpretation) for classification of vegetation types, with focus on grasslands,</p> </li> </ul>"},{"location":"data_usecases/usecase_tundra_karkonosze.html#use-case-in-the-course","title":"Use case in the course","text":"<p>The use case is featured in the following parts of the course:</p> <ul> <li>Exercise: Image processing (Module 2, Theme 3)</li> <li>Exercise: Multitemporal classification (Module 2, Theme 4)</li> <li>Case study: Monitoring tundra grasslands (Karkonosze/Krkono\u0161e Mountains) (Module 2, Case Study 1)</li> </ul>"},{"location":"data_usecases/usecase_tundra_karkonosze.html#references","title":"References","text":"<p>Beniston, M. (2016). Environmental change in mountains and uplands. Routledge.</p> <p>Wakuli\u0144ska, M., &amp; Marcinkowska-Ochtyra, A. (2020). Multi-temporal sentinel-2 data in classification of mountain vegetation. Remote Sensing, 12(17), 2696. https://doi.org/10.3390/rs12172696</p>"},{"location":"module0/module0.html","title":"Before you start the e-learning course\u2026","text":"<p>The e-learning course Time Series Analysis in Remote Sensing for Understanding Human-Environment Interactions is designed for the MSc level, and some elementary knowledge of statistics, remote sensing, and programming is required. A summary of knowledge and skills that are essential for successfully starting and completing the course can be found below. Links to online materials explaining the necessary theory with practical examples are provided.  </p> <p>The practical exercises in the course utilise open or free software packages, namely QGIS and some of its plugins, R, and Google Earth Engine. Moreover, programming in Python is required in some modules. Instructions on how to install the required software packages as well as links to user manuals or tutorials, explaining basic functions and script writing, are explained in the section software.</p> <p>You should be familiar with the following concepts or methods:</p>"},{"location":"module0/module0.html#statistics","title":"Statistics","text":"<ul> <li> <p>Variable, random variable</p> </li> <li> <p>Probability, probability distribution</p> </li> <li> <p>Errors in measurements (types and propagation)</p> </li> <li> <p>Least squares fitting</p> </li> <li> <p>Regression</p> </li> <li> <p>ANOVA</p> </li> </ul> <p>The following freely accessible resources provide introduction to statistics and explanation of above concepts:</p> <p>Probability and statistics - online E-Book</p> <p>Online Statistics Education: A Multimedia Course of Study (http://onlinestatbook.com/). Project Leader: David M. Lane, Rice University.</p>"},{"location":"module0/module0.html#geoinformation-systems-gis","title":"Geoinformation Systems (GIS)","text":"<ul> <li> <p>Working with raster data (import/export, interpolation, reclassification, resampling, raster math, ...)</p> </li> <li> <p>Working with vector data (import/export, digitizing features, attribute table manipulations, spatial overlays, ...)</p> </li> <li> <p>Understanding and handling coordinate reference systems (CRS)</p> </li> </ul>"},{"location":"module0/module0.html#remote-sensing","title":"Remote Sensing","text":"<ul> <li> <p>Remote sensing principles</p> </li> <li> <p>Optical (multi- and hyperspectral) and LiDAR data</p> </li> <li> <p>Basic knowledge of radiometric corrections</p> </li> <li> <p>Geometric correction of images (orthorectification)</p> </li> <li> <p>Pre-processing of point clouds (from raw measurements to a georeferenced point cloud) \u2013 TLS &amp; ULS/ALS</p> </li> <li> <p>Generating point clouds from imagery (basic workflow, details on SfM or high density image matching are not necessary)</p> </li> </ul> <p>Links to tutorials and resources where you can obtain the required skills or knowledge in Remote Sensing:</p> <p> ESA</p> <p>Land in Focus \u2013 Basics of Remote Sensing - online (registration required)</p> <p> Canada Centre for Mapping and Earth Observation</p> <p>Remote Sensing Tutorials - online and PDF version (access without registration)</p> <p> University of Twente</p> <p>Principles of Remote Sensing - online textbook</p> <p>Tempfli, K., Kerle, N., Huurneman, G. C., Janssen, L. L. F. (2009). Principles of remote sensing : an introductory textbook. (ITC Educational Textbook Series; Vol. 2). International Institute for Geo-Information Science and Earth Observation.</p>"},{"location":"module0/module0.html#software-and-programming","title":"Software and programming","text":"<p>This course makes use of various tools and software packages for different analysis tasks throughout the modules. Below you find an overview of all required software, with further material for setting them up, if applicable. Check the individual modules for the software/tools that are specifically required.</p>"},{"location":"module0/module0.html#software","title":"Software","text":"<p>The following list contains all software that is used throughout the course. The links for each software lead to descriptions and (where required) specific information how to set up them up for this course.</p> <ul> <li> <p>QGIS (including GRASS GIS and SAGA GIS) + EnMAP-Box</p> </li> <li> <p>R</p> </li> <li> <p>Python (including Jupyter notebook)</p> </li> <li> <p>CloudCompare (direct download and installation from the website)</p> </li> <li> <p>Google Earth Engine</p> </li> </ul>"},{"location":"module0/module0.html#programming","title":"Programming","text":"<p>Programming is an important skill for any scientific analyses, where off-the-shelve software is not available, workflows should be automated, or own/new methods are developed. The programming languages Python and R are most widely used in data science and scientific analyses in general, and will be used in this course.</p> <p>Independent of the specific programming language, you should have basic programming skills before taking this course. Importantly, these regard knowledge of fundamental programming concepts (e.g., algorithms, syntax).</p> <p>You should be familiar with the following concepts and operations prior to the course:</p> <ul> <li>Data types</li> <li>Variables</li> <li>Conditions</li> <li>Arrays</li> <li>Loops</li> <li>Optional: Functions, Classes</li> </ul> <p>Ideally, you also have some experience with:</p> <ul> <li>Data visualization (plotting in R and/or Python)</li> <li>Reading and writing tabular data as well as raster and vector geodata</li> </ul> <p>The following resources provide introductions to programming regarding above concepts:</p> <ul> <li>Downey, A.B. (2015): Think Python - How to Think Like a Computer Scientist. O\u2018Reilly. (open source): https://greenteapress.com/wp/think-python-2e</li> <li>Coursera is a platform that offers a variety of online courses</li> </ul> <p>Below, we provide links to tutorials and resources where you can obtain the required skills using Python and/or R.</p>"},{"location":"module0/module0.html#programming-in-python","title":"Programming in Python","text":"<p>If you are new to Python, you might look into this tutorial on Python Code Fundamentals by Earth Lab.</p> <p>Advanced tutorials or material can be found among the following resources: * Online course on Practical Python Programming by David Beazley * Online course/textbook on Use Data for Earth and Environmental Science in Open Source Python by Earth Lab</p>"},{"location":"module0/module0.html#programming-in-r","title":"Programming in R","text":"<p>If you are new to R you can check out recommended tutorials and books here</p>"},{"location":"module0/teaching_manual/forking.html","title":"Setting up online resources for teaching E-TRAINEE as a university course","text":"<p>When using the course, i.e., when teaching or training with it,  we recommend to fork the repository into your GitHub account/organization and use the latest version of the main branch.  This ensures that the course content is stable while updates may be merged into this repository.  If you want to integrate updates from this repository into your fork,  you can do so by creating a pull request from this repository to your forked repository, or by using the syncing offered by GitHub.</p>"},{"location":"module0/teaching_manual/forking.html#set-up-the-course-fork","title":"Set up the course fork","text":"<ol> <li>Create a fork of the original etrainee repository, choose an appropriate name according to your needs as it's going to be publicly visible. A fork is a copy of a repository.  Forking a repository allows you to freely experiment with changes without affecting the original project  and without being affected by the changes made in the original project.</li> </ol> <ol> <li>In the Actions tab of your newly created forked repository enable workflows.</li> </ol> <p> <p> </p>  3. Then, **run the workflow** *deploy_to_gh_pages* using the main branch. This takes some time,  once the workflow has finished, a green check mark will appear next to it. The workflow is an automatically triggered event, meaning every time a change in the future is made in the main branch, the github-pages will update on  their own.  <p> <p> </p>  4. In the Settings tab -&gt; Pages -&gt; Build and Deployment **enable GitHub Pages**. As the source choose *Deploy from a branch* and as the branch select *gh-pages*. Save the options.  <p> <p> </p>  5. The previous step takes some minutes. After the deployment has successfully finished a message with the wording \"Your site is live at ...\" will appear.  The **URL link** can be used to access the course website generated from your forked repository.  You can make the link visible on the repositories landing page by including it in the About section (option: Use your GitHub Pages website)    <p> <p> </p>  6. It is recommended to **add protection to the main branch**. This can be done in Settings -&gt; Branches -&gt; Add Branch Protection Rule -&gt; Branch name pattern: *main* and check *Require a pull request before merging*. You can further decide if  pull requests targeting a matching branch should require approval or not.   <p> <p> </p>  ## Communicate with students Students can suggest changes, fix typos, give feedback, etc., either by raising **issues** on GitHub or by directly applying updates to branches via **pull/merge requests**. Issues need to be enabled first in the  Settings -&gt; General -&gt; Features: Issues (check box).  Issues and pull requests can be linked together to show the progress and automatically close the issues upon their resolution.   <p> </p>   ## Modify and update course info 1. The **course website landing page** is generated from *course/index.md*. You can modify this file to contain the needed information such as the time schedule, topics, contact information, etc. A template that can be edited and used is provided [here](index_template.md).  The easiest way is to copy the content of this file.   2. You can **edit the course website structure** and select only some modules to be built by modifying *mkdocs/mkdocs.yml* and, e.g., deleting lines for modules not to be included.   3. **Updates and corrections** applied to the main branch of the fork are automatically posted on website  due to the previously defined actions.     ## After the semester The changes in the forked repository can be transferred back to the original etrainee repository via pull/merge requests. It is important to not transfer back the altered *course/index.md*  and other course specific files, only the corrections. This can be done in several ways:  A. Create a new branch in your repository from the [remote upstream](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/configuring-a-remote-repository-for-a-fork) (original etrainee repository) -&gt; **cherry-pick** and push only the specific commits to the branch -&gt; make a pull request.   <pre><code># set remote upstream \ngit remote add upstream https://github.com/&lt;upstream_github_username&gt;/&lt;upstream_github_repo_name&gt;.git\n# fetch changes\ngit fetch --all\n# create new branch from\ngit checkout -b branch_name upstream/master\n# cherry-pick commits\ngit cherry-pick &lt;hash of commit&gt;\n# push the commit\ngit push -u origin branch_name\n</code></pre>   or  B. Create a new branch in your repository -&gt; replace the course specific altered files with the  the ones from the original etrainee repository -&gt; make a pull request."},{"location":"module0/teaching_manual/index_template.html","title":"Home","text":"<p>Welcome to the course on \"Time Series Analysis in Remote Sensing for Understanding Human-Environment Interactions\".</p> <p>The main objective is to learn new tools and methods for time series analysis of remote sensing data and to understand their fit-for-purpose application in Geography and environmental sciences. </p> <p>The course consists of X selected modules of a larger e-learning course called E-TRAINEE that is jointly developed by the Universities of Prague, Warsaw, Innsbruck and Heidelberg in the framework of Erasmus+ and 4EU+ (see https://web.natur.cuni.cz/gis/etrainee/):</p> <ul> <li>Module 1: Methods of Time Series Analysis in Remote Sensing</li> <li>Module 2: Satellite Multispectral Images Time Series Analysis</li> <li>Module 3: 3D/4D Geographic Point Cloud Time Series Analysis</li> <li>Module 4: Airborne Imaging and Laboratory Spectroscopy Time Series Analysis</li> </ul>"},{"location":"module0/teaching_manual/index_template.html#contact","title":"Contact","text":""},{"location":"module0/teaching_manual/index_template.html#dates-and-topics","title":"Dates and Topics","text":""},{"location":"module0/teaching_manual/index_template.html#location-time","title":"Location &amp; time","text":"# Date Topics 00 01 02 03 04 05 06 07 08 09 10 11 12"},{"location":"module0/teaching_manual/index_template.html#how-to-use-the-course","title":"How To Use the Course","text":"<ul> <li>The course contents are provided via a website (which you are probably on right now). </li> <li>This website is automatically generated from the corresponding Github repository: xxx. The repository is linked at the top right of the website on all pages. </li> <li>Some parts of the course are conducted in Jupyter Notebooks. These are static pages on the website, but it is recommended that you follow them interactively. Use the download button at the top of the respective pages. This will open the raw file in your browser, just right click and use \"Save page as / Seite speichern unter...\" to store the ipynb file.</li> <li>All data required for the course is contained in the central data repository. Each theme contains information which of the data (directories) is relevant for the current task.<ul> <li>You can report issues via the Issue Tracker on the Github repository. Depending on the type of issue, they will be solved throughout the course or considered in future development.</li> </ul> </li> </ul>"},{"location":"module0/teaching_manual/index_template.html#set-up-your-python-conda-environment","title":"Set up your Python conda environment","text":"<p>Installation instructions and links to the respective conda environments are given in the software section of this course. </p>"},{"location":"module0/teaching_manual/index_template.html#data-repository","title":"Data Repository","text":"<p>All data used in the course can be accessed via Zenodo and downloaded before starting the respective module: https://zenodo.org/records/10003575</p>"},{"location":"module1/module1.html","title":"Methods of Time Series Analysis in Remote Sensing","text":"<p>Module 1 covers a range of basic principles and methods of remote sensing time series analysis that are applicable to data from different platforms and sensors. While the hands-on parts of the module focus on optical satellite imagery, many of the approaches you learn here will be helpful for working with other data types (such as close-range imagery time series or time series of 3D point clouds from photogrammetry or laser scanning). Hence, in this module you will learn about:</p> <ul> <li>Principles of time series in general and remote sensing time series in specific</li> <li>Major Earth observation missions, data archives and access options</li> <li>Strategies and computing facilities for large remote sensing time series, including introductions with Python and with the Google Earth Engine</li> <li>Classification approaches and methods for remote sensing time series</li> <li>Trajectory-based views on remotely sensed variables</li> <li>Approaches to land surface monitoring and change detection</li> <li>Fusion of multi-modal remote sensing time series</li> <li>Possibilities and best practices for validating your analyses with remote sensing time series</li> </ul>"},{"location":"module1/module1.html#structure-overview","title":"Structure overview","text":"<p>This module covers the following themes:</p> <ul> <li>Principles of remote sensing time series</li> <li>Large time series datasets in remote sensing</li> <li>Time series analysis based on classification</li> <li>Trajectory-based analysis</li> <li>Spatio-temporal data fusion</li> <li>Reference data, validation and accuracy assessment</li> </ul>"},{"location":"module1/module1.html#prerequisites-to-perform-this-module","title":"Prerequisites to perform this module","text":"<p>The following skills and background knowledge are required for this module:</p> <ul> <li>Basics of statistics</li> <li>Basics of geoinformation systems and handling raster/vector data</li> <li>Some familiarity with QGIS</li> <li>Basic programming skills in Python</li> <li>Principles of remote sensing</li> </ul>"},{"location":"module1/module1.html#software","title":"Software","text":"<p>For the practical parts of this module (excercises and tutorials), you will need:</p> <ul> <li>QGIS - In some of the Module 1 excercises, the graphical user interface of QGIS is used for visualization of data or for digitizing polygons (used to label training samples).</li> <li>Google Earth Engine - For several tutorials and excercises of Module 1, a registered user account for this service is required (create one here, if you don't have one).</li> <li>Python - You can use the package and environment management system Conda and the <code>etrainee_m1.yml</code> file to install the packages needed for the tutorials and excercises into a fresh Python environment. The yaml file can be downloaded here: etrainee_m1.yml.</li> </ul>"},{"location":"module1/module1.html#toolbox-intro","title":"Toolbox intro","text":"<p>Before you start with Theme 1 of Module 1, we recommend that you go through the Toolbox intro. There, you will learn how to</p> <ul> <li>set up your working environment with Python and all required packages</li> <li>create, modify and run interactive Jupyter Notebooks containing Python code</li> <li>use Python for basic processing steps and visualization of geospatial data</li> </ul>"},{"location":"module1/module1.html#practical-parts-of-this-module-overview","title":"Practical parts of this module (overview)","text":"<p>Module 1 contains at least one practical part (tutorial or excercise) per theme. The tutorials are linked as separate documents in the respective sections of a theme. Many of them are provided as Python Jupyter Notebooks and, if you download them, you can explore them interactively.</p>"},{"location":"module1/module1.html#mandatory-parts","title":"Mandatory parts","text":"<p>It is recommended to go through the following tutorials and excercises (one per theme). They are focusing on image time series analysis with Python's <code>xarray</code> package and the Google Earth Engine (GEE) Python API, and they (partly) build upon each other:</p> <ul> <li>Theme 1<ul> <li>Tutorial 1: Raster Time Series in Python using xarray, introducing the xarray package for handling labelled, multi-dimensional arrays at the example of a Sentinel-2 satellite image time series.</li> </ul> </li> <li>Theme 2<ul> <li>Tutorial 1: Sentinel-2 via STAC in Python, showing how to access Sentinel-2 data via a SpatioTemporal Asset Catalog (STAC) and get it into the Python xarray processing framework.</li> </ul> </li> <li>Theme 3<ul> <li>Tutorial 1: Image time series classification in Python, showing a machine learning workflow with spectral-temporal metrics (derived from one season of satellite imagery) as features for landcover classification. The result is one landcover map (which we will validate in theme 6).</li> </ul> </li> <li>Theme 4<ul> <li>Tutorial: Forest disturbance assessment with Python and the GEE, examining a Landsat 8 NDVI time series (spectral-temporal trajectory) to assess the timing of forest disturbance.</li> </ul> </li> <li>Theme 5<ul> <li>Tutorial: Sentinel-1/-2 surface water monitoring, where you learn how to combine Sentinel-1 SAR data and Sentinel-2 optical imagery for monitoring the extent of a water reservoir in a relatively simple workflow.</li> </ul> </li> <li>Theme 6<ul> <li>Exercise: Assessment of landcover classification accuracy, with a solution provided in this Notebook.</li> </ul> </li> </ul>"},{"location":"module1/module1.html#optional-parts","title":"Optional parts","text":"<p>In case you want to explore further topics and methods, there are more tutorials and excercises available:</p> <ul> <li>Theme 1<ul> <li>Tutorial 2: Exploring a Sentinel-2 time series using QGIS and the GEE Timeseries Explorer plugin</li> <li>Tutorial 3: Explore temporal profiles of a vegetation index in Python with pandas</li> <li>Tutorial 4: Exploring and processing a Sentinel-2 time series using the GRASS GIS temporal framework</li> </ul> </li> <li>Theme 2<ul> <li>Tutorial 2: Google Earth Engine (GEE) in Python, showing how to use the GEE cloud computing environment and its Python API for accessing, cloud-masking and downloading a Sentinel-2 time series.</li> <li>Tutorial 3: Large point clouds in Python, providing a couple of hints for handling and exploring large point clouds efficiently in Python (so far not time-series specific).</li> <li>Excercise: Search and load Landsat data to QGIS via a STAC API</li> </ul> </li> <li>Theme 3<ul> <li>Tutorial 2 with excercise:<ul> <li>The Snow cover time series in Python tutorial, introduces a very basic procedure for (binary) snow cover mapping with a Sentinel-2 time series. The result is a time series of snow cover maps.</li> <li>Excercise: Based on the tutorial, try to interpret the spatial patterns of snow cover duration and investigate the sensitivity of the rule-based classification regarding the classification threshold. For a suggested solution to this excercise see the Notebook Snow cover time series: Interpretation and sensitivity analysis.</li> </ul> </li> </ul> </li> </ul>"},{"location":"module1/module1.html#data-credits","title":"Data credits","text":"<p>Landsat imagery courtesy of the U.S. Geological Survey / Terms of use</p> <p>Copernicus Sentinel data courtesy of the European Space Agency - ESA / Terms of use</p>"},{"location":"module1/module1.html#start-the-module","title":"Start the module","text":"<p>... by proceeding to the Toolbox intro or (if you are already familiar with Conda, Jupyter Notebooks and GeoPython) skip this and</p> <p>... go directly to the first theme on Principles of remote sensing time series.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html","title":"Principles of remote sensing time series","text":"<p>This theme provides general knowledge about the principles of remote sensing time series. At first, we look at time series in general and at some important principles of remote sensing. Then we go on to see how time series principles can be transferred to remote sensing data.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#objectives","title":"Objectives","text":"<p>At the end of this theme, you will know concepts and typical properties of time series data in general and how to look at these properties. Further, you will understand how remote sensing time series are acquired, and what needs to be considered generally when using them. Finally, you will explore exemplary data and perform some first analysis steps using Python.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#time-series-principles","title":"Time series principles","text":""},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#what-is-a-time-series","title":"What is a time series?","text":"<p>A time series is a set of observations that are ordered by time (i.e., by their temporal occurrence; Castle and Hendry 2019). That means the observations have a clear and unchangeable chronology and distance to each other. The chronology of observations may be seen like neighbourhood and topology in spatial data analysis.</p> <p>Time series are recorded at different frequencies (number of measurements in given time period), which are often expressed through a measurement interval (i.e. the temporal spacing between consecutive measurements). This can be a (more or less) fixed interval (e.g. hourly, daily, every five days, \u2026) or an irregular one. The moment in time when a measurement is recorded is usually referenced as a time stamp (e.g. June 5th, 2019 at 7:00 am).</p> <p>Imagine two sets of temperature measurements, one with time (day and daytime) included for each measurement and one where the time is missing \u2026 Which temperature data set is more valuable for analysis (and why)?</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#properties-of-time-series","title":"Properties of time series","text":"<p>The next sections will illustrate some important properties that are commonly found in time series and you will get to know possibilities of looking at these properties, including</p> <ul> <li>Persistence</li> <li>(Non-)stationarity</li> <li>Trends, cyclic components and noise</li> <li>Temporal aggregates and resampling</li> <li>Filtering and noise reduction</li> </ul>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#patterns-of-persistence","title":"Patterns of persistence","text":"<p>Time series may contain patterns of persistence (Castle and Hendry 2019), with closely succeeding observations being more similar than observations that are longer apart. This is the case in this time series of daily air temperature from an automatic weather station in the Central Alps (Institute of Meteorology and Geophysics 2013)), contrasted by an example with random numbers:</p> <p></p> <p></p> <p>The persistent patterns in the temperature time series can be clearly seen if we calculate and plot the correlations between temperature values n days apart (i.e. with a lag of n days). On a warm day, chances are quite good that the next day is also quite warm, but often the weather changes after a few days. Accordingly, temperatures measured within a few days show some positive correlation with each other. This effect is decreasing rapidly with increasing time lag, and autocorrelation values range between the 95% and 99% confidence bands (solid grey and dashed lines), suggesting that these autocorrelations are not statistically significant. The series of random numbers shows much less patterns of persistence and lacks the autocorrelation of a few consecutive observation values.</p> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#non-stationarity","title":"(Non-)Stationarity","text":"<p>A time series where the mean and variance of the observations stay constant over time is considered stationary (see Castle and Hendry (2019, p. 10) for more detailed explanations and examples). In many environmantal and socio-economic time series this is not the case (non-stationarity). Staying with the air temperature example, it matters a lot if temperature on a specific day of the year (DOY) was measured in the 1920ies or very recently, with climate change having now strongly affected the temperatures to be expected with a certain probability. Non-stationarity can be related to gradual developments (like atmospheric CO<sub>2</sub> increase) but also to sudden events (like volcanic eruptions), which cause shifts in the means and variances of the data. If undetected, non-stationarity is a problem for empirical forecasting and for an analysis of relations between variables by their temporal development (Castle and Hendry 2019). Therefore, it can be advisable to check the time series for stationarity, e.g. using the Kwiatkowski-Phillips-Schmidt-Shin test (Kwiatkowski et al. 1992) or by visual inspection of plotted values.</p> <p></p> <p>Examples of (a) a stationary time series with constant mean and variance and (b) three non-stationary time series in the form of a shift in mean (trend and step change) and a shift in variance. Solid and dashed black lines represent the mean and the variance of the time series, respectively. (figure by Slater et al. 2021/ CC BY 4.0).</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#trend-detection-and-removal","title":"Trend detection and removal","text":"<p>A trend in a time series is often detected by fitting a regression. Here, we detect a linear trend, but note that trends do not need to be linear and other functions (e.g. exponential) may be more appropriate.</p> <p></p> <p>Note also that the time period we look at can matter a lot: In this example, we can detect an overall positive trend, but a negative trend if we take only the observations of a shorter time period (dotted line). Detection of breaks as another source of non-stationarity in time series is treated in Module 1 Theme 4. Some analyses may require to first remove a trend. The detrended series below simply contains the residuals of a regression of a given order.</p> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#decomposition-of-a-time-series-trends-cyclic-components-and-noise","title":"Decomposition of a time series: Trends, cyclic components and noise","text":"<p>It may also be informative to further split the variation of a time series. In addition to a level and some amount of noise, a time series may contain a trend and a seasonal or daily (cyclic or periodic) component (sometimes periods span even multiple years, e.g. (due to) the sunspot activity or the El Ni\u00f1o\u2013Southern Oscillation (ENSO)). In a simple model, these components can be combined either additively or multiplicatively with an observation y at time t given as</p> <pre><code>y(t) = Level + Trend + Seasonality + Noise\n</code></pre> <p>or</p> <pre><code>y(t) = Level * Trend * Seasonality * Noise\n</code></pre> <p>The next figure shows (from top to bottom) weekly observations of atmospheric CO<sub>2</sub> at Maona Loa (Hawaii), which can be decomposed into a trend, a seasonal component and a residual component based on a simple additive model.</p> <p></p> <p>After subtraction of the trend and the seasonal component, we get relatively small residuals centred around zero, which is usually a good result. Such a decomposition may be a helpful tool for understanding your time series data and conceiving the analysis but should always be used with caution. There are more advanced methods available, such as STL (Season-Trend decomposition using LOESS; based on the work by Cleveland et al. (1990). Nevertheless, a time series can also contain abrupt changes (\u201cbreaks\u201d) \u2013 we will get back to this later.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#time-domain-vs-frequency-domain","title":"Time domain vs. frequency domain","text":"<p>Time series can be visualized and analysed in the time domain or in the frequency domain. We can use the Fast Fourier Transform (FFT) to calculate the Discrete Fourier Transform (DFT), and such a decomposition of time series into different frequencies can reveal different cyclic components. Here, we see a distinct periodicity of 52 weeks in the CO<sub>2</sub> dataset, as the CO<sub>2</sub> concentration is fluctuating seasonally:</p> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#temporal-resampling-aggregationbinning-into-specific-temporal-units","title":"Temporal resampling: aggregation/binning into specific temporal units","text":"<p>Time series data is often resampled by aggregating (or binning) the observations into specific temporal units by calculating a descriptive statistical measure such as the mean (or minimum, maximum, standard deviation, ...) on a daily (weekly, monthly or yearly) basis. This is a useful step for many applications but one might also loose important information.</p> <p></p> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#time-series-filtering-and-noise-reduction","title":"Time series filtering and noise reduction","text":"<p>As far as possible, (systematic) errors in a data set should be corrected (e.g. atmospheric and topographic correction of satellite imagery) or, if this is impossible, erroneous points should be removed prior to an analysis (e.g. exclude cloudy satellite images completely or mask clouds and cloud shadows, see the exercises (tutorials) in this and the next theme). Often, however, some unsystematic errors remain and cause noise in a time series. Apart from random errors, also high-frequency cyclic components (e.g. diurnal temperature cycles) can be regarded as noise if this is not what you are actually interested in. Especially if the sampling rate (temporal resolution) is high, we can reduce noise (by sacrificing some temporal resolution) using temporal filter techniques, while preserving more longer-term variation and trends. In the following example, we calculated the daily mean air temperature from a time series consisting of three measurements per day, and then we smooth this further by calculating a 10-day rolling mean. In this case, a window/kernel of length 10 days is incrementally moved by observation along the time series, in this case centered on the currently re-computed data point, to compute an average value from all values in the window. We could also aggregate daily means further, e.g., into the monthly mean air temperature.</p> <p></p> <p>Another popular method for time-series smoothing is the Savitzky-Golay filter (Savitzky and Golay 1964), that fits local polynomials to successive subsets of the data to produce values of a new time series.</p> <p></p> <p>Smoothing of noisy data by the Savitzky-Golay method (3rd degree polynomial, 9 points wide sliding window). Blue curve: raw data; blue circle: point after smoothing; yellow curve: polynomial used to determine the current point; red curve: polynomial restricted to the sliding window around the current point (figure by Cdang 2013/Wikipedia/ CC BY-SA 3.0).</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#remote-sensing-systems-platforms-and-sensors","title":"Remote sensing systems: Platforms and sensors","text":"<p>Following earlier definitions in the literature (Lillesand et al. 2015, Toth and J\u00f3zk\u00f3w 2016), we define remote sensing as the collection of information about the Earth's surface or other objects without making physical contact with it/them, instead using electromagnetic radiation. Today a large variety of remote sensing systems is in operational use, with their main components being the sensor and the platform carrying the sensor(s). Furthermore, some systems contain a ground segment dedicated to mission control and data processing.</p> <p>A general idea of these platforms and sensors is also important for anybody working with their data, as this is related to a number of crucial aspects of data acquisition:</p> <ul> <li>distance between sensor and Earth surface/object of interest</li> <li>viewing angle</li> <li>spatial and temporal extent (coverage)</li> <li>accuracy and resolution (error sources for measurements, sampling density, repetition frequency, etc.)</li> </ul> <p>More details on the acquisition systems and the characteristics of data covered by this course are provided in the respective modules.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#platforms","title":"Platforms","text":"<p>Satellite</p> <p>Earth observation satellites carry a wide range of specifically developed sensors and have played a major role in remote sensing for decades. They often travel around the Earth in a near-polar and sun-synchronous orbit, allowing them to cross the same latitude at the same local solar time (i.e. with similar illumination, which is important for sensing reflected sunlight). Often the orbit exactly repeats after completing a distinct pattern of sub-satellite tracks. Thereby almost the entire Earth is captured during one repeat cycle, with a defined revisit period (repetition frequency). Other (very high spatial resolution) satellite missions are designed to choose the direction of the sensor and thus capture selected parts of the Earth more frequently according to user needs as a commercial service (tasking). The continuity of time series provided by such satellite missions over multiple years to decades offers great potential for monitoring Earth surface dynamics and analyses of human-environment interactions.</p> <p>Aircraft</p> <p>Various types of (occupied) aircraft are flexibly operated to carry a wide range of sensors at hundreds to thousands of metres above ground. Along with the capabilities of the sensor, flying speed determines the covered spatial extent as well as the resulting data resolution and quality.</p> <p>Uncrewed aircraft system (UAS)</p> <p>During the last decade, drones (uncrewed aircraft) have evolved into an important platform for remote sensing, a development that has been fostered by the rapid miniaturisation of suitable sensors (Colomina and Molina 2014, Cummings et al. 2017). In addition to the term drone, many alternative terms and acronyms are in use, most commonly uncrewed aerial vehicle (UAV), uncrewed aircraft system (UAS) and remotely piloted aircraft system (RPAS) (Granshaw 2018).</p> <p>Terrestrial systems</p> <p>Terrestrial (i.e. ground-based) systems can be static (such as terrestrial laser scanners (TLS), which are usually operated on a tripod) or mobile (with sensors mounted on a car, ship, backpack, etc.). While the above platforms mainly acquire data from above the object of interest, terrestrial systems can have very different viewing angles. Sensor-to-object distances are often short (in the order of 10<sup>0</sup> m to 10<sup>2</sup> m; \"close-range sensing\"), although longer ranges may be possible with suitable equipment and terrain. Recently, more and more large time series are being built with autonomously operating terrestrial systems installed permanently at monitoring sites.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#sensors","title":"Sensors","text":"<p>Depending on the application, sensors record radiation in different parts of the electromagnetic spectrum (wavelegths) ranging from ultraviolet to radio frequencies. Sensors follow either a passive or an active measurement principle, based on measuring electromagnetic radiation that is emitted either from an external source or actively by the system itself, respectively.</p> <p>Passive measurement principles use radiation from an external source, which is (in earth observation) very often the Sun. Examples are passive microwave sensing and imaging in the visible and near-infrared (NIR) to thermal infrared (TIR) parts of the spectrum. Today most people carry an optical imaging sensor in their pocket (in the camera of their smartphone), which can produce colour images with three bands in the visible spectrum (sensing red, green and blue light). In this Module and in Module 2 you will learn how to use time series of multispectral satellite images, which include additional bands in the NIR to TIR spectrum. Even more (and narrower) bands are recorded by hyperspectral imaging, which is the focus of Module 4.</p> <p>Systems with an active measurement principle record radiation emitted by themselves and then reflected by a surface/object. This includes synthetic aperture radar interferometry and laser scanning, also known as LiDAR (Light detection and ranging). In addition to the quantity of radiation also the time-structure of the emitted and detected radiation can be important, as this contains information about sensor-to-object distances (range measurement). In Module 3 you will learn how to work with time series of 3D point clouds acquired by laser scanning. </p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#resolution-and-scale-of-remote-sensing-data","title":"Resolution and scale of remote sensing data","text":"<p>Typically, the capabilities of these remote sensing systems in terms of coverage/extent, accuracy and resolution (in the spectral, spatial, radiometric and temporal domains) are inversely related, so a large extent usually precludes a very high spatial and temporal resolution. Moreover, a high resolution in multiple domains is largely restricted by sensor properties and their physical limitations. Different remote sensing platforms tend to be associated with typical spatial and temporal acquisition scales related to repetition rate, operative range and spatial resolution.</p> <p></p> <p>Different remote sensing platforms along with their typical repetition rate, operative range, and spatial resolution (modified from Rutzinger et al. 2021).</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#remote-sensing-time-series","title":"Remote sensing time series","text":""},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#contents-of-remote-sensing-time-series","title":"Contents of remote sensing time series","text":"<p>Typical remote sensing time series (RSTS) can be built from</p> <ul> <li>Geophysical or biophysical variables (e.g. temperature in \u00b0C or K, leaf-area-index (LAI), surface reflectance, surface elevation, ...)</li> <li>Index variables (often spectral indices, e.g. Normalized Difference Vegetation Index (NDVI), Normalized Difference Snow Index (NDSI), and many more ...)</li> <li>Thematic variables<ul> <li>Discrete thematic variables (e.g. a landcover category, or a binary snow/no snow value)</li> <li>Continuous field estimates of a thematic variable (e.g. fractional snow cover, or fractional tree cover) indicating how much of a spatial unit (often one pixel) the thematic variable covers</li> </ul> </li> </ul> <p></p> <p>Time series of spectral indices computed from Sentinel-2 satellite imagery. Each of the nine spectral indices (a) to (i) forms a rectangular cuboid with three dimensions (x, y, time) of length {l<sub>x</sub>\u2009=\u2009512, l<sub>y</sub>\u2009=\u2009512, l<sub>time</sub>\u2009=\u2009195}. The initial date is 2018-07-09 (front) and the final date is 2021-09-26 (back) (figure by Montero et al. 2023/ CC BY 4.0).</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#structure-of-remote-sensing-time-series","title":"Structure of remote sensing time series","text":"<p>The structure of most remote sensing time series (RSTS) can be imagined as a raster stack. In case of a single-band image time series or an elevation model time series (3D array), each pixel (defined by location in x and y dimension) contains one series of observations with unique time stamps (date-time index along the time dimension). </p> <p>Often, each timestamp is associated with a couple of variables that are observed (e.g. multispectral image time series from an optical sensor with several bands at different wavelengths) or even hundreds of them (e.g. in hyperspectral time series). A multi-/hyperspectral image time series with n bands can be considered as an array with four dimensions (x, y, time, band), where each pixel contains n series of observations (i.e. length of the band dimension). Compared to raster data with its regular spatial structure, the structure of 3D point cloud time series is a bit less straightforward - this will be detailed in Module 3. A good spatial alignment is an important prerequisite of remote sensing time series. If this precondition is not met directly after the data acquisition, it must be established by a co-registration procedure.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#extent-resolution-and-scale-of-phenomena-to-be-observed","title":"Extent, resolution and scale of phenomena to be observed","text":"<p>Extent and resolution in various domains are key properties of remote sensing data. The extent is the range covered in space (area in 2D, but possibly also in 3D), or in another domain, such as time (the entire Landsat archive, for instance covers the time period since 1972 (Wulder et al. 2016), whereas most other RSTS have a much shorter temporal extent). Similarly, the resolution of remote sensing data is defined in different domains, i.e. in the spatial, spectral, radiometric and temporal domain (Liang et al. 2020).</p> <p>Spatial resolution</p> <p>Spatial resolution most often refers to the (geometric) distance between samples or the size of sampling units in object space (i.e. very often on the ground, sometimes on other objects (such as trees or buildings)), thus also referred to as ground sample distance (GSD). Together with the sharpness (blur) and noise of an image, the GSD largely determines the size of the smallest objects or phenomena that can be discriminated (ECCOE 2020/I2R Corp). Depending on the sensor and on the acquisition geometry, the spatial resolution can be (more or less) homogeneous (e.g. satellite imagery) or strongly varying across the captured area (e.g. point density in terrestrial laser scanning point clouds).</p> <p>Spectral resolution</p> <p>The spectral resolution of a sensor system is determined by the number and width of its spectral bands. A normal digital camera, for instance, can detect visible light and has three bands in the red, green and blue part of the electromagnetic spectrum, whereas hyperspectral systems have hundreds of narrow bands. Multispectral sensors typically record a number of bands somewhere in between, i.e. between three and a few tens of bands.</p> <p>Radiometric resolution</p> <p>Radiometric resolution defines the dynamic range and is given as number of bits used to represent the recorded radiation. It determines how many different levels can be differentiated (in each band). With 1 bit, only two levels (e.g. black and white) can be stored, and an 8 bit image can already resolve 256 (2<sup>8</sup>) grey levels per band.</p> <p>Temporal resolution</p> <p>The repeat cycle or frequency of observations is expressed by the temporal resolution. Regular repeat cycles can range from minutes (e.g. ground-based time-lapse cameras/webcams) to days (many satellite systems) and years (typical repeat cycle for aerial imagery). Clearly, this is a critical aspect regarding the frequency and speed of changes to be observed.</p> <p></p> <p>Remote sensing data resolution in different domains.</p> <p></p> <p>Comparison of spectral bands for Sentinel-2, Landsat 8, Landsat 7, ASTER and MODIS and atmospheric transmission curve (figure by USGS Landsat). Details and other versions of this graphic are provided by NASA/Barsi and Rocchio 2020.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#towards-analysis-ready-remote-sensing-time-series-some-preprocessing-considerations","title":"Towards analysis-ready remote sensing time series - Some preprocessing considerations","text":"<p>Important for applied remote sensing, the data and its resolution in various domains must be appropriate to resolve the studied phenomena with enough detail (further reading in Kennedy et al. 2009). While this often means that we need a data acquisition at high resolution to capture small variations, it can also mean that we need to resample or smooth our data spatially or temporally to extract only the relevant variation, excluding small changes occurring over short spatial distances or in short time periods (depending on the application and data).</p> <p>In practice, the temporal interval of observations in a remote sensing time series is often irregular due to various constraints of data acquisition (dependence on suitable environmental conditions, temporary system outage, change of the measurement protocol, etc.) or because invalid data has been removed (e.g. by a cloud mask, an outlier filter or some other form of preprocessing). Depending on the further analysis steps, consider if your data should be resampled to regular intervals (e.g. when observations are averaged over a fixed time interval).</p> <p>Most data sets contain at least some missing values (gaps in certain spatial areas or time periods) and some methods cannot be used adequately with missing values (e.g. many of the popular machine learning algorithms propagate a missing value in the feature vector to the prediction). Therefore, an important consideration is if and how to fill gaps (e.g. impute missing values by a dummy value or by the preceeding/succeeding valid observation ('forward/backward filling'), or interpolate between observations?).</p> <p>A further pre-processing step potentially needed is some form of harmonization or normalization of data acquired under different conditions or with different sensors. This can prevent a systematic bias being interpreted as change.</p> <p>Moreover, it is generally a good idea to get to know the quality of your input data before you dive into more complex processing and interpretation. Thus, it is often worth analysing the completeness (proportion and distribution of missing values/gaps) and correctness (comparison with reference data or visual plausibility assessment) of observations.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"What are some of the main characteristics of a time series?  All values are measured without any errors. The time of observation is referenced by a time stamp. A time series is a set of observations recorded at the same time. The observed values are ordered clearly by time. The time of observation is referenced by a time stamp.&amp;The observed values are ordered clearly by time.  How are patterns of persistence in a time series recognized?  Temporally succeeding observations are more similar to each other than those observed longer apart. Temporally close observations show some negative correlation, while those observed longer apart are positively correlated. Temporally close observations tend to show some positive correlation, while those observed longer do not. Temporally succeeding observations are more similar to each other than those observed longer apart.&amp;Temporally close observations tend to show some positive correlation, while those observed longer do not.  Trends in a time series: Which statement is correct?  A trend is always linear. A trend is always positive (i.e., values are increasing with time). A trend is always the same in one particular time series, regardless if we look at the entire time series or only a part of it. Trends can be computationally removed to reveal other components of a time series. Trends can be computationally removed to reveal other components of a time series.  Which of these sensors use a passive measurement principle?  Terrestrial laser scanner Hyperspectral camera onboard a drone Multi-spectral instrument onboard the Sentinel-2 satellites Synthetic aperture radar onboard the Sentinel-1 satellites Hyperspectral camera onboard a drone&amp;Multi-spectral instrument onboard the Sentinel-2 satellites  Extent, resolution, and scale of remote sensing time series: Which statement is correct?  A high spatial resolution of imagery means each pixel corresponds to a large area on the ground. A high spatial resolution of imagery means each pixel corresponds to a small area on the ground. We should avoid recording time series with very high temporal resolution because this makes it difficult to detect long term changes and the resolution cannot be changed. To constrain the timing of rapid changes, a high temporal resolution is required. A high spatial resolution of imagery means each pixel corresponds to a small area on the ground.&amp;To constrain the timing of rapid changes, a high temporal resolution is required."},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#hands-on-remote-sensing-time-series","title":"Hands-on remote sensing time series","text":""},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#tutorial-1-raster-time-series-in-python-using-xarray","title":"Tutorial 1: Raster Time Series in Python using xarray","text":"<p>This Jupyter notebook introduces Python's <code>xarray</code>, a package for processing large multi-dimensional arrays (Hoyer and Hamman 2017) and shows how to use it for handling and analysing a Sentinel-2 image time series. The next themes will build on this tutorial, as they use xarray as well (but they also repeat some explanations).</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#tutorial-2-optional-exploring-a-sentinel-2-time-series-using-qgis-and-the-gee-timeseries-explorer-plugin","title":"Tutorial 2 (optional): Exploring a Sentinel-2 time series using QGIS and the GEE Timeseries Explorer plugin","text":"<p>As a straightforward option to access huge collections of Earth observation data we try out the QGIS GEE Timeseries Explorer plugin (Rufin et al. 2021).</p> <p>Overview:</p> <ul> <li>Load a few points as geopackage or shapefile (download from the course data repository as <code>T1_sample_points_001.gpkg</code> or digitize your own points in a new layer in QGIS) and browse for S2 cloud-free NDVI time series data at these points</li> <li>Query NDVI data for a defined time period and for a defined area-of-interest</li> <li>Visualize search result: time series plot and single scenes</li> <li>Apply a reducer to aggregate data to temporal units</li> <li>Visualize aggregated data</li> <li>Optionally continue with Tutorial 3: Download temporal profiles as raw text and import them to Python</li> </ul> <p>For more detailed explanations see this tutorial and the official plugin documentation.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#tutorial-3-optional-explore-temporal-profiles-of-a-vegetation-index-in-python-with-pandas","title":"Tutorial 3 (optional): Explore temporal profiles of a vegetation index in Python with pandas","text":"<p>Export temporal profiles of a vegetation index from GEE via QGIS and explore them in Python</p> <p>In this tutorial we explore temporal profiles of Sentinel-2 Normalized Difference Vegetation Index (NDVI). Sample points for different landcover classes are used. They are all located around the village of Obergurgl (Central Alps, Tyrol, Austria).</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#tutorial-4-optional-exploring-and-processing-a-sentinel-2-time-series-using-the-grass-gis-temporal-framework","title":"Tutorial 4 (optional): Exploring and processing a Sentinel-2 time series using the GRASS GIS temporal framework","text":"<p>This tutorial shows how an entire workflow for Sentinel-2 optical image time series can be implemented in GRASS GIS. This includes the search and download of scenes from the Copernicus Open Access Hub, a number of preparatory steps within the GRASS GIS temporal framework and, finally, the exploration and analysis of a Space-Time Raster Dataset (STRDS).</p> <p>WARNING:</p> <p>This excercise is provided for interested course participants who are motivated to experiment with GRASS GIS (and ideally have already some experience with the software). Currently, however, we do not recommend this excercise because of known occasional problems with the download from the Copernicus Open Access Hub (especially with data older than one month), with installation of the i.sentinel addon, and a bug in the creation of the temporal register file. Maybe these issues are (partly) resolved in current versions, but this needs further testing. So don't get frustrated if you still encounter problems ...</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#references","title":"References","text":""},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#key-references","title":"Key references","text":"<p>Castle, J. L., &amp; Hendry, D. F. (2019). Modelling our changing world. Springer Nature. 128 p. https://doi.org/10.1007/978-3-030-21432-6</p> <p>Eitel, J. U., H\u00f6fle, B., Vierling, L. A., Abell\u00e1n, A., Asner, G. P., Deems, J. S., ... &amp; Vierling, K. T. (2016). Beyond 3-D: The new spectrum of lidar applications for earth and ecological sciences. Remote Sensing of Environment, 186, 372-392. https://doi.org/10.1016/j.rse.2016.08.018</p> <p>Kennedy, R. E., Townsend, P. A., Gross, J. E., Cohen, W. B., Bolstad, P., Wang, Y. Q., &amp; Adams, P. (2009). Remote sensing change detection tools for natural resource managers: Understanding concepts and tradeoffs in the design of landscape monitoring projects. Remote Sensing of Environment, 113(7), 1382-1396. https://doi.org/10.1016/j.rse.2008.07.018</p> <p>K\u00fcnzer, C., Dech, S., &amp; Wagner, W. (2015). Remote sensing time series - Revealing land surface dynamics, Springer, 441 p. https://doi.org/10.1007/978-3-319-15967-6</p> <p>Liang, S., &amp; Wang, J. (2020). Advanced remote sensing: Terrestrial information extraction and applications, 2nd edition, Academic Press, 1010 p. https://doi.org/10.1016/C2017-0-03489-4</p> <p>Lillesand, T. M., R. W. Kiefer, and J. W. Chipman, 2015: Remote sensing and image interpretation. 7th ed., John Wiley &amp; Sons, New York, 736 p.</p> <p>Rees, W. G. (2012). Physical principles of remote sensing, 3rd edition, Cambridge University Press, 441 p. https://doi.org/10.1017/CBO9781139017411</p> <p>Toth, C. and G. J\u00f3zk\u00f3w, 2016. Remote sensing platforms and sensors: A survey. ISPRS Journal of Photogrammetry and Remote Sensing, 115, 22\u201336. https://doi.org/10.1016/j.isprsjprs.2015.10.004</p> <p>Vosselman, G., &amp; Maas, H.-G. (2010). Airborne and terrestrial laser scanning, CRC Press, 336 p.</p> <p>Woodcock, C. E., Loveland, T. R., Herold, M., &amp; Bauer, M. E. (2020). Transitioning from change detection to monitoring with remote sensing: A paradigm shift. Remote Sensing of Environment, 238, 111558. https://doi.org/10.1016/j.rse.2019.111558</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/01_principles_of_remote_sensing_time_series.html#additional-references-cited-in-this-theme","title":"Additional references cited in this theme","text":"<p>Cleveland, R. B., Cleveland, W. S., McRae, J. E., &amp; Terpenning, I. (1990). STL: A seasonal-trend decomposition procedure based on LOESS. Journal of Official Statistics, 6, 3-73.</p> <p>Colomina, I., &amp; Molina, P. (2014). Unmanned aerial systems for photogrammetry and remote sensing: A review. ISPRS Journal of Photogrammetry and Remote Sensing, 92, 79-97. https://doi.org/10.1016/j.isprsjprs.2014.02.013</p> <p>Cummings, A. R., McKee, A., Kulkarni, K., &amp; Markandey, N. (2017). The rise of UAVs. Photogrammetric Engineering &amp; Remote Sensing, 83(4), 317-325. https://doi.org/10.14358/PERS.83.4.317</p> <p>ECCOE (EROS CalVal Center of Excellence)/I2R Corp (2020). Spatial Resolution Digital Imagery Guideline. https://www.usgs.gov/media/images/spatial-resolution-digital-imagery-guideline</p> <p>Gebbert, S., Pebesma, E. (2014). A temporal GIS for field based environmental modeling. Environmental Modelling &amp; Software, 53, 1-12. https://doi.org/10.1016/j.envsoft.2013.11.001</p> <p>Gebbert, S., Pebesma, E. (2017). The GRASS GIS temporal framework. International Journal of Geographical Information Science, 31, 1273-1292. http://dx.doi.org/10.1080/13658816.2017.1306862</p> <p>Granshaw, S. I. (2018). RPV, UAV, UAS, RPAS \u2026 or just drone? The Photogrammetric Record, 33(162), 160-170. https://doi.org/10.1111/phor.12244</p> <p>Hoyer, S., &amp; Hamman, J. (2017). xarray: ND labeled arrays and datasets in Python. Journal of Open Research Software, 5(1). https://doi.org/10.5334/jors.148</p> <p>Institute of Meteorology and Geophysics (2013). Air temperature and precipitation time series from weather station Obergurgl, 2000-2009. University of Innsbruck, PANGAEA. https://doi.org/10.1594/PANGAEA.806623</p> <p>Montero, D., Aybar, C., Mahecha, M. D., Martinuzzi, F., S\u00f6chting, M., &amp; Wieneke, S. (2023). A standardized catalogue of spectral indices to advance the use of remote sensing in Earth system research. Scientific Data, 10(1), 197. https://doi.org/10.1038/s41597-023-02096-0</p> <p>Rouse, J.W, Haas, R.H., Scheel, J.A., and Deering, D.W. (1974) 'Monitoring Vegetation Systems in the Great Plains with ERTS.' Proceedings, 3rd Earth Resource Technology Satellite (ERTS) Symposium, Vol. 1, p. 48-62. https://ntrs.nasa.gov/citations/19740022614</p> <p>Rutzinger, M., Bremer, M., Zieher, T., Mayr, A. (2021): Fernerkundung. In: Bork-H\u00fcffer, Tabea; F\u00fcller, Henning; Straube, Till (Eds.): Handbuch Digitale Geographien: Welt - Wissen - Werkzeuge. Bern, Stuttgart, Wien: UTB Hauptverlag, ISBN: 9783825255671. https://elibrary.utb.de/doi/book/10.36198/9783838555676</p> <p>Savitzky, A., &amp; Golay, M. J. (1964). Smoothing and differentiation of data by simplified least squares procedures. Analytical chemistry, 36(8), 1627-1639. https://doi.org/10.1021%2Fac60214a047</p> <p>Wulder, M. A., White, J. C., Loveland, T. R., Woodcock, C. E., Belward, A. S., Cohen, W. B., ... &amp; Roy, D. P. (2016). The global Landsat archive: Status, consolidation, and direction. Remote Sensing of Environment, 185, 271-283. https://doi.org/10.1016/j.rse.2015.11.032</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html","title":"Exploring and processing a Sentinel-2 time series using the GRASS GIS temporal framework","text":"<p>This section shows how an entire workflow for Sentinel-2 optical image time series can be implemented in GRASS GIS. This includes the search and download of scenes from the Copernicus Open Access Hub, a number of preparatory steps within the GRASS GIS temporal framework and, finally, the exploration and analysis of a Space-Time Raster Dataset (STRDS).</p> <p>WARNING:</p> <p>This excercise is provided for interested course participants who are motivated to experiment with GRASS GIS (and ideally have already some experience with the software). Currently, however, we do not recommend this excercise because of known problems with the download from the Copernicus Open Access Hub (especially with data older than one month), with installation of the i.sentinel addon, and a bug in the creation of the temporal register file. Maybe these issues are (partly) resolved in current versions, but this needs further testing. So don't get frustrated if you still encounter problems ...</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#prerequisites","title":"Prerequisites","text":"<p>To download and use the Snetinel-2 imagery you will need</p> <ul> <li>an account for the Copernicus Open Access Hub (register here)</li> <li>the sentinelsat Python package installed (requires pandas)</li> <li>a GRASS GIS installation including the i.sentinel addon</li> <li>some GB of free storage on your disc (&gt; 2 GB per Sentinel-2 scene; &gt; 2 GB to unzip it)</li> </ul>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#some-preparations","title":"Some preparations","text":"<p>Create a settings textfile with credentials for Copernicus Open Access Hub account (two lines with user and PW --&gt; ...\\credentials_ESA.txt).</p> <p>Download an area-of-interest polygon or create your own and save as ...\\my_aoi.gpkg.</p> <p>Launch GRASS GIS and create a new location, set the CRS to WGS84 / UTM zone 32N (EPSG code 32632).</p> <pre><code>grass78         # start GRASS GIS from the commandline\n\ng.gui wxpython  # start GUI (if not started yet)\n</code></pre> <p>Import my_aoi.gpkg</p> <pre><code>v.import input=C:\\work\\testing\\lv_fe\\AOI\\my_aoi.gpkg --o\n</code></pre> <p>Check region and set to AOI extent</p> <pre><code>g.region -p\ng.region vect=my_aoi\n</code></pre>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#search-and-download-of-sentinel-2-scenes","title":"Search and download of Sentinel-2 scenes","text":"<p>Search the Copernicus Open Access Hub and list only (-l) all available Sentinel-2 products containing the AOI (or for the current region) that meet the search criteria (between two defined dates and with cloud cover &lt;=15%).</p> <pre><code>i.sentinel.download -l settings=C:\\work\\testing\\lv_fe\\credentials_ESA.txt producttype=S2MSI2A map=my_aoi area_relation=Contains start=2020-03-01 end=2020-05-31 sort=ingestiondate order=desc clouds=15\n</code></pre> <p></p> <p>Download these Sentinel-2 products.</p> <pre><code>i.sentinel.download settings=C:\\work\\testing\\lv_fe\\credentials_ESA.txt producttype=S2MSI2A map=my_aoi area_relation=Contains start=2020-03-01 end=2020-05-31 sort=ingestiondate order=desc clouds=15 output=C:\\work\\testing\\lv_fe\\S2_time_series\n</code></pre>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#import-these-sentinel-2-products-into-grass-gis","title":"Import these Sentinel-2 products into GRASS GIS","text":"<p>By default input data are imported into GRASS format, alternatively data can be linked if -l is given. Limit import to selected bands only, here with 10 m resolution. Double quotes may be needed to define a pattern!</p> <p>At first, print list of raster files to be imported by -p. For each file also the projection match with current location is printed including detected input data EPSG code. If projection of input data differs from current location consider reprojection (-r) or creating a new location for import.</p> <pre><code>i.sentinel.import -p input=C:\\work\\testing\\lv_fe\\S2_time_series pattern=\"B0(4|8)_10m\"\n</code></pre> <p>It is also useful to import cloud mask features by -c flag (as vector map). Now import/link (-l) and provide a name for the output file to use later with t.register</p> <pre><code>i.sentinel.import -l -c input=C:\\work\\testing\\lv_fe\\S2_time_series pattern=\"B0(4|8)_10m\" register_output=C:\\work\\testing\\lv_fe\\S2_time_series\\t_register.txt memory=3000 --o                    # only import bands 4 and 8\n</code></pre>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#temporal-registration-of-the-imported-rasters-into-a-space-time-dataset","title":"Temporal registration of the imported rasters into a Space Time Dataset","text":"<p>GRASS GIS includes a temporal framework (Gebbert and Pebesma 2014, 2017), where spatio-temporal data is be organized in Space Time Datasets.</p> <p></p> <p>Such a space-time dataset (STDS) is a collection of time stamped maps (snapshots) of the same variable</p> <ul> <li>Space time raster datasets (strds)</li> <li>Space time 3D raster datasets (str3ds)</li> <li>Space time vector datasets (stvds)</li> </ul> <p>Space-time datasets are mapset specific, they can only register raster, 3D raster or vector maps from the same mapset.</p> <p>STDS are processed via a dedicated group of tools (t.*), e.g.:</p> <ul> <li>t.connect, t.create, t.rename, t.register, \u2026              (data management)</li> <li>t.rast.extract, t.rast.gapfill, t.rast.algebra, \u2026         (querying and map calculation)</li> <li>t.rast.series, t.rast.aggregate, t.rast.accumulate, \u2026     (aggregation and accumulation analysis)</li> </ul> <p>Visualization of space-time datasets is achieved with</p> <ul> <li>g.gui.animation</li> <li>g.gui.timeline</li> <li>g.gui.mapswipe</li> <li>g.gui.tplot</li> </ul> <p>For more info, see the GRASS GIS documentation here and here.</p> <p>So the next step is to create a new Space Time Dataset, more specifically a Space Time Raster Dataset (STRDS) called sentinel, since we want to use our Sentinel-2 bands (which are rasters).</p> <pre><code>t.create type=strds temporaltype=absolute output=sentinel title=\"S2_strds\" description=\"Sentinel-2 time series spring 2020\"\n</code></pre> <p>Register all imported Sentinel bands into the prepared STRDS (which is still empty): Assigns timestamps and registers raster maps in a space time raster dataset</p> <pre><code>t.register input=sentinel file=C:\\work\\testing\\lv_fe\\S2_time_series\\t_register.txt      \n</code></pre> <p>We use the text file with timestamps for the scenes (created during import of the bands). In case of problems with teh following steps, check if there are empty lines between entries of the t_register.txt file (created by a bug in t.register) and delete such empty lines.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#handling-of-a-space-time-dataset","title":"Handling of a Space Time Dataset","text":"<p>Before analyse the data itself, we check basic metadata of the space time raster dataset (STRDS).</p> <pre><code>t.info input=sentinel                   # print metadata of the *sentinel* STRDS\nt.rast.list input=sentinel              # list all rasters contained in the *sentinel* STRDS\n</code></pre> <p>Using the g.gui.timeline tool, we can plot the timeline of the Sentinel-2 scenes, and (optionally) show also a 3D plot of spatio-temporal extents (\"space-time cube\").</p> <p></p> <p>Note that the observations (Sentinel-2 scenes) are not regularly spaced in time; especially if we queried only scenes with low cloud cover percentage, this can result in larger gaps.</p> <p>There are also ways to undo the previous steps, i.e. remove data from the temporal database if needed:</p> <pre><code>t.unregister --h        # Unregisters raster, vector and raster3d maps from the temporal database or a specific space time dataset (---h to show help).\nt.remove --h            # Removes entire space time datasets from temporal database (---h to show help).\n</code></pre>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#ndvi-time-series","title":"NDVI time series","text":"<p>For Normalized Difference Vegetation index (NDVI) computation we will create two new space time datasets (subsets) for 4th and 8th bands, respectively.</p> <pre><code>t.rast.extract input=sentinel where=\"name like '%B04%'\" output=b4\nt.rast.extract input=sentinel where=\"name like '%B08%'\" output=b8\n</code></pre> <p>Check the content of these strds</p> <pre><code>t.rast.list input=b4\nt.rast.list input=b8\n</code></pre> <p>Calculate NDVI</p> <pre><code>t.rast.algebra expression=\"ndvi=float(b8 - b4)/(b8 + b4)\" basename=ndvi --o\n</code></pre> <p>Use <code>t.rast.list in=ndvi</code> to print a list of the calculated NDVI maps (registered in the new strds ndvi) and print info about the ndvi STRDS with <code>t.info ndvi</code>.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#visualization-of-the-ndvi-time-series","title":"Visualization of the NDVI time series","text":"<p>Set an NDVI-specific color table (which is pre-configured in GRASS GIS)</p> <pre><code>t.rast.colors input=ndvi color=ndvi\n</code></pre> <p></p> <p>Using g.gui.animation, we created the animation above (in case of problems, add multiple rasters, not STRDS, because there is a problem with the way timestamps were created). Note that the cloud masks (boundaries in pink) do not look ideal in all cases. Furthermore, we can plot (and also export) the values of the NDVI time series at a defined point (with the possibility to export values to CSV file) via the GUI or the commandline</p> <pre><code>g.gui.tplot strds=ndvi coordinates=659071.2,5244350.1 title=NDVI xlabel=Date ylabel=NDVI\n</code></pre> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#univariate-statistics-for-each-registered-raster","title":"Univariate statistics for each registered raster","text":"<p>We can easily calculate some univariate statistics for each raster registered in a STRDS and, optionally, export the statistics to a textfile:</p> <pre><code>t.rast.univar input=ndvi out=C:\\work\\FE-Kurs\\sentinel-2\\stats.txt sep=tab\n</code></pre> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#temporal-aggregation-of-time-series-values-for-each-pixel","title":"Temporal aggregation of time series values for each pixel","text":"<p>t.rast.series performs different aggregation algorithms on all or a subset of raster maps in a space time raster dataset.</p> <p>Aggregate operations to be performed on the raster maps include, e.g.:</p> <ul> <li>average</li> <li>count</li> <li>median</li> <li>standard deviation</li> <li>range</li> <li>minimum or maximum</li> <li>min/max_raster (raster map number with the maximum (or the minimum) time-series value)</li> <li>arbitrary quantiles</li> <li>skewness</li> <li>parameters of a linear regression model (\"trend line\") fitted to the time series of each pixel (slope, intercept/offset, coefficient of determination)</li> <li>...</li> <li>see the t.rast.series manual for details</li> </ul> <p>Here, we produce two new rasters containing, for each pixel, the standard deviation and the maximum value of the time series, respectively:</p> <pre><code>t.rast.series in=ndvi method=stddev out=ndvi_stddev --o\nt.rast.series in=ndvi method=maximum out=ndvi_max --o\n</code></pre> <p></p> <p>Rasters created in GRASS GIS can be exported as GeoTIFF, e.g.:</p> <pre><code>r.out.gdal in=ndvi_stddev out=C:\\work\\FE-Kurs\\sentinel-2\\ndvi_stddev.tif format=GTiff --o\n</code></pre> <p>... and create a map layout in another software (e.g. QGIS):</p> <p></p> <p>Finally, we want to look at the trend of NDVI across our time series. Therefore, we let GRASS GIS fit a linear regression to the NDVI time series of each pixel and write the slope to a new raster.  Here, a positive trend of the NDVI reflects the disappearance of snow and the greening of the vegetation, especially at higher elevations and on ski runs (in the southern part of the AOI). This is contrasted by a negative NDVI trend of some agricultural fields due to the harvest of certain crops.</p> <pre><code>t.rast.series in=ndvi method=slope out=ndvi_slope --o\n</code></pre> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_GRASS_raster_time_series.html#summary-and-outlook","title":"Summary and outlook","text":"<p>To conclude, this tutorial has povided an introduction to working with remote sensing time series in GRASS GIS. You have learned</p> <ul> <li>how to search and download Sentinel-2 satellite data from the Copernicus Open Access Hub</li> <li>how to import and register the data into the GRASS GIS framework for spatio-temporal processing</li> <li>how to run basic analysis steps and visually explore the data</li> </ul> <p>For those who want to dive deeper into spatio-temporal analysis with GRASS GIS, there are a couple of [add-ons] https://grass.osgeo.org/grass80/manuals/addons/) that could be interesting, such as:</p> <ul> <li>r.regression.series - Makes each output cell value a function of the values assigned to the corresponding cells in the input raster map layers.</li> <li>r.mregression.series - Calculates multiple regression between time series: Y(t) = b1 * X1(t) + ... + bn * Xn(t).</li> <li>r.seasons - Extracts seasons from a time series.</li> <li>r.series.decompose - Calculates decomposition of time series X.</li> <li>r.series.diversity - Compute diversity indices over input layers.</li> <li>r.series.filter - Performs filtering of raster time series X (in time domain).</li> <li>r.series.lwr - Approximates a time series and creates approximated, gap-filled output.</li> <li>r.hants - Approximates a periodic time series and creates approximated output.</li> </ul>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_QGIS_GEE_TS_Explorer.html","title":"Exploring a Sentinel-2 time series using QGIS and the GEE Timeseries Explorer plugin","text":"<p>As a straightforward option to access huge collections of Earth observation data we try out the QGIS GEE Timeseries Explorer plugin (Rufin et al. 2021).</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_QGIS_GEE_TS_Explorer.html#overview","title":"Overview","text":"<ul> <li>load a few points as geopackage (download from the course data repository as <code>T1_sample_points_001.gpkg</code> or digitize your own points in a new layer in QGIS) and browse for S2 cloud-free NDVI time series data at these points (inspect and download, import them to Python later)</li> <li>query NDVI data for a defined time period and for a defined area-of-interest</li> <li>visualize search result: TS plot and single scenes</li> <li>apply a reducer to aggregate data to temporal units</li> <li>visualize aggregated data</li> </ul> <p>Note that the following steps were tested with an earlier version of the plugin. Therefore see also the latest plugin documentation.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_QGIS_GEE_TS_Explorer.html#steps","title":"Steps","text":"<ol> <li> <p>Browse for data at a specific location in the Central Alps (close to Obergurgl (Austria))</p> <p>Lon 46.85542120893998</p> <p>Lat 11.00044842646236</p> <p>collection: Sentinel-2 MSI Level-2A</p> <p>We use filters to constrain the search to a time period and to obtain only data from scenes with less than 40% of pixels classified as cloudy</p> <p>We visualize the time series profile for band 4 and band 8 at this point (the same bands as used for NDVI calculation).</p> <p></p> </li> <li> <p>Create a monthly aggregate of the RGB bands</p> <p>The scenes of a temporal bin are aggregated (reduced) to one composite layer which can be visualized in the map panel. You can play with different reducers (e.g. Median) and percentile stretches controlling the color visualization of reflectance values. The temporal bin is marked by a yellow box in the temporal profile plot (There seems to be an offset in the time scale labels on the y-axis, i.e. a small bug?).</p> <p></p> </li> <li> <p>Pin each aggregate image to the layer panel</p> <p>We repeat this for subsequent bins to get a series of monthly aggregate map layers. We have a single-band RGB for each month now.</p> <p></p> <p>Note that we apply a rather strong percentile stretch here to get enough contrast in the darker (snow-free) areas. There is still snow in higher elevations. Note also that we did not apply cloud masking, and here spurious cloud effects are visible (some of the bright areas), although the median aggregation mitigates this problem a lot.</p> <p>Alternatively, you can adjust the color rendering in the Layer Properties (Symbology), e.g.:</p> <p></p> <p>Now we enable the temporal control on each layer and set its time range (when it should be displayed in an animation).</p> <p></p> <p>Here, we exported the frames and built a GIF.</p> <p></p> </li> </ol>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html","title":"T1 S2 xarray","text":"Metadata     title: \"E-TRAINEE Tutorial - Raster time series processing in Python using xarray\"     description: \"This is a tutorial within the first theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-20     authors: Andreas Mayr In\u00a0[23]: Copied! <pre>import pathlib\nimport glob\nimport os\nfrom datetime import datetime\nimport rioxarray\nimport xarray\n</pre> import pathlib import glob import os from datetime import datetime import rioxarray import xarray In\u00a0[13]: Copied! <pre># Define the path to the s2 directory with the Sentinel-2 image subsets on your local system\nin_dir = pathlib.Path('F:/data/etrainee/gee/s2')\n\n# Obtain a list of all files to load from the s2 directory (defined previously as in_dir) on your local system\ngeotiff_list = glob.glob(str(in_dir / '**TPS.tif')) # We use 'TPS' to constrain the granules with better coverage in our AOI which is covered by TPT and TPS granules.\n\n# Helper function to get the datetime index from the filenames\ndef paths_to_datetimeindex(paths):\n    return [datetime.strptime((os.path.basename(i)[0:8]), '%Y%m%d') for i in paths]\n\n# Create a variable used for the time dimension\ntime_var = xarray.Variable('time', paths_to_datetimeindex(geotiff_list))\n\n# Load in and concatenate all individual GeoTIFFs\nS2_da = xarray.concat([rioxarray.open_rasterio(i, masked=True) for i in geotiff_list], dim=time_var)\n\n# Convert the xarray.DataArray into an xarray.Dataset\nS2_ds = S2_da.to_dataset('band')\n\n# Rename the variables to the more useful band names\nband_names = {1: 'B2', 2: 'B3', 3: 'B4', 4: 'B8', 5: 'B11', 6: 'B12'}     # A dictionary with the names of the Sentinel-2 bands exported earlier.\nS2_ds = S2_ds.rename(band_names)\n</pre> # Define the path to the s2 directory with the Sentinel-2 image subsets on your local system in_dir = pathlib.Path('F:/data/etrainee/gee/s2')  # Obtain a list of all files to load from the s2 directory (defined previously as in_dir) on your local system geotiff_list = glob.glob(str(in_dir / '**TPS.tif')) # We use 'TPS' to constrain the granules with better coverage in our AOI which is covered by TPT and TPS granules.  # Helper function to get the datetime index from the filenames def paths_to_datetimeindex(paths):     return [datetime.strptime((os.path.basename(i)[0:8]), '%Y%m%d') for i in paths]  # Create a variable used for the time dimension time_var = xarray.Variable('time', paths_to_datetimeindex(geotiff_list))  # Load in and concatenate all individual GeoTIFFs S2_da = xarray.concat([rioxarray.open_rasterio(i, masked=True) for i in geotiff_list], dim=time_var)  # Convert the xarray.DataArray into an xarray.Dataset S2_ds = S2_da.to_dataset('band')  # Rename the variables to the more useful band names band_names = {1: 'B2', 2: 'B3', 3: 'B4', 4: 'B8', 5: 'B11', 6: 'B12'}     # A dictionary with the names of the Sentinel-2 bands exported earlier. S2_ds = S2_ds.rename(band_names) <p>Now <code>S2_ds</code> is a DataSet with three dimensions (x, y, time) and six variables (the spectral bands). We add some attributes and fill them with metadata retrieved by the rio accessor. See the result:</p> In\u00a0[14]: Copied! <pre># Fill in some of the missing metadata\nS2_ds.attrs[\"crs\"] = S2_ds.rio.crs\nS2_ds.attrs[\"resolution\"] = S2_ds.rio.resolution()\nS2_ds.attrs[\"units\"] = \"meters\"\n\nS2_ds\n</pre>  # Fill in some of the missing metadata S2_ds.attrs[\"crs\"] = S2_ds.rio.crs S2_ds.attrs[\"resolution\"] = S2_ds.rio.resolution() S2_ds.attrs[\"units\"] = \"meters\"  S2_ds Out[14]: <pre>&lt;xarray.Dataset&gt;\nDimensions:      (time: 335, y: 201, x: 251)\nCoordinates:\n  * x            (x) float64 6.516e+05 6.516e+05 ... 6.566e+05 6.566e+05\n  * y            (y) float64 5.194e+06 5.194e+06 5.194e+06 ... 5.19e+06 5.19e+06\n    spatial_ref  int32 0\n  * time         (time) datetime64[ns] 2018-01-02 2018-01-12 ... 2021-09-08\nData variables:\n    B2           (time, y, x) float32 nan nan nan nan ... 1.095e+03 nan nan\n    B3           (time, y, x) float32 nan nan nan nan ... 1.351e+03 nan nan\n    B4           (time, y, x) float32 nan nan nan nan ... 1.445e+03 nan nan\n    B8           (time, y, x) float32 nan nan nan nan ... 2.208e+03 nan nan\n    B11          (time, y, x) float32 nan nan nan nan ... 2.162e+03 nan nan\n    B12          (time, y, x) float32 nan nan nan nan ... 1.658e+03 nan nan\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    scale_factor:            1.0\n    add_offset:              0.0\n    crs:                     EPSG:32632\n    resolution:              (20.0, -20.0)\n    units:                   meters</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 335</li><li>y: 201</li><li>x: 251</li></ul></li><li>Coordinates: (4)<ul><li>x(x)float646.516e+05 6.516e+05 ... 6.566e+05<pre>array([651590., 651610., 651630., ..., 656550., 656570., 656590.])</pre></li><li>y(y)float645.194e+06 5.194e+06 ... 5.19e+06<pre>array([5193750., 5193730., 5193710., ..., 5189790., 5189770., 5189750.])</pre></li><li>spatial_ref()int320crs_wkt :PROJCS[\"WGS 84 / UTM zone 32N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32632\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 32Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :9.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 32N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32632\"]]GeoTransform :651580.0 20.0 0.0 5193760.0 0.0 -20.0<pre>array(0)</pre></li><li>time(time)datetime64[ns]2018-01-02 ... 2021-09-08<pre>array(['2018-01-02T00:00:00.000000000', '2018-01-12T00:00:00.000000000',\n       '2018-01-27T00:00:00.000000000', ..., '2021-09-03T00:00:00.000000000',\n       '2021-09-05T00:00:00.000000000', '2021-09-08T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (6)<ul><li>B2(time, y, x)float32nan nan nan ... 1.095e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  7116.,  6893.,  6975.],\n        [   nan,    nan,    nan, ...,  7284.,  6826.,  6941.],\n        [   nan,    nan,    nan, ...,  7578.,  7313.,  7179.],\n        ...,\n        [10715., 10118., 10227., ...,  6981.,  6980.,  5471.],\n        [ 9605.,  9503., 10095., ...,  8117.,  8512.,  5604.],\n        [ 8790.,  9014.,  9334., ...,  8695.,  8726.,  7408.]],\n\n       [[ 8002.,  8192.,  8645., ...,  7557.,  7214.,  7777.],\n        [ 9680.,  8141.,  7723., ...,  7704.,  7163.,  7302.],\n        [10009.,  9963.,  8712., ...,  8089.,  7683.,  7587.],\n        ...,\n...\n        [  497.,   486.,   464., ...,   774.,   913.,   776.],\n        [  502.,   454.,   415., ...,   781.,   927.,   694.],\n        [  479.,   476.,   412., ...,   823.,   667.,   559.]],\n\n       [[  688.,   768.,   801., ...,  1071.,  1124.,   811.],\n        [  794.,   757.,   679., ...,   929.,  1109.,  1225.],\n        [  838.,   751.,   724., ...,   847.,  1017.,  1118.],\n        ...,\n        [  512.,   496.,   487., ...,   821.,   905.,   817.],\n        [  512.,   463.,   439., ...,   818.,   916.,   734.],\n        [  512.,   480.,   430., ...,   832.,   745.,   647.]],\n\n       [[   nan,    nan,    nan, ...,  1267.,  1419.,  1130.],\n        [   nan,    nan,    nan, ...,  1190.,  1335.,  1522.],\n        [   nan,    nan,    nan, ...,  1149.,  1255.,  1380.],\n        ...,\n        [  554.,   557.,   544., ...,  1122.,    nan,    nan],\n        [  583.,   503.,   486., ...,  1059.,    nan,    nan],\n        [  532.,   525.,   473., ...,  1095.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B3(time, y, x)float32nan nan nan ... 1.351e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  5978.,  5810.,  6208.],\n        [   nan,    nan,    nan, ...,  6139.,  5621.,  5896.],\n        [   nan,    nan,    nan, ...,  6440.,  6103.,  5997.],\n        ...,\n        [10655., 10096., 10182., ...,  6781.,  6842.,  5474.],\n        [ 9626.,  9502., 10141., ...,  7924.,  8409.,  5659.],\n        [ 8782.,  8944.,  9354., ...,  8530.,  8645.,  7237.]],\n\n       [[ 8117.,  8269.,  8972., ...,  6509.,  6257.,  7102.],\n        [ 9813.,  8405.,  7894., ...,  6683.,  6100.,  6382.],\n        [10242.,  9839.,  8840., ...,  7019.,  6668.,  6504.],\n        ...,\n...\n        [  795.,   776.,   790., ...,  1084.,  1220.,  1073.],\n        [  830.,   751.,   638., ...,  1094.,  1223.,   977.],\n        [  816.,   784.,   639., ...,  1148.,  1026.,   891.]],\n\n       [[  895.,  1016.,  1046., ...,  1476.,  1504.,   973.],\n        [ 1051.,   999.,   897., ...,  1318.,  1572.,  1684.],\n        [ 1075.,  1032.,  1004., ...,  1093.,  1335.,  1499.],\n        ...,\n        [  792.,   756.,   788., ...,  1145.,  1242.,  1100.],\n        [  832.,   726.,   664., ...,  1126.,  1255.,   993.],\n        [  813.,   761.,   608., ...,  1166.,  1110.,  1008.]],\n\n       [[   nan,    nan,    nan, ...,  1566.,  1669.,  1264.],\n        [   nan,    nan,    nan, ...,  1447.,  1659.,  1835.],\n        [   nan,    nan,    nan, ...,  1241.,  1465.,  1669.],\n        ...,\n        [  817.,   804.,   824., ...,  1349.,    nan,    nan],\n        [  848.,   767.,   687., ...,  1326.,    nan,    nan],\n        [  836.,   785.,   654., ...,  1351.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B4(time, y, x)float32nan nan nan ... 1.445e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  5187.,  4992.,  5774.],\n        [   nan,    nan,    nan, ...,  5256.,  4782.,  5134.],\n        [   nan,    nan,    nan, ...,  5649.,  5427.,  5273.],\n        ...,\n        [10998., 10420., 10512., ...,  6941.,  7017.,  5465.],\n        [ 9933.,  9851., 10476., ...,  8076.,  8508.,  5783.],\n        [ 9070.,  9271.,  9623., ...,  8672.,  8696.,  7421.]],\n\n       [[ 8208.,  8373.,  8949., ...,  5550.,  5398.,  6620.],\n        [ 9820.,  8390.,  8020., ...,  5718.,  5078.,  5425.],\n        [10243.,  9897.,  8857., ...,  6095.,  5775.,  5661.],\n        ...,\n...\n        [  775.,   769.,   764., ...,  1193.,  1364.,  1175.],\n        [  790.,   726.,   679., ...,  1193.,  1369.,  1022.],\n        [  738.,   762.,   646., ...,  1250.,  1056.,   955.]],\n\n       [[ 1019.,  1111.,  1139., ...,  1410.,  1493.,  1009.],\n        [ 1136.,  1104.,   991., ...,  1264.,  1481.,  1698.],\n        [ 1210.,  1122.,  1142., ...,  1130.,  1273.,  1535.],\n        ...,\n        [  810.,   789.,   779., ...,  1277.,  1401.,  1247.],\n        [  800.,   740.,   700., ...,  1233.,  1391.,  1065.],\n        [  776.,   774.,   651., ...,  1275.,  1190.,  1089.]],\n\n       [[   nan,    nan,    nan, ...,  1547.,  1639.,  1208.],\n        [   nan,    nan,    nan, ...,  1355.,  1571.,  1820.],\n        [   nan,    nan,    nan, ...,  1130.,  1404.,  1637.],\n        ...,\n        [  840.,   839.,   814., ...,  1475.,    nan,    nan],\n        [  855.,   778.,   742., ...,  1444.,    nan,    nan],\n        [  802.,   836.,   683., ...,  1445.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B8(time, y, x)float32nan nan nan ... 2.208e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  4276.,  4089.,  5000.],\n        [   nan,    nan,    nan, ...,  4322.,  3919.,  4197.],\n        [   nan,    nan,    nan, ...,  4684.,  4451.,  4310.],\n        ...,\n        [10667., 10119., 10220., ...,  6789.,  6765.,  5422.],\n        [ 9642.,  9625., 10164., ...,  7770.,  8218.,  5703.],\n        [ 8856.,  9046.,  9407., ...,  8321.,  8372.,  7258.]],\n\n       [[ 7688.,  7870.,  8241., ...,  4393.,  4298.,  5567.],\n        [ 8961.,  7822.,  7538., ...,  4488.,  4061.,  4483.],\n        [ 9254.,  9074.,  8255., ...,  4840.,  4612.,  4475.],\n        ...,\n...\n        [ 3299.,  3043.,  3446., ...,  2016.,  1802.,  1492.],\n        [ 3497.,  3196.,  2681., ...,  2136.,  1993.,  2335.],\n        [ 3557.,  3366.,  2658., ...,  2082.,  2654.,  2835.]],\n\n       [[ 1535.,  1779.,  1725., ...,  5187.,  5241.,  3208.],\n        [ 1689.,  1687.,  1717., ...,  4914.,  5625.,  5237.],\n        [ 1694.,  1749.,  1784., ...,  3391.,  4121.,  4569.],\n        ...,\n        [ 3162.,  2974.,  3239., ...,  2032.,  1848.,  1540.],\n        [ 3349.,  3044.,  2672., ...,  2234.,  1945.,  2323.],\n        [ 3347.,  3170.,  2343., ...,  2111.,  2656.,  3014.]],\n\n       [[   nan,    nan,    nan, ...,  4832.,  5022.,  3458.],\n        [   nan,    nan,    nan, ...,  4571.,  5241.,  5036.],\n        [   nan,    nan,    nan, ...,  3000.,  3851.,  4310.],\n        ...,\n        [ 3175.,  3096.,  3361., ...,  2087.,    nan,    nan],\n        [ 3332.,  3154.,  2701., ...,  2206.,    nan,    nan],\n        [ 3405.,  3286.,  2539., ...,  2208.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B11(time, y, x)float32nan nan nan ... 2.162e+03 nan nan<pre>array([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        ...,\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   35.,   50.,   86.],\n        [  nan,   nan,   nan, ...,   94.,   54.,   61.],\n        [  nan,   nan,   nan, ...,  101.,  126.,  130.],\n        ...,\n        [1356., 1331., 1330., ..., 1038., 1095., 1011.],\n        [1263., 1274., 1333., ..., 1047., 1077., 1035.],\n        [1193., 1209., 1275., ..., 1077., 1092., 1142.]],\n\n       [[ 824.,  810.,  796., ...,   40.,   44.,   69.],\n        [ 840.,  803.,  812., ...,   32.,   40.,   65.],\n        [ 829.,  822.,  827., ...,   12.,   45.,   53.],\n        ...,\n...\n        ...,\n        [2406., 2307., 2287., ..., 1990., 1866., 1749.],\n        [2454., 2242., 2079., ..., 1934., 1962., 2062.],\n        [2454., 2350., 2060., ..., 2048., 2293., 2423.]],\n\n       [[2289., 2594., 2779., ..., 4196., 4083., 3178.],\n        [2516., 2651., 2776., ..., 3809., 4438., 4386.],\n        [2518., 2746., 2777., ..., 3333., 3912., 4490.],\n        ...,\n        [2509., 2390., 2469., ..., 2143., 2002., 1930.],\n        [2540., 2280., 2218., ..., 2073., 1990., 2036.],\n        [2640., 2424., 2123., ..., 2080., 2351., 2600.]],\n\n       [[  nan,   nan,   nan, ..., 3937., 3844., 3249.],\n        [  nan,   nan,   nan, ..., 3667., 4190., 4182.],\n        [  nan,   nan,   nan, ..., 2992., 3647., 4200.],\n        ...,\n        [2477., 2393., 2369., ..., 2132.,   nan,   nan],\n        [2496., 2326., 2216., ..., 2071.,   nan,   nan],\n        [2542., 2395., 2168., ..., 2162.,   nan,   nan]]], dtype=float32)</pre></li><li>B12(time, y, x)float32nan nan nan ... 1.658e+03 nan nan<pre>array([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        ...,\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   62.,  142.,  100.],\n        [  nan,   nan,   nan, ...,   89.,   62.,  100.],\n        [  nan,   nan,   nan, ...,  100.,  100.,  145.],\n        ...,\n        [1512., 1467., 1478., ..., 1123., 1173., 1009.],\n        [1411., 1414., 1479., ..., 1188., 1209., 1055.],\n        [1322., 1324., 1419., ..., 1227., 1246., 1197.]],\n\n       [[ 882.,  878.,  836., ...,  101.,  137.,  123.],\n        [ 889.,  870.,  863., ...,   79.,  114.,  106.],\n        [ 872.,  867.,  870., ...,   97.,   92.,  128.],\n        ...,\n...\n        ...,\n        [1350., 1284., 1255., ..., 1509., 1484., 1336.],\n        [1372., 1245., 1159., ..., 1440., 1468., 1487.],\n        [1390., 1290., 1127., ..., 1463., 1527., 1605.]],\n\n       [[1857., 2110., 2247., ..., 2527., 2581., 2188.],\n        [2008., 2116., 2225., ..., 2318., 2767., 2799.],\n        [2028., 2187., 2198., ..., 2048., 2532., 2998.],\n        ...,\n        [1450., 1360., 1385., ..., 1683., 1617., 1533.],\n        [1439., 1312., 1252., ..., 1542., 1560., 1582.],\n        [1512., 1376., 1230., ..., 1562., 1655., 1764.]],\n\n       [[  nan,   nan,   nan, ..., 2653., 2550., 2230.],\n        [  nan,   nan,   nan, ..., 2424., 2809., 2842.],\n        [  nan,   nan,   nan, ..., 2098., 2443., 2885.],\n        ...,\n        [1460., 1415., 1361., ..., 1757.,   nan,   nan],\n        [1483., 1355., 1289., ..., 1645.,   nan,   nan],\n        [1480., 1364., 1264., ..., 1658.,   nan,   nan]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>xPandasIndex<pre>PandasIndex(Float64Index([651590.0, 651610.0, 651630.0, 651650.0, 651670.0, 651690.0,\n              651710.0, 651730.0, 651750.0, 651770.0,\n              ...\n              656410.0, 656430.0, 656450.0, 656470.0, 656490.0, 656510.0,\n              656530.0, 656550.0, 656570.0, 656590.0],\n             dtype='float64', name='x', length=251))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5193750.0, 5193730.0, 5193710.0, 5193690.0, 5193670.0, 5193650.0,\n              5193630.0, 5193610.0, 5193590.0, 5193570.0,\n              ...\n              5189930.0, 5189910.0, 5189890.0, 5189870.0, 5189850.0, 5189830.0,\n              5189810.0, 5189790.0, 5189770.0, 5189750.0],\n             dtype='float64', name='y', length=201))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2018-01-02', '2018-01-12', '2018-01-27', '2018-01-29',\n               '2018-02-08', '2018-02-11', '2018-02-13', '2018-02-16',\n               '2018-02-18', '2018-02-21',\n               ...\n               '2021-08-16', '2021-08-19', '2021-08-21', '2021-08-24',\n               '2021-08-26', '2021-08-31', '2021-09-03', '2021-09-03',\n               '2021-09-05', '2021-09-08'],\n              dtype='datetime64[ns]', name='time', length=335, freq=None))</pre></li></ul></li><li>Attributes: (9)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1scale_factor :1.0add_offset :0.0crs :EPSG:32632resolution :(20.0, -20.0)units :meters</li></ul> <p>A <code>spatial_ref</code> coordinate with length one was automatically created but is not needed. Let's get rid of this.</p> In\u00a0[15]: Copied! <pre>S2_ds = S2_ds.drop_vars(names='spatial_ref')    # Drop the variable with specified name\nS2_ds\n</pre> S2_ds = S2_ds.drop_vars(names='spatial_ref')    # Drop the variable with specified name S2_ds Out[15]: <pre>&lt;xarray.Dataset&gt;\nDimensions:  (time: 335, y: 201, x: 251)\nCoordinates:\n  * x        (x) float64 6.516e+05 6.516e+05 6.516e+05 ... 6.566e+05 6.566e+05\n  * y        (y) float64 5.194e+06 5.194e+06 5.194e+06 ... 5.19e+06 5.19e+06\n  * time     (time) datetime64[ns] 2018-01-02 2018-01-12 ... 2021-09-08\nData variables:\n    B2       (time, y, x) float32 nan nan nan nan ... 1.095e+03 nan nan\n    B3       (time, y, x) float32 nan nan nan nan ... 1.36e+03 1.351e+03 nan nan\n    B4       (time, y, x) float32 nan nan nan nan ... 1.445e+03 nan nan\n    B8       (time, y, x) float32 nan nan nan nan ... 2.208e+03 nan nan\n    B11      (time, y, x) float32 nan nan nan nan ... 1.98e+03 2.162e+03 nan nan\n    B12      (time, y, x) float32 nan nan nan nan ... 1.56e+03 1.658e+03 nan nan\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    scale_factor:            1.0\n    add_offset:              0.0\n    crs:                     EPSG:32632\n    resolution:              (20.0, -20.0)\n    units:                   meters</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 335</li><li>y: 201</li><li>x: 251</li></ul></li><li>Coordinates: (3)<ul><li>x(x)float646.516e+05 6.516e+05 ... 6.566e+05<pre>array([651590., 651610., 651630., ..., 656550., 656570., 656590.])</pre></li><li>y(y)float645.194e+06 5.194e+06 ... 5.19e+06<pre>array([5193750., 5193730., 5193710., ..., 5189790., 5189770., 5189750.])</pre></li><li>time(time)datetime64[ns]2018-01-02 ... 2021-09-08<pre>array(['2018-01-02T00:00:00.000000000', '2018-01-12T00:00:00.000000000',\n       '2018-01-27T00:00:00.000000000', ..., '2021-09-03T00:00:00.000000000',\n       '2021-09-05T00:00:00.000000000', '2021-09-08T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (6)<ul><li>B2(time, y, x)float32nan nan nan ... 1.095e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  7116.,  6893.,  6975.],\n        [   nan,    nan,    nan, ...,  7284.,  6826.,  6941.],\n        [   nan,    nan,    nan, ...,  7578.,  7313.,  7179.],\n        ...,\n        [10715., 10118., 10227., ...,  6981.,  6980.,  5471.],\n        [ 9605.,  9503., 10095., ...,  8117.,  8512.,  5604.],\n        [ 8790.,  9014.,  9334., ...,  8695.,  8726.,  7408.]],\n\n       [[ 8002.,  8192.,  8645., ...,  7557.,  7214.,  7777.],\n        [ 9680.,  8141.,  7723., ...,  7704.,  7163.,  7302.],\n        [10009.,  9963.,  8712., ...,  8089.,  7683.,  7587.],\n        ...,\n...\n        [  497.,   486.,   464., ...,   774.,   913.,   776.],\n        [  502.,   454.,   415., ...,   781.,   927.,   694.],\n        [  479.,   476.,   412., ...,   823.,   667.,   559.]],\n\n       [[  688.,   768.,   801., ...,  1071.,  1124.,   811.],\n        [  794.,   757.,   679., ...,   929.,  1109.,  1225.],\n        [  838.,   751.,   724., ...,   847.,  1017.,  1118.],\n        ...,\n        [  512.,   496.,   487., ...,   821.,   905.,   817.],\n        [  512.,   463.,   439., ...,   818.,   916.,   734.],\n        [  512.,   480.,   430., ...,   832.,   745.,   647.]],\n\n       [[   nan,    nan,    nan, ...,  1267.,  1419.,  1130.],\n        [   nan,    nan,    nan, ...,  1190.,  1335.,  1522.],\n        [   nan,    nan,    nan, ...,  1149.,  1255.,  1380.],\n        ...,\n        [  554.,   557.,   544., ...,  1122.,    nan,    nan],\n        [  583.,   503.,   486., ...,  1059.,    nan,    nan],\n        [  532.,   525.,   473., ...,  1095.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B3(time, y, x)float32nan nan nan ... 1.351e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  5978.,  5810.,  6208.],\n        [   nan,    nan,    nan, ...,  6139.,  5621.,  5896.],\n        [   nan,    nan,    nan, ...,  6440.,  6103.,  5997.],\n        ...,\n        [10655., 10096., 10182., ...,  6781.,  6842.,  5474.],\n        [ 9626.,  9502., 10141., ...,  7924.,  8409.,  5659.],\n        [ 8782.,  8944.,  9354., ...,  8530.,  8645.,  7237.]],\n\n       [[ 8117.,  8269.,  8972., ...,  6509.,  6257.,  7102.],\n        [ 9813.,  8405.,  7894., ...,  6683.,  6100.,  6382.],\n        [10242.,  9839.,  8840., ...,  7019.,  6668.,  6504.],\n        ...,\n...\n        [  795.,   776.,   790., ...,  1084.,  1220.,  1073.],\n        [  830.,   751.,   638., ...,  1094.,  1223.,   977.],\n        [  816.,   784.,   639., ...,  1148.,  1026.,   891.]],\n\n       [[  895.,  1016.,  1046., ...,  1476.,  1504.,   973.],\n        [ 1051.,   999.,   897., ...,  1318.,  1572.,  1684.],\n        [ 1075.,  1032.,  1004., ...,  1093.,  1335.,  1499.],\n        ...,\n        [  792.,   756.,   788., ...,  1145.,  1242.,  1100.],\n        [  832.,   726.,   664., ...,  1126.,  1255.,   993.],\n        [  813.,   761.,   608., ...,  1166.,  1110.,  1008.]],\n\n       [[   nan,    nan,    nan, ...,  1566.,  1669.,  1264.],\n        [   nan,    nan,    nan, ...,  1447.,  1659.,  1835.],\n        [   nan,    nan,    nan, ...,  1241.,  1465.,  1669.],\n        ...,\n        [  817.,   804.,   824., ...,  1349.,    nan,    nan],\n        [  848.,   767.,   687., ...,  1326.,    nan,    nan],\n        [  836.,   785.,   654., ...,  1351.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B4(time, y, x)float32nan nan nan ... 1.445e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  5187.,  4992.,  5774.],\n        [   nan,    nan,    nan, ...,  5256.,  4782.,  5134.],\n        [   nan,    nan,    nan, ...,  5649.,  5427.,  5273.],\n        ...,\n        [10998., 10420., 10512., ...,  6941.,  7017.,  5465.],\n        [ 9933.,  9851., 10476., ...,  8076.,  8508.,  5783.],\n        [ 9070.,  9271.,  9623., ...,  8672.,  8696.,  7421.]],\n\n       [[ 8208.,  8373.,  8949., ...,  5550.,  5398.,  6620.],\n        [ 9820.,  8390.,  8020., ...,  5718.,  5078.,  5425.],\n        [10243.,  9897.,  8857., ...,  6095.,  5775.,  5661.],\n        ...,\n...\n        [  775.,   769.,   764., ...,  1193.,  1364.,  1175.],\n        [  790.,   726.,   679., ...,  1193.,  1369.,  1022.],\n        [  738.,   762.,   646., ...,  1250.,  1056.,   955.]],\n\n       [[ 1019.,  1111.,  1139., ...,  1410.,  1493.,  1009.],\n        [ 1136.,  1104.,   991., ...,  1264.,  1481.,  1698.],\n        [ 1210.,  1122.,  1142., ...,  1130.,  1273.,  1535.],\n        ...,\n        [  810.,   789.,   779., ...,  1277.,  1401.,  1247.],\n        [  800.,   740.,   700., ...,  1233.,  1391.,  1065.],\n        [  776.,   774.,   651., ...,  1275.,  1190.,  1089.]],\n\n       [[   nan,    nan,    nan, ...,  1547.,  1639.,  1208.],\n        [   nan,    nan,    nan, ...,  1355.,  1571.,  1820.],\n        [   nan,    nan,    nan, ...,  1130.,  1404.,  1637.],\n        ...,\n        [  840.,   839.,   814., ...,  1475.,    nan,    nan],\n        [  855.,   778.,   742., ...,  1444.,    nan,    nan],\n        [  802.,   836.,   683., ...,  1445.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B8(time, y, x)float32nan nan nan ... 2.208e+03 nan nan<pre>array([[[   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        ...,\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan],\n        [   nan,    nan,    nan, ...,    nan,    nan,    nan]],\n\n       [[   nan,    nan,    nan, ...,  4276.,  4089.,  5000.],\n        [   nan,    nan,    nan, ...,  4322.,  3919.,  4197.],\n        [   nan,    nan,    nan, ...,  4684.,  4451.,  4310.],\n        ...,\n        [10667., 10119., 10220., ...,  6789.,  6765.,  5422.],\n        [ 9642.,  9625., 10164., ...,  7770.,  8218.,  5703.],\n        [ 8856.,  9046.,  9407., ...,  8321.,  8372.,  7258.]],\n\n       [[ 7688.,  7870.,  8241., ...,  4393.,  4298.,  5567.],\n        [ 8961.,  7822.,  7538., ...,  4488.,  4061.,  4483.],\n        [ 9254.,  9074.,  8255., ...,  4840.,  4612.,  4475.],\n        ...,\n...\n        [ 3299.,  3043.,  3446., ...,  2016.,  1802.,  1492.],\n        [ 3497.,  3196.,  2681., ...,  2136.,  1993.,  2335.],\n        [ 3557.,  3366.,  2658., ...,  2082.,  2654.,  2835.]],\n\n       [[ 1535.,  1779.,  1725., ...,  5187.,  5241.,  3208.],\n        [ 1689.,  1687.,  1717., ...,  4914.,  5625.,  5237.],\n        [ 1694.,  1749.,  1784., ...,  3391.,  4121.,  4569.],\n        ...,\n        [ 3162.,  2974.,  3239., ...,  2032.,  1848.,  1540.],\n        [ 3349.,  3044.,  2672., ...,  2234.,  1945.,  2323.],\n        [ 3347.,  3170.,  2343., ...,  2111.,  2656.,  3014.]],\n\n       [[   nan,    nan,    nan, ...,  4832.,  5022.,  3458.],\n        [   nan,    nan,    nan, ...,  4571.,  5241.,  5036.],\n        [   nan,    nan,    nan, ...,  3000.,  3851.,  4310.],\n        ...,\n        [ 3175.,  3096.,  3361., ...,  2087.,    nan,    nan],\n        [ 3332.,  3154.,  2701., ...,  2206.,    nan,    nan],\n        [ 3405.,  3286.,  2539., ...,  2208.,    nan,    nan]]],\n      dtype=float32)</pre></li><li>B11(time, y, x)float32nan nan nan ... 2.162e+03 nan nan<pre>array([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        ...,\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   35.,   50.,   86.],\n        [  nan,   nan,   nan, ...,   94.,   54.,   61.],\n        [  nan,   nan,   nan, ...,  101.,  126.,  130.],\n        ...,\n        [1356., 1331., 1330., ..., 1038., 1095., 1011.],\n        [1263., 1274., 1333., ..., 1047., 1077., 1035.],\n        [1193., 1209., 1275., ..., 1077., 1092., 1142.]],\n\n       [[ 824.,  810.,  796., ...,   40.,   44.,   69.],\n        [ 840.,  803.,  812., ...,   32.,   40.,   65.],\n        [ 829.,  822.,  827., ...,   12.,   45.,   53.],\n        ...,\n...\n        ...,\n        [2406., 2307., 2287., ..., 1990., 1866., 1749.],\n        [2454., 2242., 2079., ..., 1934., 1962., 2062.],\n        [2454., 2350., 2060., ..., 2048., 2293., 2423.]],\n\n       [[2289., 2594., 2779., ..., 4196., 4083., 3178.],\n        [2516., 2651., 2776., ..., 3809., 4438., 4386.],\n        [2518., 2746., 2777., ..., 3333., 3912., 4490.],\n        ...,\n        [2509., 2390., 2469., ..., 2143., 2002., 1930.],\n        [2540., 2280., 2218., ..., 2073., 1990., 2036.],\n        [2640., 2424., 2123., ..., 2080., 2351., 2600.]],\n\n       [[  nan,   nan,   nan, ..., 3937., 3844., 3249.],\n        [  nan,   nan,   nan, ..., 3667., 4190., 4182.],\n        [  nan,   nan,   nan, ..., 2992., 3647., 4200.],\n        ...,\n        [2477., 2393., 2369., ..., 2132.,   nan,   nan],\n        [2496., 2326., 2216., ..., 2071.,   nan,   nan],\n        [2542., 2395., 2168., ..., 2162.,   nan,   nan]]], dtype=float32)</pre></li><li>B12(time, y, x)float32nan nan nan ... 1.658e+03 nan nan<pre>array([[[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        ...,\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n        [  nan,   nan,   nan, ...,   nan,   nan,   nan]],\n\n       [[  nan,   nan,   nan, ...,   62.,  142.,  100.],\n        [  nan,   nan,   nan, ...,   89.,   62.,  100.],\n        [  nan,   nan,   nan, ...,  100.,  100.,  145.],\n        ...,\n        [1512., 1467., 1478., ..., 1123., 1173., 1009.],\n        [1411., 1414., 1479., ..., 1188., 1209., 1055.],\n        [1322., 1324., 1419., ..., 1227., 1246., 1197.]],\n\n       [[ 882.,  878.,  836., ...,  101.,  137.,  123.],\n        [ 889.,  870.,  863., ...,   79.,  114.,  106.],\n        [ 872.,  867.,  870., ...,   97.,   92.,  128.],\n        ...,\n...\n        ...,\n        [1350., 1284., 1255., ..., 1509., 1484., 1336.],\n        [1372., 1245., 1159., ..., 1440., 1468., 1487.],\n        [1390., 1290., 1127., ..., 1463., 1527., 1605.]],\n\n       [[1857., 2110., 2247., ..., 2527., 2581., 2188.],\n        [2008., 2116., 2225., ..., 2318., 2767., 2799.],\n        [2028., 2187., 2198., ..., 2048., 2532., 2998.],\n        ...,\n        [1450., 1360., 1385., ..., 1683., 1617., 1533.],\n        [1439., 1312., 1252., ..., 1542., 1560., 1582.],\n        [1512., 1376., 1230., ..., 1562., 1655., 1764.]],\n\n       [[  nan,   nan,   nan, ..., 2653., 2550., 2230.],\n        [  nan,   nan,   nan, ..., 2424., 2809., 2842.],\n        [  nan,   nan,   nan, ..., 2098., 2443., 2885.],\n        ...,\n        [1460., 1415., 1361., ..., 1757.,   nan,   nan],\n        [1483., 1355., 1289., ..., 1645.,   nan,   nan],\n        [1480., 1364., 1264., ..., 1658.,   nan,   nan]]], dtype=float32)</pre></li></ul></li><li>Indexes: (3)<ul><li>xPandasIndex<pre>PandasIndex(Float64Index([651590.0, 651610.0, 651630.0, 651650.0, 651670.0, 651690.0,\n              651710.0, 651730.0, 651750.0, 651770.0,\n              ...\n              656410.0, 656430.0, 656450.0, 656470.0, 656490.0, 656510.0,\n              656530.0, 656550.0, 656570.0, 656590.0],\n             dtype='float64', name='x', length=251))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5193750.0, 5193730.0, 5193710.0, 5193690.0, 5193670.0, 5193650.0,\n              5193630.0, 5193610.0, 5193590.0, 5193570.0,\n              ...\n              5189930.0, 5189910.0, 5189890.0, 5189870.0, 5189850.0, 5189830.0,\n              5189810.0, 5189790.0, 5189770.0, 5189750.0],\n             dtype='float64', name='y', length=201))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2018-01-02', '2018-01-12', '2018-01-27', '2018-01-29',\n               '2018-02-08', '2018-02-11', '2018-02-13', '2018-02-16',\n               '2018-02-18', '2018-02-21',\n               ...\n               '2021-08-16', '2021-08-19', '2021-08-21', '2021-08-24',\n               '2021-08-26', '2021-08-31', '2021-09-03', '2021-09-03',\n               '2021-09-05', '2021-09-08'],\n              dtype='datetime64[ns]', name='time', length=335, freq=None))</pre></li></ul></li><li>Attributes: (9)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1scale_factor :1.0add_offset :0.0crs :EPSG:32632resolution :(20.0, -20.0)units :meters</li></ul> In\u00a0[16]: Copied! <pre>S2_ds.B2.sel(time='2020-01-02').plot()\n</pre> S2_ds.B2.sel(time='2020-01-02').plot() Out[16]: <pre>&lt;matplotlib.collections.QuadMesh at 0x27b2bcbd060&gt;</pre> <p>Other possibilities for plotting include scatter plots, e.g. with two bands plotted against each other:</p> In\u00a0[17]: Copied! <pre>S2_ds.sel(time='2020-01-02').plot.scatter(x='B3', y='B4', marker='.', edgecolor='none', alpha=0.01)\n</pre> S2_ds.sel(time='2020-01-02').plot.scatter(x='B3', y='B4', marker='.', edgecolor='none', alpha=0.01) Out[17]: <pre>&lt;matplotlib.collections.PathCollection at 0x27b2cf8c400&gt;</pre> <p>Let's see how many valid observations we have for each pixel.</p> In\u00a0[18]: Copied! <pre>S2_count = S2_ds.B2.count(dim='time')   # Count valid data values of one band along the time dimension\nS2_count = S2_count.rename(new_name_or_name_dict='Valid observations')\nS2_count.plot()\n</pre> S2_count = S2_ds.B2.count(dim='time')   # Count valid data values of one band along the time dimension S2_count = S2_count.rename(new_name_or_name_dict='Valid observations') S2_count.plot() Out[18]: <pre>&lt;matplotlib.collections.QuadMesh at 0x27b2cfe88e0&gt;</pre> <p>To plot an RGB image we create a new DataArray with the respective bands from our DataSet, then plot this (plotting functionality of xarray is mainly focused on DataArrays).</p> In\u00a0[18]: Copied! <pre>S2_RGB_da = S2_ds[['B4', 'B3', 'B2']].sel(time='2020-01-02').to_array()\nS2_RGB_da.plot.imshow(robust=True)\n</pre> S2_RGB_da = S2_ds[['B4', 'B3', 'B2']].sel(time='2020-01-02').to_array() S2_RGB_da.plot.imshow(robust=True) Out[18]: <pre>&lt;matplotlib.image.AxesImage at 0x258b2a1b7f0&gt;</pre> <p>Plot multiple observations within a defined time period, one band only:</p> In\u00a0[19]: Copied! <pre>S2_ds.B2.sel(time='2020-05').plot.imshow(col='time')\n</pre> S2_ds.B2.sel(time='2020-05').plot.imshow(col='time') Out[19]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258b2d369b0&gt;</pre> <p>The same for a 3-band combination (here SWIR-1, near-infrared and blue) and a longer time period:</p> In\u00a0[20]: Copied! <pre># Create a new DataArray with the respective bands and time period, then plot this\nS2_RGB_da = S2_ds[['B11', 'B8', 'B2']].sel(time=slice('2020-05-01', '2020-07-31')).to_array()\nS2_RGB_da.plot.imshow(col='time', robust=True)\n</pre> # Create a new DataArray with the respective bands and time period, then plot this S2_RGB_da = S2_ds[['B11', 'B8', 'B2']].sel(time=slice('2020-05-01', '2020-07-31')).to_array() S2_RGB_da.plot.imshow(col='time', robust=True) Out[20]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258b2dd3d90&gt;</pre> In\u00a0[21]: Copied! <pre># Compute the NDSI from the SWIR-1 (B11) and green (B3) bands and add as a new variable to our Sentinel-2 Dataset\nS2_ds['NDSI'] = (S2_ds.B3 - S2_ds.B11) / (S2_ds.B3 + S2_ds.B11)\n\n# For convenience define NDSI as a new DataArray (Note: metadata, such as the CRS, are not propagated to the new DataArray by default)\nNDSI = S2_ds.NDSI\n\n# Show the resulting rasters of one month\n# for other colormaps see https://matplotlib.org/stable/gallery/color/colormap_reference.html\nNDSI.sel(time='2020-05').plot(col='time', cmap='ocean')\nNDSI.sel(time='2020-05').plot(col='time', cmap='coolwarm_r', center=0.4, robust=True)    # center cmap around 0.4, a typical threshold for snow discrimination\n</pre> # Compute the NDSI from the SWIR-1 (B11) and green (B3) bands and add as a new variable to our Sentinel-2 Dataset S2_ds['NDSI'] = (S2_ds.B3 - S2_ds.B11) / (S2_ds.B3 + S2_ds.B11)  # For convenience define NDSI as a new DataArray (Note: metadata, such as the CRS, are not propagated to the new DataArray by default) NDSI = S2_ds.NDSI  # Show the resulting rasters of one month # for other colormaps see https://matplotlib.org/stable/gallery/color/colormap_reference.html NDSI.sel(time='2020-05').plot(col='time', cmap='ocean') NDSI.sel(time='2020-05').plot(col='time', cmap='coolwarm_r', center=0.4, robust=True)    # center cmap around 0.4, a typical threshold for snow discrimination Out[21]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258b79eb760&gt;</pre> In\u00a0[22]: Copied! <pre># Plot the original (observed) NDSI values for one month\nNDSI.sel(time='2020-05').plot(col='time', cmap='ocean')\n\n# Interpolate missing values in these images and plot (default: linear interpolation)\nNDSI.sel(time='2020-05').interpolate_na(dim='time').plot(col='time', cmap='ocean')#(time=new_dates)\n\n# Forward fill missing values in these images and plot (requires the bottleneck package to be installed)\nNDSI.sel(time='2020-05').ffill(dim='time').plot(col='time', cmap='ocean')\n\n# Backward fill missing values in these images and plot\nNDSI.sel(time='2020-05').bfill(dim='time').plot(col='time', cmap='ocean')\n</pre> # Plot the original (observed) NDSI values for one month NDSI.sel(time='2020-05').plot(col='time', cmap='ocean')  # Interpolate missing values in these images and plot (default: linear interpolation) NDSI.sel(time='2020-05').interpolate_na(dim='time').plot(col='time', cmap='ocean')#(time=new_dates)  # Forward fill missing values in these images and plot (requires the bottleneck package to be installed) NDSI.sel(time='2020-05').ffill(dim='time').plot(col='time', cmap='ocean')  # Backward fill missing values in these images and plot NDSI.sel(time='2020-05').bfill(dim='time').plot(col='time', cmap='ocean') Out[22]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258b9096800&gt;</pre> <p>Via matplotlib we have many options for customizing plots (e.g., create subplots), and here we use xarray's possibility to make line plots directly from a DataArray:</p> In\u00a0[23]: Copied! <pre>import matplotlib.pyplot as plt                         # for extended plotting functionality\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 4))         # make two subplots next to each other\n\n# Plot NDSI from the original time series\nNDSI.isel(x=10, y=[10, 20, 30]).plot.line(ax=axes[0], x='time')\n\n# Forward fill missing values of the entire NDSI time series, select a few locations with constant x and plot the NDSI against time\nNDSI.ffill(dim='time').isel(x=10, y=[10, 20, 30]).plot.line(ax=axes[1], x='time')\n\nplt.tight_layout()\n</pre> import matplotlib.pyplot as plt                         # for extended plotting functionality  fig, axes = plt.subplots(1, 2, figsize=(15, 4))         # make two subplots next to each other  # Plot NDSI from the original time series NDSI.isel(x=10, y=[10, 20, 30]).plot.line(ax=axes[0], x='time')  # Forward fill missing values of the entire NDSI time series, select a few locations with constant x and plot the NDSI against time NDSI.ffill(dim='time').isel(x=10, y=[10, 20, 30]).plot.line(ax=axes[1], x='time')  plt.tight_layout() <p>... or heatmaps:</p> In\u00a0[24]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(10, 5))            # make two subplots next to each other\n\n# Plot NDSI heatmap from the original time series (values at locations with constant x and varying y)\nNDSI.isel(x=100, y=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]).plot(ax=axes[0], cmap='coolwarm_r', center=0.4, robust=True)\n\n# Forward fill missing values of the entire NDSI time series, select a few locations with constant x and plot the NDSI heatmap\nNDSI.ffill(dim='time').isel(x=100, y=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]).plot(ax=axes[1], cmap='coolwarm_r', center=0.4, robust=True)\n\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(10, 5))            # make two subplots next to each other  # Plot NDSI heatmap from the original time series (values at locations with constant x and varying y) NDSI.isel(x=100, y=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]).plot(ax=axes[0], cmap='coolwarm_r', center=0.4, robust=True)  # Forward fill missing values of the entire NDSI time series, select a few locations with constant x and plot the NDSI heatmap NDSI.ffill(dim='time').isel(x=100, y=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]).plot(ax=axes[1], cmap='coolwarm_r', center=0.4, robust=True)  plt.tight_layout() <p>It is also possible to interpolate other time steps (not covered by a Sentinel-2 overpass). Of course, such processing steps must be used carefully and considered during further analysis and interpretation. Note that the last \"new\" date cannot be interpolated because there is now data (in the selected subset) after this date.</p> In\u00a0[25]: Copied! <pre># Interpolate new dates and create a new DataArray\nnew_dates = ['2020-05-05', '2020-05-10', '2020-05-15', '2020-05-20', '2020-05-25', '2020-05-30']\nNDSI_new_dates_da = NDSI.sel(time='2020-05').interp(time=new_dates)\n\n# Concatenate original data and data for new dates, and sort by 'time' (to maintain a chronological order)\nNDSI_all_dates_da = xarray.concat(objs=[NDSI.sel(time='2020-05'), NDSI_new_dates_da], dim='time').sortby('time')\n\n# Plot \nNDSI_all_dates_da.plot(col='time', cmap='ocean')\n</pre> # Interpolate new dates and create a new DataArray new_dates = ['2020-05-05', '2020-05-10', '2020-05-15', '2020-05-20', '2020-05-25', '2020-05-30'] NDSI_new_dates_da = NDSI.sel(time='2020-05').interp(time=new_dates)  # Concatenate original data and data for new dates, and sort by 'time' (to maintain a chronological order) NDSI_all_dates_da = xarray.concat(objs=[NDSI.sel(time='2020-05'), NDSI_new_dates_da], dim='time').sortby('time')  # Plot  NDSI_all_dates_da.plot(col='time', cmap='ocean') Out[25]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258bcbd2170&gt;</pre> In\u00a0[26]: Copied! <pre>NDSI.groupby('time.month').mean(dim='time').plot(col='month', cmap='ocean')\n</pre> NDSI.groupby('time.month').mean(dim='time').plot(col='month', cmap='ocean') Out[26]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258bcc3be50&gt;</pre> <p>If we are interested in the variation of snow cover in different months, we can calculate the variance (or the standard deviation) over the months:</p> In\u00a0[27]: Copied! <pre># Group by month and compute the variance of NDSI, then plot\nNDSI.groupby('time.month').var(dim='time').plot(col='month')\n</pre> # Group by month and compute the variance of NDSI, then plot NDSI.groupby('time.month').var(dim='time').plot(col='month') Out[27]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258b931c6d0&gt;</pre> <p>... or over seasons (named by the months' first letters):</p> In\u00a0[28]: Copied! <pre># Group by seasons and compute the variance of NDSI, then plot\n# I would prefer a chronological order of seasons but cannot get them sorted easily\nNDSI.groupby('time.season').var(dim='time').plot(col='season')\n</pre> # Group by seasons and compute the variance of NDSI, then plot # I would prefer a chronological order of seasons but cannot get them sorted easily NDSI.groupby('time.season').var(dim='time').plot(col='season') Out[28]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258c1207310&gt;</pre> <p>Now let's have a look at monthly NDSI aggregates of different years. We resample to one month temporal resolution by computing the mean, then make a facet plot with 12 columns, so that we get each year in a separate row.</p> In\u00a0[29]: Copied! <pre>NDSI.resample(time='1M').mean().plot(col='time', col_wrap=12, cmap='coolwarm_r', center=0.4, robust=True)\n</pre> NDSI.resample(time='1M').mean().plot(col='time', col_wrap=12, cmap='coolwarm_r', center=0.4, robust=True) Out[29]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258c9b535e0&gt;</pre> <p>There seem to be no images from May 2018 (resulting in a NoData aggregate), and we may be interested in the number of observations for all other months as well. Using <code>count()</code> instead of <code>mean()</code>, we can inspect the number of valid observations per month:</p> In\u00a0[30]: Copied! <pre>NDSI.resample(time='1M').count().plot(col='time', col_wrap=12)\n</pre> NDSI.resample(time='1M').count().plot(col='time', col_wrap=12) Out[30]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x258d32b9b70&gt;</pre> In\u00a0[31]: Copied! <pre># We use pandas' .duplicated() to check for duplicates\nimport pandas as pd\n\n# Print the dates with duplicates\nfor i,j in zip(pd.Index(NDSI['time']).duplicated(), NDSI['time'].values):\n    if i == True:\n        print(j)\n</pre> # We use pandas' .duplicated() to check for duplicates import pandas as pd  # Print the dates with duplicates for i,j in zip(pd.Index(NDSI['time']).duplicated(), NDSI['time'].values):     if i == True:         print(j) <pre>2019-03-28T00:00:00.000000000\n2020-10-18T00:00:00.000000000\n2020-10-28T00:00:00.000000000\n2020-11-17T00:00:00.000000000\n2020-11-27T00:00:00.000000000\n2020-12-17T00:00:00.000000000\n2021-01-16T00:00:00.000000000\n2021-02-15T00:00:00.000000000\n2021-09-03T00:00:00.000000000\n</pre> In\u00a0[32]: Copied! <pre># Remove duplicates and keep only the first occurrences (default: keep='first')) in a new DataArray with unique time index\nNDSI_unique = NDSI.drop_duplicates('time')\n\n# Compare the shape of the two DataArrays\nprint(NDSI.shape)\nprint(NDSI_unique.shape)\n</pre> # Remove duplicates and keep only the first occurrences (default: keep='first')) in a new DataArray with unique time index NDSI_unique = NDSI.drop_duplicates('time')  # Compare the shape of the two DataArrays print(NDSI.shape) print(NDSI_unique.shape) <pre>(335, 201, 251)\n(326, 201, 251)\n</pre> <p>Now we resample to one day temporal resolution and fill daily values by linear interpolation between observations.</p> In\u00a0[33]: Copied! <pre># Resample to 1 day temporal resolution\nNDSI_interpolated = NDSI_unique.resample(time='1d').interpolate()\n\n# Interpolate the missing values along the time dimension\nNDSI_interpolated = NDSI_interpolated.interpolate_na(dim='time')\n\n# Plot the resulting time series at three points\nNDSI_interpolated.isel(x=100, y=[50, 100, 150]).plot.line(x='time')\n\n# Or in one line of code\n#NDSI_unique.resample(time='1d').interpolate().interpolate_na(dim='time').isel(x=100, y=[50, 100, 150]).plot.line(x='time')\n</pre> # Resample to 1 day temporal resolution NDSI_interpolated = NDSI_unique.resample(time='1d').interpolate()  # Interpolate the missing values along the time dimension NDSI_interpolated = NDSI_interpolated.interpolate_na(dim='time')  # Plot the resulting time series at three points NDSI_interpolated.isel(x=100, y=[50, 100, 150]).plot.line(x='time')  # Or in one line of code #NDSI_unique.resample(time='1d').interpolate().interpolate_na(dim='time').isel(x=100, y=[50, 100, 150]).plot.line(x='time') Out[33]: <pre>[&lt;matplotlib.lines.Line2D at 0x258deee70d0&gt;,\n &lt;matplotlib.lines.Line2D at 0x258def2f7f0&gt;,\n &lt;matplotlib.lines.Line2D at 0x258def2f760&gt;]</pre> <p>Compute the 30-day rolling average of the NDSI time series and plot for one point (aggregation results are assigned the coordinate at the end of each window by default, but can be centered by passing center=True when constructing the rolling object):</p> In\u00a0[34]: Copied! <pre># Set up the plot\nfig, axes = plt.subplots(1, 1, figsize=(12, 4))\n\n# The daily NDSI values\nNDSI_interpolated.isel(x=100, y=50).plot.line(label='Interpolated NDSI')\n\n# Take the mean value over the NDSI values in a 30-day rolling window and plot the time series for one specific point\nNDSI_interpolated.rolling(time=30, center=True).mean().isel(x=100, y=50).plot.line(label='Interpolated NDSI, 30-days rolling average')\n\n# Show the legend and show the plot\nfig.legend()\nplt.tight_layout()\n</pre> # Set up the plot fig, axes = plt.subplots(1, 1, figsize=(12, 4))  # The daily NDSI values NDSI_interpolated.isel(x=100, y=50).plot.line(label='Interpolated NDSI')  # Take the mean value over the NDSI values in a 30-day rolling window and plot the time series for one specific point NDSI_interpolated.rolling(time=30, center=True).mean().isel(x=100, y=50).plot.line(label='Interpolated NDSI, 30-days rolling average')  # Show the legend and show the plot fig.legend() plt.tight_layout()"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#raster-time-series-processing-in-python-using-xarray","title":"Raster time series processing in Python using xarray\u00b6","text":"<p>This tutorial provides an introduction to raster time series processing with Python using (mainly) the xarray package (Hoyer and Hamman 2017) for multi-dimensional data. Xarray's data structures (<code>DataArray</code> and <code>Dataset</code>, see here) complement NumPy-like arrays by labels in the form of dimensions, coordinates and attributes, thereby encoding information about how the array values are absolutely related to space, time etc. The labels can also be used to access and analyse the array values. These properties and functionality are very useful if you work with time series of remote sensing images or other raster data.</p> <p>The Python packages used in this tutorial are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p> <p>Data:</p> <p>As a sample dataset, we use a time series of Sentinel-2 satellite data covering a relatively small area around the village of Obergurgl, located in the Central Alps of Tyrol (Austria), and the years 2018 to 2021. This imagery has been queried from the Google Earth Engine (GEE), then cloud-masked and exported from GEE as 6-band GeoTiffs, one GeoTiff per scene (see theme 2 for a tutorial on exporting cloudless Sentinel-2 imagery from GEE). You find this dataset in a folder named <code>s2</code> in the data repository of the course.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#loading-many-files-of-an-image-time-series","title":"Loading many files of an image time series\u00b6","text":"<p>We take the set of Sentinel-2 images stored locally as GeoTIFFs (after you downloaded them as the folder <code>s2</code> from the course data repository). Using rioxarray, we load all these images to a DataArray and construct a DataSet from them. We follow more or less the approaches described here and here.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#working-with-images-and-time-series","title":"Working with images and time series\u00b6","text":""},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#plotting","title":"Plotting\u00b6","text":"<p>As a next step we will explore some of the xarray plotting possibilities with our image time series. If you need more advanced functionality, try out more specialized packages (such as Cartopy for making maps and hvplot for interactive plots, see this tutorial).</p> <p>First, select one scene and one band and plot this:</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#band-calculations","title":"Band calculations\u00b6","text":"<p>As an example for xarray's raster calculator functionality we calculate the Normalized Difference Snow Index (NDSI; Hall et al. 1995) for the entire Sentinel-2 time series. We can add it as a new variable to our Sentinel-2 Dataset, thereby keeping our data organised. Alternatively, we can define it as a new DataArray.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#time-series-interpolation","title":"Time series interpolation\u00b6","text":"<p>For temporal interpolation and aggregation xarray contains some very useful functionality, and much of this works similar as with Pandas. In the rather short time series above we can see the snow disappearing and we also encounter different amounts of valid pixels (white NoData pixels should result from cloud-masking). As a next step, we will fill missing values with different methods.</p> <p>Can you explain the method-specific differences in the resulting images? Why does the linear interpolation not fill all missing values?</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#time-series-aggregation","title":"Time series aggregation\u00b6","text":"<p>Aggregating over temporal units is straightforward thanks to the time indexing. Here we group all observations of our NDSI time series by month and calculate the averages over these groups, then plot the resulting monthly composite rasters. This temporal aggregation gives a good impression of the typical snow cover distribution in our mountainous area-of-interest across the year.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_S2_xarray.html#time-series-smoothing","title":"Time series smoothing\u00b6","text":"<p>A couple of other operations related to time series processing are straightforward with xarrays, such as smoothing and filtering with rolling windows.</p> <p>To test such methods, we first resample to a higher temporal resolution (\"upsampling\" in the xarrays documentation) by linear interpolation. This will not work if there are duplicates in the time index (e.g. due to S2 products sensed on the same day but available in two versions with different product generation time stamps). Then you will get an \"InvalidIndexError\" (Reindexing only valid with uniquely valued Index objects). So we treat the duplicates first, e.g. by searching, printing and removing them.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_spectral-temporal_profiles.html","title":"Explore temporal profiles of a vegetation index in Python with pandas","text":"<p>In this tutorial, we explore temporal profiles drawn from Sentinel-2 data showing the variation of spectral values over time at the example of the popular Normalized Difference Vegetation Index (NDVI). We use Python with the packages pandas, matplotlib and seaborn.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_spectral-temporal_profiles.html#optionally-query-the-required-spectral-bands-from-gee-via-qgis-and-download-temporal-profiles-for-sample-points","title":"Optionally Query the required spectral bands from GEE via QGIS and download temporal profiles for sample points","text":"<p>(Shortcut: Download the folder <code>T1_point_profiles.zip</code> with the spectral profiles directly from the course data repository.)</p> <p>First, download and import the geopackage <code>T1_sample_points_001.gpkg</code> from the course data repository containing a few sample points located around the village of Obergurgl (Central Alps, Tyrol, Austria). These points were labelled as</p> <ul> <li>meadow</li> <li>pasture</li> <li>forest</li> <li>bare earth</li> </ul> <p>by visual interpretation of an aerial orthophoto (acquired in 2020 and provided by the federal authorities of Tyrol).</p> <p>Query Sentinel-2 Level-2A data as before, this time by choosing your sample points layer and its landcover field in the GEE Timeseries Explorer's point browser (bottom line). You can browse through the points and display a temporal profile for each point (arrow buttons). We choose band 4 and band 8 now, plus the Scene Classification Map (SCL) and the Cloud Mask (QA60).</p> <p>Again we run a monthly binning on the time series, this time rendering a false-color composite (bands 843 as RGB) with each bin's minimum as a reducer. This prefers cloud- and snow-free pixels if available in the bin (but unfortunately also cloud shadows are preferred).</p> <p> </p> <p>What spectral-temporal patterns can you identify?</p> <p>Finally, we download all point profile data as a set of textfiles, using the button in the lower right corner.</p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_spectral-temporal_profiles.html#analyse-spectral-temporal-profiles-for-the-sample-points","title":"Analyse spectral-temporal profiles for the sample points","text":"<p>For further analysis, we continue in Python and load all point profiles into a pandas dataframe:</p> <pre><code># load some packages we'll use\nimport os\nfrom glob import glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntxtfiles = glob(os.path.join(\n    'T_1_point_profiles', 'profile*.txt'))           # profiles in directory /T_1-1_point_profiles/\n\n# read all point profiles (*.txt) into a dataframe and add it to a list\ndf_list= []                                           # create empty list to store dataframes\nfor f in txtfiles:\n    tmp_df = pd.read_csv(f, delimiter=\";\",\n                        na_values=\"None\",                                      # the profiles may contain NoData values marked as \"None\"\n                        index_col=\"time\", parse_dates=True)                    # index the data by sensing time of the Sentinel scenes\n    df_list.append(tmp_df)                                                      # append df to list (any better suggestions?)\n    print(f'Successfully created dataframe for {f} with shape {tmp_df.shape}')\n\n# concatenate our list of dataframes into one\ndf = pd.concat(df_list, axis=0, sort=True)\n# inspect the new dataframe\nprint(df.shape)\ndf.info()\ndf.head()\n</code></pre> <p>We calculate the NDVI (Rouse et al. 1974) for each observation as</p> <pre><code>df[\"NDVI\"] = (df[\"B8\"] - df[\"B4\"]) / (df[\"B8\"] + df[\"B4\"])\n ```\n\nNow let's plot using the matplotlib and seaborn data visualization libraries. Passing the entire NDVI column in long-form mode will aggregate over repeated values (each date) to show the mean of our observations and the 95% confidence interval:\n\n```python\nplt.figure(figsize=(10, 5))\nsns_plot = sns.lineplot(data=df, x=df.index, y='NDVI')\n</code></pre> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_spectral-temporal_profiles.html#using-cloud-and-snow-masks","title":"Using cloud and snow masks","text":"<p>Above, we see a clear seasonal component but also lots of noise, caused probably by clouds (and snow). Remember that we did not constrain our search for S-2 scenes very strictly (&lt; 40% cloudy pixels in a scene). As an easy way to omit cloudy observations, we use the quality band (QA60) delivered with the Sentinel-2 L-2A product. Clouds are encoded as \"0\" there.</p> <pre><code>df_cloudless = df[df['QA60']==0]\n</code></pre> <p>Alternatively, we could use the Sentinel-2 scene classification (SCL) to filter.</p> <pre><code>df_filtered = df[df.SCL.isin([4, 5, 7])]    # take only observations where pixels are classified as vegetation, not vegetated, unclassified\nprint(df.shape)\nprint(df_cloudless.shape)   # the new dataframes have less rows\nprint(df_filtered.shape)    # the new dataframes have less rows\n</code></pre> <pre><code># plot the cloud-masked NDVI time series\nplt.figure(figsize=(10, 5))\nsns_plot = sns.lineplot(data=df_cloudless, x=df_cloudless.index, y='NDVI')\nplt.title('NDVI cloud-masked')\n\n# plot the filtered NDVI time series\nplt.figure(figsize=(10, 5))\nsns_plot = sns.lineplot(data=df_filtered, x=df_filtered.index, y='NDVI')\nplt.title('NDVI SCL-filtered')\n</code></pre> <p> </p> <p>While the results looks much improved, they differ and they are still far from a smooth curve.</p> <p>Now let's focus on one landcover class. Note that the bottom plot lacks observations in winter as snow is filtered via the SCL.</p> <pre><code>sns.set_style(\"whitegrid\")\nfig, axes = plt.subplots(3, 1, figsize=(10, 5), sharex=True)\nsns_plot = sns.lineplot(data=df[df['landcover']=='meadow'],\n                        x=df[df['landcover']=='meadow'].index, y='NDVI',\n                        color='limegreen',\n                        ci='sd',        # show standard deviation\n                        ax=axes[0])\nsns_plot.set_title('Meadow - unfiltered')\nsns_plot.set(ylim=(-0.2, 1))\n\nsns_plot = sns.lineplot(data=df_cloudless[df_cloudless['landcover']=='meadow'],\n                        x=df_cloudless[df_cloudless['landcover']=='meadow'].index, y='NDVI',\n                        color='limegreen',\n                        ci='sd',\n                        ax=axes[1])\nsns_plot.set_title('Meadow - cloud-masked')\nsns_plot.set(ylim=(-0.2, 1))\n\nsns_plot = sns.lineplot(data=df_filtered[df_filtered['landcover']=='meadow'],\n                        x=df_filtered[df_filtered['landcover']=='meadow'].index, y='NDVI',\n                        color='limegreen',\n                        ci='sd',\n                        ax=axes[2])\nsns_plot.set_title('Meadow - SCL-filtered')\nsns_plot.set(ylim=(-0.2, 1))\nfig.tight_layout()\n</code></pre> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_spectral-temporal_profiles.html#visualizing-ndvi-time-series-per-point","title":"Visualizing NDVI time series per point","text":"<p>To get an impression of the entire set of NDVI observations at all our sample points we can plot a heatmap for each landcover class.</p> <pre><code># separate dataframes by landcover\ndf_cl_m = df_cloudless[df_cloudless['landcover']=='meadow']\ndf_cl_p = df_cloudless[df_cloudless['landcover']=='pasture']\ndf_cl_f = df_cloudless[df_cloudless['landcover']=='forest']\ndf_cl_b = df_cloudless[df_cloudless['landcover']=='bare earth']\n\n# pivot tables (each point profile in a separate column)\ndf_cl_m_pivot = pd.pivot_table(df_cl_m, values='NDVI', index='time', columns='fid')     # fid: feature identifier of point\ndf_cl_p_pivot = pd.pivot_table(df_cl_p, values='NDVI', index='time', columns='fid')\ndf_cl_f_pivot = pd.pivot_table(df_cl_f, values='NDVI', index='time', columns='fid')\ndf_cl_b_pivot = pd.pivot_table(df_cl_b, values='NDVI', index='time', columns='fid')\n\n# keep only the date part of the datetime index (scene timestamps)\ndf_cl_m_pivot.index=df_cl_m_pivot.index.date            # meadow\ndf_cl_p_pivot.index=df_cl_p_pivot.index.date            # pasture\ndf_cl_f_pivot.index=df_cl_f_pivot.index.date            # forest\ndf_cl_b_pivot.index=df_cl_b_pivot.index.date            # bare earth\n\n# plot heatmaps\nfig, axes = plt.subplots(1,4, figsize=(12, 7), sharey=True)\nsns_plot = sns.heatmap(data=df_cl_m_pivot, vmin=-0.3, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=20, ax=axes[0])\nsns_plot.set_title('NDVI meadow points')\nsns_plot = sns.heatmap(data=df_cl_p_pivot, vmin=-0.3, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=20, ax=axes[1])\nsns_plot.set_title('NDVI pasture points')\nsns_plot = sns.heatmap(data=df_cl_f_pivot, vmin=-0.3, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=20, ax=axes[2])\nsns_plot.set_title('NDVI forest points')\nsns_plot = sns.heatmap(data=df_cl_b_pivot, vmin=-0.3, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=20, ax=axes[3])\nsns_plot.set_title('NDVI bare earth points')\n</code></pre> <p></p>"},{"location":"module1/01_principles_of_remote_sensing_time_series/T1_spectral-temporal_profiles.html#temporal-aggregates","title":"Temporal aggregates","text":"<p>To get a more generalized view we can calculate temporal aggregates (such as the monthly maximum NDVI), also called temporal resampling or reducing. First, we have to restore the pivot tables with the full DatetimeIndex, needed for temporal resampling (we reduced this to the date to get cleaner labels on the time axis). (Maybe omit this step?)</p> <pre><code># restore pivot tables\ndf_cl_m_pivot = pd.pivot_table(df_cl_m, values='NDVI', index='time', columns='fid')     # fid: feature identifier of point\ndf_cl_p_pivot = pd.pivot_table(df_cl_p, values='NDVI', index='time', columns='fid')\ndf_cl_f_pivot = pd.pivot_table(df_cl_f, values='NDVI', index='time', columns='fid')\ndf_cl_b_pivot = pd.pivot_table(df_cl_b, values='NDVI', index='time', columns='fid')\n\n# aggregate over temporal units (monthly maximum NDVI)\ndf_cl_m_pivot_monthly = df_cl_m_pivot.resample('M').max()\ndf_cl_p_pivot_monthly = df_cl_p_pivot.resample('M').max()\ndf_cl_f_pivot_monthly = df_cl_f_pivot.resample('M').max()\ndf_cl_b_pivot_monthly = df_cl_b_pivot.resample('M').max()\n\n# keep only the date part of the datetime index (scene timestamps)\ndf_cl_m_pivot_monthly.index=df_cl_m_pivot_monthly.index.date            # meadow\ndf_cl_p_pivot_monthly.index=df_cl_p_pivot_monthly.index.date            # pasture\ndf_cl_f_pivot_monthly.index=df_cl_f_pivot_monthly.index.date            # forest\ndf_cl_b_pivot_monthly.index=df_cl_b_pivot_monthly.index.date            # bare earth\n\n# plot heatmaps\nfig, axes = plt.subplots(1,4, figsize=(12, 7), sharey=True)\nsns_plot = sns.heatmap(data=df_cl_m_pivot_monthly, vmin=-0.2, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=4, ax=axes[0])\nsns_plot.set_title('NDVI meadow')\nsns_plot = sns.heatmap(data=df_cl_p_pivot_monthly, vmin=-0.2, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=4, ax=axes[1])\nsns_plot.set_title('NDVI pasture')\nsns_plot = sns.heatmap(data=df_cl_f_pivot_monthly, vmin=-0.2, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=4, ax=axes[2])\nsns_plot.set_title('NDVI forest')\nsns_plot = sns.heatmap(data=df_cl_b_pivot_monthly, vmin=-0.2, vmax=0.9, cmap='PRGn', center=0.1, yticklabels=4, ax=axes[3])\nsns_plot.set_title('NDVI bare earth')\n</code></pre> <p></p> <p>Try other options for aggregation (min, mean, standard deviation, ...) and see the effect on the plot. What could be applications for the specific methods?</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html","title":"Theme 2 - Large time series datasets in remote sensing","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#introduction","title":"Introduction","text":"<p>Remote sensing (RS) applications are increasingly based on large time series, which have opened new possibilities for understanding land surface dynamics and human-environment interactions. The data is often too big (mainly as an effect of large extent and high resolution in space, time and other dimensions, such as spectral wavelength) to be processed with standard methods.</p> <p>Objective</p> <p>The objective of this theme is to overview methods and approaches to query, access and handle such large time series datasets in remote sensing. The scope ranges from improvement of processes on a single local machine to efficiently querying and downloading the data you need from huge archives and to scalability of entire workflows using distributed computing on high-performance clusters and cloud infrastructure. Data-wise, the focus is on medium resolution satellite data but 3D point clouds, typically acquired from airborne and terrestrial platforms, are also touched briefly.</p> <p>At the end of this theme, you will</p> <ul> <li>know several of the most important remote sensing missions and public archives hosting remote sensing time series data</li> <li>be able to search and download such open data efficiently; the focus here is on time series of optical images with medium spatial resolution from<ul> <li>the NASA/USGS Landsat program</li> <li>the Copernicus Sentinel-2 mission</li> </ul> </li> <li>have an overview of the possibilities currently available for big data management and processing in remote sensing</li> </ul> <p>Working through the tutorials and exercises of this theme you will try some convenient methods to access and start working with large remote sensing time series. This includes various state-of-the-art approaches, such as Spatio-Temporal Asset Catalogs (STAC) or the Google Earth Engine (GEE) which enables you to utilize cloud storage and computing infrastructure.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#major-earth-observation-missions-data-archives-and-access","title":"Major Earth observation missions, data archives, and access","text":"<p>This section overviews a selection of the most important global Earth observation missions and the resulting data products along with their spatial, spectral and temporal properties (see also the respective sections in Theme 1 and Theme 2 of the second E-TRAINEE Module). A few examples for public data archives of national or regional remote sensing data are also presented. Furthermore, the main possibilities for accessing this data are outlined (see also this section in Module 2, with links to additional archives). A searchable and comprehensive collection of satellites and sensors is provided in the Land Remote Sensing Satellites Online Compendium.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#landsat-missions","title":"Landsat missions","text":"<p>The Landsat program, jointly run by the NASA and the USGS, provides the longest and most continuous time series of Earth observation images. Starting with the launch of Landsat 1 in 1972, a series of satellite missions have produced millions of images covering a time period where major anthropogenic and natural changes of the Earth's surface took place. The revisit period of a single Landsat is 16 days (18 days for L1-3). Whereas the use of Landsat time series had long been limited by data charges and access, the consolidation of all data into a centralized and open archive (at a time when also computational ressources and storage improved rapidly) have marked a new era of Earth observation (Woodcock et al. 2008, Wulder et al. 2016). Hence, Landsat data have been used in numerous applications and scientific studies on long-term ecosystem monitoring, human-environment interactions and other topics (Wulder et al. 2019).</p> <p></p> <p>Timeline of Landsat satellite missions (figure by NASA's Scientific Visualization Studio).</p> <p>Although continuity and interoperability is a priority of the program, the technology employed by the Landsat missions has undergone considerable improvements over the decades. Based on sensor similarities, the missions and their data can be grouped into L1-3, L4-5, L7, and L8-9. Landsat 8 and 9 (launched in 2013 and 2021, respectively) feature 11 spectral bands with spatial resolutions ranging from 15 m (panchromatic) to 30 m (VNIR and SWIR) and 100 m (thermal; resampled to 30 m for delivery). The successior mission Landsat Next has been announced to improve on temporal, spectral and spatial resolution.</p> <p>Data products and access</p> <p>Global Landsat data has been reprocessed by the USGS and archived into different Collections and processing levels:</p> <ul> <li>Landsat Collection 2 is currently the collection processed with state-of-the-art algorithms and auxiliary data, with its products of Level-1 and Level-2 being available for download from EarthExplorer</li> <li>Level-1 data for all Landsat sensors 1-9 since 1972 are delivered as Digital Numbers (DN) in an unsigned 16-bit integer format. DN can be converted to Top of Atmosphere (TOA) reflectance or radiance using the radiometric scaling factors provided in each scene metadata file.</li> <li>Level-2 data contains surface reflectances and land surface temperatures along with Quality Assessment (QA) masks and is currently available for Landsat 4-9 (i.e. from 1982 on) in Collection 2. These products are generated from Level-1 inputs that meet the &lt;76 degrees Solar Zenith Angle constraint (i.e. no processing of images taken with low sun angles) and include the required auxiliary data inputs to generate a scientifically viable product.</li> <li>For the United States, Landsat Collection 2 also contains ARD (analysis-ready data), with TOA refelctances/brightness temperatures and surface reflectances/temperatures as well as pixel-level quality assessment data gridded in Albers projection and distributed as tiles of 5,000 x 5,000 30-m pixels.</li> <li>Frome the above products the USGS is deriving a few Level-3 science products (currently on-demand?) that represent biophysical properties of the Earth's surface, such as Dynamic Surface Water Extent, Fractional Snow Covered Area, Burned Area, or Provisional Actual Evapotranspiration.</li> </ul> <p></p> <p>Landsat 8 Collection 2 products for an area over the Sapta Koshi River in Bairawa, Nepal (path 140 row 41) acquired on May 3, 2013. Left: Level-1 top of atmosphere reflectance (TOA) image. Center: Atmospherically corrected surface reflectance (SR) image. Right: Level-2 surface temperature (ST [K]) image (figure by M. A. Bouchard, USGS (2013)).</p> <p></p> <p>An animation of the Omaha area (Nebraska, USA) that shows spring flooding rendered first in a Landsat 8 Operational Land Imager (OLI) false color composite, and second by the Landsat Level-3 Dynamic Surface Water Extent (DSWE) product. (figure by Earth Resources Observation and Science (EROS) Center (2019)).</p> <p>You can inspect Landsat images with RGB natural color, a range of other band combinations (composites) and a few different indices (band ratios) in the LandsatLook viewer.</p> <p>As shown in this overview page, there are several options for downloading Landsat data:</p> <ul> <li>EarthExplorer with a graphical user interface (GUI)</li> <li>EE Bulk Download Web Application (BDWA): Web-based tool useful for downloading large quantities of Landsat Collection 2 products with limited user interaction.</li> <li>The The EROS Science Processing Architecture On Demand Interface produces Landsat Surface Reflectance-derived Spectral Indices on demand and includes options for reprojection and spatial sub-setting (https://www.usgs.gov/media/files/eros-science-processing-architecture-demand-interface-user-guide).</li> <li>The landsatexplore package provides an interface to the EarthExplorer portal to search and download Landsat Collections scenes through a command-line interface or a Python API.</li> <li>If you need to download a lot of scenes, see this description of how to create a file list and use it to download the scenes listed.</li> <li>Landsat ARD (analysis-ready data) can be accessed and visualized via the AppEEARS API (see this Jupyter Notebook Python Tutorial)</li> <li>Landsat Collection 2 Level-1 and Level-2 global scene-based data products and Landsat Collection 2 ARD are hosted on the Amazon Web Services (AWS) cloud platform in a Simple Storage Service (S3) requester pays bucket, and can be accessed via the SpatioTemporal Asset Catalog (STAC) API (as shown in this tutorial and in this post).</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#sentinel-2-mission","title":"Sentinel-2 mission","text":"<p>Copernicus is the European Union's Earth Observation Programme. It includes, among others, the Sentinel missions, a series of (currently) seven space missions dedicated to radar and multi-spectral imaging for land, ocean and atmosphere observation, each comprising a constellation of two satellites. Among these missions we focus on Sentinel-2, with its two satellites (Sentinel-2A/B) acquiring optical images for land monitoring services (e.g. land cover change, vegetation state, water or snow extent etc.) since 2015 and 2017, respectively. Important for time series analysis, the revisit period of the two satellites together is 5 days at the equator, and shorter at higher latitudes. Unfortunately, the true temporal resolution of useful data can be heavily impaired by clouds.</p> <p>The multi-spectral instrument (MSI) onboard each Sentinel-2 features 13 spectral bands in the visible-to-near infrared (VNIR) and short wave infrared (SWIR) ranges. In addition to the four VNIR bands at 10-m resolution, coarser resolution bands (20/60 m) are available for vegetation monitoring in the so-called red-edge (Horler et al. 1983), and for snow, ice and cloud discrimination as well as for aerosols, water vapour and cirrus cloud detection. In the examples of this theme we will work with the VNIR bands (2, 3, 4, and 8) and the SWIR-1 band.</p> <p></p> <p>Sentinel-2 bands overview (figure by Digital Earth Australia, reused from ESA).</p> <p>Data products and access</p> <p>The easiest way to look at Sentinel-2 imagery (and often also other free satellite data) is through one of the online viewers, such as</p> <ul> <li>Copernicus Browser</li> <li>Sentinel Playground</li> <li>Sentinel Hub EO Browser</li> </ul> <p>The core products generated from the raw Sentinel-2 data correspond to two processing levels:</p> <ul> <li>Level-1C: Top-of-atmosphere reflectances in cartographic geometry, i.e. radiometrically and geometrically corrected (including orthorectification) but without correction of atmospheric effects.</li> <li>Level-2A: This is the product typically used for analysis. It contains surface reflectances in cartographic geometry (UTM/WGS84) as well as a scene classification map and cloud masks. It can either be generated from Level-1C data on the user side with the sen2cor processor from ESA's SNAP Toolbox or directly obtained from the Copernicus Data Space Ecosystem or other archives.</li> </ul> <p>The Copernicus Data Space Ecosystem has been announced to become operational as the \"official\" access point for Sentinel data in the course of 2023. It features the the Copernicus Browser, for search, visualization and download of scenes (after registration for the service). The scenes are provided as ZIP-compressed archives in a folder structure called SAFE (Standard Archive Format for Europe). Each SAFE archive contains a 110 x 110 km tile with the corresponding rasters in JPEG2000 format. Various other tools and APIs to explore and analyse Earth observaton data and services are announced. </p> <p></p> <p>Browser-based graphical user interface of the Copernicus Open Accesss Hub, useful mainly for downloading one or a few scenes.</p> <p>Furthermore, Copernicus data are distributed by several national mirrors, and by partial mirrors (integrating specific Sentinel data (products or geographic coverages) into their platforms). In addition, Sentinel data can be obtained via several free or paid versions of Copernicus Data and Information Access Services (DIAS), such as Mundi or CREODIAS.</p> <p>Finally, cloud providers (such as Amazon Web Services (AWS) or Google Cloud Storage) are hosting various collections of Earth observation data. Tools like sen2R and GRASS GIS i.sentinel.download are increasingly adding these sources to their download options. If such data collections are combined with a powerful cloud computing infrastructure as well as basic and advanced processing algorithms, this opens interesting possibilities for large scale analysis (as detailed below).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#the-harmonized-landsat-and-sentinel-2-hls-archive","title":"The Harmonized Landsat and Sentinel-2 (HLS) archive","text":"<p>For many time series applications the temporal resolution of a single archive (i.e. either Landsat or Sentinel) alone is not enough, and it is therefore necessary to combine the data from the Landsat and the Sentinel missions. Since the two original archives contain differences in terms of data structure and content (related to sensor characteristics and data management), the user has to perform sophisticated preprocessing steps to make the data consistent and ready to use in a joint analysis.</p> <p>For some applications this effort can now be omitted with NASA EOSDIS Land Processes DAAC having released a Harmonized Landsat and Sentinel-2 (HLS) archive. These HLS products provide consistent surface reflectance (SR) and top of atmosphere (TOA) brightness data from the Landsat 8 OLI and the Sentinel-2 MSI at 30 m resolution with global observations of the land every 2\u20133 days (Claverie et al. 2018). Preprocessing steps performed by the HLS project to create seamless products from both sensors (OLI and MSI) include</p> <ul> <li>atmospheric correction</li> <li>masking of cloud and cloud shadows</li> <li>spatial co-registration, common gridding and tiling</li> <li>illumination and view angle normalization</li> <li>spectral bandpass adjustment</li> </ul> <p> </p> <p>Timeline of the Landsat/Sentinel-2 Virtual Constellation, contributing to the Harmonized Landsat and Sentinel-2 (HLS) archive with consistent surface reflectance at improved temporal resolution (figure by HLS Science Team/NASA EarthData). See also this animation of the different satellites with orbits and data swaths.</p> <p>You can download HLS products through NASA's Earthdata Search or use the following ressources to get started:</p> <ul> <li>HLS Quick Guide</li> <li>Jupyter Notebook Tutorials with HLS data<ul> <li>in Python</li> <li>in R</li> </ul> </li> <li>HLS subsetting, processing, and exporting reformatted data Python script</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#low-spatial-resolution-missions","title":"Low (spatial) resolution missions","text":"<p>While Landsat and Sentinel-2 data are, due to their spatial resolution of a few decametres, often used for land use and land cover monitoring, lower resolution data is often not suitable for mapping discrete objects (like land parcels). However, for continental scale monitoring of more continuous phenomena (such as sea and land surface temperature, vegetation phenology, or snow cover) spatial resolutions of several hundred meters are often sufficient. Missions dedicated to such applications typically prioritize higher resolutions in the spectral and temporal domains (i.e. more bands and shorter revisit time).</p> <p>The main missions in this context are the Moderate Resolution Imaging Spectroradimeter (MODIS) and its predecessor and successor missions AVHRR and VIIRS, respectively. Moreover, the Sentinel-3 mission is an important initiative in low-spatial, high-temporal resolution Earth Observation. The Sentinel-3 mission is intensively discussed - also in comparison with similar missions named above - in a series of posts (1, 2, 3) of the imagico.de blog by Hormann (2016).</p> <p>These two lessons on using MODIS data with Python are provided by the Earth Lab at University of Colorado, Boulder:</p> <ul> <li>How to find and download MODIS products from the USGS Earth Explorer website</li> <li>Working with MODIS data in Python</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#commercial-satellite-imagery","title":"Commercial satellite imagery","text":"<p>In addition to the satellite imagery acquired by public organizations which are accessible as open data, there are a number of commercial providers for satellite imagery. Their imagery has often a very high spatial resolution but is acquired on-demand, i.e. acquisitions of specific areas are tasked upon request by paying clients (in contrast to e.g. Sentinels and Landsats capturing most of the Earth surface in a fixed schedule defined by their orbits and sensor orientations).</p> <p>SpyMeSat provides a smartphone app and a web interface for Earth observation satellite overpass notifications (when is your point of interest captured by which satellite system), options for purchasing the imagery from leading providers, and management and observation of tasking requests (for taking new imagery on-demand).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#national-and-regional-scale-remote-sensing-data-from-aerial-platforms","title":"National and regional scale remote sensing data from aerial platforms","text":"<p>Aerial remote sensing data (mostly orthophotos and airborne laser scanning data) is typically acquired at frequencies of several years. Private companies are usually contracted by the public authorities to acquire and process the data and to deliver a specified product. These products are usually distributed on a regional/national basis by the respective public authorities, often via web services and sometimes (and increasingly) with a query and download platform, e.g.:</p> <p>Autonomous Province of South Tyrol - Bolzano (Italy)</p> <p>A comprehensive set of public geodata covering the Autonomous Province of South Tyrol - Bolzano (Italy) can be searched and downloaded via the GeoKatalog under a CC0 1.0 license. Amongst others, the archive comprises RGB and color-infrared orthophotos and ALS point clouds from different campaigns. Spatial subsets can be defined (by a box, a polygon, a place or a map tile) and then downloaded in a chosen coordinate reference system. Links to the respective Web Mapping Services (WMS) can be obtained from the same application.</p> <p>Tyrol (Austria)</p> <p>In the federal state of Tyrol (Austria; Land Tirol - data.gv.at), multitemporal orthophotos, digital terrain models (DTM) from airborne laser scanning (and derivatives, such as shaded relief or slope gradient rasters), and some other geodata are provided as WMS, WCS, WFS, and WMTS (see this website and this document for instructions and links to the services). While these services are very convenient for displaying (background) maps, download capabilities are limited (to subsets of raster DTMs and vector data).</p> <p>Download of data (under a CC BY 4.0 license) is possible at two applications for recent RGB orthophotos (0.2 m resolution) and for airborne laser scanning data (1-m DTM, DSM, and contours) from the latest acquisition.</p> <p>An overview of data acquisition campaigns for orthophotos and airborne laser scanning is provided at this application, as a possibility to search for available datasets in a specific area. The data (including RGB/CIR orthophotos, 0.5-m elevation models, and ALS point clouds) could then be ordered from the Geoinformation Department at the Office of the Federal Government of Tyrol.</p> <p>Switzerland</p> <p>The Swiss Geodownloader plugin for QGIS allows you to download Swiss open geodata (such as the swissALTI3D digital elevation model or Swisstopo maps). You can search for datasets, filter files by extent, timestamp or file type and add the geodata directly to QGIS.</p> <p>USA</p> <p>In the continental USA, the National Agriculture Imagery Program (NAIP) acquires aerial imagery at 1 m resolution during the agricultural growing seasons every three years. The following notebooks demonstrate the access of NAIP orthophotos using Microsoft cloud services:</p> <ul> <li>Accessing NAIP data with the Microsoft Planetary Computer STAC API</li> <li>Accessing NAIP data on Microsoft Azure</li> </ul> <p>New Zealand</p> <p>For New Zealand a rich archive of national and regional geospatial data is available from the Land Information New Zealand (LINZ) data service.</p> <p>OpenAerialMap</p> <p>OpenAerialMap is a platform for openly licensed imagery, where you can browse for available imagery in a map viewer or share your own images. It contains both imagery from occupied aircraft and from unoccupied aircraft/drones.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#strategies-and-computing-facilities-for-large-remote-sensing-time-series-an-overview","title":"Strategies and computing facilities for large remote sensing time series: An overview","text":"<p>In remote sensing analyses covering a single or a few points in time the data sets can already be quite large, depending on the extent and resolution (a single Sentinel-2 scene for instance has more than 1 GB). If we move to time series of remote sensing data, our data sets can easily grow huge, meaning that we need dedicated tools for management and analysis.</p> <p>This section gives a short overview of the strategies commonly used for processing large remote sensing data. This includes:  </p> <ul> <li>Approaches to handle (moderately large) datasets and processing tasks on a single, local machine (laptop, desktop computer/workstation)</li> <li>Processing on a high-performance computing (HPC) cluster</li> <li>Cloud computing</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#local-data-management-and-processing","title":"Local data management and processing","text":"<p>Clearly, the possibilities for processing large time series of remote sensing data locally on your laptop and even on more powerful workstations are limited. Nevertheless, there are ways to make ideal use of the available ressources or to reduce the data amount and the processing to what is really needed (without necessarily sacrificing too much in terms of accuracy or completeness of the outcome). Some considerations for local data management and processing in the context of remote sensing time series include:</p> <ul> <li>Tiling and virtual data sets (rasters or point clouds): E.g., using GDAL you can create Virtual Rasters that do not copy the original raster data but reference them in a rather small XML file (normally with the extension .vrt) specifiying, e.g., that multiple bands compose a multi-band raster. See the documentation of the GDAL Virtual Format. In QGIS this is possible with the Virtual Raster Builder plugin or via the standard save or export file dialogs.</li> <li>Data formats (NetCDF, HDF5, Parquet, Zarr, ...), see e.g. here</li> <li>For vector data it may be worth investigating the relatively new Geoparquet format, that adds interoperable geospatial types (Point, Line, Polygon) to Apache Parquet.</li> <li>Point cloud thinning, i.e. keeping only a subset of the points. The motivation can be to ease processing (reduce data) or to homogenize the point density. Various algorithms and tools for thinning are available, e.g. in SAGA GIS, PDAL, or CloudCompare.</li> <li>Such a subset of a larger point cloud is sometimes referred to as core points, and it can make sense to perform certain processing steps only for these core points (e.g. for distance calculation with the M3C2 method (Lague et al. 2013) in CloudCompare).</li> <li>Keep in mind that not all applications need 64-bit precision, so you might save memory by changing the data type to one that is sufficient for the specific dataset (e.g. 32-bit (float32)). Satellite imagery level 2 products (e.g. Landsat and Sentinel-2) often come with a scaling factor applied to surface reflectance (e.g., multiplied by 10000) to store integers instead of floating point values.</li> <li>Dimensionality reduction (e.g. by a Principal Components Analysis (PCA))</li> <li>Parallelization of processing tasks</li> <li>Out-of-core solutions where only those parts of the data are read into memory that are currently needed</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#high-performance-computing-on-a-central-cluster","title":"High performance computing on a central cluster","text":"<p>High Performance Compute (HPC) systems are large networks of multicore processors. They make it possible to run programs which require more ressources than those available with a single workstation. Such applications include:</p> <ul> <li>Parallel computations on many CPUs</li> <li>Parallel computations with large demand of memory</li> <li>Computations with extensive temporary disk space usage</li> </ul> <p>In other words: Why use a HPC?</p> <ul> <li>Parallelization - run many small tasks at the same time; it is important that your code is designed for parallel computing to actually make use of it</li> <li>Run a very large task that requires a lot of computational power or RAM</li> </ul> <p>Typical examples for parallel computing would be a sensitivity/parameter study with many runs of a program, a task that is performed for many spatial or temporal subsets of the data (pixel time series or tiles/chunks, as long as no interaction between these subsets is required). In many RS projects with high computational demand the input data is huge. Thus, you should first consider if and how your data can be transferred from its archive to the HPC infrastructure.</p> <p>Academic research cluster facilities can be divided into institutional (university-based) infrastracture, national collaborations and international/supranational collaborations. Examples maintained at university level are:</p> <ul> <li>LEO4 HPC infrastructure of the University of Innsbruck (A), consisting of 50 nodes with a total of 1452 cores and 8.4 TB of memory. An introduction for LEO4 users is provided here.</li> <li>CRIB, the platform of the Univesity of Twente (NL) for parallel, distributed, or GPU-assisted computing capabilities for (big) geospatial data analysis.</li> </ul> <p>While institutional clusters are usually quite well accessible for staff (and often also for students of the institution) the computing ressources established through national and supranational collaborations can typically be used only after (peer-reviewed) application. Examples are:</p> <ul> <li>Vienna Scientific Cluster (VSC), a collaboration of several Austrian universities that provides HPC resources and corresponding services to their users</li> <li>Partnership for Advanced Computing in Europe (PRACE)</li> </ul> <p>The way how you actually connect to the HPC system, set up your software environment, and submit your computing tasks (jobs) to be scheduled and distributed depends on the specific HPC system. Therefore, we do not go into the details here and encourage you to look into the instructions provided by the system administrator.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#distributed-storage-and-computing-in-the-cloud","title":"Distributed storage and computing in the cloud","text":"<p>If the data to be processed is huge, it becomes impractical to transfer the data (including unnecessary parts of it) to the computing ressource. Then, it rather makes sense to ship the requests with processing instructions for the computer to the data and run the processes there on dedicated, shared hardware, as it is done in cloud computing.</p> <p>Cloud computing is the \"on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user\" (Wikipedia). Providers deliver cloud computing services (such as storage, data, software and computing power) via the internet to their users, who pay for the services they use (or use a limited amount of ressources for free). Read more about the general concepts of cloud computing here.</p> <p></p> <p>Concepts of cloud computing, where networked user elements access the services, hardware and software managed by a provider in a system that can be thought of as an amorphous cloud (figure by Johnston 2009/Wikipedia/ CC BY-SA 3.0).</p> <p>For remote sensing and geospatial cloud computing, prominent providers of such services are:</p> <ul> <li>Google with its Google Earth Engine (GEE)</li> <li>Amazon with its Amazon Web Services (AWS)</li> <li>Microsoft with its Microsoft Planetary Computer (MPC), for details see e.g. this talk or the Planetary Computer tutorials repository</li> </ul> <p>Despite the impressive benefits of cloud computing services in terms of available computing power, you have to expect limitations due to computing ressources allocated to a user by the specific service. This may require some kind of sampling or another kind of workaround (see e.g. Anderson et al. 2020 or Yang et el. 2021). So far, especially the Google Earth Engine has been extensively used to study land surface processes and human-environment interactions. Examples include:</p> <ul> <li>Forest monitoring: Detection and analysis of tree cover change on a global scale, see Hansen et al. (2013) and the Global Forest Watch (GFW) online platform providing maps of forest change, monthly and weekly deforestation alerts as well as data showing the dominant drivers of tree cover loss</li> <li>Climate change related greening trends in sparsely vegetated ecosystems: Anderson et al. (2020) used data from the Landsat archive to study trends and patterns of greening in the Hindu Kush Himalaya</li> <li>National-scale monitoring of agricultural practices and use intensities: Stumpf et al. (2018) mapped annual grass-/cropland distributions, and Stumpf et al. (2020) differentiated grassland management practices (mowing or grazing) and use intensities across Switzerland based on Landsat time series metrics. Together with a database from field survey programs this revealed the effects of landuse practices on key ecosystem parameters such as soil organic carbon and plant diversity.</li> </ul> <p>To make access of geospatial data from clouds and other web-based services more efficient, specific cloud-native data formats have been specified:</p> <ul> <li>Cloud-optimized GeoTIFFs (COGs) are essentially regular GeoTIFFs with a special internal structure that makes it possible to download only parts of them (blocks) or lower-resolution versions (overviews).</li> <li>Zarr is a cloud-native format for the storage of large multidimensional arrays. Watch this video to learn more about Zarr.</li> <li>For point clouds the Cloud Optimized Point Cloud (COPC) format has recently been developed. A COPC file is a LAZ 1.4 file that stores chunks of point data organized in a clustered octree. Read the blog post about COPC in QGIS. Sample COPC datasets can be seen in this viewer or downloaded here and here. A point cloud dataset from the USGS 3D Elevation Program can be accessed via the Microsoft Planetary Computer.</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#data-cubes","title":"Data cubes","text":"<p>Although proposed already in the 1990s (Baumann et al. 1993), data cubes have recently emerged as a powerful approach for the management and analytics of multi-dimensional EO data (more correctly: \"hyper-rectangles\" (Pebesma and Bivand 2023). The data cubes are based on gridded spatio-temporal data with spatial (e.g. latitude and longitude) and time dimensions creating a three-dimensional cube of data. Various thematic data parameters (e.g. spectral bands) can be represented by additional dimensions of the data cube. A data cube can be deployed on various scales and infrastructures: from local workstations for use in smaller projects to large scale (cloud) back-ends serving a wider user community (Sudmanns et al. 2022).</p> <p>There are various data cube initiatives which share the intention to facilitate access and processing of big geo-spatial data for users. By providing Analysis-Ready-Data (ARD) structured uniformly (in a common grid) and indexed by their dimensions (as opposed to a sensor and provider dependent structure) they reduce the burden of data search, handling and pre-processing. For practical implementation of a data cube, however, there are various approaches and technologies around, which we try to grasp in the following.</p> <p></p> <p>Data cube with five dimensions: latitude, longitude, time, spectral band, and sensor (figure by Pebesma 2023/r-spatial/ Apache License 2.0).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#open-data-cube-approach-and-implementations","title":"Open Data Cube approach and implementations","text":"<p>Open Data Cube (ODC) is an open source geospatial data management and analysis software project that helps you harness the power of satellite data. At its core, ODC is a set of Python libraries and PostgreSQL database that helps you work with geospatial raster data. The Open Data Cube can be deployed on HPC, cloud, and local installations and accessed via a Python API which loads the data as xarray objects. Check out the ODC manual or the ODC Notebooks.</p> <p></p> <p>The Open Data Cube ecosystem (figure by Open Data Cube/ Apache License 2.0).</p> <p>ODC projects and implementations include:</p> <ul> <li>Digital Earth Australia - A platform to make satellite data and its derivatives accessible and manageable for Australian purposes. This concept has evolved into the Open Data Cube (ODC), a not-for-profit open source project and community developing.     Check out the DEA Sandbox, a learning and analysis environment created as an introduction to Digital Earth Australia and the Open Data Cube, or have a look at the notebooks by Krause et al. (2021).</li> <li>Swiss Data Cube (Chatenoux et al. 2021) -      The Swiss Data Cube was one of the first adopters of the Data Cube system for a national scale platform. It includes data from Landsat, and Sentinel-1 and -2 over Switzerland, as well as products related to urbanization, cloud free mosaics, and snow cover (from https://www.opendatacube.org/overview).</li> <li>Digital Earth Africa deploys the Africa Regional Data Cube. For more information see this video, try the Sandbox and see the documentation.</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#euro-data-cube-and-xcube","title":"Euro Data Cube and xcube","text":"<p>Euro Data Cube</p> <p>The Euro Data Cube (EDC) is operated by a consortium of private Earth observation companies and cloud providers and supported by the European Space Agency. The EDC is offered as a subscription-based, paid service. Its main objective is to provide a data archive and computing platform for commercial applications, including the possibility for companies to develop applications on top of EDC and sell it to their customers. However, a (small) trial is available for free and researchers may apply for a free account - check out the announcements of opportunity and the FAQ. A good impression of various variables relevant for Earth System Science (such as gross primary productivity, fractional snow cover, ...) with map views at coarse resolutions (of &gt;20 km) and temporal point profiles is provided by this variant of the EDC. All further information can be found in the official documentation. There are different options for access and analysis of the EDC, including a QGIS plugin and xcube.</p> <p>xcube</p> <p>xcube is an open-source Python package and toolkit used for the Euro Data Cube. It helps to create self-contained data cubes that can be published in the cloud and it offers customizable analysis and processing based on Python's xarray.</p> <p>Plugins for xcube add support for the Sentinel Hub Cloud API, the ESA Climate Change Initiative Open Data Portal, or the Copernicus Climate Data Store.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#data-cubes-based-on-array-databases","title":"Data cubes based on array databases","text":"<p>An alternative approach to data cubes is based on array databases (Baumann et al. 2019). The pioneering implementation of an array database management system is rasdaman (\"raster data manager\"), featuring</p> <ul> <li>support of the Coverage data model for multi-dimensional raster data and related Open Geospatial Consortium (OGC) web service standards, most notably Web Coverage Service (WCS) and Web Coverage Processing Service (WCPS), as explained here</li> <li>the query language rasql</li> <li>server-side tiled storage in a relational database with spatial indexing</li> <li>tile streaming</li> <li>dynamic scheduling of processes on a rasdaman server installation</li> <li>client APIs for C++ (raslib), Java (rasj) and Python (rasdapy)</li> </ul> <p>These demos show how to use Coverages by connecting to the rasdaman service endpoint via QGIS or Python:</p> <ul> <li>QGIS</li> <li>Python (Jupyter Notebook)</li> </ul> <p>The EarthServer federation is an initiative that uses this data cube approach to share and unify spatio-temporal data from multiple large-scale Earth data providers. The federation members include research centers, supercomputing centers, companies, and agencies.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#data-cubes-with-otherunknown-architecture","title":"Data cubes with other/unknown architecture","text":"<p>Further implementations and prototypes of a data cube include:</p> <ul> <li>Germany's Copernicus Data and Exploitation Platform CODE-DE (Storch et al. 2019) amongst other services hosts two data cubes, one based on the Sentinel Hub software by Sinergise, and one based on the FORCE software by David Frantz (2019).</li> <li>The Earth System Data Lab (ESDL) follows a data cube approach with a multivariate focus. It prioritizes efficient exploitation of multiple data streams for a joint analysis over efficiency with a high-resolution data stream (Mahecha et al. 2020). Analyses can be deployed in the ESDL with Python or Julia and via user-defined functions (UDFs).</li> <li>The Brasil Data Cube (Simoes et al. 2021) is implemented using the R programming language and the SITS package.</li> <li>The Austrian Data Cube (ACube) is run by the Earth Observation Data Centre for Water Resources Monitoring (EODC) and provides pre-processed Sentinel-1 &amp; -2 data. The ACube can be accessed throught an application programming interface (API) or via WMS/WCS Geoserver (e.g. with QGIS). (?)</li> <li>The Austrian semantic EO data cube Sen2Cube is a prototype(?) for a semantically enriched, web-based data cube (Sudmanns et al. 2021).</li> <li>The SemantiX project aims to retrieve Essential Climate Variables (ECVs) from AVHHR data (1981-today) and Sentinel-3 data(2016-today) covering the entire European Alps. The ECVs are made publicly available via a data cube and a smartphone app.</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#more-tools-for-building-your-own-data-cube","title":"More tools for building your own data cube","text":"<p>If you want to build your own data cube:</p> <ul> <li>gdalcubes - Earth observation data cubes from GDAL image collections. The library is written in C++ and includes a basic command line interface and an R package. A Python package is planned for the future.</li> <li>gdalcubes_R - A package for creating and analyzing Earth observation data cubes in R (limitations: no vector data support, only 4 D; opposed to starspackage).</li> <li>stars - An R package for spatiotemporal arrays, raster and vector data cubes.</li> <li>SITS - An R package for satellite image time series analysis and machine learning based classification of image time series obtained from data cubes (Simoes et al. 2021, Camara et al. 2023).</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#working-with-a-data-cube-external-demos-and-tutorials","title":"Working with a data cube: External demos and tutorials","text":"<p>If you want to try working with data cubes, check out these external ressources:</p> <ul> <li>Open Data Cube Sandbox, which is a JupyterHub Python notebook server with individual work spaces and with global Landsat 8 data indexed</li> <li>This collection of notebooks covering, e.g., landslide detection and machine learning</li> <li>ODC-Colab - This aims at a demonstration of Open Data Cubes within Google Colab and performs an automated setup of an ODC environment with GEE data.</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#large-time-series-datasets-in-python","title":"Large time series datasets in Python","text":"<p>Clearly, automatisation of workflows by scripting your processing pipelines is key to any successful working with large datasets, especially when scientific criteria such as reproducibility and objectivity are important. Compared to clicking your way through a graphical user interface (GUI) of a specialized GIS/remote sensing software, this is much more efficient and enables you to flexibly customize the processing exactly to your needs. In this context, the Python ecosystem provides a rich set of tools and interfaces to data and computing platforms and is, thus, the preferred choice for many professional applications. </p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#python-libraries","title":"Python libraries","text":"<p>Some Python libraries you might find useful for the management and analysis of (large) remote sensing data sets (including remote sensing time series):</p> <ul> <li>Numba: You may accelerate your code with Numba, which translates Python functions into optimized machine code at runtime.</li> <li> <p>Scale your Python workflows (whether they are based on arrays or on dataframes) by dynamic task scheduling, parallel computing and lazy evaluation using Dask. This library extends the size of convenient datasets from \u201cfits in memory\u201d to \u201cfits on disk\u201d and can be useful both on a single machine and in distributed environments (clusters with 100s of cores). Further reading in Daniel (2019).</p> </li> <li> <p>Handling tabular (column-oriented) data:</p> <ul> <li>Pandas and its DataFrames are a popular choice in data science and can also be useful in remote sensing applications (when data is not array-like but tabular, e.g. image objects, sample points, ...). However, it is neither  specifically made for geodata nor for very large datasets.</li> <li>GeoPandas extends the capabilities of pandas by possibilities for spatial operations, making it ideal for vector geodata. Recent efforts included performance improvements and support for storing geospatial vector data (point, line, polygon) efficiently according to the GeoParquet specification.</li> <li>Dask-Geopandas merges the geospatial capabilities of GeoPandas and scalability of Dask. Thus, it can be an interesting option if you have large GeoDataFrames that either do not comfortably fit in memory or require expensive computation that can be easily parallelised.</li> <li>Spatialpandas is developed for vectorized spatial and geometric operations on DataFrames using numba (with support for Parquet and Dask).</li> <li> <p>Modin provides a convenient way to speed up pandas code by parallelization (without changes in your existing pandas code). It uses Ray or Dask internally to schedule the computations on multiple cores (which pandas normally does not do). Moreover, it supports out-of-core processing to prevent pandas from running out of memory.</p> </li> <li> <p>Vaex is a library developed for large tabular datasets (that otherwise tend to raise memory errors). With its memory mapping and lazy reading capabilities, Vaex works well with file formats like HDF5, Apache Arrow and Apache Parquet. It has a focus on</p> <ul> <li>Visualization: Histograms, density plots and 3D volume rendering.</li> <li>Exploration: Calculate statistics (such as mean, sum, count, standard deviation etc.) on an N-dimensional grid with up to a billion (10<sup>9</sup>) objects/rows per second.</li> <li>Machine learning with vaex.ml: So far with data transformers, wrappers to ML libraries (Scikit-Learn, xgboost) and a performant KMeans implementation.</li> </ul> </li> <li> <p>dask.dataframe is a large parallel DataFrame composed of many smaller pandas DataFrames, split along the index. It is a more flexible option than Dask-Geopandas but lacks the convenient implementation of geospatial functionality.</p> </li> <li>In summary: Small data: use Pandas. Out of memory error: move to vaex. Crazy amount of data (many TB) that will never fit onto 1 computer: dask.dataframe (as recommended here)</li> </ul> </li> <li> <p>Handling array data:</p> <ul> <li>xarray (Hoyer and Hamman 2017) is a package for labelled multi-dimensional arrays and integrates tightly with Dask (see this example). Rasterio (a library for geospatial raster data access) extends the general purpose Xarray library to Rioxarray, with improved geospatial capabilities.</li> <li>The Satpy library also builds on xarray and is made to conveniently handle satellite data, i.e. to import various formats, create composites, perform atmospheric corrections and visual enhancements.</li> <li>The Iris library can be seen as an alternative to xarray, with a focus on keeping metadata (such as information on units following the CF conventions) while working with multi-dimensional Earth Science data (e.g. meteorological and oceanographic data). </li> <li>RasterFrames is a data model and processing software system developed by Astraea, Inc. It organizes raster data in Spark DataFrames and has a focus on scalability of Earth observation data analytics.</li> </ul> </li> <li> <p>Spectral indices: The Spyndex package is specialized on computing a wide range of spectral indices for arrays and dataframes together with pandas, geopandas, xarrays and dask.</p> </li> <li>EOReader is a package for reading (your locally stored) data from optical and SAR constellations, loading and stacking bands, clouds, DEM and spectral indices in a sensor-agnostic way. (Comparable to what eemont does compared to the raw GEE Python API) it simplifies these tasks as compared to rasterio/rioxarray:</li> </ul> <p></p> <p>EOReader simplifications overview (figure by ICube-SERTIT/EOReader documentation/ Apache License 2.0).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#tutorials-remote-sensing-time-series-in-python","title":"Tutorials: Remote sensing time series in Python","text":"<p>Some of the tools listed above are demonstrated in the following notebooks:</p> <ul> <li> <p>Large point clouds in Python tutorial, providing a couple of hints for handling and exploring large point clouds efficiently in Python (so far not time-series specific). Use sample data provided or try with your own point cloud.</p> </li> <li> <p>External ressources:</p> <ul> <li>Why Xarray and Dask? (Pangeo)</li> <li>Parallel computations on dataframes and arrays using Dask (Pangeo tutorial)</li> <li>Parallel raster computations using Dask (Carpentries Incubator tutorial)</li> </ul> </li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#data-access-from-spatiotemporal-asset-catalogs-stac","title":"Data access from SpatioTemporal Asset Catalogs (STAC)","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#introduction-to-spatiotemporal-asset-catalogs-stac","title":"Introduction to SpatioTemporal Asset Catalogs (STAC)","text":"<p>The SpatioTemporal Asset Catalog (STAC) specification enables online search and discovery of geospatial assets (i.e. specifc data products) by providing a common language to describe a range of geospatial information, so it can more easily be indexed and discovered. A 'spatiotemporal asset' is any file that represents information about the Earth captured in a certain space and time.</p> <p>When analyzing optical imagery, in many cases you need only certain spectral bands covering your area-of-interest, instead of entire scenes. An important advantage of accessing a STAC server is that you can constrain the download to the specific data you actually need, not only in terms of Earth observation mission, sensor and time period, but also in terms of bands and spatial coverage.</p> <p>Watch this video to learn more about STAC. A list of STAC APIs and catalogs is provided on the STAC Index website. QGIS users can access data from several endpoints via the QGIS STAC API plugin).</p> <p> </p> <p>The QGIS STAC API Browser, the search results, and footprint of one item (a Landsat 9 scene) loaded to the map canvas.</p> <p></p> <p>One asset (Surface Temperature Band) of the STAC item (Landsat 9 scene) found is loaded as a layer to the QGIS map canvas.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#tutorials-getting-satellite-data-from-a-stac-catalog","title":"Tutorials: Getting satellite data from a STAC catalog","text":"<p>This ETRAINEE notebook illustrates how to query and download Sentinel-2 satellite imagery from the Amazon Web Service (AWS) cloud using the pystac-client library. Furthermore, it shows how to work with the data using the xarray library and how to perform tasks like subsetting, cloud masking, spectral index calculation, temporal aggregation, and trend analysis. A few simple processing steps let you explore the development of the Normalized Difference Vegetation Index (NDVI) in spring and summer, including e.g. the seasonal greening of mountain grasslands.</p> <p>More (external) Python Jupyter Notebooks:</p> <ul> <li>Access Landsat data from Microsoft Planetary Computer</li> <li>Access Sentinel 2 Data from AWS (1), including visualization of asset footprints</li> <li>Access Sentinel 2 Data from AWS (2)</li> <li>Access Landsat data from AWS, a rendered version is here</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#google-earth-engine","title":"Google Earth Engine","text":"<p>Earth Engine is a service offered by Google for analysis and visualization of geospatial datasets, based on data storage and computing on a large cloud (Gorelick et al. 2017, Amani et al. 2020, Tamiminia et al. 2020, ). The Google Earth Engine (GEE) is free to use for research, education, and nonprofit use after signing up for access to the service.</p> <p>There are different ways to use GEE:</p> <ul> <li>the JavaScript Code Editor used in a web browser</li> <li>an Earth Engine client library for Python</li> <li>an Earth Engine client library for R (rgee; Aybar et al. 2020), which wraps the Earth Engine Python API</li> <li>a command line tool</li> <li>via QGIS with the GEE Plugin or the GEE Timeseries Explorer</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#the-javascript-code-editor","title":"The JavaScript Code Editor","text":"<p>The \"standard\" way of using GEE is via its JavaScript Code Editor in a web browser, which is introduced here and here. In this editor you can simply run one of the many scripts already available (in the Scripts tab in the left pane) or develop your own code for a custom analysis. The results are displayed in the map pane or in the Inspector and Console tabs in the right pane.</p> <p></p> <p>The Google Earth Engine has a web browser based graphical user interface (figure by Google Developers/ CC BY 4.0).</p> <p>To get a quick impression of the data archive and the computational power of GEE, for example, click this link, You will find a script that creates an animated thumbnail of a 1-year time series with MODIS NDVI composites over Australia and Tasmania. In the Code Editor, simply click Run and you should see something like this added to the map pane:</p> <p></p> <p>One year of MODIS NDVI over Australia and Tasmania (Created with Google Earth Engine).</p> <p>A comprehensive online course with well-structured material is provided by Ujaval Gandhi at the Spatial Thoughts website. It is focused on the JavaScript Code Editor but contains also an introduction to the Python API.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#a-selection-of-cool-apps","title":"A selection of cool apps","text":"<p>There is a growing variety of easy to use apps based on the Google Earth Engine and developed by the remote sensing community using the Google App Engine. Some of the apps make requests to the Earth Engine backend using a service account, thus allowing anyone to use the app without logging in or being a registered GEE user. Other apps use client-side authentication, meaning that you need to sign up with your own account to use the app. A few examples are listed in the following:</p> <ul> <li>Landsat time series animations for any rectangular area on Earth can be created quite easily by these two GEE apps. First, the EE-TS-GIF app creates a GIF from a smoothed Landsat time series with a choice of three bands. The output of this can be further enriched by annotating years and adding a context map through using the Snazzy-EE-TS-GIF app.</li> <li>GEE JavaScript module to color time series chart points as stretched 3-band RGB</li> <li>Annual cloud-free Landsat composite time series app</li> <li>Sentinel-2 and Landsat-8 image time series app</li> <li>Snow monitors (created with the method described in Gascoin et al. 2022) for the Alps, the Pyrenees, the Western USA, and the Sierra Nevada (Spain)</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#using-gee-with-python","title":"Using GEE with Python","text":"<p>The Earth Engine Python client is a library called <code>ee</code>. To do the tutorials and excercises of this module we recommend to set up the environment with the <code>m1_etrainee.yml</code> file which contains the <code>ee</code> package and some related ones.</p> <p>Other helpful Python packages for GEE include geemap, eemont and wxee. geemap is a Python package by Wu (2020), developed for interactive mapping with Google Earth Engine (GEE) within a Jupyter-based environment. Using GEE with Python becomes even more convenient with the eemont package by Montero et al. (2021). The eemont package extends the GEE Python API with pre-processing and processing tools for the most used satellite platforms by adding utility methods for different Earth Engine Objects that are friendly with the Python method chaining. It facilitates for instance cloud masking, calculation of spectral indices, extraction of time series by region and other tasks. For time series processing, eemont combines well with the wxee package, which integrates the data catalog and processing power of Google Earth Engine with the flexibility of xarray.</p> <p>An introduction to the GEE Python API is provided in this E-TRAINEE notebook, which illustrates how to query the Sentinel-2 collection for a defined region and time period, mask clouds efficiently (optionally calculate also the NDVI) - all in the cloud - and then get the resulting time series on your local harddrive. This gives you the flexibility to run subsequent analyses with your preferred software (e.g. the scientific Python stack or R). More tutorials using GEE and some Python packages extending this are waiting for you in the next themes. This external example provides a very good introduction to the GEE Python API, working on a time series of MODIS landcover and land surface temperature.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#openeo","title":"OpenEO","text":"<p>Another interesting initiative targeting large remote sensing time series with cloud computing and the data cube concept is openEO. This project develops an open application programming interface (API) that allows you to connect clients for R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. With this openEO API you can run a selection of defined processes on the cloud infrastructure. The processes available are depending on the back-end; they cover mostly data download, handling and basic processing steps (aggregations, resampling, filters, math, pixel operations, NDVI, ...). Currently, openEO features</p> <ul> <li>Three official client libraries for R, Python, and JavaScript</li> <li>A web-based editor - Supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers.</li> <li>A QGIS plugin</li> </ul> <p>See this list for known openEO providers (back-ends) and their services, including the openEO Platform. To dive into the openEO functionality see the documentation and the cookbook therein, or try the full example on retrieving monthly mean Sentinel-1 backscatter data with Python and the Google Earth Engine back-end.</p> <p></p> <p>Concept of the openEO API (figure by Schramm et al. 2021/ CC BY 4.0).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#summary-and-outlook","title":"Summary and outlook","text":"<p>Accessing and handling large remote sensing time series data or selected parts of them can be a major obstacle for anyone starting to use that in his work. Hence, this theme has introduced the most important data sources and tools needed as a prerequisite for any more advanced analysis. For further reading on recent and anticipated future development and trends in the area of big remote sensing data see, e.g., Sudmanns et al. 2022, Xu et al. 2022 and Backeberg et al. 2022.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"Optical satellite data from the Landsat program: Which statements are correct?  To map recent land cover changes in small-structured agricultural systems, Landsat-9 data is ideal, because it can provide vegetation indices at the highest spatial and temporal resolution. The long period of relatively continuous observation across large parts of the Earth is one of the biggest strengths of the Landsat archive. To ensure continuity, all Landsat satellites used exactly the same sensor. Level-1 Landsat data from Landsat Collection 1 is the best choice for any remote sensing time series analysis. The long period of relatively continuous observation across large parts of the Earth is one of the biggest strengths of the Landsat archive.  Which of these statements about data cubes are correct?  Data cubes always have three dimensions. Observations and derivatives of different variables and even from multiple different remote sensing systems can be managed in a well-structured form by a data cube. Data cubes can be deployed on a local machine but most commonly they are hosted on larger (cluster/cloud) infrastructure to serve more users. Observations and derivatives of different variables and even from multiple different remote sensing systems can be managed in a well-structured form by a data cube.&amp;Data cubes can be deployed on a local machine but most commonly they are hosted on larger (cluster/cloud) infrastructure to serve more users.  True or false: Clouds are not a problem for monitoring crop growth or snow cover with Sentinel-2 time series because very good cloud removal algorithms exist, and they can reveal the ground under cloud cover.  True False False"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#excercise","title":"Excercise","text":"<p>Search and load Landsat data to QGIS via a STAC API</p> <p>The goal of this excercise is learn how to search and load Landsat data of selected scenes from large archives of cloud providers to QGIS. Try to accomplish the following tasks using QGIS and the STAC API Plugin:</p> <ol> <li>Install the (QGIS STAC API Plugin) and read the documentation.</li> <li>Connect to a STAC API and search for Landsat scenes of a specific time period (e.g. July 2022) covering an area of your choice (e.g. your university's city). For the different Landsat sensors, how many scenes do you find? (Hint: AWS/EarthSearch and Microsoft Planetary Computer make their STAC APIs available without authentication (as of 03/2023))</li> <li>Select a scene for further use, preferably one with little cloud cover. Load the footprint of the scene to your map canvas (along with a background map, e.g. Open Street Map).</li> <li>Add selected bands (e.g. red, green, blue bands) as layers to the map canvas (Hint: To directly downolad the assets, set download folder under Settings).</li> <li>Create a composite Virtual Raster (VRT) out of the selected bands.</li> <li>Clip VRT to a relatively small area-of-interest (AOI) polygon.</li> <li>Save the clipped VRT as GeoTiff (RGB composite for the AOI in the bands' native resolution).</li> </ol> <p> </p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/02_large_time_series_datasets_in_remote_sensing.html#references","title":"References","text":"<p>Amani, M., Ghorbanian, A., Ahmadi, S. A., Kakooei, M., Moghimi, A., Mirmazloumi, S. M., ... &amp; Brisco, B. (2020). Google earth engine cloud computing platform for remote sensing big data applications: A comprehensive review. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 13, 5326-5350. https://doi.org/10.1109/JSTARS.2020.3021052</p> <p>Backeberg, B., \u0160ustr, Z., Fern\u00e1ndez, E., Donchyts, G., Haag, A., Oonk, J. R., ... &amp; Chatzikyriakou, C. (2022). An open compute and data federation as an alternative to monolithic infrastructures for big Earth data analytics. Big Earth Data, 1-19. https://doi.org/10.1080/20964471.2022.2094953</p> <p>Baumann, P. (1993). Language Support for Raster Image Manipulation in Databases. In: Graphics Modeling and Visualization in Science and Technology (pp. 236-245). https://doi.org/10.1007%2F978-3-642-77811-7_19</p> <p>Baumann, P., Misev, D., Merticariu, V., Huu, B.P. (2019). Datacubes: Towards Space/Time Analysis-Ready Data. In: D\u00f6llner, J., Jobst, M., Schmitz, P. (eds). Service-Oriented Mapping. Lecture Notes in Geoinformation and Cartography. Springer, Cham. https://doi.org/10.1007/978-3-319-72434-8_14</p> <p>Daniel, J. (2019). Data Science with Python and Dask.  ISBN 9781617295607, 296 p., https://www.manning.com/books/data-science-with-python-and-dask</p> <p>Gascoin, S., Monteiro, D., &amp; Morin, S. (2022). Reanalysis-based contextualization of real-time snow cover monitoring from space. Environmental Research Letters, 17(11), 114044. https://doi.org/10.1088/1748-9326/ac9e6a</p> <p>Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., &amp; Moore, R. (2017). Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote sensing of Environment, 202, 18-27. https://doi.org/10.1016/j.rse.2017.06.031</p> <p>Hansen, M. C., Potapov, P. V., Moore, R., Hancher, M., Turubanova, S. A., Tyukavina, A., ... &amp; Townshend, J. (2013). High-resolution global maps of 21st-century forest cover change. Science, 342(6160), 850-853.</p> <p>Horler, D. N. H., DOCKRAY, M., &amp; Barber, J. (1983). The red edge of plant leaf reflectance. International Journal of Remote Sensing, 4(2), 273-288 https://doi.org/10.1080/01431168308948546</p> <p>Hoyer, S., &amp; Hamman, J. (2017). xarray: ND labeled arrays and datasets in Python. Journal of Open Research Software, 5(1). https://doi.org/10.5334/jors.148</p> <p>Krause, C., Dunn, B., Bishop-Taylor, R., Adams, C., Burton, C., Alger, M., Chua, S., Phillips, C., Newey, V., Kouzoubov, K., Leith, A., Ayers, D., Hicks, A., DEA Notebooks contributors (2021). Digital Earth Australia notebooks and tools repository. Geoscience Australia, Canberra. https://doi.org/10.26186/145234</p> <p>Lewis, A., Oliver, S., Lymburner, L., Evans, B., Wyborn, L., Mueller, N., ... &amp; Wang, L. W. (2017). The Australian geoscience data cube\u2014foundations and lessons learned. Remote Sensing of Environment, 202, 276-292. https://doi.org/10.1016/j.rse.2017.03.015</p> <p>Li, J., &amp; Roy, D. P. (2017). A global analysis of Sentinel 2A, Sentinel 2B and Landsat 8 data revisit intervals and implications for terrestrial monitoring. Remote Sensing, 9 (9). https://doi.org/10.3390/rs9090902</p> <p>Mahecha, M. D., Gans, F., Brandt, G., Christiansen, R., Cornell, S. E., Fomferra, N., ... &amp; Reichstein, M. (2020). Earth system data cubes unravel global multivariate dynamics. Earth System Dynamics, 11(1), 201-234. https://doi.org/10.5194/esd-11-201-2020</p> <p>Montero, D., (2021). eemont: A Python package that extends Google Earth Engine. Journal of Open Source Software, 6(62), 3168, https://doi.org/10.21105/joss.03168</p> <p>Pebesma, E.; Bivand, R. (2023). Spatial Data Science: With Applications in R (1st ed.). 314 pages. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016</p> <p>Ranghetti, L., Boschetti, M., Nutini, F., &amp; Busetto, L. (2020). \u201csen2r\u201d: An R toolbox for automatically downloading and preprocessing Sentinel-2 satellite data. Computers &amp; Geosciences, 139, 104473. https://doi.org/10.1016/j.cageo.2020.104473</p> <p>Schramm, M., Pebesma, E., Milenkovi\u0107, M., Foresta, L., Dries, J., Jacob, A., ... &amp; Reiche, J. (2021). The openEO API \u2013 Harmonising the use of Earth Observation cloud services using virtual data cube functionalities. Remote Sensing, 13(6), 1125. https://doi.org/10.3390/rs13061125</p> <p>Simoes, R., Camara, G., Queiroz, G., Souza, F., Andrade, P. R., Santos, L., Carvalho, A., &amp; Ferreira, K. (2021). Satellite Image Time Series Analysis for Big Earth Observation Data. Remote Sensing, 13(13), 2428. https://doi.org/10.3390/rs13132428</p> <p>Storch, T., Reck, C., Holzwarth, S., Wiegers, B., Mandery, N., Raape, U., Strobl, C., Volkmann, R., B\u00f6ttcher, M., Hirner, A., Senft, J., Plesia, N., Kukuk, T., Meissl, S., Felske, J.-R., Heege, T.,Keuck, V., Schmidt, M., &amp; Staudenrausch, H. (2019). Insights into CODE-DE Germany\u2019s Copernicus data and exploitation platform. Big Earth Data, 3(4), 338\u2013361. https://doi.org/10.1080/20964471.2019.1692297</p> <p>Strobl, P., Baumann, P., Lewis, A., Szantoi, Z., Killough, B., Purss, M., Craglia, M., Nativi, S., Held, A., &amp; Dhu, T. (2017). The six faces of the data cube. Proc. of the 2017 Conference on Big Data from Space (BIDS' 2017), 28th - 30th Nov 2017, Toulouse (France). https://doi.org/10.2760/383579</p> <p>Sudmanns, M., Augustin, H., Killough, B., Giuliani, G., Tiede, D., Leith, A., ... &amp; Lewis, A. (2022). Think global, cube local: an Earth Observation Data Cube\u2019s contribution to the Digital Earth vision. Big Earth Data, 1-29. https://doi.org/10.1080/20964471.2022.2099236</p> <p>Tamiminia, H., Salehi, B., Mahdianpari, M., Quackenbush, L., Adeli, S., &amp; Brisco, B. (2020). Google Earth Engine for geo-big data applications: A meta-analysis and systematic review. ISPRS Journal of Photogrammetry and Remote Sensing, 164, 152-170. https://doi.org/10.1016/j.isprsjprs.2020.04.001</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html","title":"PC explore v02","text":"Metadata     title: \"E-TRAINEE Tutorial - Point cloud exploration with Python\"     description: \"This is a tutorial within the second theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-17     authors: Andreas Mayr In\u00a0[1]: Copied! <pre>import laspy\nimport vaex\nimport pandas as pd\nimport pathlib\n</pre> import laspy import vaex import pandas as pd import pathlib In\u00a0[3]: Copied! <pre>data_dir = pathlib.Path('C:/work/etrainee/point_clouds')    # Define path on your local system\ndata_dir\n</pre> data_dir = pathlib.Path('C:/work/etrainee/point_clouds')    # Define path on your local system data_dir Out[3]: <pre>WindowsPath('C:/work/etrainee/point_clouds')</pre> In\u00a0[4]: Copied! <pre>PC_las = data_dir / 'pc_subset_hut.las'\n#PC_las = data_dir / 'ahk_2020_uls.laz'             # use AHK rock glacier point cloud (lacks intensity values)\n\n# read LAS file to Vaex dataframe (with selected attributes, e.g. intensity)\ninFile = laspy.read(PC_las)\nvdf_las = vaex.from_dict({'X': inFile.x, 'Y': inFile.y, 'Z': inFile.z, 'intensity': inFile.intensity})\nprint(f'preview the first rows\\n: {vdf_las.head(5)}')\nprint(f'column names: {vdf_las.column_names}')\n\nprint(f'number of rows: {vdf_las.shape[0]:,}')\nprint(f'number of columns: {vdf_las.shape[1]}')\n</pre> PC_las = data_dir / 'pc_subset_hut.las' #PC_las = data_dir / 'ahk_2020_uls.laz'             # use AHK rock glacier point cloud (lacks intensity values)  # read LAS file to Vaex dataframe (with selected attributes, e.g. intensity) inFile = laspy.read(PC_las) vdf_las = vaex.from_dict({'X': inFile.x, 'Y': inFile.y, 'Z': inFile.z, 'intensity': inFile.intensity}) print(f'preview the first rows\\n: {vdf_las.head(5)}') print(f'column names: {vdf_las.column_names}')  print(f'number of rows: {vdf_las.shape[0]:,}') print(f'number of columns: {vdf_las.shape[1]}') <pre>preview the first rows\n:   #       X            Y        Z    intensity\n  0  714607  5.16844e+06  2333.22        39255\n  1  714610  5.16844e+06  2334.51        38425\n  2  714609  5.16844e+06  2334.6         38971\n  3  714609  5.16844e+06  2334.76        38053\n  4  714609  5.16844e+06  2334.93        39823\ncolumn names: ['X', 'Y', 'Z', 'intensity']\nnumber of rows: 4,839,997\nnumber of columns: 4\n</pre> <p>Export the point cloud to CSV (using <code>pandas_df.to_csv</code> under the hood):</p> In\u00a0[32]: Copied! <pre>%%time\nPC_csv = data_dir / 'pc_subset_hut.csv'\nvdf_las.export_csv(PC_csv)\n</pre> %%time PC_csv = data_dir / 'pc_subset_hut.csv' vdf_las.export_csv(PC_csv) <pre>CPU times: total: 23.9 s\nWall time: 27.1 s\n</pre> <p>Compare this to the speed of using the Apache Arrow backend:</p> In\u00a0[5]: Copied! <pre>%%time\nPC_csv = data_dir / 'pc_subset_hut.csv'\nvdf_las.export_csv_arrow(PC_csv)\n</pre> %%time PC_csv = data_dir / 'pc_subset_hut.csv' vdf_las.export_csv_arrow(PC_csv) <pre>CPU times: total: 3.77 s\nWall time: 4.32 s\n</pre> In\u00a0[6]: Copied! <pre>vdf = vaex.from_csv(PC_csv)    # read textfile (csv) to Vaex dataframe (use sep='\\t' if your data is tab-separated)\nvdf.head(n=5)\n</pre> vdf = vaex.from_csv(PC_csv)    # read textfile (csv) to Vaex dataframe (use sep='\\t' if your data is tab-separated) vdf.head(n=5) Out[6]: #                                 X          Y      Z  intensity 07146075.16844e+062333.22      39255 17146105.16844e+062334.51      38425 27146095.16844e+062334.6       38971 37146095.16844e+062334.76      38053 47146095.16844e+062334.93      39823 In\u00a0[35]: Copied! <pre>vdf = vaex.from_csv(PC_csv, convert=True, chunk_size=5_000_000, progress=True, copy_index=True)    # Does not make much sense with only 4.8M points, try with a larger point cloud.\n</pre> vdf = vaex.from_csv(PC_csv, convert=True, chunk_size=5_000_000, progress=True, copy_index=True)    # Does not make much sense with only 4.8M points, try with a larger point cloud. <p>Here, we also copied the index column of the Pandas DataFrame as a regular column (just to demonstrate the option).</p> In\u00a0[7]: Copied! <pre>vdf.viz.histogram(vdf.Z)\n</pre> vdf.viz.histogram(vdf.Z) Out[7]: <pre>[&lt;matplotlib.lines.Line2D at 0x18a26ecb220&gt;]</pre> <p>... then the intensity values:</p> In\u00a0[8]: Copied! <pre>vdf.viz.histogram(vdf.intensity, limits='99.7%')\n</pre> vdf.viz.histogram(vdf.intensity, limits='99.7%') Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x18a26fe1e10&gt;]</pre> <p>Now we plot 2D maps with aggregating the mean and standard deviation of the Z value (i.e. elevation in meters) per visualization cell, as well as the number of points. Note that we set the <code>aspect</code> parameter to fix the axis aspect ratio (and prevent a spatial distortion). See the Vaex documentation for more options.</p> In\u00a0[9]: Copied! <pre>#vdf.viz.heatmap(vdf.X, vdf.Y, what=vaex.stat.mean(vdf.Z), colorbar=False) # With a single plot there seems to be a problem with colorbar placement.\nvdf.viz.heatmap(vdf.X, vdf.Y, what=[vaex.stat.mean(vdf.Z), vaex.stat.std(vdf.Z), vaex.stat.count(vdf.Z)],\n                title=\"Values aggregated per cell\",\n                aspect=\"equal\",\n                figsize=(10, 5))\n</pre> #vdf.viz.heatmap(vdf.X, vdf.Y, what=vaex.stat.mean(vdf.Z), colorbar=False) # With a single plot there seems to be a problem with colorbar placement. vdf.viz.heatmap(vdf.X, vdf.Y, what=[vaex.stat.mean(vdf.Z), vaex.stat.std(vdf.Z), vaex.stat.count(vdf.Z)],                 title=\"Values aggregated per cell\",                 aspect=\"equal\",                 figsize=(10, 5)) Out[9]: <pre>&lt;matplotlib.image.AxesImage at 0x18a279c3fa0&gt;</pre> In\u00a0[10]: Copied! <pre>import pandas as pd\ndf = pd.read_csv(PC_csv)                    # load point cloud as pandas dataframe\n</pre> import pandas as pd df = pd.read_csv(PC_csv)                    # load point cloud as pandas dataframe <p>Now import the required packages and use Datashader to quickly plot a 2D planimetric view of our point cloud, where the colors represent the 2D point density.</p> In\u00a0[11]: Copied! <pre>import colorcet\nimport datashader as ds, datashader.transfer_functions as tf\n\ncvs = ds.Canvas(plot_width=850)\nagg = cvs.points(df, 'X', 'Y')                  # we plot the point density\ntf.shade(agg, cmap=colorcet.fire, how='log')    # plot with fire colormap and log colorscale\n</pre> import colorcet import datashader as ds, datashader.transfer_functions as tf  cvs = ds.Canvas(plot_width=850) agg = cvs.points(df, 'X', 'Y')                  # we plot the point density tf.shade(agg, cmap=colorcet.fire, how='log')    # plot with fire colormap and log colorscale Out[11]: In\u00a0[12]: Copied! <pre>tf.shade(agg, cmap=colorcet.fire, how='eq_hist')    # plot with fire colormap and histogram equalization\n</pre> tf.shade(agg, cmap=colorcet.fire, how='eq_hist')    # plot with fire colormap and histogram equalization Out[12]: <p>... and we plot the maximum elevation per bin:</p> In\u00a0[13]: Copied! <pre>agg = cvs.points(df, 'X', 'Y', ds.max('Z'))                  # we plot the maximum elevation per bin\ntf.shade(agg, cmap=colorcet.gray)\n</pre> agg = cvs.points(df, 'X', 'Y', ds.max('Z'))                  # we plot the maximum elevation per bin tf.shade(agg, cmap=colorcet.gray) Out[13]: <p>Now we create an interactive version of the map, using Holoviews with the Bokeh plotting backend and again with datashader for fast rendering. We switch to aggregating the standard deviation of z-values per bin.</p> In\u00a0[14]: Copied! <pre>import holoviews.operation.datashader as hd\nimport holoviews as hv\n\nhv.extension('bokeh')\nshaded = hd.datashade(hv.Points(df, ['X', 'Y']), cmap=colorcet.bmy, aggregator=ds.std('Z'))\nhd.dynspread(shaded, threshold=0.5, max_px=4).opts(bgcolor='black', xaxis=None, yaxis=None, width=900, height=500)\n</pre> import holoviews.operation.datashader as hd import holoviews as hv  hv.extension('bokeh') shaded = hd.datashade(hv.Points(df, ['X', 'Y']), cmap=colorcet.bmy, aggregator=ds.std('Z')) hd.dynspread(shaded, threshold=0.5, max_px=4).opts(bgcolor='black', xaxis=None, yaxis=None, width=900, height=500) Out[14]: In\u00a0[15]: Copied! <pre>import hvplot.pandas\n# Make colors represent point density\ndf.hvplot.scatter(x='X', y='Y', rasterize=True, aspect=\"equal\", logz=True, title='Point count')\n</pre> import hvplot.pandas # Make colors represent point density df.hvplot.scatter(x='X', y='Y', rasterize=True, aspect=\"equal\", logz=True, title='Point count') Out[15]: In\u00a0[17]: Copied! <pre>df.hvplot.scatter(x='X', y='Y', rasterize=True, aggregator=ds.std('Z'), aspect=\"equal\", title='Elevation standard deviation [m]')\n</pre> df.hvplot.scatter(x='X', y='Y', rasterize=True, aggregator=ds.std('Z'), aspect=\"equal\", title='Elevation standard deviation [m]') Out[17]: <p>Now we want to use the Matplotlib plotting backend (instead of the default Bokeh backend) and show map along with a histogram.</p> In\u00a0[18]: Copied! <pre>hvplot.extension('matplotlib')\n\n(df.hvplot.scatter(x='X', y='Y', rasterize=True, aggregator=ds.mean('Z'), aspect=\"equal\", title='Elevation mean [m]') +\ndf.hvplot.hist('Z'))\n</pre> hvplot.extension('matplotlib')  (df.hvplot.scatter(x='X', y='Y', rasterize=True, aggregator=ds.mean('Z'), aspect=\"equal\", title='Elevation mean [m]') + df.hvplot.hist('Z')) Out[18]: <p>Maybe try also GeoPandas and spatialpandas!</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#point-cloud-exploration-with-python","title":"Point cloud exploration with Python\u00b6","text":"<p>A Notebook to explore a 3D point cloud with various state-of-the-art Python packages specialized on large datasets. Different options for loading and visualizing point clouds quickly in a 2D map view are shown.</p> <p>The packages needed for this tutorial are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p> <p>Data:</p> <p>Use the <code>pc_subset_hut.las</code> point cloud or try with your own one. The <code>pc_subset_hut.las</code> is a small subset of a point cloud acquired by unmanned aerial vehicle laser scanning (details in Mayr et al. 2020) and covers a small area in the Dolomites (IT), including the buildings of a mountain hut.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#handling-point-clouds-with-vaex","title":"Handling point clouds with Vaex\u00b6","text":"<p>Vaex is a Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. Let's see how this can be used with point clouds.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#reading-files","title":"Reading files\u00b6","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#read-las-file-to-vaex-dataframe","title":"Read LAS file to Vaex dataframe\u00b6","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#read-csv-file-to-vaex-dataframe","title":"Read CSV file to Vaex dataframe\u00b6","text":"<p>The <code>vaex.open</code> method reads a CSV file lazily, i.e. the data is streamed when needed for computations. This is useful when your data is larger the available memory. In this case, however, we use <code>vaex.from_csv</code> to read into memory.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#memory-efficient-out-of-core-processing","title":"Memory efficient out-of-core processing\u00b6","text":"<p>By specifying the <code>convert=True</code> argument, large CSVs (larger than memory) can be read in chunks, converted to the memory-mappable HDF5 format and written to disk (first as one temporary HDF5 file per chunk, then concatenated into one HDF5, hence requiring twice the disk space of the final HDF5 file). This frees memory, and allows you to efficiently work out-of-core.</p> <p>The syntax to do this is quite simple even for multiple files (e.g. a tiled point cloud; see the documentation here), and very straightforward for a single CSV file:</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#visualization-with-vaex","title":"Visualization with Vaex\u00b6","text":"<p>Let's plot histograms directly from the Vaex dataframe, first the elevation ...</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#more-vaex-tools","title":"More Vaex tools\u00b6","text":"<p>Vaex offers other tools you may find useful for handling 3D point clouds, such as:</p> <ul> <li>Geo operations - For \"clipping-like\" geometric operations, e.g. <code>inside_polygons()</code>?</li> <li>Fast group-by operations</li> <li>Arrow support - Make use of the Arrow memory format capable of zero-copy reads for fast data access, visualizing quick overviews, shallow copies, and creating new virtual columns.</li> <li>Machine learning - <code>vaex.ml</code> offers ML functionality, including a fast and scalable K-means clustering implementation and many other tools.</li> <li>Propagation of uncertainties</li> <li>For more point cloud plotting examples see: https://examples.pyviz.org/ and https://examples.pyviz.org/seattle_lidar/Seattle_Lidar.html#seattle-lidar-gallery-seattle-lidar</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#datashader","title":"Datashader\u00b6","text":"<p>Let's try plotting from Pandas DataFrames with Datashader, a toolset for rendering graphics of large datasets quickly and conveniently.</p> <p>First we load a point cloud in csv format as Pandas DataFrame.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/PC_explore_v02.html#hvplot","title":"hvPlot\u00b6","text":"<p>The next plotting option we try is hvPlot, which provides a high-level plotting API built on HoloViews. By specifying <code>rasterize</code>, we get a colorbar and the possibility to fix the axes aspect ratio (preventing spatial distortion). This uses datashader to aggregate the data into a viewable image, but to get a logarithmic colorscale we must use hvPlot's option <code>logz()</code> (using datashader directly it would be how='log'). See the user guide for more customization options.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html","title":"T2 GEE s2cloudless v03 export time series","text":"Metadata     title: \"Introduction to the Google Earth Engine Python API: Sentinel-2 cloud masking, interactive mapping and download\"     description: \"This is a tutorial within the second theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-14     author: Andreas Mayr In\u00a0[\u00a0]: Copied! <pre>import ee\nimport geemap\nimport folium\n</pre> import ee import geemap import folium <p>Before we can start using the GEE API, we must authenticate with our credentials and initialize the API. Run the cell and follow the instructions on how to grant the notebook access with your account.</p> In\u00a0[\u00a0]: Copied! <pre>try:                            # If you have authenticated recently, you might be able to initialize directly\n        ee.Initialize()\nexcept Exception as e:          # If initialize does not work, you probably have to authenticate first\n        ee.Authenticate()\n        ee.Initialize()\n</pre> try:                            # If you have authenticated recently, you might be able to initialize directly         ee.Initialize() except Exception as e:          # If initialize does not work, you probably have to authenticate first         ee.Authenticate()         ee.Initialize() <p>To authorize access needed by Earth Engine, open the following         URL in a web browser and follow the instructions:</p> <p>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&amp;request_id=o9ZzBVzyJR13EyQBEu2VNNBzTL7LNG6QEJXugm294Pk&amp;tc=e4sKaJOMDInEIkerBOKmGZ8UPFZHHWNEcS3OQnv4Bvg&amp;cc=XWPr4hodeP6fTK__uzv8kb9ulc1E1CrKM3tEx0NqIFY</p> <p>The authorization workflow will generate a code, which you should paste in the box below.</p> <pre>\nSuccessfully saved authorization token.\n</pre> <p>We want to query the Sentinel-2 Multi-Spectral instrument (MSI) image collection with Level-2A (surface reflectance) products and apply filters to the collection. First, we define a geometry and some other parameters that will be used for filtering:</p> In\u00a0[\u00a0]: Copied! <pre>my_point = ee.Geometry.Point(11.0, 46.8)    # Point of interest with latitude and longitude\nAOI = my_point                              # Area of interest (map display will be centered on this, here we simply use a point)\nSTART_DATE = '2021-06-09'                   # Start date of our time period of interest\nEND_DATE = '2021-07-11'                     # End date of our time period of interest\nCLOUD_FILTER = 60                           # Maximum image cloud cover percent allowed in image collection\nCLD_PRB_THRESH = 40                         # Cloud probability (%); values greater than are considered cloud\nNIR_DRK_THRESH = 0.15                       # NIR reflectance; values less than this are considered potential cloud shadow\nCLD_PRJ_DIST = 1.5                          # Maximum distance (km) to search for cloud shadows from cloud edges\nBUFFER = 60                                 # Distance [m] to dilate the edge of cloud-identified objects\n</pre> my_point = ee.Geometry.Point(11.0, 46.8)    # Point of interest with latitude and longitude AOI = my_point                              # Area of interest (map display will be centered on this, here we simply use a point) START_DATE = '2021-06-09'                   # Start date of our time period of interest END_DATE = '2021-07-11'                     # End date of our time period of interest CLOUD_FILTER = 60                           # Maximum image cloud cover percent allowed in image collection CLD_PRB_THRESH = 40                         # Cloud probability (%); values greater than are considered cloud NIR_DRK_THRESH = 0.15                       # NIR reflectance; values less than this are considered potential cloud shadow CLD_PRJ_DIST = 1.5                          # Maximum distance (km) to search for cloud shadows from cloud edges BUFFER = 60                                 # Distance [m] to dilate the edge of cloud-identified objects <p>You may fine tune the cloud-mask related thresholds to get the best results for your needs. The strictness of cloud masking is essentially a trade-off between data completeness (little spatial and temporal gaps) and reliability of land surface observations. It will depend on your further processing steps (such as compositing) and how any remaining cloudy pixels can compromise the reliability of intermediate products and final results.</p> <p>We will use two different ImageCollections:</p> <ul> <li>The Sentinel-2 Level-2A collection provides the surface reflectances produced by the sen2cor processor and downloaded from the Copernicus Open Access Hub.</li> <li>The Sentinel 2 Cloud Probability collection is created using a machine learning approach to cloud detection, which is detailed here and here. Alternatively, the QA60 cloud mask or the SCL scene classification map provided by ESA with the Sentinel-2 L2A product could be used. See this blog post for a comparative discussion of the cloud masks.</li> </ul> <p>These two collections are filtered by bounds and date and then joined into a single collection.</p> In\u00a0[5]: Copied! <pre>def get_s2_sr_cld_col(aoi, start_date, end_date):\n    # Import and filter S2 surface reflectance\n    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n        .filterBounds(aoi)\n        .filterDate(start_date, end_date)\n        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n\n    # Import and filter cloud probabilities produced by s2cloudless\n    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n        .filterBounds(aoi)\n        .filterDate(start_date, end_date))\n\n    # Join the filtered cloud probabilities to the surface reflectances by the 'system:index' property\n    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n        'primary': s2_sr_col,\n        'secondary': s2_cloudless_col,\n        'condition': ee.Filter.equals(**{\n            'leftField': 'system:index',\n            'rightField': 'system:index'\n        })\n    }))\n\n# Apply the function defined above to build a collection containing both S2 SR and cloud probabilities \nmy_S2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n</pre> def get_s2_sr_cld_col(aoi, start_date, end_date):     # Import and filter S2 surface reflectance     s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')         .filterBounds(aoi)         .filterDate(start_date, end_date)         .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))      # Import and filter cloud probabilities produced by s2cloudless     s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')         .filterBounds(aoi)         .filterDate(start_date, end_date))      # Join the filtered cloud probabilities to the surface reflectances by the 'system:index' property     return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{         'primary': s2_sr_col,         'secondary': s2_cloudless_col,         'condition': ee.Filter.equals(**{             'leftField': 'system:index',             'rightField': 'system:index'         })     }))  # Apply the function defined above to build a collection containing both S2 SR and cloud probabilities  my_S2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)  In\u00a0[6]: Copied! <pre>def add_cloud_bands(img):\n    # Get s2cloudless image, subset the probability band.\n    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n\n    # Condition s2cloudless by the probability threshold value (defined at the beginning).\n    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n\n    # Add the cloud probability layer and cloud mask as image bands.\n    return img.addBands(ee.Image([cld_prb, is_cloud]))\n</pre> def add_cloud_bands(img):     # Get s2cloudless image, subset the probability band.     cld_prb = ee.Image(img.get('s2cloudless')).select('probability')      # Condition s2cloudless by the probability threshold value (defined at the beginning).     is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')      # Add the cloud probability layer and cloud mask as image bands.     return img.addBands(ee.Image([cld_prb, is_cloud]))  In\u00a0[7]: Copied! <pre>def add_shadow_bands(img):\n    # Exclude water pixels using the scene classification (SCL) band.\n    not_water = img.select('SCL').neq(6)\n\n    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n    SR_BAND_SCALE = 1e4\n    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n\n    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n\n    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n        .select('distance')\n        .mask()\n        .rename('cloud_transform'))\n\n    # Identify the intersection of dark pixels with cloud shadow projection.\n    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n\n    # Add dark pixels, cloud projection, and identified shadows as image bands.\n    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n</pre> def add_shadow_bands(img):     # Exclude water pixels using the scene classification (SCL) band.     not_water = img.select('SCL').neq(6)      # Identify dark NIR pixels that are not water (potential cloud shadow pixels).     SR_BAND_SCALE = 1e4     dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')      # Determine the direction to project cloud shadow from clouds (assumes UTM projection).     shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));      # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.     cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)         .reproject(**{'crs': img.select(0).projection(), 'scale': 100})         .select('distance')         .mask()         .rename('cloud_transform'))      # Identify the intersection of dark pixels with cloud shadow projection.     shadows = cld_proj.multiply(dark_pixels).rename('shadows')      # Add dark pixels, cloud projection, and identified shadows as image bands.     return img.addBands(ee.Image([dark_pixels, cld_proj, shadows])) In\u00a0[8]: Copied! <pre>def add_cld_shdw_mask(img):\n    # Add cloud component bands.\n    img_cloud = add_cloud_bands(img)\n\n    # Add cloud shadow component bands.\n    img_cloud_shadow = add_shadow_bands(img_cloud)\n\n    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n\n    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n        .rename('cloudmask'))\n\n    # Add the final cloud-shadow mask to the image.\n    return img_cloud_shadow.addBands(is_cld_shdw)\n    # or (to include only the final cloud/cloud shadow mask along with the original image bands)\n    #return img.addBands(is_cld_shdw)\n</pre> def add_cld_shdw_mask(img):     # Add cloud component bands.     img_cloud = add_cloud_bands(img)      # Add cloud shadow component bands.     img_cloud_shadow = add_shadow_bands(img_cloud)      # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.     is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)      # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.     # 20 m scale is for speed, and assumes clouds don't require 10 m precision.     is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)         .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})         .rename('cloudmask'))      # Add the final cloud-shadow mask to the image.     return img_cloud_shadow.addBands(is_cld_shdw)     # or (to include only the final cloud/cloud shadow mask along with the original image bands)     #return img.addBands(is_cld_shdw) In\u00a0[9]: Copied! <pre># Define a method for displaying Earth Engine image tiles to a folium map.\ndef add_ee_layer(self, ee_image_object, vis_params, name, show=True, opacity=1, min_zoom=0):\n    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n    folium.raster_layers.TileLayer(\n        tiles=map_id_dict['tile_fetcher'].url_format,\n        attr='Map Data &amp;copy; &lt;a href=\"https://earthengine.google.com/\"&gt;Google Earth Engine&lt;/a&gt;',\n        name=name,\n        show=show,\n        opacity=opacity,\n        min_zoom=min_zoom,\n        overlay=True,\n        control=True\n        ).add_to(self)\n\n# Add the Earth Engine layer method to folium.\nfolium.Map.add_ee_layer = add_ee_layer\n</pre> # Define a method for displaying Earth Engine image tiles to a folium map. def add_ee_layer(self, ee_image_object, vis_params, name, show=True, opacity=1, min_zoom=0):     map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)     folium.raster_layers.TileLayer(         tiles=map_id_dict['tile_fetcher'].url_format,         attr='Map Data \u00a9 Google Earth Engine',         name=name,         show=show,         opacity=opacity,         min_zoom=min_zoom,         overlay=True,         control=True         ).add_to(self)  # Add the Earth Engine layer method to folium. folium.Map.add_ee_layer = add_ee_layer <p>Define a function to display all of the cloud and cloud shadow components to an interactive Folium map. The input is an image collection where each image is the result of the add_cld_shdw_mask function defined previously.</p> <p>To facilitate display in a single map, we create a mosaic of the collection. ee.ImageCollection.mosaic composites images according to their position in the collection (priority is last to first) and pixel mask status, where invalid (mask value 0) pixels are filled by preceding valid (mask value &gt;0) [pixels.</p> In\u00a0[10]: Copied! <pre>def display_cloud_layers(col):\n    # Mosaic the image collection.\n    img = col.mosaic()\n\n    # Subset layers and prepare them for display.\n    clouds = img.select('clouds').selfMask()\n    shadows = img.select('shadows').selfMask()\n    dark_pixels = img.select('dark_pixels').selfMask()\n    probability = img.select('probability')\n    cloudmask = img.select('cloudmask').selfMask()\n    cloud_transform = img.select('cloud_transform')\n\n    # Create a folium map object.\n    center = AOI.centroid(10).coordinates().reverse().getInfo()\n    m = folium.Map(location=center, zoom_start=12)\n\n    # Add layers to the folium map.\n    m.add_ee_layer(img,\n                   {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 2500, 'gamma': 1.1},\n                   'S2 image', True, 1, 9)\n    m.add_ee_layer(probability,\n                   {'min': 0, 'max': 100},\n                   'probability (cloud)', False, 1, 9)\n    m.add_ee_layer(clouds,\n                   {'palette': 'e056fd'},\n                   'clouds', False, 1, 9)\n    m.add_ee_layer(cloud_transform,\n                   {'min': 0, 'max': 1, 'palette': ['white', 'black']},\n                   'cloud_transform', False, 1, 9)\n    m.add_ee_layer(dark_pixels,\n                   {'palette': 'orange'},\n                   'dark_pixels', False, 1, 9)\n    m.add_ee_layer(shadows, {'palette': 'yellow'},\n                   'shadows', False, 1, 9)\n    m.add_ee_layer(cloudmask, {'palette': 'orange'},\n                   'cloudmask', True, 0.5, 9)\n\n    # Add a layer control panel to the map.\n    m.add_child(folium.LayerControl())\n\n    # Display the map.\n    display(m)\n\n    # Export the map as HTML file (to be viewed in a web browser), this is optional.\n    outfp = \"output/cloud_mask_map.html\"\n    m.save(outfp)\n</pre> def display_cloud_layers(col):     # Mosaic the image collection.     img = col.mosaic()      # Subset layers and prepare them for display.     clouds = img.select('clouds').selfMask()     shadows = img.select('shadows').selfMask()     dark_pixels = img.select('dark_pixels').selfMask()     probability = img.select('probability')     cloudmask = img.select('cloudmask').selfMask()     cloud_transform = img.select('cloud_transform')      # Create a folium map object.     center = AOI.centroid(10).coordinates().reverse().getInfo()     m = folium.Map(location=center, zoom_start=12)      # Add layers to the folium map.     m.add_ee_layer(img,                    {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 2500, 'gamma': 1.1},                    'S2 image', True, 1, 9)     m.add_ee_layer(probability,                    {'min': 0, 'max': 100},                    'probability (cloud)', False, 1, 9)     m.add_ee_layer(clouds,                    {'palette': 'e056fd'},                    'clouds', False, 1, 9)     m.add_ee_layer(cloud_transform,                    {'min': 0, 'max': 1, 'palette': ['white', 'black']},                    'cloud_transform', False, 1, 9)     m.add_ee_layer(dark_pixels,                    {'palette': 'orange'},                    'dark_pixels', False, 1, 9)     m.add_ee_layer(shadows, {'palette': 'yellow'},                    'shadows', False, 1, 9)     m.add_ee_layer(cloudmask, {'palette': 'orange'},                    'cloudmask', True, 0.5, 9)      # Add a layer control panel to the map.     m.add_child(folium.LayerControl())      # Display the map.     display(m)      # Export the map as HTML file (to be viewed in a web browser), this is optional.     outfp = \"output/cloud_mask_map.html\"     m.save(outfp) In\u00a0[11]: Copied! <pre>my_S2_sr_cld_col_disp = my_S2_sr_cld_col.map(add_cld_shdw_mask)\ndisplay_cloud_layers(my_S2_sr_cld_col_disp)\n</pre> my_S2_sr_cld_col_disp = my_S2_sr_cld_col.map(add_cld_shdw_mask) display_cloud_layers(my_S2_sr_cld_col_disp) Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[12]: Copied! <pre># Define a function to apply the cloud mask to each image in the collection\ndef apply_cld_shdw_mask(img):\n    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n    not_cld_shdw = img.select('cloudmask').Not()\n\n    # Subset reflectance bands and update their masks, return the result.\n    return img.select('B.*').updateMask(not_cld_shdw)\n\n# Add the mask components to each image and apply the function to process the collection\nS2_sr_cloudless = my_S2_sr_cld_col.map(add_cld_shdw_mask).map(apply_cld_shdw_mask)\n</pre> # Define a function to apply the cloud mask to each image in the collection def apply_cld_shdw_mask(img):     # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.     not_cld_shdw = img.select('cloudmask').Not()      # Subset reflectance bands and update their masks, return the result.     return img.select('B.*').updateMask(not_cld_shdw)  # Add the mask components to each image and apply the function to process the collection S2_sr_cloudless = my_S2_sr_cld_col.map(add_cld_shdw_mask).map(apply_cld_shdw_mask) In\u00a0[13]: Copied! <pre>s2_sr_median = S2_sr_cloudless.median()\ns2_sr_mosaic = S2_sr_cloudless.mosaic()\ns2_sr_count = S2_sr_cloudless.count()\n</pre> s2_sr_median = S2_sr_cloudless.median() s2_sr_mosaic = S2_sr_cloudless.mosaic() s2_sr_count = S2_sr_cloudless.count() In\u00a0[14]: Copied! <pre># Create a folium map object.\ncenter = AOI.centroid(10).coordinates().reverse().getInfo()\nm = folium.Map(location=center, zoom_start=12)\n\n# Add layers to the folium map.\nm.add_ee_layer(s2_sr_count,\n                {'bands': ['B4'], 'min': 0, 'max': 20},\n                'S2 cloud-free count', True, 1, 9)\nm.add_ee_layer(s2_sr_median,\n                {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 2500, 'gamma': 1.1},\n                'S2 cloud-free median', True, 1, 9)\nm.add_ee_layer(s2_sr_mosaic,\n                {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 2500, 'gamma': 1.1},\n                'S2 cloud-free mosaic', True, 1, 9)\n\n# Add a layer control panel to the map.\nm.add_child(folium.LayerControl())\n\n# Display the map.\ndisplay(m)\n</pre> # Create a folium map object. center = AOI.centroid(10).coordinates().reverse().getInfo() m = folium.Map(location=center, zoom_start=12)  # Add layers to the folium map. m.add_ee_layer(s2_sr_count,                 {'bands': ['B4'], 'min': 0, 'max': 20},                 'S2 cloud-free count', True, 1, 9) m.add_ee_layer(s2_sr_median,                 {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 2500, 'gamma': 1.1},                 'S2 cloud-free median', True, 1, 9) m.add_ee_layer(s2_sr_mosaic,                 {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 2500, 'gamma': 1.1},                 'S2 cloud-free mosaic', True, 1, 9)  # Add a layer control panel to the map. m.add_child(folium.LayerControl())  # Display the map. display(m)  Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[16]: Copied! <pre># Define a region-of-interest (ROI) from a list of GeoJSON 'Polygon' formatted coordinates (Lon-Lat pairs).\ngeom = ee.Geometry.Polygon(\n        [ \n            [ \n                [10.990786901656424, 46.845543799137999],\n                [10.990786901656424, 46.878736564096471],\n                [11.055145371135509, 46.878736564096471],\n                [11.055145371135509, 46.845543799137999],\n                [10.990786901656424, 46.845543799137999]    # matching the first vertex is optional\n            ]\n        ]\n    )\nfeature = ee.Feature(geom, {})\nmy_roi = feature.geometry()\n</pre> # Define a region-of-interest (ROI) from a list of GeoJSON 'Polygon' formatted coordinates (Lon-Lat pairs). geom = ee.Geometry.Polygon(         [              [                  [10.990786901656424, 46.845543799137999],                 [10.990786901656424, 46.878736564096471],                 [11.055145371135509, 46.878736564096471],                 [11.055145371135509, 46.845543799137999],                 [10.990786901656424, 46.845543799137999]    # matching the first vertex is optional             ]         ]     ) feature = ee.Feature(geom, {}) my_roi = feature.geometry() <p>Now we define an output path and actually export ... Using either geemap or this solution. Note that we export all 12 bands into a single GeoTIFF file.</p> In\u00a0[17]: Copied! <pre>ee_object = s2_sr_median\nfile_name = \"C:\\work\\etrainee\\gee\\s2_sr_median.tif\"\ngeemap.ee_export_image(ee_object, file_name, scale=10, region=my_roi, file_per_band=False)  # Scale sets the resolution in meters per pixel. Defaults to 1000.\n</pre> ee_object = s2_sr_median file_name = \"C:\\work\\etrainee\\gee\\s2_sr_median.tif\" geemap.ee_export_image(ee_object, file_name, scale=10, region=my_roi, file_per_band=False)  # Scale sets the resolution in meters per pixel. Defaults to 1000. <pre>Generating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/07d9e7b5b1e229f0676eb0d1e318379c-d62a70e682fd862abdb9429e5d318c86:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\s2_sr_median.tif\n</pre> In\u00a0[18]: Copied! <pre>ee_object = S2_sr_cloudless\nout_dir = \"C:\\work\\etrainee\\gee\"\ngeemap.ee_export_image_collection(ee_object, out_dir, scale=10, region=my_roi, file_per_band=False)\nprint(\"Finished!\")\n</pre> ee_object = S2_sr_cloudless out_dir = \"C:\\work\\etrainee\\gee\" geemap.ee_export_image_collection(ee_object, out_dir, scale=10, region=my_roi, file_per_band=False) print(\"Finished!\") <pre>Total number of images: 9\n\nExporting 1/9: 20210610T101559_20210610T102220_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/da77d9b40ecfc8d490a5bf035fdaef95-68c21680a834d24ced92e9e4f2437579:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210610T101559_20210610T102220_T32TPS.tif\n\n\nExporting 2/9: 20210612T101031_20210612T101329_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/f5870ed4cb317edc28dfc05d86b2ff60-92df9db8a30f72880986d1b3a66a9201:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210612T101031_20210612T101329_T32TPS.tif\n\n\nExporting 3/9: 20210615T102021_20210615T102602_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/a8cfa850d5348b5c032a0b6639a6baf2-e5550c87efc288ec5b909d16ac07042b:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210615T102021_20210615T102602_T32TPS.tif\n\n\nExporting 4/9: 20210617T100559_20210617T101301_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/6332cb72f442ccc10346b5b25597cac0-c2b8f17ffe1e982083c4ee91d57f2786:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210617T100559_20210617T101301_T32TPS.tif\n\n\nExporting 5/9: 20210622T101031_20210622T101416_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/d533affbd9a2a64295e827d4f73f2a81-cb4ce0734f4bed78b1ad8bc00b2a4d1a:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210622T101031_20210622T101416_T32TPS.tif\n\n\nExporting 6/9: 20210625T102021_20210625T102545_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/92c0cb84a4fd5930c9e620d04816f5e0-71b8a6e296513974d7036af38c123baa:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210625T102021_20210625T102545_T32TPS.tif\n\n\nExporting 7/9: 20210627T100559_20210627T101702_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/5001a95b748c21799a5e2d13f2cd6463-b844f2ea6f00a21b02f297b888b96fec:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210627T100559_20210627T101702_T32TPS.tif\n\n\nExporting 8/9: 20210702T101031_20210702T101157_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/c9c0c9560dc073cd9a3545329298b2b1-f3d9aa4f17a1de9028f931aa8e731193:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210702T101031_20210702T101157_T32TPS.tif\n\n\nExporting 9/9: 20210710T101559_20210710T102312_T32TPS.tif\nGenerating URL ...\nDownloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/4ad22c03193120c71b4f7f461dde986d-9539fa20c7cdf2d8d39c10e89bfda327:getPixels\nPlease wait ...\nData downloaded to C:\\work\\etrainee\\gee\\20210710T101559_20210710T102312_T32TPS.tif\n\n\nFinished!\n</pre> In\u00a0[19]: Copied! <pre>S2_ndvi = S2_sr_cloudless.map(lambda image: image.normalizedDifference(['B8', 'B4']).rename(['ndvi']))\nprint(S2_ndvi.__sizeof__())\nee_object = S2_ndvi\nout_dir = \"C:\\work\\etrainee\\gee\"\ngeemap.ee_export_image_collection(ee_object, out_dir, scale=10, region=my_roi, file_per_band=False)\nprint(\"Finished!\")\n</pre> S2_ndvi = S2_sr_cloudless.map(lambda image: image.normalizedDifference(['B8', 'B4']).rename(['ndvi'])) print(S2_ndvi.__sizeof__()) ee_object = S2_ndvi out_dir = \"C:\\work\\etrainee\\gee\" geemap.ee_export_image_collection(ee_object, out_dir, scale=10, region=my_roi, file_per_band=False) print(\"Finished!\") <pre>32\nFinished!\n</pre>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#introduction-to-the-google-earth-engine-python-api-sentinel-2-cloud-masking-interactive-mapping-and-download","title":"Introduction to the Google Earth Engine Python API: Sentinel-2 cloud masking, interactive mapping and download\u00b6","text":"<p>This Notebook performs a procedure for cloud masking in Sentinel-2 imagery, which is an important basis for further time series analysis. It is shown how to (i) access Google Earth Engine (GEE) image collections via the GEE Python API, (ii) process the imagery in the cloud, (iii) visualize the results in interactive maps, and (iv) download data from GEE. More specifically, you will learn how to achieve the following steps:</p> <ul> <li>Query and filter GEE image collections of Sentinel-2 imagery and associated cloud probabilities</li> <li>Join the two different, filtered image collections to build a new collection</li> <li>Derive clouds and cloud shadows as components of a cloud mask</li> <li>Dilate the edge of cloud mask objects</li> <li>Visualize the imagery, the final cloud mask and its components in an interactive Leaflet map using the Folium package</li> <li>Download cloud masked data produced in the Earth Engine to a local hard drive (using the geemap package)</li> <li>Apply a function (e.g. for calculating a vegetation index) to an image collection</li> </ul> <p>Much of the Notebook is based on Sentinel-2 Cloud Masking with s2cloudless by Justin Braaten, licensed under the CC BY 4.0 License and the Apache 2.0 License (code samples).</p> <p>Requirements:</p> <ul> <li>You will need a Google account with GEE activated (sign up here, if you do not already have one).</li> <li>The Python packages used in this tutorial are listed in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</li> </ul>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#authenticate-and-initialize-api","title":"Authenticate and initialize API\u00b6","text":"<p>First, we load the required packages: Google Earth Engine Python API (which is called <code>ee</code>), <code>geemap</code> and <code>folium</code>.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#query-and-filter-image-collections","title":"Query and filter image collections\u00b6","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#define-functions-for-cloud-mask-components","title":"Define functions for cloud mask components\u00b6","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#cloud-components","title":"Cloud components\u00b6","text":"<p>Define a function to add the s2cloudless probability layer and derived cloud mask as bands to an S2 SR image input.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#cloud-shadow-components","title":"Cloud shadow components\u00b6","text":"<p>Define a function to add dark pixels, cloud projection, and identified shadows as bands to an S2 SR image input. Note that the image input needs to be the result of the above add_cloud_bands function because it relies on knowing which pixels are considered cloudy ('clouds' band).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#final-cloud-shadow-mask","title":"Final cloud-shadow mask\u00b6","text":"<p>Define a function to assemble all of the cloud and cloud shadow components and produce the final mask.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#visualize-and-evaluate-cloud-mask-components","title":"Visualize and evaluate cloud mask components\u00b6","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#define-functions-to-display-image-and-mask-component-layers","title":"Define functions to display image and mask component layers.\u00b6","text":"<p>Folium will be used to display map layers. Import folium and define a method to display Earth Engine image tiles.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#display-mask-component-layers","title":"Display mask component layers\u00b6","text":"<p>Map the add_cld_shdw_mask function over the collection to add mask component bands to each image, then display the results.</p> <p>Give the system some time to render everything, it should take less than a minute.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#apply-cloud-mask","title":"Apply cloud mask\u00b6","text":"<p>We use these cloud mask components to set the cloud-affected pixels to NoData.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#build-a-cloud-free-composite","title":"Build a cloud-free composite\u00b6","text":"<p>Reduce the collection by median (in your application, you might consider using medoid reduction to build a composite from actual data values, instead of per-band statistics). For comparison, we also create a mosaic (filled with most recent pixel values, see above). Moreover, we want to know how many cloud-free observations are available for each pixel and, thus, use the <code>count()</code> funtion.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#display-the-cloud-free-composites","title":"Display the cloud-free composites\u00b6","text":"<p>Display the results. Be patient while the map renders, it may take a minute; <code>ee.Image.reproject()</code> is forcing computations to happen at 100 and 20 m scales (i.e. it is not relying on appropriate pyramid level scales for analysis). The issues with <code>ee.Image.reproject()</code> being resource-intensive in this case are mostly confined to interactive map viewing. Batch image exports and table reduction exports where the scale parameter is set to typical Sentinel-2 scales (10-60 m) are less affected. For the count layer's colour scale a simple legend would be good but we skip this for simplicity here (this tutorial describes adding legends with the geemap package).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#export-images-to-local-machine","title":"Export images to local machine\u00b6","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#export-the-composite-image","title":"Export the composite image\u00b6","text":"<p>First, we will export the composite (i.e. a single image) for our study area to a folder on our local hard drive. Here, we make use of geemap, a Python package described in Wu (2020) The geemap tools facilitate interactive mapping but also some other tasks with Google Earth Engine greatly.</p> <p>We define a region for the export as a polygon. Clipping seems not to be necessary then. Nevertheless, see here for clipping image collections, or here for how to use filterBounds.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#export-the-cloud-masked-image-collection","title":"Export the cloud-masked image collection\u00b6","text":"<p>We use geemap because this contains a nice function to export data directly to a local hard drive.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_GEE_s2cloudless_v03_export_time_series.html#calculate-the-ndvi-from-the-cloud-masked-image-collection-and-export","title":"Calculate the NDVI from the cloud-masked image collection and export\u00b6","text":"<p>From the collection of cloudmasked S2 images we calculate the NDVI. In GEE there is a helper function <code>normalizedDifference()</code> for such purposes and a more flexible <code>expression()</code> function. We use <code>lambda</code> to define a function and then map it over the collection. Using <code>map()</code> over an image collection is similar to a for-loop but it can make the computations running in parallel. In addition, we print the number of scenes in this collection.</p> <p>Finally, we will export the cloud-free NDVI image collection for the ROI to a directory on our local hard drive. Again, we make use of geemap.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html","title":"T2 Sentinel STAC v03","text":"Metadata     title: \"E-TRAINEE Tutorial - Sentinel-2 data STAC access and time series processing\"     description: \"This is a tutorial within the second theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-13     authors: Andreas Mayr In\u00a0[3]: Copied! <pre>import pystac_client\nimport stackstac\nimport pyproj\nimport xarray\nimport rioxarray\n</pre> import pystac_client import stackstac import pyproj import xarray import rioxarray <p>Define a point of interest to intersect our query.</p> In\u00a0[4]: Copied! <pre>lon, lat = 11.39, 47.26     # Innsbruck (Austria)\n</pre> lon, lat = 11.39, 47.26     # Innsbruck (Austria) <p>Use the pystac_client package and element84\u2019s search endpoint to look for items from the <code>sentinel-s2-l2a-cogs</code> collection on Amazon Web Service (AWS). As search criteria we use a maximum cloud cover (per scene) and a time period.</p> In\u00a0[5]: Copied! <pre>URL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\n</pre> URL = \"https://earth-search.aws.element84.com/v1\" catalog = pystac_client.Client.open(URL) In\u00a0[7]: Copied! <pre>%%time                                  \nitems = catalog.search(\n    intersects=dict(type=\"Point\", coordinates=[lon, lat]),\n    collections=[\"sentinel-2-l2a\"],\n    query =  ['eo:cloud_cover&lt;50'],\n    datetime=\"2021-04-01/2021-08-31\"\n).item_collection()\nprint(f'{len(items)} items found')      # How many items in the catalog match these criteria?\n</pre> %%time                                   items = catalog.search(     intersects=dict(type=\"Point\", coordinates=[lon, lat]),     collections=[\"sentinel-2-l2a\"],     query =  ['eo:cloud_cover&lt;50'],     datetime=\"2021-04-01/2021-08-31\" ).item_collection() print(f'{len(items)} items found')      # How many items in the catalog match these criteria? <pre>42 items found\nCPU times: total: 141 ms\nWall time: 2.67 s\n</pre> <p>Use <code>stackstac</code> to turn those STAC items into a lazy xarray. Without specifying anything, this makes use of dask and reads the data in chunks, with one chunk shaped (1, 1, 1024, 1024) in the time, band, y, x dimensions (Read more about chunk sizes here, here, and here.). Using all the defaults, our data will be in its native coordinate reference system, at the finest resolution of all the assets.</p> In\u00a0[34]: Copied! <pre>%time stack = stackstac.stack(items)\n</pre> %time stack = stackstac.stack(items) <pre>CPU times: total: 78.1 ms\nWall time: 87.7 ms\n</pre> <p>Now print info about this xarray.DataArray called 'stack'. Note that everything has run quickly so far, despite the large data volume covered by our query. Up to now, we are only working with metadata, no download or processing of any pixel values.</p> In\u00a0[35]: Copied! <pre>stack\n</pre> stack Out[35]: <pre>&lt;xarray.DataArray 'stackstac-d35de160ec768a0c51870cec97c80190' (time: 42,\n                                                                band: 32,\n                                                                y: 10980,\n                                                                x: 10980)&gt;\ndask.array&lt;fetch_raster_window, shape=(42, 32, 10980, 10980), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray&gt;\nCoordinates: (12/52)\n  * time                                     (time) datetime64[ns] 2021-04-01...\n    id                                       (time) &lt;U24 'S2B_32TPT_20210401_...\n  * band                                     (band) &lt;U12 'aot' ... 'wvp-jp2'\n  * x                                        (x) float64 6e+05 ... 7.098e+05\n  * y                                        (y) float64 5.3e+06 ... 5.19e+06\n    s2:medium_proba_clouds_percentage        (time) float64 1.463 16.5 ... 2.701\n    ...                                       ...\n    title                                    (band) &lt;U31 'Aerosol optical thi...\n    gsd                                      (band) object None 10 ... None None\n    common_name                              (band) object None 'blue' ... None\n    center_wavelength                        (band) object None 0.49 ... None\n    full_width_half_max                      (band) object None 0.098 ... None\n    epsg                                     int32 32632\nAttributes:\n    spec:        RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0...\n    crs:         epsg:32632\n    transform:   | 10.00, 0.00, 600000.00|\\n| 0.00,-10.00, 5300040.00|\\n| 0.0...\n    resolution:  10.0</pre>xarray.DataArray'stackstac-d35de160ec768a0c51870cec97c80190'<ul><li>time: 42</li><li>band: 32</li><li>y: 10980</li><li>x: 10980</li></ul><ul><li>dask.array&lt;chunksize=(1, 1, 1024, 1024), meta=np.ndarray&gt;  Array   Chunk   Bytes   1.18 TiB   8.00 MiB   Shape   (42, 32, 10980, 10980)   (1, 1, 1024, 1024)   Dask graph   162624 chunks in 3 graph layers   Data type   float64 numpy.ndarray  42 1 10980 10980 32 </li><li>Coordinates: (52)<ul><li>time(time)datetime64[ns]2021-04-01T10:27:39.520000 ... 2...<pre>array(['2021-04-01T10:27:39.520000000', '2021-04-08T10:17:41.934000000',\n       '2021-04-08T10:17:41.935000000', '2021-04-16T10:27:35.384000000',\n       '2021-04-16T10:27:35.385000000', '2021-04-21T10:27:35.793000000',\n       '2021-04-23T10:17:40.745000000', '2021-04-23T10:17:40.747000000',\n       '2021-04-28T10:17:40.197000000', '2021-05-03T10:17:42.916000000',\n       '2021-05-08T10:17:42.592000000', '2021-05-08T10:17:42.594000000',\n       '2021-05-23T10:17:45.371000000', '2021-05-26T10:27:42.080000000',\n       '2021-05-28T10:17:45.418000000', '2021-05-28T10:17:45.421000000',\n       '2021-05-31T10:27:42.147000000', '2021-05-31T10:27:42.149000000',\n       '2021-06-02T10:17:45.639000000', '2021-06-02T10:17:45.641000000',\n       '2021-06-10T10:27:42.429000000', '2021-06-15T10:27:41.696000000',\n       '2021-06-15T10:27:41.697000000', '2021-06-17T10:17:45.721000000',\n       '2021-06-17T10:17:45.724000000', '2021-06-22T10:17:45.729000000',\n       '2021-06-22T10:17:45.732000000', '2021-06-27T10:17:45.604000000',\n       '2021-06-27T10:17:45.605000000', '2021-07-02T10:17:46.959000000',\n       '2021-07-02T10:17:46.961000000', '2021-07-10T10:27:43.233000000',\n       '2021-07-12T10:17:47.866000000', '2021-07-20T10:27:43.424000000',\n       '2021-07-22T10:17:48.210000000', '2021-08-06T10:17:45.959000000',\n       '2021-08-09T10:27:42.199000000', '2021-08-11T10:17:47.398000000',\n       '2021-08-14T10:27:43.878000000', '2021-08-16T10:17:44.680000000',\n       '2021-08-19T10:27:40.757000000', '2021-08-21T10:17:47.043000000'],\n      dtype='datetime64[ns]')</pre></li><li>id(time)&lt;U24'S2B_32TPT_20210401_0_L2A' ... '...<pre>array(['S2B_32TPT_20210401_0_L2A', 'S2B_32TPT_20210408_2_L2A',\n       'S2B_32TPT_20210408_0_L2A', 'S2A_32TPT_20210416_1_L2A',\n       'S2A_32TPT_20210416_0_L2A', 'S2B_32TPT_20210421_0_L2A',\n       'S2A_32TPT_20210423_2_L2A', 'S2A_32TPT_20210423_0_L2A',\n       'S2B_32TPT_20210428_0_L2A', 'S2A_32TPT_20210503_0_L2A',\n       'S2B_32TPT_20210508_1_L2A', 'S2B_32TPT_20210508_0_L2A',\n       'S2A_32TPT_20210523_0_L2A', 'S2A_32TPT_20210526_0_L2A',\n       'S2B_32TPT_20210528_1_L2A', 'S2B_32TPT_20210528_0_L2A',\n       'S2B_32TPT_20210531_1_L2A', 'S2B_32TPT_20210531_0_L2A',\n       'S2A_32TPT_20210602_1_L2A', 'S2A_32TPT_20210602_0_L2A',\n       'S2B_32TPT_20210610_0_L2A', 'S2A_32TPT_20210615_0_L2A',\n       'S2A_32TPT_20210615_1_L2A', 'S2B_32TPT_20210617_1_L2A',\n       'S2B_32TPT_20210617_0_L2A', 'S2A_32TPT_20210622_1_L2A',\n       'S2A_32TPT_20210622_0_L2A', 'S2B_32TPT_20210627_1_L2A',\n       'S2B_32TPT_20210627_0_L2A', 'S2A_32TPT_20210702_1_L2A',\n       'S2A_32TPT_20210702_0_L2A', 'S2B_32TPT_20210710_0_L2A',\n       'S2A_32TPT_20210712_0_L2A', 'S2B_32TPT_20210720_0_L2A',\n       'S2A_32TPT_20210722_0_L2A', 'S2B_32TPT_20210806_0_L2A',\n       'S2B_32TPT_20210809_0_L2A', 'S2A_32TPT_20210811_0_L2A',\n       'S2A_32TPT_20210814_0_L2A', 'S2B_32TPT_20210816_0_L2A',\n       'S2B_32TPT_20210819_0_L2A', 'S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U24')</pre></li><li>band(band)&lt;U12'aot' 'blue' ... 'wvp-jp2'<pre>array(['aot', 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09', 'red',\n       'rededge1', 'rededge2', 'rededge3', 'scl', 'swir16', 'swir22', 'visual',\n       'wvp', 'aot-jp2', 'blue-jp2', 'coastal-jp2', 'green-jp2', 'nir-jp2',\n       'nir08-jp2', 'nir09-jp2', 'red-jp2', 'rededge1-jp2', 'rededge2-jp2',\n       'rededge3-jp2', 'scl-jp2', 'swir16-jp2', 'swir22-jp2', 'visual-jp2',\n       'wvp-jp2'], dtype='&lt;U12')</pre></li><li>x(x)float646e+05 6e+05 ... 7.098e+05 7.098e+05<pre>array([600000., 600010., 600020., ..., 709770., 709780., 709790.])</pre></li><li>y(y)float645.3e+06 5.3e+06 ... 5.19e+06<pre>array([5300040., 5300030., 5300020., ..., 5190270., 5190260., 5190250.])</pre></li><li>s2:medium_proba_clouds_percentage(time)float641.463 16.5 14.33 ... 4.624 2.701<pre>array([ 1.46334 , 16.504081, 14.328757, 10.298105, 16.633525,  7.706635,\n        1.899826,  2.313006,  3.444555, 12.165798,  6.054812,  6.262551,\n        8.735961,  8.490147,  6.304692,  6.609003,  0.209097,  0.926137,\n       12.20682 ,  3.0033  ,  6.494882,  0.308812,  0.057903,  9.846946,\n        2.624075, 10.38753 ,  2.671621,  2.953941,  1.214524, 14.579228,\n        6.361181,  1.917733,  5.046743,  5.886542,  3.49712 ,  5.288142,\n        8.231435,  4.027119,  0.841558, 19.751492,  4.62429 ,  2.701251])</pre></li><li>earthsearch:boa_offset_applied(time)boolFalse True False ... False False<pre>array([False,  True, False,  True, False, False,  True, False, False,\n       False,  True, False, False, False,  True, False,  True, False,\n        True, False, False, False,  True,  True, False,  True, False,\n        True, False,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False])</pre></li><li>processing:software(time)object{'sentinel2-to-stac': '0.1.0'} ....<pre>array([{'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'}],\n      dtype=object)</pre></li><li>proj:epsg()int3232632<pre>array(32632)</pre></li><li>s2:generation_time(time)&lt;U27'2021-04-01T14:19:13.000000Z' .....<pre>array(['2021-04-01T14:19:13.000000Z', '2023-05-13T12:44:55.000000Z',\n       '2021-04-08T13:26:17.000000Z', '2023-05-19T13:18:55.000000Z',\n       '2021-04-16T13:20:57.000000Z', '2021-04-21T13:46:13.000000Z',\n       '2023-05-12T19:45:03.000000Z', '2021-04-23T12:55:30.000000Z',\n       '2021-04-28T13:40:12.000000Z', '2021-05-03T12:17:41.000000Z',\n       '2023-02-27T09:29:40.000000Z', '2021-05-08T14:56:55.000000Z',\n       '2021-05-23T12:13:33.000000Z', '2021-05-26T13:34:25.000000Z',\n       '2023-02-27T08:23:12.000000Z', '2021-05-28T13:08:36.000000Z',\n       '2023-03-01T00:24:41.000000Z', '2021-05-31T14:00:40.000000Z',\n       '2023-03-13T04:04:11.000000Z', '2021-06-02T17:13:07.000000Z',\n       '2021-06-10T13:27:39.000000Z', '2021-06-15T13:16:59.000000Z',\n       '2023-03-21T22:10:50.000000Z', '2023-03-21T22:17:28.000000Z',\n       '2021-06-17T13:12:19.000000Z', '2023-01-29T15:30:31.000000Z',\n       '2021-06-22T12:13:05.000000Z', '2023-03-18T23:38:59.000000Z',\n       '2021-06-27T13:58:56.000000Z', '2023-01-25T13:05:14.000000Z',\n       '2021-07-02T12:13:53.000000Z', '2021-07-10T13:23:21.000000Z',\n       '2021-07-12T13:22:19.000000Z', '2021-07-20T13:22:07.000000Z',\n       '2021-07-22T12:02:14.000000Z', '2021-08-06T15:00:37.000000Z',\n       '2021-08-14T13:01:57.000000Z', '2021-08-11T16:05:21.000000Z',\n       '2021-08-14T13:23:26.000000Z', '2021-08-16T13:08:48.000000Z',\n       '2021-08-19T13:53:08.000000Z', '2021-08-21T12:05:55.000000Z'],\n      dtype='&lt;U27')</pre></li><li>created(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-03T11:38:25.089Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-03T11:19:16.880Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-03T11:12:48.494Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:dark_features_percentage(time)float647.618 3.489 2.558 ... 2.943 2.726<pre>array([7.618114, 3.489247, 2.558084, 5.739467, 4.485189, 2.807637,\n       5.218234, 2.874529, 1.733305, 1.861065, 2.493159, 1.334918,\n       0.416572, 1.220573, 1.255598, 0.764608, 2.384401, 1.463244,\n       0.931227, 1.316954, 1.090162, 0.8525  , 1.352069, 0.733103,\n       1.135998, 1.516507, 1.200472, 1.417252, 0.957145, 1.22749 ,\n       1.375679, 1.512192, 0.560178, 1.777653, 1.21489 , 2.177998,\n       2.577018, 1.8398  , 3.048143, 1.672873, 2.943032, 2.726397])</pre></li><li>s2:datatake_type()&lt;U8'INS-NOBS'<pre>array('INS-NOBS', dtype='&lt;U8')</pre></li><li>s2:water_percentage(time)float641.035 0.3132 ... 0.3022 0.8797<pre>array([1.034616, 0.313168, 0.424308, 0.617732, 0.819981, 0.987005,\n       0.857674, 1.026752, 0.751145, 0.66969 , 0.589903, 0.582099,\n       0.374695, 0.464075, 0.43538 , 0.426498, 0.905694, 0.953868,\n       0.768842, 1.276473, 0.616264, 0.743187, 0.848236, 0.84215 ,\n       1.075244, 0.699068, 0.829451, 0.73024 , 0.757141, 0.394683,\n       0.510065, 0.539884, 0.785743, 0.768489, 1.082259, 0.712835,\n       0.559107, 0.910735, 1.067886, 0.465221, 0.302152, 0.879734])</pre></li><li>eo:cloud_cover(time)float646.149 45.79 45.55 ... 44.97 11.88<pre>array([ 6.149054, 45.791623, 45.550924, 25.692633, 33.007648, 43.998277,\n        3.953265,  4.693773, 35.548758, 39.265612, 15.81282 , 15.84441 ,\n       49.168627, 45.008284, 26.705834, 28.351225,  0.365331,  1.409963,\n       33.068579, 24.769196, 47.15134 ,  1.805315,  1.849951, 26.098341,\n       19.611905, 25.549465, 18.602191, 10.19    ,  9.07656 , 44.637647,\n       35.395256, 16.658998, 10.469738, 24.9592  , 18.490819, 28.947858,\n       27.45625 , 24.8152  ,  3.156408, 49.208091, 44.972815, 11.884551])</pre></li><li>view:sun_azimuth(time)float64161.2 157.5 157.5 ... 158.4 155.0<pre>array([161.22281178, 157.46645516, 157.46542095, 160.80126617,\n       160.80014335, 160.57908911, 156.51420567, 156.51306183,\n       156.0684864 , 155.56597523, 154.95741812, 154.95616171,\n       152.69773459, 156.88007906, 151.8422183 , 151.84086683,\n       156.07844848, 156.07704495, 150.97550979, 150.97415895,\n       154.47087908, 153.7373982 , 153.738758  , 148.65682968,\n       148.65553825, 148.11111459, 148.10983402, 147.7307707 ,\n       147.72954199, 147.54553355, 147.54435203, 152.24342398,\n       147.75100857, 152.92620053, 148.7434059 , 151.42303732,\n       156.18470534, 152.56163454, 157.28195674, 153.75067448,\n       158.41022444, 155.02778045])</pre></li><li>s2:sequence(time)&lt;U1'0' '2' '0' '1' ... '0' '0' '0' '0'<pre>array(['0', '2', '0', '1', '0', '0', '2', '0', '0', '0', '1', '0', '0',\n       '0', '1', '0', '1', '0', '1', '0', '0', '0', '1', '1', '0', '1',\n       '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n       '0', '0', '0'], dtype='&lt;U1')</pre></li><li>s2:datastrip_id(time)&lt;U64'S2B_OPER_MSI_L2A_DS_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_DS_VGS4_20210401T141913_S20210401T102326_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230513T124455_S20210408T101148_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210408T132617_S20210408T101148_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230519T131855_S20210416T102600_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210416T132057_S20210416T102600_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210421T134613_S20210421T102336_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230512T194503_S20210423T101704_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210423T125530_S20210423T101704_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210428T134012_S20210428T101657_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210503T121741_S20210503T101204_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T092940_S20210508T101551_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210508T145655_S20210508T101551_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210523T121333_S20210523T101416_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210526T133425_S20210526T102625_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T082312_S20210528T100555_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210528T130836_S20210528T100555_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230301T002441_S20210531T102618_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210531T140040_S20210531T102618_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230313T040411_S20210602T101235_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210602T171307_S20210602T101235_N03.00',\n...\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230321T221728_S20210617T101301_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210617T131219_S20210617T101301_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230129T153031_S20210622T101416_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210622T121305_S20210622T101416_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230318T233859_S20210627T101702_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210627T135856_S20210627T101702_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230125T130514_S20210702T101157_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210702T121353_S20210702T101157_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210710T132321_S20210710T102312_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210712T132219_S20210712T101027_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210720T132207_S20210720T101617_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210722T120214_S20210722T101419_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210806T150037_S20210806T101207_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS1_20210814T130157_S20210809T102637_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210811T160521_S20210811T101411_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210814T132326_S20210814T102609_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210816T130848_S20210816T101300_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210819T135308_S20210819T102635_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210821T120555_S20210821T101418_N03.01'],\n      dtype='&lt;U64')</pre></li><li>mgrs:grid_square()&lt;U2'PT'<pre>array('PT', dtype='&lt;U2')</pre></li><li>instruments()&lt;U3'msi'<pre>array('msi', dtype='&lt;U3')</pre></li><li>s2:thin_cirrus_percentage(time)float640.4854 0.4475 ... 0.004964 3.404<pre>array([4.8540100e-01, 4.4747100e-01, 1.9159610e+00, 2.4920600e-01,\n       8.8436700e-01, 2.2049110e+00, 8.2425000e-02, 7.5833000e-02,\n       1.1564730e+00, 2.1922420e+00, 8.2073200e+00, 7.6982920e+00,\n       1.3726480e+00, 2.5636340e+00, 1.5205705e+01, 1.6151080e+01,\n       4.1297000e-02, 4.4017000e-02, 5.1187000e-01, 8.5886000e-01,\n       1.7029480e+00, 1.1063510e+00, 1.7621680e+00, 1.0222800e-01,\n       2.7094100e-01, 1.3869100e-01, 2.2755900e-01, 4.5100390e+00,\n       4.5370160e+00, 5.0657700e+00, 3.5785940e+00, 9.4725660e+00,\n       3.9136060e+00, 2.2286300e-01, 5.0741000e-02, 8.8339000e-02,\n       8.7864210e+00, 1.1323400e-01, 3.8490000e-03, 8.7228200e-01,\n       4.9640000e-03, 3.4040560e+00])</pre></li><li>s2:vegetation_percentage(time)float6426.34 3.007 3.274 ... 36.39 66.22<pre>array([26.339936,  3.007327,  3.273584, 11.632005, 12.624834, 18.579322,\n       34.706536, 36.717805, 22.266363, 18.495288, 33.440745, 34.884864,\n       14.592455, 19.679427, 29.31349 , 30.193511, 59.83513 , 60.953248,\n       39.538234, 42.743748, 29.41438 , 71.402311, 70.151877, 51.991493,\n       55.270773, 53.398359, 56.250197, 68.942064, 69.730705, 38.577574,\n       43.56502 , 66.226125, 71.196151, 49.621934, 62.576228, 49.759224,\n       54.908115, 55.141944, 78.077263, 32.121503, 36.389408, 66.223991])</pre></li><li>s2:high_proba_clouds_percentage(time)float644.2 28.84 29.31 ... 40.34 5.779<pre>array([4.2003130e+00, 2.8840071e+01, 2.9306206e+01, 1.5145321e+01,\n       1.5489756e+01, 3.4086731e+01, 1.9710150e+00, 2.3049340e+00,\n       3.0947730e+01, 2.4907573e+01, 1.5506890e+00, 1.8835670e+00,\n       3.9060017e+01, 3.3954504e+01, 5.1954370e+00, 5.5911420e+00,\n       1.1493700e-01, 4.3980900e-01, 2.0349889e+01, 2.0907035e+01,\n       3.8953510e+01, 3.9015200e-01, 2.9880000e-02, 1.6149166e+01,\n       1.6716889e+01, 1.5023246e+01, 1.5703011e+01, 2.7260200e+00,\n       3.3250190e+00, 2.4992649e+01, 2.5455481e+01, 5.2686990e+00,\n       1.5093900e+00, 1.8849795e+01, 1.4942957e+01, 2.3571377e+01,\n       1.0438395e+01, 2.0674847e+01, 2.3110010e+00, 2.8584316e+01,\n       4.0343562e+01, 5.7792450e+00])</pre></li><li>s2:unclassified_percentage(time)float645.542 1.249 4.882 ... 4.302 3.051<pre>array([5.542233, 1.249116, 4.882131, 1.901653, 8.202816, 3.343896,\n       0.369042, 4.536037, 5.607506, 5.386119, 0.42674 , 5.226327,\n       5.411838, 6.440622, 1.1914  , 7.262506, 0.095849, 2.831393,\n       0.502138, 3.365285, 5.884814, 2.064657, 0.053503, 0.43447 ,\n       3.136941, 0.979966, 3.44101 , 0.372121, 1.875458, 1.251701,\n       4.816858, 1.852196, 2.070124, 6.984119, 4.280468, 5.570791,\n       3.381409, 4.446889, 1.608293, 5.839212, 4.302198, 3.050546])</pre></li><li>s2:snow_ice_percentage(time)float6440.65 40.93 39.09 ... 1.161 0.858<pre>array([40.654615, 40.934548, 39.088675, 45.368615, 33.28197 , 21.687628,\n       44.689843, 40.108246, 25.425422, 26.355195, 37.636858, 35.119075,\n       27.416307, 21.791345, 30.055815, 26.366636, 27.95321 , 24.107485,\n       13.354345, 16.522175,  8.769823, 12.960622, 16.334337,  8.544581,\n        8.742414,  6.522872,  6.389479,  7.112196,  5.816885,  4.461629,\n        4.215506,  2.77441 ,  3.27045 ,  1.624512,  1.15363 ,  1.572428,\n        1.036823,  0.380159,  0.842441,  2.167679,  1.161288,  0.857967])</pre></li><li>earthsearch:s3_path(time)&lt;U79's3://sentinel-cogs/sentinel-s2-...<pre>array(['s3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210401_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210421_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210428_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210503_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210523_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210526_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_0_L2A',\n...\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210710_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210712_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210720_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210722_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210806_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210809_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210811_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210814_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210816_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210819_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U79')</pre></li><li>s2:granule_id(time)&lt;U62'S2B_OPER_MSI_L2A_TL_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_TL_VGS4_20210401T141913_A021255_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230513T124455_A021355_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210408T132617_A021355_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230519T131855_A030378_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210416T132057_A030378_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210421T134613_A021541_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230512T194503_A030478_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210423T125530_A030478_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210428T134012_A021641_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210503T121741_A030621_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T092940_A021784_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210508T145655_A021784_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210523T121333_A030907_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210526T133425_A030950_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T082312_A022070_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210528T130836_A022070_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230301T002441_A022113_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210531T140040_A022113_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230313T040411_A031050_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210602T171307_A031050_T32TPT_N03.00',\n...\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230321T221728_A022356_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210617T131219_A022356_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230129T153031_A031336_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210622T121305_A031336_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230318T233859_A022499_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210627T135856_A022499_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230125T130514_A031479_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210702T121353_A031479_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210710T132321_A022685_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210712T132219_A031622_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210720T132207_A022828_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210722T120214_A031765_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210806T150037_A023071_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS1_20210814T130157_A023114_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210811T160521_A032051_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210814T132326_A032094_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210816T130848_A023214_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210819T135308_A023257_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210821T120555_A032194_T32TPT_N03.01'],\n      dtype='&lt;U62')</pre></li><li>constellation()&lt;U10'sentinel-2'<pre>array('sentinel-2', dtype='&lt;U10')</pre></li><li>s2:reflectance_conversion_factor(time)float641.004 0.9999 ... 0.9753 0.9761<pre>array([1.00391722, 0.99989016, 0.99989016, 0.99531243, 0.99531243,\n       0.99251188, 0.99141059, 0.99141059, 0.98870861, 0.98609676,\n       0.98359974, 0.98359974, 0.97693835, 0.97577919, 0.97504789,\n       0.97504789, 0.97399904, 0.97399904, 0.97334113, 0.97334113,\n       0.97102521, 0.96985123, 0.96985123, 0.969445  , 0.969445  ,\n       0.96858002, 0.96858002, 0.96794362, 0.96794362, 0.96753822,\n       0.96753822, 0.96737784, 0.96743179, 0.96802315, 0.96826343,\n       0.97121659, 0.97204341, 0.9726335 , 0.97358286, 0.97425016,\n       0.97531576, 0.97606015])</pre></li><li>view:sun_elevation(time)float6445.93 48.03 48.03 ... 53.63 52.36<pre>array([45.92877963, 48.02966446, 48.0295066 , 51.57725751, 51.57711889,\n       53.32274987, 53.3762054 , 53.37603569, 54.97817306, 56.47292004,\n       57.84102398, 57.84083568, 61.10137652, 62.31302901, 61.87597965,\n       61.87575968, 63.010744  , 63.01055231, 62.48509786, 62.4848721 ,\n       63.89999116, 64.08851703, 64.08872077, 63.30105997, 63.30082564,\n       63.24315182, 63.24291386, 63.02832992, 63.02809443, 62.66520394,\n       62.66497104, 62.62975879, 61.51305241, 61.04463203, 59.85139625,\n       56.52628032, 56.4892853 , 55.22451323, 55.10440682, 53.83083839,\n       53.63004967, 52.36396242])</pre></li><li>platform(time)&lt;U11'sentinel-2b' ... 'sentinel-2a'<pre>array(['sentinel-2b', 'sentinel-2b', 'sentinel-2b', 'sentinel-2a',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a'], dtype='&lt;U11')</pre></li><li>mgrs:latitude_band()&lt;U1'T'<pre>array('T', dtype='&lt;U1')</pre></li><li>s2:product_uri(time)&lt;U65'S2B_MSIL2A_20210401T101559_N030...<pre>array(['S2B_MSIL2A_20210401T101559_N0300_R065_T32TPT_20210401T141913.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0500_R022_T32TPT_20230513T124455.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0300_R022_T32TPT_20210408T132617.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0500_R065_T32TPT_20230519T131855.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0300_R065_T32TPT_20210416T132057.SAFE',\n       'S2B_MSIL2A_20210421T101549_N0300_R065_T32TPT_20210421T134613.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0500_R022_T32TPT_20230512T194503.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0300_R022_T32TPT_20210423T125530.SAFE',\n       'S2B_MSIL2A_20210428T100549_N0300_R022_T32TPT_20210428T134012.SAFE',\n       'S2A_MSIL2A_20210503T101021_N0300_R022_T32TPT_20210503T121741.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0500_R022_T32TPT_20230227T092940.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0300_R022_T32TPT_20210508T145655.SAFE',\n       'S2A_MSIL2A_20210523T101031_N0300_R022_T32TPT_20210523T121333.SAFE',\n       'S2A_MSIL2A_20210526T102021_N0300_R065_T32TPT_20210526T133425.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0500_R022_T32TPT_20230227T082312.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0300_R022_T32TPT_20210528T130836.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0500_R065_T32TPT_20230301T002441.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0300_R065_T32TPT_20210531T140040.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0500_R022_T32TPT_20230313T040411.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0300_R022_T32TPT_20210602T171307.SAFE',\n...\n       'S2B_MSIL2A_20210617T100559_N0500_R022_T32TPT_20230321T221728.SAFE',\n       'S2B_MSIL2A_20210617T100559_N0300_R022_T32TPT_20210617T131219.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0500_R022_T32TPT_20230129T153031.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0300_R022_T32TPT_20210622T121305.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0500_R022_T32TPT_20230318T233859.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0300_R022_T32TPT_20210627T135856.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0500_R022_T32TPT_20230125T130514.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0301_R022_T32TPT_20210702T121353.SAFE',\n       'S2B_MSIL2A_20210710T101559_N0301_R065_T32TPT_20210710T132321.SAFE',\n       'S2A_MSIL2A_20210712T101031_N0301_R022_T32TPT_20210712T132219.SAFE',\n       'S2B_MSIL2A_20210720T101559_N0301_R065_T32TPT_20210720T132207.SAFE',\n       'S2A_MSIL2A_20210722T101031_N0301_R022_T32TPT_20210722T120214.SAFE',\n       'S2B_MSIL2A_20210806T100559_N0301_R022_T32TPT_20210806T150037.SAFE',\n       'S2B_MSIL2A_20210809T101559_N0301_R065_T32TPT_20210814T130157.SAFE',\n       'S2A_MSIL2A_20210811T101031_N0301_R022_T32TPT_20210811T160521.SAFE',\n       'S2A_MSIL2A_20210814T102031_N0301_R065_T32TPT_20210814T132326.SAFE',\n       'S2B_MSIL2A_20210816T100559_N0301_R022_T32TPT_20210816T130848.SAFE',\n       'S2B_MSIL2A_20210819T101559_N0301_R065_T32TPT_20210819T135308.SAFE',\n       'S2A_MSIL2A_20210821T101031_N0301_R022_T32TPT_20210821T120555.SAFE'],\n      dtype='&lt;U65')</pre></li><li>updated(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-06T14:14:42.036Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-06T04:47:25.002Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-06T04:45:55.146Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:product_type()&lt;U7'S2MSI2A'<pre>array('S2MSI2A', dtype='&lt;U7')</pre></li><li>s2:saturated_defective_pixel_percentage()int320<pre>array(0)</pre></li><li>s2:degraded_msi_data_percentage(time)object0 0.1898 0 0.0825 0 0 ... 0 0 0 0 0<pre>array([0, 0.1898, 0, 0.0825, 0, 0, 0.0161, 0, 0, 0, 0.0259, 0, 0, 0,\n       0.0119, 0, 0.0204, 0, 0.0664, 0, 0, 0, 0.0225, 0.0382, 0, 0.0347,\n       0, 0.012, 0, 0.021, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      dtype=object)</pre></li><li>grid:code()&lt;U10'MGRS-32TPT'<pre>array('MGRS-32TPT', dtype='&lt;U10')</pre></li><li>s2:nodata_pixel_percentage(time)object0.002973 9.284004 ... 8.773684<pre>array([0.002973, 9.284004, 8.883444, 0, 3.6e-05, 6e-05, 9.639984,\n       9.534544, 8.656428, 9.141818, 9.115126, 8.715048, 8.926022,\n       0.001904, 8.633243, 8.246658, 0.023792, 0.013444, 9.047445,\n       8.948952, 0.009396, 0.000113, 1.7e-05, 8.783989, 8.390699, 9.33119,\n       9.234183, 8.918451, 8.525366, 9.049369, 8.950624, 0.010153,\n       8.783641, 0.010524, 8.777927, 8.489713, 0.004336, 8.84221,\n       0.003726, 8.610921, 0.000985, 8.773684], dtype=object)</pre></li><li>s2:cloud_shadow_percentage(time)float644.934 0.4877 2.028 ... 2.541 4.589<pre>array([4.934268, 0.487673, 2.027822, 0.962452, 3.386088, 5.310493,\n       0.704442, 2.288749, 3.619292, 4.858182, 0.008787, 0.596837,\n       1.866554, 1.606319, 0.439432, 0.873589, 0.13241 , 0.22497 ,\n       6.900295, 4.897763, 5.058461, 0.094422, 0.031267, 5.442425,\n       4.135132, 2.465164, 2.53105 , 0.682236, 0.868314, 3.576069,\n       2.60878 , 1.147319, 0.079854, 6.248215, 3.624339, 3.247687,\n       1.521886, 5.362982, 1.95151 , 2.662793, 2.541466, 4.588702])</pre></li><li>s2:not_vegetated_percentage(time)float647.727 4.727 2.194 ... 7.388 9.788<pre>array([ 7.727165,  4.727299,  2.194477,  8.085445,  4.191478,  3.285741,\n        9.500962,  7.754106,  5.04821 ,  3.108844,  9.590986,  6.41147 ,\n        0.752953,  3.789353, 10.603052,  5.761432,  8.327983,  8.055834,\n        4.936339,  5.108408,  2.014758, 10.076986,  9.378749,  5.913443,\n        6.891589,  8.868597, 10.756154, 10.553885, 10.917792,  5.873202,\n        7.512838,  9.288875, 11.567761,  8.01588 ,  7.577366,  8.01118 ,\n        8.559394,  7.102291, 10.248062,  5.862629,  7.38764 ,  9.788115])</pre></li><li>s2:datatake_id(time)&lt;U34'GS2B_20210401T101559_021255_N03...<pre>array(['GS2B_20210401T101559_021255_N03.00',\n       'GS2B_20210408T100549_021355_N05.00',\n       'GS2B_20210408T100549_021355_N03.00',\n       'GS2A_20210416T102021_030378_N05.00',\n       'GS2A_20210416T102021_030378_N03.00',\n       'GS2B_20210421T101549_021541_N03.00',\n       'GS2A_20210423T101021_030478_N05.00',\n       'GS2A_20210423T101021_030478_N03.00',\n       'GS2B_20210428T100549_021641_N03.00',\n       'GS2A_20210503T101021_030621_N03.00',\n       'GS2B_20210508T100549_021784_N05.00',\n       'GS2B_20210508T100549_021784_N03.00',\n       'GS2A_20210523T101031_030907_N03.00',\n       'GS2A_20210526T102021_030950_N03.00',\n       'GS2B_20210528T100559_022070_N05.00',\n       'GS2B_20210528T100559_022070_N03.00',\n       'GS2B_20210531T101559_022113_N05.00',\n       'GS2B_20210531T101559_022113_N03.00',\n       'GS2A_20210602T101031_031050_N05.00',\n       'GS2A_20210602T101031_031050_N03.00',\n...\n       'GS2A_20210615T102021_031236_N05.00',\n       'GS2B_20210617T100559_022356_N05.00',\n       'GS2B_20210617T100559_022356_N03.00',\n       'GS2A_20210622T101031_031336_N05.00',\n       'GS2A_20210622T101031_031336_N03.00',\n       'GS2B_20210627T100559_022499_N05.00',\n       'GS2B_20210627T100559_022499_N03.00',\n       'GS2A_20210702T101031_031479_N05.00',\n       'GS2A_20210702T101031_031479_N03.01',\n       'GS2B_20210710T101559_022685_N03.01',\n       'GS2A_20210712T101031_031622_N03.01',\n       'GS2B_20210720T101559_022828_N03.01',\n       'GS2A_20210722T101031_031765_N03.01',\n       'GS2B_20210806T100559_023071_N03.01',\n       'GS2B_20210809T101559_023114_N03.01',\n       'GS2A_20210811T101031_032051_N03.01',\n       'GS2A_20210814T102031_032094_N03.01',\n       'GS2B_20210816T100559_023214_N03.01',\n       'GS2B_20210819T101559_023257_N03.01',\n       'GS2A_20210821T101031_032194_N03.01'], dtype='&lt;U34')</pre></li><li>s2:processing_baseline(time)&lt;U5'03.00' '05.00' ... '03.01' '03.01'<pre>array(['03.00', '05.00', '03.00', '05.00', '03.00', '03.00', '05.00',\n       '03.00', '03.00', '03.00', '05.00', '03.00', '03.00', '03.00',\n       '05.00', '03.00', '05.00', '03.00', '05.00', '03.00', '03.00',\n       '03.00', '05.00', '05.00', '03.00', '05.00', '03.00', '05.00',\n       '03.00', '05.00', '03.01', '03.01', '03.01', '03.01', '03.01',\n       '03.01', '03.01', '03.01', '03.01', '03.01', '03.01', '03.01'],\n      dtype='&lt;U5')</pre></li><li>mgrs:utm_zone()int3232<pre>array(32)</pre></li><li>earthsearch:payload_id(time)&lt;U74'roda-sentinel2/workflow-sentine...<pre>array(['roda-sentinel2/workflow-sentinel2-to-stac/ca1fac60c9640967f45658d79d1523ba',\n       'roda-sentinel2/workflow-sentinel2-to-stac/d1881ee109d59d1df5a0cfc2d85ce430',\n       'roda-sentinel2/workflow-sentinel2-to-stac/f9aa8a3191a8be696d2ddddd9734621c',\n       'roda-sentinel2/workflow-sentinel2-to-stac/57765d545d606d43efe2820f6e0d1159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/1ac3805c5018ad2b5d5989908e95d38d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/21d39a00de8e19a12451c18ba24f832b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/778637c0ddea5fd397ac6695ffe344dd',\n       'roda-sentinel2/workflow-sentinel2-to-stac/29497ba4096ac30ecf99e9603229329b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/03a8dfb01cc4bd0fbcfbb149e25da06f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/c4e5ae224b1aa55e00c6903aa25f65ee',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4f41cddc1225338c2ce7a50a413d05ff',\n       'roda-sentinel2/workflow-sentinel2-to-stac/aa4edf7495a6f12421d1142a8371e750',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4a84c0a1c9ec26453a988d58b3a1e236',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b7752cd2d7341f1cdf30140de86e4992',\n       'roda-sentinel2/workflow-sentinel2-to-stac/8d9d4f770bdb5a0a1a614b2fde70290d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/050ec18e7aef56ed8ce1d702e586175d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/fef3c2c9435cfa8276430d9f2c8fbcdb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ae91b5b1381789ef2f526f90e0d7a89f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/e987ce36035c8be203930ef9d5bcfb55',\n       'roda-sentinel2/workflow-sentinel2-to-stac/64087c1420a6067f46412cfc0215cf07',\n...\n       'roda-sentinel2/workflow-sentinel2-to-stac/1dc42ce3106743876f3fb4adc694ce4d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/af36520f9b99c3e44bf2830bbb217b2b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/5065c12a38660808deee8961e203498a',\n       'roda-sentinel2/workflow-sentinel2-to-stac/6ff82a306f3bf3f579816ae10d063d01',\n       'roda-sentinel2/workflow-sentinel2-to-stac/0081724da96931a83c6f7515f7562943',\n       'roda-sentinel2/workflow-sentinel2-to-stac/737df6bf70bb0f7b565fa5e6ee4129bb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ca09639b46ee5bb24b92fd5b2987cc8b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/82f456e00cccfe29386bd26a12a6e588',\n       'roda-sentinel2/workflow-sentinel2-to-stac/391f5e95d69f9ebd8a98bcc3fc3589eb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cbc04fd5c89f2b587d6dadfc9b9fffbb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/2ee326f71919bb8db20326fefc2dfbb1',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4e6ecb562c70758e02b09233082200a3',\n       'roda-sentinel2/workflow-sentinel2-to-stac/453b45d4b7891c6f3e0afe979aa1538e',\n       'roda-sentinel2/workflow-sentinel2-to-stac/665c266d9bc61d6315aded7c7690b159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/762a8a493aae7d67b31d512131bc22d4',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cebdfaff85f595f644f4d89f380d1a04',\n       'roda-sentinel2/workflow-sentinel2-to-stac/7fd514bec1e5e63f8b389286d09303d5',\n       'roda-sentinel2/workflow-sentinel2-to-stac/9ed53be7f1a72cb5913c128c31eb27b0',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b40cc8ad73175b8ce928a3bc78e9ae28'],\n      dtype='&lt;U74')</pre></li><li>raster:bands(band)object[{'nodata': 0, 'data_type': 'uin...<pre>array([list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'scale': 0.001, 'offset': 0}]),\n       None, None, None, None, None, None, None, None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint8', 'spatial_resolution': 20}]),\n       None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'unit': 'cm', 'scale': 0.001, 'offset': 0}]),\n       list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'scale': 0.001, 'offset': 0}]),\n       None, None, None, None, None, None, None, None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint8', 'spatial_resolution': 20}]),\n       None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'unit': 'cm', 'scale': 0.001, 'offset': 0}])],\n      dtype=object)</pre></li><li>title(band)&lt;U31'Aerosol optical thickness (AOT)...<pre>array(['Aerosol optical thickness (AOT)', 'Blue (band 2) - 10m',\n       'Coastal aerosol (band 1) - 60m', 'Green (band 3) - 10m',\n       'NIR 1 (band 8) - 10m', 'NIR 2 (band 8A) - 20m',\n       'NIR 3 (band 9) - 60m', 'Red (band 4) - 10m',\n       'Red edge 1 (band 5) - 20m', 'Red edge 2 (band 6) - 20m',\n       'Red edge 3 (band 7) - 20m', 'Scene classification map (SCL)',\n       'SWIR 1 (band 11) - 20m', 'SWIR 2 (band 12) - 20m',\n       'True color image', 'Water vapour (WVP)',\n       'Aerosol optical thickness (AOT)', 'Blue (band 2) - 10m',\n       'Coastal aerosol (band 1) - 60m', 'Green (band 3) - 10m',\n       'NIR 1 (band 8) - 10m', 'NIR 2 (band 8A) - 20m',\n       'NIR 3 (band 9) - 60m', 'Red (band 4) - 10m',\n       'Red edge 1 (band 5) - 20m', 'Red edge 2 (band 6) - 20m',\n       'Red edge 3 (band 7) - 20m', 'Scene classification map (SCL)',\n       'SWIR 1 (band 11) - 20m', 'SWIR 2 (band 12) - 20m',\n       'True color image', 'Water vapour (WVP)'], dtype='&lt;U31')</pre></li><li>gsd(band)objectNone 10 60 10 ... 20 20 None None<pre>array([None, 10, 60, 10, 10, 20, 60, 10, 20, 20, 20, None, 20, 20, None,\n       None, None, 10, 60, 10, 10, 20, 60, 10, 20, 20, 20, None, 20, 20,\n       None, None], dtype=object)</pre></li><li>common_name(band)objectNone 'blue' 'coastal' ... None None<pre>array([None, 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09', 'red',\n       'rededge', 'rededge', 'rededge', None, 'swir16', 'swir22', None,\n       None, None, 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09',\n       'red', 'rededge', 'rededge', 'rededge', None, 'swir16', 'swir22',\n       None, None], dtype=object)</pre></li><li>center_wavelength(band)objectNone 0.49 0.443 ... 2.19 None None<pre>array([None, 0.49, 0.443, 0.56, 0.842, 0.865, 0.945, 0.665, 0.704, 0.74,\n       0.783, None, 1.61, 2.19, None, None, None, 0.49, 0.443, 0.56,\n       0.842, 0.865, 0.945, 0.665, 0.704, 0.74, 0.783, None, 1.61, 2.19,\n       None, None], dtype=object)</pre></li><li>full_width_half_max(band)objectNone 0.098 0.027 ... None None<pre>array([None, 0.098, 0.027, 0.045, 0.145, 0.033, 0.026, 0.038, 0.019,\n       0.018, 0.028, None, 0.143, 0.242, None, None, None, 0.098, 0.027,\n       0.045, 0.145, 0.033, 0.026, 0.038, 0.019, 0.018, 0.028, None,\n       0.143, 0.242, None, None], dtype=object)</pre></li><li>epsg()int3232632<pre>array(32632)</pre></li></ul></li><li>Indexes: (4)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2021-04-01 10:27:39.520000', '2021-04-08 10:17:41.934000',\n               '2021-04-08 10:17:41.935000', '2021-04-16 10:27:35.384000',\n               '2021-04-16 10:27:35.385000', '2021-04-21 10:27:35.793000',\n               '2021-04-23 10:17:40.745000', '2021-04-23 10:17:40.747000',\n               '2021-04-28 10:17:40.197000', '2021-05-03 10:17:42.916000',\n               '2021-05-08 10:17:42.592000', '2021-05-08 10:17:42.594000',\n               '2021-05-23 10:17:45.371000', '2021-05-26 10:27:42.080000',\n               '2021-05-28 10:17:45.418000', '2021-05-28 10:17:45.421000',\n               '2021-05-31 10:27:42.147000', '2021-05-31 10:27:42.149000',\n               '2021-06-02 10:17:45.639000', '2021-06-02 10:17:45.641000',\n               '2021-06-10 10:27:42.429000', '2021-06-15 10:27:41.696000',\n               '2021-06-15 10:27:41.697000', '2021-06-17 10:17:45.721000',\n               '2021-06-17 10:17:45.724000', '2021-06-22 10:17:45.729000',\n               '2021-06-22 10:17:45.732000', '2021-06-27 10:17:45.604000',\n               '2021-06-27 10:17:45.605000', '2021-07-02 10:17:46.959000',\n               '2021-07-02 10:17:46.961000', '2021-07-10 10:27:43.233000',\n               '2021-07-12 10:17:47.866000', '2021-07-20 10:27:43.424000',\n               '2021-07-22 10:17:48.210000', '2021-08-06 10:17:45.959000',\n               '2021-08-09 10:27:42.199000', '2021-08-11 10:17:47.398000',\n               '2021-08-14 10:27:43.878000', '2021-08-16 10:17:44.680000',\n               '2021-08-19 10:27:40.757000', '2021-08-21 10:17:47.043000'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>bandPandasIndex<pre>PandasIndex(Index(['aot', 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09', 'red',\n       'rededge1', 'rededge2', 'rededge3', 'scl', 'swir16', 'swir22', 'visual',\n       'wvp', 'aot-jp2', 'blue-jp2', 'coastal-jp2', 'green-jp2', 'nir-jp2',\n       'nir08-jp2', 'nir09-jp2', 'red-jp2', 'rededge1-jp2', 'rededge2-jp2',\n       'rededge3-jp2', 'scl-jp2', 'swir16-jp2', 'swir22-jp2', 'visual-jp2',\n       'wvp-jp2'],\n      dtype='object', name='band'))</pre></li><li>xPandasIndex<pre>PandasIndex(Float64Index([600000.0, 600010.0, 600020.0, 600030.0, 600040.0, 600050.0,\n              600060.0, 600070.0, 600080.0, 600090.0,\n              ...\n              709700.0, 709710.0, 709720.0, 709730.0, 709740.0, 709750.0,\n              709760.0, 709770.0, 709780.0, 709790.0],\n             dtype='float64', name='x', length=10980))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5300040.0, 5300030.0, 5300020.0, 5300010.0, 5300000.0, 5299990.0,\n              5299980.0, 5299970.0, 5299960.0, 5299950.0,\n              ...\n              5190340.0, 5190330.0, 5190320.0, 5190310.0, 5190300.0, 5190290.0,\n              5190280.0, 5190270.0, 5190260.0, 5190250.0],\n             dtype='float64', name='y', length=10980))</pre></li></ul></li><li>Attributes: (4)spec :RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0, 5300040.0), resolutions_xy=(10.0, 10.0))crs :epsg:32632transform :| 10.00, 0.00, 600000.00| | 0.00,-10.00, 5300040.00| | 0.00, 0.00, 1.00|resolution :10.0</li></ul> <p>We are only interested in a relatively small area around our main point of interest (defined above for intersection). So we convert the coordinates of this point into the native CRS of the satellite data, add a buffer and then use this rectangle to slice (\"clip\") the stacked array spatially.</p> In\u00a0[36]: Copied! <pre>x_utm, y_utm = pyproj.Proj(stack.crs)(lon, lat)     # Re-project the geographic point coordinates to the sat data CRS (i.e. EPSG:32632)\nbuffer = 5000  # meters\n</pre> x_utm, y_utm = pyproj.Proj(stack.crs)(lon, lat)     # Re-project the geographic point coordinates to the sat data CRS (i.e. EPSG:32632) buffer = 5000  # meters <p>Now we have a much smaller DataArray (with 1000 by 1000 cells in x/y) called 'stack_aoi'.</p> In\u00a0[37]: Copied! <pre>stack_aoi = stack.loc[..., y_utm+buffer:y_utm-buffer, x_utm-buffer:x_utm+buffer]\nstack_aoi\n</pre> stack_aoi = stack.loc[..., y_utm+buffer:y_utm-buffer, x_utm-buffer:x_utm+buffer] stack_aoi Out[37]: <pre>&lt;xarray.DataArray 'stackstac-d35de160ec768a0c51870cec97c80190' (time: 42,\n                                                                band: 32,\n                                                                y: 1000, x: 1000)&gt;\ndask.array&lt;getitem, shape=(42, 32, 1000, 1000), dtype=float64, chunksize=(1, 1, 678, 610), chunktype=numpy.ndarray&gt;\nCoordinates: (12/52)\n  * time                                     (time) datetime64[ns] 2021-04-01...\n    id                                       (time) &lt;U24 'S2B_32TPT_20210401_...\n  * band                                     (band) &lt;U12 'aot' ... 'wvp-jp2'\n  * x                                        (x) float64 6.758e+05 ... 6.858e+05\n  * y                                        (y) float64 5.242e+06 ... 5.232e+06\n    s2:medium_proba_clouds_percentage        (time) float64 1.463 16.5 ... 2.701\n    ...                                       ...\n    title                                    (band) &lt;U31 'Aerosol optical thi...\n    gsd                                      (band) object None 10 ... None None\n    common_name                              (band) object None 'blue' ... None\n    center_wavelength                        (band) object None 0.49 ... None\n    full_width_half_max                      (band) object None 0.098 ... None\n    epsg                                     int32 32632\nAttributes:\n    spec:        RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0...\n    crs:         epsg:32632\n    transform:   | 10.00, 0.00, 600000.00|\\n| 0.00,-10.00, 5300040.00|\\n| 0.0...\n    resolution:  10.0</pre>xarray.DataArray'stackstac-d35de160ec768a0c51870cec97c80190'<ul><li>time: 42</li><li>band: 32</li><li>y: 1000</li><li>x: 1000</li></ul><ul><li>dask.array&lt;chunksize=(1, 1, 322, 610), meta=np.ndarray&gt;  Array   Chunk   Bytes   10.01 GiB   3.16 MiB   Shape   (42, 32, 1000, 1000)   (1, 1, 678, 610)   Dask graph   5376 chunks in 4 graph layers   Data type   float64 numpy.ndarray  42 1 1000 1000 32 </li><li>Coordinates: (52)<ul><li>time(time)datetime64[ns]2021-04-01T10:27:39.520000 ... 2...<pre>array(['2021-04-01T10:27:39.520000000', '2021-04-08T10:17:41.934000000',\n       '2021-04-08T10:17:41.935000000', '2021-04-16T10:27:35.384000000',\n       '2021-04-16T10:27:35.385000000', '2021-04-21T10:27:35.793000000',\n       '2021-04-23T10:17:40.745000000', '2021-04-23T10:17:40.747000000',\n       '2021-04-28T10:17:40.197000000', '2021-05-03T10:17:42.916000000',\n       '2021-05-08T10:17:42.592000000', '2021-05-08T10:17:42.594000000',\n       '2021-05-23T10:17:45.371000000', '2021-05-26T10:27:42.080000000',\n       '2021-05-28T10:17:45.418000000', '2021-05-28T10:17:45.421000000',\n       '2021-05-31T10:27:42.147000000', '2021-05-31T10:27:42.149000000',\n       '2021-06-02T10:17:45.639000000', '2021-06-02T10:17:45.641000000',\n       '2021-06-10T10:27:42.429000000', '2021-06-15T10:27:41.696000000',\n       '2021-06-15T10:27:41.697000000', '2021-06-17T10:17:45.721000000',\n       '2021-06-17T10:17:45.724000000', '2021-06-22T10:17:45.729000000',\n       '2021-06-22T10:17:45.732000000', '2021-06-27T10:17:45.604000000',\n       '2021-06-27T10:17:45.605000000', '2021-07-02T10:17:46.959000000',\n       '2021-07-02T10:17:46.961000000', '2021-07-10T10:27:43.233000000',\n       '2021-07-12T10:17:47.866000000', '2021-07-20T10:27:43.424000000',\n       '2021-07-22T10:17:48.210000000', '2021-08-06T10:17:45.959000000',\n       '2021-08-09T10:27:42.199000000', '2021-08-11T10:17:47.398000000',\n       '2021-08-14T10:27:43.878000000', '2021-08-16T10:17:44.680000000',\n       '2021-08-19T10:27:40.757000000', '2021-08-21T10:17:47.043000000'],\n      dtype='datetime64[ns]')</pre></li><li>id(time)&lt;U24'S2B_32TPT_20210401_0_L2A' ... '...<pre>array(['S2B_32TPT_20210401_0_L2A', 'S2B_32TPT_20210408_2_L2A',\n       'S2B_32TPT_20210408_0_L2A', 'S2A_32TPT_20210416_1_L2A',\n       'S2A_32TPT_20210416_0_L2A', 'S2B_32TPT_20210421_0_L2A',\n       'S2A_32TPT_20210423_2_L2A', 'S2A_32TPT_20210423_0_L2A',\n       'S2B_32TPT_20210428_0_L2A', 'S2A_32TPT_20210503_0_L2A',\n       'S2B_32TPT_20210508_1_L2A', 'S2B_32TPT_20210508_0_L2A',\n       'S2A_32TPT_20210523_0_L2A', 'S2A_32TPT_20210526_0_L2A',\n       'S2B_32TPT_20210528_1_L2A', 'S2B_32TPT_20210528_0_L2A',\n       'S2B_32TPT_20210531_1_L2A', 'S2B_32TPT_20210531_0_L2A',\n       'S2A_32TPT_20210602_1_L2A', 'S2A_32TPT_20210602_0_L2A',\n       'S2B_32TPT_20210610_0_L2A', 'S2A_32TPT_20210615_0_L2A',\n       'S2A_32TPT_20210615_1_L2A', 'S2B_32TPT_20210617_1_L2A',\n       'S2B_32TPT_20210617_0_L2A', 'S2A_32TPT_20210622_1_L2A',\n       'S2A_32TPT_20210622_0_L2A', 'S2B_32TPT_20210627_1_L2A',\n       'S2B_32TPT_20210627_0_L2A', 'S2A_32TPT_20210702_1_L2A',\n       'S2A_32TPT_20210702_0_L2A', 'S2B_32TPT_20210710_0_L2A',\n       'S2A_32TPT_20210712_0_L2A', 'S2B_32TPT_20210720_0_L2A',\n       'S2A_32TPT_20210722_0_L2A', 'S2B_32TPT_20210806_0_L2A',\n       'S2B_32TPT_20210809_0_L2A', 'S2A_32TPT_20210811_0_L2A',\n       'S2A_32TPT_20210814_0_L2A', 'S2B_32TPT_20210816_0_L2A',\n       'S2B_32TPT_20210819_0_L2A', 'S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U24')</pre></li><li>band(band)&lt;U12'aot' 'blue' ... 'wvp-jp2'<pre>array(['aot', 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09', 'red',\n       'rededge1', 'rededge2', 'rededge3', 'scl', 'swir16', 'swir22', 'visual',\n       'wvp', 'aot-jp2', 'blue-jp2', 'coastal-jp2', 'green-jp2', 'nir-jp2',\n       'nir08-jp2', 'nir09-jp2', 'red-jp2', 'rededge1-jp2', 'rededge2-jp2',\n       'rededge3-jp2', 'scl-jp2', 'swir16-jp2', 'swir22-jp2', 'visual-jp2',\n       'wvp-jp2'], dtype='&lt;U12')</pre></li><li>x(x)float646.758e+05 6.758e+05 ... 6.858e+05<pre>array([675820., 675830., 675840., ..., 685790., 685800., 685810.])</pre></li><li>y(y)float645.242e+06 5.242e+06 ... 5.232e+06<pre>array([5241820., 5241810., 5241800., ..., 5231850., 5231840., 5231830.])</pre></li><li>s2:medium_proba_clouds_percentage(time)float641.463 16.5 14.33 ... 4.624 2.701<pre>array([ 1.46334 , 16.504081, 14.328757, 10.298105, 16.633525,  7.706635,\n        1.899826,  2.313006,  3.444555, 12.165798,  6.054812,  6.262551,\n        8.735961,  8.490147,  6.304692,  6.609003,  0.209097,  0.926137,\n       12.20682 ,  3.0033  ,  6.494882,  0.308812,  0.057903,  9.846946,\n        2.624075, 10.38753 ,  2.671621,  2.953941,  1.214524, 14.579228,\n        6.361181,  1.917733,  5.046743,  5.886542,  3.49712 ,  5.288142,\n        8.231435,  4.027119,  0.841558, 19.751492,  4.62429 ,  2.701251])</pre></li><li>earthsearch:boa_offset_applied(time)boolFalse True False ... False False<pre>array([False,  True, False,  True, False, False,  True, False, False,\n       False,  True, False, False, False,  True, False,  True, False,\n        True, False, False, False,  True,  True, False,  True, False,\n        True, False,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False])</pre></li><li>processing:software(time)object{'sentinel2-to-stac': '0.1.0'} ....<pre>array([{'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'}],\n      dtype=object)</pre></li><li>proj:epsg()int3232632<pre>array(32632)</pre></li><li>s2:generation_time(time)&lt;U27'2021-04-01T14:19:13.000000Z' .....<pre>array(['2021-04-01T14:19:13.000000Z', '2023-05-13T12:44:55.000000Z',\n       '2021-04-08T13:26:17.000000Z', '2023-05-19T13:18:55.000000Z',\n       '2021-04-16T13:20:57.000000Z', '2021-04-21T13:46:13.000000Z',\n       '2023-05-12T19:45:03.000000Z', '2021-04-23T12:55:30.000000Z',\n       '2021-04-28T13:40:12.000000Z', '2021-05-03T12:17:41.000000Z',\n       '2023-02-27T09:29:40.000000Z', '2021-05-08T14:56:55.000000Z',\n       '2021-05-23T12:13:33.000000Z', '2021-05-26T13:34:25.000000Z',\n       '2023-02-27T08:23:12.000000Z', '2021-05-28T13:08:36.000000Z',\n       '2023-03-01T00:24:41.000000Z', '2021-05-31T14:00:40.000000Z',\n       '2023-03-13T04:04:11.000000Z', '2021-06-02T17:13:07.000000Z',\n       '2021-06-10T13:27:39.000000Z', '2021-06-15T13:16:59.000000Z',\n       '2023-03-21T22:10:50.000000Z', '2023-03-21T22:17:28.000000Z',\n       '2021-06-17T13:12:19.000000Z', '2023-01-29T15:30:31.000000Z',\n       '2021-06-22T12:13:05.000000Z', '2023-03-18T23:38:59.000000Z',\n       '2021-06-27T13:58:56.000000Z', '2023-01-25T13:05:14.000000Z',\n       '2021-07-02T12:13:53.000000Z', '2021-07-10T13:23:21.000000Z',\n       '2021-07-12T13:22:19.000000Z', '2021-07-20T13:22:07.000000Z',\n       '2021-07-22T12:02:14.000000Z', '2021-08-06T15:00:37.000000Z',\n       '2021-08-14T13:01:57.000000Z', '2021-08-11T16:05:21.000000Z',\n       '2021-08-14T13:23:26.000000Z', '2021-08-16T13:08:48.000000Z',\n       '2021-08-19T13:53:08.000000Z', '2021-08-21T12:05:55.000000Z'],\n      dtype='&lt;U27')</pre></li><li>created(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-03T11:38:25.089Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-03T11:19:16.880Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-03T11:12:48.494Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:dark_features_percentage(time)float647.618 3.489 2.558 ... 2.943 2.726<pre>array([7.618114, 3.489247, 2.558084, 5.739467, 4.485189, 2.807637,\n       5.218234, 2.874529, 1.733305, 1.861065, 2.493159, 1.334918,\n       0.416572, 1.220573, 1.255598, 0.764608, 2.384401, 1.463244,\n       0.931227, 1.316954, 1.090162, 0.8525  , 1.352069, 0.733103,\n       1.135998, 1.516507, 1.200472, 1.417252, 0.957145, 1.22749 ,\n       1.375679, 1.512192, 0.560178, 1.777653, 1.21489 , 2.177998,\n       2.577018, 1.8398  , 3.048143, 1.672873, 2.943032, 2.726397])</pre></li><li>s2:datatake_type()&lt;U8'INS-NOBS'<pre>array('INS-NOBS', dtype='&lt;U8')</pre></li><li>s2:water_percentage(time)float641.035 0.3132 ... 0.3022 0.8797<pre>array([1.034616, 0.313168, 0.424308, 0.617732, 0.819981, 0.987005,\n       0.857674, 1.026752, 0.751145, 0.66969 , 0.589903, 0.582099,\n       0.374695, 0.464075, 0.43538 , 0.426498, 0.905694, 0.953868,\n       0.768842, 1.276473, 0.616264, 0.743187, 0.848236, 0.84215 ,\n       1.075244, 0.699068, 0.829451, 0.73024 , 0.757141, 0.394683,\n       0.510065, 0.539884, 0.785743, 0.768489, 1.082259, 0.712835,\n       0.559107, 0.910735, 1.067886, 0.465221, 0.302152, 0.879734])</pre></li><li>eo:cloud_cover(time)float646.149 45.79 45.55 ... 44.97 11.88<pre>array([ 6.149054, 45.791623, 45.550924, 25.692633, 33.007648, 43.998277,\n        3.953265,  4.693773, 35.548758, 39.265612, 15.81282 , 15.84441 ,\n       49.168627, 45.008284, 26.705834, 28.351225,  0.365331,  1.409963,\n       33.068579, 24.769196, 47.15134 ,  1.805315,  1.849951, 26.098341,\n       19.611905, 25.549465, 18.602191, 10.19    ,  9.07656 , 44.637647,\n       35.395256, 16.658998, 10.469738, 24.9592  , 18.490819, 28.947858,\n       27.45625 , 24.8152  ,  3.156408, 49.208091, 44.972815, 11.884551])</pre></li><li>view:sun_azimuth(time)float64161.2 157.5 157.5 ... 158.4 155.0<pre>array([161.22281178, 157.46645516, 157.46542095, 160.80126617,\n       160.80014335, 160.57908911, 156.51420567, 156.51306183,\n       156.0684864 , 155.56597523, 154.95741812, 154.95616171,\n       152.69773459, 156.88007906, 151.8422183 , 151.84086683,\n       156.07844848, 156.07704495, 150.97550979, 150.97415895,\n       154.47087908, 153.7373982 , 153.738758  , 148.65682968,\n       148.65553825, 148.11111459, 148.10983402, 147.7307707 ,\n       147.72954199, 147.54553355, 147.54435203, 152.24342398,\n       147.75100857, 152.92620053, 148.7434059 , 151.42303732,\n       156.18470534, 152.56163454, 157.28195674, 153.75067448,\n       158.41022444, 155.02778045])</pre></li><li>s2:sequence(time)&lt;U1'0' '2' '0' '1' ... '0' '0' '0' '0'<pre>array(['0', '2', '0', '1', '0', '0', '2', '0', '0', '0', '1', '0', '0',\n       '0', '1', '0', '1', '0', '1', '0', '0', '0', '1', '1', '0', '1',\n       '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n       '0', '0', '0'], dtype='&lt;U1')</pre></li><li>s2:datastrip_id(time)&lt;U64'S2B_OPER_MSI_L2A_DS_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_DS_VGS4_20210401T141913_S20210401T102326_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230513T124455_S20210408T101148_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210408T132617_S20210408T101148_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230519T131855_S20210416T102600_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210416T132057_S20210416T102600_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210421T134613_S20210421T102336_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230512T194503_S20210423T101704_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210423T125530_S20210423T101704_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210428T134012_S20210428T101657_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210503T121741_S20210503T101204_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T092940_S20210508T101551_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210508T145655_S20210508T101551_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210523T121333_S20210523T101416_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210526T133425_S20210526T102625_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T082312_S20210528T100555_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210528T130836_S20210528T100555_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230301T002441_S20210531T102618_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210531T140040_S20210531T102618_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230313T040411_S20210602T101235_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210602T171307_S20210602T101235_N03.00',\n...\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230321T221728_S20210617T101301_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210617T131219_S20210617T101301_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230129T153031_S20210622T101416_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210622T121305_S20210622T101416_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230318T233859_S20210627T101702_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210627T135856_S20210627T101702_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230125T130514_S20210702T101157_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210702T121353_S20210702T101157_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210710T132321_S20210710T102312_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210712T132219_S20210712T101027_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210720T132207_S20210720T101617_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210722T120214_S20210722T101419_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210806T150037_S20210806T101207_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS1_20210814T130157_S20210809T102637_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210811T160521_S20210811T101411_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210814T132326_S20210814T102609_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210816T130848_S20210816T101300_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210819T135308_S20210819T102635_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210821T120555_S20210821T101418_N03.01'],\n      dtype='&lt;U64')</pre></li><li>mgrs:grid_square()&lt;U2'PT'<pre>array('PT', dtype='&lt;U2')</pre></li><li>instruments()&lt;U3'msi'<pre>array('msi', dtype='&lt;U3')</pre></li><li>s2:thin_cirrus_percentage(time)float640.4854 0.4475 ... 0.004964 3.404<pre>array([4.8540100e-01, 4.4747100e-01, 1.9159610e+00, 2.4920600e-01,\n       8.8436700e-01, 2.2049110e+00, 8.2425000e-02, 7.5833000e-02,\n       1.1564730e+00, 2.1922420e+00, 8.2073200e+00, 7.6982920e+00,\n       1.3726480e+00, 2.5636340e+00, 1.5205705e+01, 1.6151080e+01,\n       4.1297000e-02, 4.4017000e-02, 5.1187000e-01, 8.5886000e-01,\n       1.7029480e+00, 1.1063510e+00, 1.7621680e+00, 1.0222800e-01,\n       2.7094100e-01, 1.3869100e-01, 2.2755900e-01, 4.5100390e+00,\n       4.5370160e+00, 5.0657700e+00, 3.5785940e+00, 9.4725660e+00,\n       3.9136060e+00, 2.2286300e-01, 5.0741000e-02, 8.8339000e-02,\n       8.7864210e+00, 1.1323400e-01, 3.8490000e-03, 8.7228200e-01,\n       4.9640000e-03, 3.4040560e+00])</pre></li><li>s2:vegetation_percentage(time)float6426.34 3.007 3.274 ... 36.39 66.22<pre>array([26.339936,  3.007327,  3.273584, 11.632005, 12.624834, 18.579322,\n       34.706536, 36.717805, 22.266363, 18.495288, 33.440745, 34.884864,\n       14.592455, 19.679427, 29.31349 , 30.193511, 59.83513 , 60.953248,\n       39.538234, 42.743748, 29.41438 , 71.402311, 70.151877, 51.991493,\n       55.270773, 53.398359, 56.250197, 68.942064, 69.730705, 38.577574,\n       43.56502 , 66.226125, 71.196151, 49.621934, 62.576228, 49.759224,\n       54.908115, 55.141944, 78.077263, 32.121503, 36.389408, 66.223991])</pre></li><li>s2:high_proba_clouds_percentage(time)float644.2 28.84 29.31 ... 40.34 5.779<pre>array([4.2003130e+00, 2.8840071e+01, 2.9306206e+01, 1.5145321e+01,\n       1.5489756e+01, 3.4086731e+01, 1.9710150e+00, 2.3049340e+00,\n       3.0947730e+01, 2.4907573e+01, 1.5506890e+00, 1.8835670e+00,\n       3.9060017e+01, 3.3954504e+01, 5.1954370e+00, 5.5911420e+00,\n       1.1493700e-01, 4.3980900e-01, 2.0349889e+01, 2.0907035e+01,\n       3.8953510e+01, 3.9015200e-01, 2.9880000e-02, 1.6149166e+01,\n       1.6716889e+01, 1.5023246e+01, 1.5703011e+01, 2.7260200e+00,\n       3.3250190e+00, 2.4992649e+01, 2.5455481e+01, 5.2686990e+00,\n       1.5093900e+00, 1.8849795e+01, 1.4942957e+01, 2.3571377e+01,\n       1.0438395e+01, 2.0674847e+01, 2.3110010e+00, 2.8584316e+01,\n       4.0343562e+01, 5.7792450e+00])</pre></li><li>s2:unclassified_percentage(time)float645.542 1.249 4.882 ... 4.302 3.051<pre>array([5.542233, 1.249116, 4.882131, 1.901653, 8.202816, 3.343896,\n       0.369042, 4.536037, 5.607506, 5.386119, 0.42674 , 5.226327,\n       5.411838, 6.440622, 1.1914  , 7.262506, 0.095849, 2.831393,\n       0.502138, 3.365285, 5.884814, 2.064657, 0.053503, 0.43447 ,\n       3.136941, 0.979966, 3.44101 , 0.372121, 1.875458, 1.251701,\n       4.816858, 1.852196, 2.070124, 6.984119, 4.280468, 5.570791,\n       3.381409, 4.446889, 1.608293, 5.839212, 4.302198, 3.050546])</pre></li><li>s2:snow_ice_percentage(time)float6440.65 40.93 39.09 ... 1.161 0.858<pre>array([40.654615, 40.934548, 39.088675, 45.368615, 33.28197 , 21.687628,\n       44.689843, 40.108246, 25.425422, 26.355195, 37.636858, 35.119075,\n       27.416307, 21.791345, 30.055815, 26.366636, 27.95321 , 24.107485,\n       13.354345, 16.522175,  8.769823, 12.960622, 16.334337,  8.544581,\n        8.742414,  6.522872,  6.389479,  7.112196,  5.816885,  4.461629,\n        4.215506,  2.77441 ,  3.27045 ,  1.624512,  1.15363 ,  1.572428,\n        1.036823,  0.380159,  0.842441,  2.167679,  1.161288,  0.857967])</pre></li><li>earthsearch:s3_path(time)&lt;U79's3://sentinel-cogs/sentinel-s2-...<pre>array(['s3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210401_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210421_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210428_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210503_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210523_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210526_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_0_L2A',\n...\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210710_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210712_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210720_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210722_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210806_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210809_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210811_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210814_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210816_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210819_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U79')</pre></li><li>s2:granule_id(time)&lt;U62'S2B_OPER_MSI_L2A_TL_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_TL_VGS4_20210401T141913_A021255_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230513T124455_A021355_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210408T132617_A021355_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230519T131855_A030378_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210416T132057_A030378_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210421T134613_A021541_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230512T194503_A030478_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210423T125530_A030478_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210428T134012_A021641_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210503T121741_A030621_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T092940_A021784_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210508T145655_A021784_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210523T121333_A030907_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210526T133425_A030950_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T082312_A022070_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210528T130836_A022070_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230301T002441_A022113_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210531T140040_A022113_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230313T040411_A031050_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210602T171307_A031050_T32TPT_N03.00',\n...\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230321T221728_A022356_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210617T131219_A022356_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230129T153031_A031336_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210622T121305_A031336_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230318T233859_A022499_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210627T135856_A022499_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230125T130514_A031479_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210702T121353_A031479_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210710T132321_A022685_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210712T132219_A031622_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210720T132207_A022828_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210722T120214_A031765_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210806T150037_A023071_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS1_20210814T130157_A023114_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210811T160521_A032051_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210814T132326_A032094_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210816T130848_A023214_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210819T135308_A023257_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210821T120555_A032194_T32TPT_N03.01'],\n      dtype='&lt;U62')</pre></li><li>constellation()&lt;U10'sentinel-2'<pre>array('sentinel-2', dtype='&lt;U10')</pre></li><li>s2:reflectance_conversion_factor(time)float641.004 0.9999 ... 0.9753 0.9761<pre>array([1.00391722, 0.99989016, 0.99989016, 0.99531243, 0.99531243,\n       0.99251188, 0.99141059, 0.99141059, 0.98870861, 0.98609676,\n       0.98359974, 0.98359974, 0.97693835, 0.97577919, 0.97504789,\n       0.97504789, 0.97399904, 0.97399904, 0.97334113, 0.97334113,\n       0.97102521, 0.96985123, 0.96985123, 0.969445  , 0.969445  ,\n       0.96858002, 0.96858002, 0.96794362, 0.96794362, 0.96753822,\n       0.96753822, 0.96737784, 0.96743179, 0.96802315, 0.96826343,\n       0.97121659, 0.97204341, 0.9726335 , 0.97358286, 0.97425016,\n       0.97531576, 0.97606015])</pre></li><li>view:sun_elevation(time)float6445.93 48.03 48.03 ... 53.63 52.36<pre>array([45.92877963, 48.02966446, 48.0295066 , 51.57725751, 51.57711889,\n       53.32274987, 53.3762054 , 53.37603569, 54.97817306, 56.47292004,\n       57.84102398, 57.84083568, 61.10137652, 62.31302901, 61.87597965,\n       61.87575968, 63.010744  , 63.01055231, 62.48509786, 62.4848721 ,\n       63.89999116, 64.08851703, 64.08872077, 63.30105997, 63.30082564,\n       63.24315182, 63.24291386, 63.02832992, 63.02809443, 62.66520394,\n       62.66497104, 62.62975879, 61.51305241, 61.04463203, 59.85139625,\n       56.52628032, 56.4892853 , 55.22451323, 55.10440682, 53.83083839,\n       53.63004967, 52.36396242])</pre></li><li>platform(time)&lt;U11'sentinel-2b' ... 'sentinel-2a'<pre>array(['sentinel-2b', 'sentinel-2b', 'sentinel-2b', 'sentinel-2a',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a'], dtype='&lt;U11')</pre></li><li>mgrs:latitude_band()&lt;U1'T'<pre>array('T', dtype='&lt;U1')</pre></li><li>s2:product_uri(time)&lt;U65'S2B_MSIL2A_20210401T101559_N030...<pre>array(['S2B_MSIL2A_20210401T101559_N0300_R065_T32TPT_20210401T141913.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0500_R022_T32TPT_20230513T124455.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0300_R022_T32TPT_20210408T132617.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0500_R065_T32TPT_20230519T131855.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0300_R065_T32TPT_20210416T132057.SAFE',\n       'S2B_MSIL2A_20210421T101549_N0300_R065_T32TPT_20210421T134613.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0500_R022_T32TPT_20230512T194503.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0300_R022_T32TPT_20210423T125530.SAFE',\n       'S2B_MSIL2A_20210428T100549_N0300_R022_T32TPT_20210428T134012.SAFE',\n       'S2A_MSIL2A_20210503T101021_N0300_R022_T32TPT_20210503T121741.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0500_R022_T32TPT_20230227T092940.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0300_R022_T32TPT_20210508T145655.SAFE',\n       'S2A_MSIL2A_20210523T101031_N0300_R022_T32TPT_20210523T121333.SAFE',\n       'S2A_MSIL2A_20210526T102021_N0300_R065_T32TPT_20210526T133425.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0500_R022_T32TPT_20230227T082312.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0300_R022_T32TPT_20210528T130836.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0500_R065_T32TPT_20230301T002441.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0300_R065_T32TPT_20210531T140040.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0500_R022_T32TPT_20230313T040411.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0300_R022_T32TPT_20210602T171307.SAFE',\n...\n       'S2B_MSIL2A_20210617T100559_N0500_R022_T32TPT_20230321T221728.SAFE',\n       'S2B_MSIL2A_20210617T100559_N0300_R022_T32TPT_20210617T131219.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0500_R022_T32TPT_20230129T153031.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0300_R022_T32TPT_20210622T121305.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0500_R022_T32TPT_20230318T233859.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0300_R022_T32TPT_20210627T135856.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0500_R022_T32TPT_20230125T130514.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0301_R022_T32TPT_20210702T121353.SAFE',\n       'S2B_MSIL2A_20210710T101559_N0301_R065_T32TPT_20210710T132321.SAFE',\n       'S2A_MSIL2A_20210712T101031_N0301_R022_T32TPT_20210712T132219.SAFE',\n       'S2B_MSIL2A_20210720T101559_N0301_R065_T32TPT_20210720T132207.SAFE',\n       'S2A_MSIL2A_20210722T101031_N0301_R022_T32TPT_20210722T120214.SAFE',\n       'S2B_MSIL2A_20210806T100559_N0301_R022_T32TPT_20210806T150037.SAFE',\n       'S2B_MSIL2A_20210809T101559_N0301_R065_T32TPT_20210814T130157.SAFE',\n       'S2A_MSIL2A_20210811T101031_N0301_R022_T32TPT_20210811T160521.SAFE',\n       'S2A_MSIL2A_20210814T102031_N0301_R065_T32TPT_20210814T132326.SAFE',\n       'S2B_MSIL2A_20210816T100559_N0301_R022_T32TPT_20210816T130848.SAFE',\n       'S2B_MSIL2A_20210819T101559_N0301_R065_T32TPT_20210819T135308.SAFE',\n       'S2A_MSIL2A_20210821T101031_N0301_R022_T32TPT_20210821T120555.SAFE'],\n      dtype='&lt;U65')</pre></li><li>updated(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-06T14:14:42.036Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-06T04:47:25.002Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-06T04:45:55.146Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:product_type()&lt;U7'S2MSI2A'<pre>array('S2MSI2A', dtype='&lt;U7')</pre></li><li>s2:saturated_defective_pixel_percentage()int320<pre>array(0)</pre></li><li>s2:degraded_msi_data_percentage(time)object0 0.1898 0 0.0825 0 0 ... 0 0 0 0 0<pre>array([0, 0.1898, 0, 0.0825, 0, 0, 0.0161, 0, 0, 0, 0.0259, 0, 0, 0,\n       0.0119, 0, 0.0204, 0, 0.0664, 0, 0, 0, 0.0225, 0.0382, 0, 0.0347,\n       0, 0.012, 0, 0.021, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      dtype=object)</pre></li><li>grid:code()&lt;U10'MGRS-32TPT'<pre>array('MGRS-32TPT', dtype='&lt;U10')</pre></li><li>s2:nodata_pixel_percentage(time)object0.002973 9.284004 ... 8.773684<pre>array([0.002973, 9.284004, 8.883444, 0, 3.6e-05, 6e-05, 9.639984,\n       9.534544, 8.656428, 9.141818, 9.115126, 8.715048, 8.926022,\n       0.001904, 8.633243, 8.246658, 0.023792, 0.013444, 9.047445,\n       8.948952, 0.009396, 0.000113, 1.7e-05, 8.783989, 8.390699, 9.33119,\n       9.234183, 8.918451, 8.525366, 9.049369, 8.950624, 0.010153,\n       8.783641, 0.010524, 8.777927, 8.489713, 0.004336, 8.84221,\n       0.003726, 8.610921, 0.000985, 8.773684], dtype=object)</pre></li><li>s2:cloud_shadow_percentage(time)float644.934 0.4877 2.028 ... 2.541 4.589<pre>array([4.934268, 0.487673, 2.027822, 0.962452, 3.386088, 5.310493,\n       0.704442, 2.288749, 3.619292, 4.858182, 0.008787, 0.596837,\n       1.866554, 1.606319, 0.439432, 0.873589, 0.13241 , 0.22497 ,\n       6.900295, 4.897763, 5.058461, 0.094422, 0.031267, 5.442425,\n       4.135132, 2.465164, 2.53105 , 0.682236, 0.868314, 3.576069,\n       2.60878 , 1.147319, 0.079854, 6.248215, 3.624339, 3.247687,\n       1.521886, 5.362982, 1.95151 , 2.662793, 2.541466, 4.588702])</pre></li><li>s2:not_vegetated_percentage(time)float647.727 4.727 2.194 ... 7.388 9.788<pre>array([ 7.727165,  4.727299,  2.194477,  8.085445,  4.191478,  3.285741,\n        9.500962,  7.754106,  5.04821 ,  3.108844,  9.590986,  6.41147 ,\n        0.752953,  3.789353, 10.603052,  5.761432,  8.327983,  8.055834,\n        4.936339,  5.108408,  2.014758, 10.076986,  9.378749,  5.913443,\n        6.891589,  8.868597, 10.756154, 10.553885, 10.917792,  5.873202,\n        7.512838,  9.288875, 11.567761,  8.01588 ,  7.577366,  8.01118 ,\n        8.559394,  7.102291, 10.248062,  5.862629,  7.38764 ,  9.788115])</pre></li><li>s2:datatake_id(time)&lt;U34'GS2B_20210401T101559_021255_N03...<pre>array(['GS2B_20210401T101559_021255_N03.00',\n       'GS2B_20210408T100549_021355_N05.00',\n       'GS2B_20210408T100549_021355_N03.00',\n       'GS2A_20210416T102021_030378_N05.00',\n       'GS2A_20210416T102021_030378_N03.00',\n       'GS2B_20210421T101549_021541_N03.00',\n       'GS2A_20210423T101021_030478_N05.00',\n       'GS2A_20210423T101021_030478_N03.00',\n       'GS2B_20210428T100549_021641_N03.00',\n       'GS2A_20210503T101021_030621_N03.00',\n       'GS2B_20210508T100549_021784_N05.00',\n       'GS2B_20210508T100549_021784_N03.00',\n       'GS2A_20210523T101031_030907_N03.00',\n       'GS2A_20210526T102021_030950_N03.00',\n       'GS2B_20210528T100559_022070_N05.00',\n       'GS2B_20210528T100559_022070_N03.00',\n       'GS2B_20210531T101559_022113_N05.00',\n       'GS2B_20210531T101559_022113_N03.00',\n       'GS2A_20210602T101031_031050_N05.00',\n       'GS2A_20210602T101031_031050_N03.00',\n...\n       'GS2A_20210615T102021_031236_N05.00',\n       'GS2B_20210617T100559_022356_N05.00',\n       'GS2B_20210617T100559_022356_N03.00',\n       'GS2A_20210622T101031_031336_N05.00',\n       'GS2A_20210622T101031_031336_N03.00',\n       'GS2B_20210627T100559_022499_N05.00',\n       'GS2B_20210627T100559_022499_N03.00',\n       'GS2A_20210702T101031_031479_N05.00',\n       'GS2A_20210702T101031_031479_N03.01',\n       'GS2B_20210710T101559_022685_N03.01',\n       'GS2A_20210712T101031_031622_N03.01',\n       'GS2B_20210720T101559_022828_N03.01',\n       'GS2A_20210722T101031_031765_N03.01',\n       'GS2B_20210806T100559_023071_N03.01',\n       'GS2B_20210809T101559_023114_N03.01',\n       'GS2A_20210811T101031_032051_N03.01',\n       'GS2A_20210814T102031_032094_N03.01',\n       'GS2B_20210816T100559_023214_N03.01',\n       'GS2B_20210819T101559_023257_N03.01',\n       'GS2A_20210821T101031_032194_N03.01'], dtype='&lt;U34')</pre></li><li>s2:processing_baseline(time)&lt;U5'03.00' '05.00' ... '03.01' '03.01'<pre>array(['03.00', '05.00', '03.00', '05.00', '03.00', '03.00', '05.00',\n       '03.00', '03.00', '03.00', '05.00', '03.00', '03.00', '03.00',\n       '05.00', '03.00', '05.00', '03.00', '05.00', '03.00', '03.00',\n       '03.00', '05.00', '05.00', '03.00', '05.00', '03.00', '05.00',\n       '03.00', '05.00', '03.01', '03.01', '03.01', '03.01', '03.01',\n       '03.01', '03.01', '03.01', '03.01', '03.01', '03.01', '03.01'],\n      dtype='&lt;U5')</pre></li><li>mgrs:utm_zone()int3232<pre>array(32)</pre></li><li>earthsearch:payload_id(time)&lt;U74'roda-sentinel2/workflow-sentine...<pre>array(['roda-sentinel2/workflow-sentinel2-to-stac/ca1fac60c9640967f45658d79d1523ba',\n       'roda-sentinel2/workflow-sentinel2-to-stac/d1881ee109d59d1df5a0cfc2d85ce430',\n       'roda-sentinel2/workflow-sentinel2-to-stac/f9aa8a3191a8be696d2ddddd9734621c',\n       'roda-sentinel2/workflow-sentinel2-to-stac/57765d545d606d43efe2820f6e0d1159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/1ac3805c5018ad2b5d5989908e95d38d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/21d39a00de8e19a12451c18ba24f832b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/778637c0ddea5fd397ac6695ffe344dd',\n       'roda-sentinel2/workflow-sentinel2-to-stac/29497ba4096ac30ecf99e9603229329b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/03a8dfb01cc4bd0fbcfbb149e25da06f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/c4e5ae224b1aa55e00c6903aa25f65ee',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4f41cddc1225338c2ce7a50a413d05ff',\n       'roda-sentinel2/workflow-sentinel2-to-stac/aa4edf7495a6f12421d1142a8371e750',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4a84c0a1c9ec26453a988d58b3a1e236',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b7752cd2d7341f1cdf30140de86e4992',\n       'roda-sentinel2/workflow-sentinel2-to-stac/8d9d4f770bdb5a0a1a614b2fde70290d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/050ec18e7aef56ed8ce1d702e586175d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/fef3c2c9435cfa8276430d9f2c8fbcdb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ae91b5b1381789ef2f526f90e0d7a89f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/e987ce36035c8be203930ef9d5bcfb55',\n       'roda-sentinel2/workflow-sentinel2-to-stac/64087c1420a6067f46412cfc0215cf07',\n...\n       'roda-sentinel2/workflow-sentinel2-to-stac/1dc42ce3106743876f3fb4adc694ce4d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/af36520f9b99c3e44bf2830bbb217b2b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/5065c12a38660808deee8961e203498a',\n       'roda-sentinel2/workflow-sentinel2-to-stac/6ff82a306f3bf3f579816ae10d063d01',\n       'roda-sentinel2/workflow-sentinel2-to-stac/0081724da96931a83c6f7515f7562943',\n       'roda-sentinel2/workflow-sentinel2-to-stac/737df6bf70bb0f7b565fa5e6ee4129bb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ca09639b46ee5bb24b92fd5b2987cc8b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/82f456e00cccfe29386bd26a12a6e588',\n       'roda-sentinel2/workflow-sentinel2-to-stac/391f5e95d69f9ebd8a98bcc3fc3589eb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cbc04fd5c89f2b587d6dadfc9b9fffbb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/2ee326f71919bb8db20326fefc2dfbb1',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4e6ecb562c70758e02b09233082200a3',\n       'roda-sentinel2/workflow-sentinel2-to-stac/453b45d4b7891c6f3e0afe979aa1538e',\n       'roda-sentinel2/workflow-sentinel2-to-stac/665c266d9bc61d6315aded7c7690b159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/762a8a493aae7d67b31d512131bc22d4',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cebdfaff85f595f644f4d89f380d1a04',\n       'roda-sentinel2/workflow-sentinel2-to-stac/7fd514bec1e5e63f8b389286d09303d5',\n       'roda-sentinel2/workflow-sentinel2-to-stac/9ed53be7f1a72cb5913c128c31eb27b0',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b40cc8ad73175b8ce928a3bc78e9ae28'],\n      dtype='&lt;U74')</pre></li><li>raster:bands(band)object[{'nodata': 0, 'data_type': 'uin...<pre>array([list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'scale': 0.001, 'offset': 0}]),\n       None, None, None, None, None, None, None, None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint8', 'spatial_resolution': 20}]),\n       None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'unit': 'cm', 'scale': 0.001, 'offset': 0}]),\n       list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'scale': 0.001, 'offset': 0}]),\n       None, None, None, None, None, None, None, None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint8', 'spatial_resolution': 20}]),\n       None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'unit': 'cm', 'scale': 0.001, 'offset': 0}])],\n      dtype=object)</pre></li><li>title(band)&lt;U31'Aerosol optical thickness (AOT)...<pre>array(['Aerosol optical thickness (AOT)', 'Blue (band 2) - 10m',\n       'Coastal aerosol (band 1) - 60m', 'Green (band 3) - 10m',\n       'NIR 1 (band 8) - 10m', 'NIR 2 (band 8A) - 20m',\n       'NIR 3 (band 9) - 60m', 'Red (band 4) - 10m',\n       'Red edge 1 (band 5) - 20m', 'Red edge 2 (band 6) - 20m',\n       'Red edge 3 (band 7) - 20m', 'Scene classification map (SCL)',\n       'SWIR 1 (band 11) - 20m', 'SWIR 2 (band 12) - 20m',\n       'True color image', 'Water vapour (WVP)',\n       'Aerosol optical thickness (AOT)', 'Blue (band 2) - 10m',\n       'Coastal aerosol (band 1) - 60m', 'Green (band 3) - 10m',\n       'NIR 1 (band 8) - 10m', 'NIR 2 (band 8A) - 20m',\n       'NIR 3 (band 9) - 60m', 'Red (band 4) - 10m',\n       'Red edge 1 (band 5) - 20m', 'Red edge 2 (band 6) - 20m',\n       'Red edge 3 (band 7) - 20m', 'Scene classification map (SCL)',\n       'SWIR 1 (band 11) - 20m', 'SWIR 2 (band 12) - 20m',\n       'True color image', 'Water vapour (WVP)'], dtype='&lt;U31')</pre></li><li>gsd(band)objectNone 10 60 10 ... 20 20 None None<pre>array([None, 10, 60, 10, 10, 20, 60, 10, 20, 20, 20, None, 20, 20, None,\n       None, None, 10, 60, 10, 10, 20, 60, 10, 20, 20, 20, None, 20, 20,\n       None, None], dtype=object)</pre></li><li>common_name(band)objectNone 'blue' 'coastal' ... None None<pre>array([None, 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09', 'red',\n       'rededge', 'rededge', 'rededge', None, 'swir16', 'swir22', None,\n       None, None, 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09',\n       'red', 'rededge', 'rededge', 'rededge', None, 'swir16', 'swir22',\n       None, None], dtype=object)</pre></li><li>center_wavelength(band)objectNone 0.49 0.443 ... 2.19 None None<pre>array([None, 0.49, 0.443, 0.56, 0.842, 0.865, 0.945, 0.665, 0.704, 0.74,\n       0.783, None, 1.61, 2.19, None, None, None, 0.49, 0.443, 0.56,\n       0.842, 0.865, 0.945, 0.665, 0.704, 0.74, 0.783, None, 1.61, 2.19,\n       None, None], dtype=object)</pre></li><li>full_width_half_max(band)objectNone 0.098 0.027 ... None None<pre>array([None, 0.098, 0.027, 0.045, 0.145, 0.033, 0.026, 0.038, 0.019,\n       0.018, 0.028, None, 0.143, 0.242, None, None, None, 0.098, 0.027,\n       0.045, 0.145, 0.033, 0.026, 0.038, 0.019, 0.018, 0.028, None,\n       0.143, 0.242, None, None], dtype=object)</pre></li><li>epsg()int3232632<pre>array(32632)</pre></li></ul></li><li>Indexes: (4)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2021-04-01 10:27:39.520000', '2021-04-08 10:17:41.934000',\n               '2021-04-08 10:17:41.935000', '2021-04-16 10:27:35.384000',\n               '2021-04-16 10:27:35.385000', '2021-04-21 10:27:35.793000',\n               '2021-04-23 10:17:40.745000', '2021-04-23 10:17:40.747000',\n               '2021-04-28 10:17:40.197000', '2021-05-03 10:17:42.916000',\n               '2021-05-08 10:17:42.592000', '2021-05-08 10:17:42.594000',\n               '2021-05-23 10:17:45.371000', '2021-05-26 10:27:42.080000',\n               '2021-05-28 10:17:45.418000', '2021-05-28 10:17:45.421000',\n               '2021-05-31 10:27:42.147000', '2021-05-31 10:27:42.149000',\n               '2021-06-02 10:17:45.639000', '2021-06-02 10:17:45.641000',\n               '2021-06-10 10:27:42.429000', '2021-06-15 10:27:41.696000',\n               '2021-06-15 10:27:41.697000', '2021-06-17 10:17:45.721000',\n               '2021-06-17 10:17:45.724000', '2021-06-22 10:17:45.729000',\n               '2021-06-22 10:17:45.732000', '2021-06-27 10:17:45.604000',\n               '2021-06-27 10:17:45.605000', '2021-07-02 10:17:46.959000',\n               '2021-07-02 10:17:46.961000', '2021-07-10 10:27:43.233000',\n               '2021-07-12 10:17:47.866000', '2021-07-20 10:27:43.424000',\n               '2021-07-22 10:17:48.210000', '2021-08-06 10:17:45.959000',\n               '2021-08-09 10:27:42.199000', '2021-08-11 10:17:47.398000',\n               '2021-08-14 10:27:43.878000', '2021-08-16 10:17:44.680000',\n               '2021-08-19 10:27:40.757000', '2021-08-21 10:17:47.043000'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>bandPandasIndex<pre>PandasIndex(Index(['aot', 'blue', 'coastal', 'green', 'nir', 'nir08', 'nir09', 'red',\n       'rededge1', 'rededge2', 'rededge3', 'scl', 'swir16', 'swir22', 'visual',\n       'wvp', 'aot-jp2', 'blue-jp2', 'coastal-jp2', 'green-jp2', 'nir-jp2',\n       'nir08-jp2', 'nir09-jp2', 'red-jp2', 'rededge1-jp2', 'rededge2-jp2',\n       'rededge3-jp2', 'scl-jp2', 'swir16-jp2', 'swir22-jp2', 'visual-jp2',\n       'wvp-jp2'],\n      dtype='object', name='band'))</pre></li><li>xPandasIndex<pre>PandasIndex(Float64Index([675820.0, 675830.0, 675840.0, 675850.0, 675860.0, 675870.0,\n              675880.0, 675890.0, 675900.0, 675910.0,\n              ...\n              685720.0, 685730.0, 685740.0, 685750.0, 685760.0, 685770.0,\n              685780.0, 685790.0, 685800.0, 685810.0],\n             dtype='float64', name='x', length=1000))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5241820.0, 5241810.0, 5241800.0, 5241790.0, 5241780.0, 5241770.0,\n              5241760.0, 5241750.0, 5241740.0, 5241730.0,\n              ...\n              5231920.0, 5231910.0, 5231900.0, 5231890.0, 5231880.0, 5231870.0,\n              5231860.0, 5231850.0, 5231840.0, 5231830.0],\n             dtype='float64', name='y', length=1000))</pre></li></ul></li><li>Attributes: (4)spec :RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0, 5300040.0), resolutions_xy=(10.0, 10.0))crs :epsg:32632transform :| 10.00, 0.00, 600000.00| | 0.00,-10.00, 5300040.00| | 0.00, 0.00, 1.00|resolution :10.0</li></ul> <p>We want only the four 10-m bands and the Scene Classification (SCL) layer.</p> In\u00a0[39]: Copied! <pre>stack_aoi = stack_aoi.sel(band = ['blue', 'green', 'red', 'nir', 'scl'])\nstack_aoi\n</pre> stack_aoi = stack_aoi.sel(band = ['blue', 'green', 'red', 'nir', 'scl']) stack_aoi Out[39]: <pre>&lt;xarray.DataArray 'stackstac-d35de160ec768a0c51870cec97c80190' (time: 42,\n                                                                band: 5,\n                                                                y: 1000, x: 1000)&gt;\ndask.array&lt;getitem, shape=(42, 5, 1000, 1000), dtype=float64, chunksize=(1, 1, 678, 610), chunktype=numpy.ndarray&gt;\nCoordinates: (12/52)\n  * time                                     (time) datetime64[ns] 2021-04-01...\n    id                                       (time) &lt;U24 'S2B_32TPT_20210401_...\n  * band                                     (band) &lt;U12 'blue' ... 'scl'\n  * x                                        (x) float64 6.758e+05 ... 6.858e+05\n  * y                                        (y) float64 5.242e+06 ... 5.232e+06\n    s2:medium_proba_clouds_percentage        (time) float64 1.463 16.5 ... 2.701\n    ...                                       ...\n    title                                    (band) &lt;U31 'Blue (band 2) - 10m...\n    gsd                                      (band) object 10 10 10 10 None\n    common_name                              (band) object 'blue' ... None\n    center_wavelength                        (band) object 0.49 0.56 ... None\n    full_width_half_max                      (band) object 0.098 0.045 ... None\n    epsg                                     int32 32632\nAttributes:\n    spec:        RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0...\n    crs:         epsg:32632\n    transform:   | 10.00, 0.00, 600000.00|\\n| 0.00,-10.00, 5300040.00|\\n| 0.0...\n    resolution:  10.0</pre>xarray.DataArray'stackstac-d35de160ec768a0c51870cec97c80190'<ul><li>time: 42</li><li>band: 5</li><li>y: 1000</li><li>x: 1000</li></ul><ul><li>dask.array&lt;chunksize=(1, 1, 322, 610), meta=np.ndarray&gt;  Array   Chunk   Bytes   1.56 GiB   3.16 MiB   Shape   (42, 5, 1000, 1000)   (1, 1, 678, 610)   Dask graph   840 chunks in 5 graph layers   Data type   float64 numpy.ndarray  42 1 1000 1000 5 </li><li>Coordinates: (52)<ul><li>time(time)datetime64[ns]2021-04-01T10:27:39.520000 ... 2...<pre>array(['2021-04-01T10:27:39.520000000', '2021-04-08T10:17:41.934000000',\n       '2021-04-08T10:17:41.935000000', '2021-04-16T10:27:35.384000000',\n       '2021-04-16T10:27:35.385000000', '2021-04-21T10:27:35.793000000',\n       '2021-04-23T10:17:40.745000000', '2021-04-23T10:17:40.747000000',\n       '2021-04-28T10:17:40.197000000', '2021-05-03T10:17:42.916000000',\n       '2021-05-08T10:17:42.592000000', '2021-05-08T10:17:42.594000000',\n       '2021-05-23T10:17:45.371000000', '2021-05-26T10:27:42.080000000',\n       '2021-05-28T10:17:45.418000000', '2021-05-28T10:17:45.421000000',\n       '2021-05-31T10:27:42.147000000', '2021-05-31T10:27:42.149000000',\n       '2021-06-02T10:17:45.639000000', '2021-06-02T10:17:45.641000000',\n       '2021-06-10T10:27:42.429000000', '2021-06-15T10:27:41.696000000',\n       '2021-06-15T10:27:41.697000000', '2021-06-17T10:17:45.721000000',\n       '2021-06-17T10:17:45.724000000', '2021-06-22T10:17:45.729000000',\n       '2021-06-22T10:17:45.732000000', '2021-06-27T10:17:45.604000000',\n       '2021-06-27T10:17:45.605000000', '2021-07-02T10:17:46.959000000',\n       '2021-07-02T10:17:46.961000000', '2021-07-10T10:27:43.233000000',\n       '2021-07-12T10:17:47.866000000', '2021-07-20T10:27:43.424000000',\n       '2021-07-22T10:17:48.210000000', '2021-08-06T10:17:45.959000000',\n       '2021-08-09T10:27:42.199000000', '2021-08-11T10:17:47.398000000',\n       '2021-08-14T10:27:43.878000000', '2021-08-16T10:17:44.680000000',\n       '2021-08-19T10:27:40.757000000', '2021-08-21T10:17:47.043000000'],\n      dtype='datetime64[ns]')</pre></li><li>id(time)&lt;U24'S2B_32TPT_20210401_0_L2A' ... '...<pre>array(['S2B_32TPT_20210401_0_L2A', 'S2B_32TPT_20210408_2_L2A',\n       'S2B_32TPT_20210408_0_L2A', 'S2A_32TPT_20210416_1_L2A',\n       'S2A_32TPT_20210416_0_L2A', 'S2B_32TPT_20210421_0_L2A',\n       'S2A_32TPT_20210423_2_L2A', 'S2A_32TPT_20210423_0_L2A',\n       'S2B_32TPT_20210428_0_L2A', 'S2A_32TPT_20210503_0_L2A',\n       'S2B_32TPT_20210508_1_L2A', 'S2B_32TPT_20210508_0_L2A',\n       'S2A_32TPT_20210523_0_L2A', 'S2A_32TPT_20210526_0_L2A',\n       'S2B_32TPT_20210528_1_L2A', 'S2B_32TPT_20210528_0_L2A',\n       'S2B_32TPT_20210531_1_L2A', 'S2B_32TPT_20210531_0_L2A',\n       'S2A_32TPT_20210602_1_L2A', 'S2A_32TPT_20210602_0_L2A',\n       'S2B_32TPT_20210610_0_L2A', 'S2A_32TPT_20210615_0_L2A',\n       'S2A_32TPT_20210615_1_L2A', 'S2B_32TPT_20210617_1_L2A',\n       'S2B_32TPT_20210617_0_L2A', 'S2A_32TPT_20210622_1_L2A',\n       'S2A_32TPT_20210622_0_L2A', 'S2B_32TPT_20210627_1_L2A',\n       'S2B_32TPT_20210627_0_L2A', 'S2A_32TPT_20210702_1_L2A',\n       'S2A_32TPT_20210702_0_L2A', 'S2B_32TPT_20210710_0_L2A',\n       'S2A_32TPT_20210712_0_L2A', 'S2B_32TPT_20210720_0_L2A',\n       'S2A_32TPT_20210722_0_L2A', 'S2B_32TPT_20210806_0_L2A',\n       'S2B_32TPT_20210809_0_L2A', 'S2A_32TPT_20210811_0_L2A',\n       'S2A_32TPT_20210814_0_L2A', 'S2B_32TPT_20210816_0_L2A',\n       'S2B_32TPT_20210819_0_L2A', 'S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U24')</pre></li><li>band(band)&lt;U12'blue' 'green' 'red' 'nir' 'scl'<pre>array(['blue', 'green', 'red', 'nir', 'scl'], dtype='&lt;U12')</pre></li><li>x(x)float646.758e+05 6.758e+05 ... 6.858e+05<pre>array([675820., 675830., 675840., ..., 685790., 685800., 685810.])</pre></li><li>y(y)float645.242e+06 5.242e+06 ... 5.232e+06<pre>array([5241820., 5241810., 5241800., ..., 5231850., 5231840., 5231830.])</pre></li><li>s2:medium_proba_clouds_percentage(time)float641.463 16.5 14.33 ... 4.624 2.701<pre>array([ 1.46334 , 16.504081, 14.328757, 10.298105, 16.633525,  7.706635,\n        1.899826,  2.313006,  3.444555, 12.165798,  6.054812,  6.262551,\n        8.735961,  8.490147,  6.304692,  6.609003,  0.209097,  0.926137,\n       12.20682 ,  3.0033  ,  6.494882,  0.308812,  0.057903,  9.846946,\n        2.624075, 10.38753 ,  2.671621,  2.953941,  1.214524, 14.579228,\n        6.361181,  1.917733,  5.046743,  5.886542,  3.49712 ,  5.288142,\n        8.231435,  4.027119,  0.841558, 19.751492,  4.62429 ,  2.701251])</pre></li><li>earthsearch:boa_offset_applied(time)boolFalse True False ... False False<pre>array([False,  True, False,  True, False, False,  True, False, False,\n       False,  True, False, False, False,  True, False,  True, False,\n        True, False, False, False,  True,  True, False,  True, False,\n        True, False,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False])</pre></li><li>processing:software(time)object{'sentinel2-to-stac': '0.1.0'} ....<pre>array([{'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'}],\n      dtype=object)</pre></li><li>proj:epsg()int3232632<pre>array(32632)</pre></li><li>s2:generation_time(time)&lt;U27'2021-04-01T14:19:13.000000Z' .....<pre>array(['2021-04-01T14:19:13.000000Z', '2023-05-13T12:44:55.000000Z',\n       '2021-04-08T13:26:17.000000Z', '2023-05-19T13:18:55.000000Z',\n       '2021-04-16T13:20:57.000000Z', '2021-04-21T13:46:13.000000Z',\n       '2023-05-12T19:45:03.000000Z', '2021-04-23T12:55:30.000000Z',\n       '2021-04-28T13:40:12.000000Z', '2021-05-03T12:17:41.000000Z',\n       '2023-02-27T09:29:40.000000Z', '2021-05-08T14:56:55.000000Z',\n       '2021-05-23T12:13:33.000000Z', '2021-05-26T13:34:25.000000Z',\n       '2023-02-27T08:23:12.000000Z', '2021-05-28T13:08:36.000000Z',\n       '2023-03-01T00:24:41.000000Z', '2021-05-31T14:00:40.000000Z',\n       '2023-03-13T04:04:11.000000Z', '2021-06-02T17:13:07.000000Z',\n       '2021-06-10T13:27:39.000000Z', '2021-06-15T13:16:59.000000Z',\n       '2023-03-21T22:10:50.000000Z', '2023-03-21T22:17:28.000000Z',\n       '2021-06-17T13:12:19.000000Z', '2023-01-29T15:30:31.000000Z',\n       '2021-06-22T12:13:05.000000Z', '2023-03-18T23:38:59.000000Z',\n       '2021-06-27T13:58:56.000000Z', '2023-01-25T13:05:14.000000Z',\n       '2021-07-02T12:13:53.000000Z', '2021-07-10T13:23:21.000000Z',\n       '2021-07-12T13:22:19.000000Z', '2021-07-20T13:22:07.000000Z',\n       '2021-07-22T12:02:14.000000Z', '2021-08-06T15:00:37.000000Z',\n       '2021-08-14T13:01:57.000000Z', '2021-08-11T16:05:21.000000Z',\n       '2021-08-14T13:23:26.000000Z', '2021-08-16T13:08:48.000000Z',\n       '2021-08-19T13:53:08.000000Z', '2021-08-21T12:05:55.000000Z'],\n      dtype='&lt;U27')</pre></li><li>created(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-03T11:38:25.089Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-03T11:19:16.880Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-03T11:12:48.494Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:dark_features_percentage(time)float647.618 3.489 2.558 ... 2.943 2.726<pre>array([7.618114, 3.489247, 2.558084, 5.739467, 4.485189, 2.807637,\n       5.218234, 2.874529, 1.733305, 1.861065, 2.493159, 1.334918,\n       0.416572, 1.220573, 1.255598, 0.764608, 2.384401, 1.463244,\n       0.931227, 1.316954, 1.090162, 0.8525  , 1.352069, 0.733103,\n       1.135998, 1.516507, 1.200472, 1.417252, 0.957145, 1.22749 ,\n       1.375679, 1.512192, 0.560178, 1.777653, 1.21489 , 2.177998,\n       2.577018, 1.8398  , 3.048143, 1.672873, 2.943032, 2.726397])</pre></li><li>s2:datatake_type()&lt;U8'INS-NOBS'<pre>array('INS-NOBS', dtype='&lt;U8')</pre></li><li>s2:water_percentage(time)float641.035 0.3132 ... 0.3022 0.8797<pre>array([1.034616, 0.313168, 0.424308, 0.617732, 0.819981, 0.987005,\n       0.857674, 1.026752, 0.751145, 0.66969 , 0.589903, 0.582099,\n       0.374695, 0.464075, 0.43538 , 0.426498, 0.905694, 0.953868,\n       0.768842, 1.276473, 0.616264, 0.743187, 0.848236, 0.84215 ,\n       1.075244, 0.699068, 0.829451, 0.73024 , 0.757141, 0.394683,\n       0.510065, 0.539884, 0.785743, 0.768489, 1.082259, 0.712835,\n       0.559107, 0.910735, 1.067886, 0.465221, 0.302152, 0.879734])</pre></li><li>eo:cloud_cover(time)float646.149 45.79 45.55 ... 44.97 11.88<pre>array([ 6.149054, 45.791623, 45.550924, 25.692633, 33.007648, 43.998277,\n        3.953265,  4.693773, 35.548758, 39.265612, 15.81282 , 15.84441 ,\n       49.168627, 45.008284, 26.705834, 28.351225,  0.365331,  1.409963,\n       33.068579, 24.769196, 47.15134 ,  1.805315,  1.849951, 26.098341,\n       19.611905, 25.549465, 18.602191, 10.19    ,  9.07656 , 44.637647,\n       35.395256, 16.658998, 10.469738, 24.9592  , 18.490819, 28.947858,\n       27.45625 , 24.8152  ,  3.156408, 49.208091, 44.972815, 11.884551])</pre></li><li>view:sun_azimuth(time)float64161.2 157.5 157.5 ... 158.4 155.0<pre>array([161.22281178, 157.46645516, 157.46542095, 160.80126617,\n       160.80014335, 160.57908911, 156.51420567, 156.51306183,\n       156.0684864 , 155.56597523, 154.95741812, 154.95616171,\n       152.69773459, 156.88007906, 151.8422183 , 151.84086683,\n       156.07844848, 156.07704495, 150.97550979, 150.97415895,\n       154.47087908, 153.7373982 , 153.738758  , 148.65682968,\n       148.65553825, 148.11111459, 148.10983402, 147.7307707 ,\n       147.72954199, 147.54553355, 147.54435203, 152.24342398,\n       147.75100857, 152.92620053, 148.7434059 , 151.42303732,\n       156.18470534, 152.56163454, 157.28195674, 153.75067448,\n       158.41022444, 155.02778045])</pre></li><li>s2:sequence(time)&lt;U1'0' '2' '0' '1' ... '0' '0' '0' '0'<pre>array(['0', '2', '0', '1', '0', '0', '2', '0', '0', '0', '1', '0', '0',\n       '0', '1', '0', '1', '0', '1', '0', '0', '0', '1', '1', '0', '1',\n       '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n       '0', '0', '0'], dtype='&lt;U1')</pre></li><li>s2:datastrip_id(time)&lt;U64'S2B_OPER_MSI_L2A_DS_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_DS_VGS4_20210401T141913_S20210401T102326_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230513T124455_S20210408T101148_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210408T132617_S20210408T101148_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230519T131855_S20210416T102600_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210416T132057_S20210416T102600_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210421T134613_S20210421T102336_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230512T194503_S20210423T101704_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210423T125530_S20210423T101704_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210428T134012_S20210428T101657_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210503T121741_S20210503T101204_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T092940_S20210508T101551_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210508T145655_S20210508T101551_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210523T121333_S20210523T101416_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210526T133425_S20210526T102625_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T082312_S20210528T100555_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210528T130836_S20210528T100555_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230301T002441_S20210531T102618_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210531T140040_S20210531T102618_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230313T040411_S20210602T101235_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210602T171307_S20210602T101235_N03.00',\n...\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230321T221728_S20210617T101301_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210617T131219_S20210617T101301_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230129T153031_S20210622T101416_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210622T121305_S20210622T101416_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230318T233859_S20210627T101702_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210627T135856_S20210627T101702_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230125T130514_S20210702T101157_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210702T121353_S20210702T101157_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210710T132321_S20210710T102312_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210712T132219_S20210712T101027_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210720T132207_S20210720T101617_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210722T120214_S20210722T101419_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210806T150037_S20210806T101207_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS1_20210814T130157_S20210809T102637_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210811T160521_S20210811T101411_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210814T132326_S20210814T102609_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210816T130848_S20210816T101300_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210819T135308_S20210819T102635_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210821T120555_S20210821T101418_N03.01'],\n      dtype='&lt;U64')</pre></li><li>mgrs:grid_square()&lt;U2'PT'<pre>array('PT', dtype='&lt;U2')</pre></li><li>instruments()&lt;U3'msi'<pre>array('msi', dtype='&lt;U3')</pre></li><li>s2:thin_cirrus_percentage(time)float640.4854 0.4475 ... 0.004964 3.404<pre>array([4.8540100e-01, 4.4747100e-01, 1.9159610e+00, 2.4920600e-01,\n       8.8436700e-01, 2.2049110e+00, 8.2425000e-02, 7.5833000e-02,\n       1.1564730e+00, 2.1922420e+00, 8.2073200e+00, 7.6982920e+00,\n       1.3726480e+00, 2.5636340e+00, 1.5205705e+01, 1.6151080e+01,\n       4.1297000e-02, 4.4017000e-02, 5.1187000e-01, 8.5886000e-01,\n       1.7029480e+00, 1.1063510e+00, 1.7621680e+00, 1.0222800e-01,\n       2.7094100e-01, 1.3869100e-01, 2.2755900e-01, 4.5100390e+00,\n       4.5370160e+00, 5.0657700e+00, 3.5785940e+00, 9.4725660e+00,\n       3.9136060e+00, 2.2286300e-01, 5.0741000e-02, 8.8339000e-02,\n       8.7864210e+00, 1.1323400e-01, 3.8490000e-03, 8.7228200e-01,\n       4.9640000e-03, 3.4040560e+00])</pre></li><li>s2:vegetation_percentage(time)float6426.34 3.007 3.274 ... 36.39 66.22<pre>array([26.339936,  3.007327,  3.273584, 11.632005, 12.624834, 18.579322,\n       34.706536, 36.717805, 22.266363, 18.495288, 33.440745, 34.884864,\n       14.592455, 19.679427, 29.31349 , 30.193511, 59.83513 , 60.953248,\n       39.538234, 42.743748, 29.41438 , 71.402311, 70.151877, 51.991493,\n       55.270773, 53.398359, 56.250197, 68.942064, 69.730705, 38.577574,\n       43.56502 , 66.226125, 71.196151, 49.621934, 62.576228, 49.759224,\n       54.908115, 55.141944, 78.077263, 32.121503, 36.389408, 66.223991])</pre></li><li>s2:high_proba_clouds_percentage(time)float644.2 28.84 29.31 ... 40.34 5.779<pre>array([4.2003130e+00, 2.8840071e+01, 2.9306206e+01, 1.5145321e+01,\n       1.5489756e+01, 3.4086731e+01, 1.9710150e+00, 2.3049340e+00,\n       3.0947730e+01, 2.4907573e+01, 1.5506890e+00, 1.8835670e+00,\n       3.9060017e+01, 3.3954504e+01, 5.1954370e+00, 5.5911420e+00,\n       1.1493700e-01, 4.3980900e-01, 2.0349889e+01, 2.0907035e+01,\n       3.8953510e+01, 3.9015200e-01, 2.9880000e-02, 1.6149166e+01,\n       1.6716889e+01, 1.5023246e+01, 1.5703011e+01, 2.7260200e+00,\n       3.3250190e+00, 2.4992649e+01, 2.5455481e+01, 5.2686990e+00,\n       1.5093900e+00, 1.8849795e+01, 1.4942957e+01, 2.3571377e+01,\n       1.0438395e+01, 2.0674847e+01, 2.3110010e+00, 2.8584316e+01,\n       4.0343562e+01, 5.7792450e+00])</pre></li><li>s2:unclassified_percentage(time)float645.542 1.249 4.882 ... 4.302 3.051<pre>array([5.542233, 1.249116, 4.882131, 1.901653, 8.202816, 3.343896,\n       0.369042, 4.536037, 5.607506, 5.386119, 0.42674 , 5.226327,\n       5.411838, 6.440622, 1.1914  , 7.262506, 0.095849, 2.831393,\n       0.502138, 3.365285, 5.884814, 2.064657, 0.053503, 0.43447 ,\n       3.136941, 0.979966, 3.44101 , 0.372121, 1.875458, 1.251701,\n       4.816858, 1.852196, 2.070124, 6.984119, 4.280468, 5.570791,\n       3.381409, 4.446889, 1.608293, 5.839212, 4.302198, 3.050546])</pre></li><li>s2:snow_ice_percentage(time)float6440.65 40.93 39.09 ... 1.161 0.858<pre>array([40.654615, 40.934548, 39.088675, 45.368615, 33.28197 , 21.687628,\n       44.689843, 40.108246, 25.425422, 26.355195, 37.636858, 35.119075,\n       27.416307, 21.791345, 30.055815, 26.366636, 27.95321 , 24.107485,\n       13.354345, 16.522175,  8.769823, 12.960622, 16.334337,  8.544581,\n        8.742414,  6.522872,  6.389479,  7.112196,  5.816885,  4.461629,\n        4.215506,  2.77441 ,  3.27045 ,  1.624512,  1.15363 ,  1.572428,\n        1.036823,  0.380159,  0.842441,  2.167679,  1.161288,  0.857967])</pre></li><li>earthsearch:s3_path(time)&lt;U79's3://sentinel-cogs/sentinel-s2-...<pre>array(['s3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210401_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210421_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210428_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210503_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210523_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210526_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_0_L2A',\n...\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210710_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210712_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210720_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210722_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210806_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210809_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210811_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210814_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210816_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210819_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U79')</pre></li><li>s2:granule_id(time)&lt;U62'S2B_OPER_MSI_L2A_TL_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_TL_VGS4_20210401T141913_A021255_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230513T124455_A021355_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210408T132617_A021355_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230519T131855_A030378_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210416T132057_A030378_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210421T134613_A021541_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230512T194503_A030478_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210423T125530_A030478_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210428T134012_A021641_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210503T121741_A030621_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T092940_A021784_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210508T145655_A021784_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210523T121333_A030907_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210526T133425_A030950_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T082312_A022070_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210528T130836_A022070_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230301T002441_A022113_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210531T140040_A022113_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230313T040411_A031050_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210602T171307_A031050_T32TPT_N03.00',\n...\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230321T221728_A022356_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210617T131219_A022356_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230129T153031_A031336_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210622T121305_A031336_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230318T233859_A022499_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210627T135856_A022499_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230125T130514_A031479_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210702T121353_A031479_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210710T132321_A022685_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210712T132219_A031622_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210720T132207_A022828_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210722T120214_A031765_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210806T150037_A023071_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS1_20210814T130157_A023114_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210811T160521_A032051_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210814T132326_A032094_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210816T130848_A023214_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210819T135308_A023257_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210821T120555_A032194_T32TPT_N03.01'],\n      dtype='&lt;U62')</pre></li><li>constellation()&lt;U10'sentinel-2'<pre>array('sentinel-2', dtype='&lt;U10')</pre></li><li>s2:reflectance_conversion_factor(time)float641.004 0.9999 ... 0.9753 0.9761<pre>array([1.00391722, 0.99989016, 0.99989016, 0.99531243, 0.99531243,\n       0.99251188, 0.99141059, 0.99141059, 0.98870861, 0.98609676,\n       0.98359974, 0.98359974, 0.97693835, 0.97577919, 0.97504789,\n       0.97504789, 0.97399904, 0.97399904, 0.97334113, 0.97334113,\n       0.97102521, 0.96985123, 0.96985123, 0.969445  , 0.969445  ,\n       0.96858002, 0.96858002, 0.96794362, 0.96794362, 0.96753822,\n       0.96753822, 0.96737784, 0.96743179, 0.96802315, 0.96826343,\n       0.97121659, 0.97204341, 0.9726335 , 0.97358286, 0.97425016,\n       0.97531576, 0.97606015])</pre></li><li>view:sun_elevation(time)float6445.93 48.03 48.03 ... 53.63 52.36<pre>array([45.92877963, 48.02966446, 48.0295066 , 51.57725751, 51.57711889,\n       53.32274987, 53.3762054 , 53.37603569, 54.97817306, 56.47292004,\n       57.84102398, 57.84083568, 61.10137652, 62.31302901, 61.87597965,\n       61.87575968, 63.010744  , 63.01055231, 62.48509786, 62.4848721 ,\n       63.89999116, 64.08851703, 64.08872077, 63.30105997, 63.30082564,\n       63.24315182, 63.24291386, 63.02832992, 63.02809443, 62.66520394,\n       62.66497104, 62.62975879, 61.51305241, 61.04463203, 59.85139625,\n       56.52628032, 56.4892853 , 55.22451323, 55.10440682, 53.83083839,\n       53.63004967, 52.36396242])</pre></li><li>platform(time)&lt;U11'sentinel-2b' ... 'sentinel-2a'<pre>array(['sentinel-2b', 'sentinel-2b', 'sentinel-2b', 'sentinel-2a',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a'], dtype='&lt;U11')</pre></li><li>mgrs:latitude_band()&lt;U1'T'<pre>array('T', dtype='&lt;U1')</pre></li><li>s2:product_uri(time)&lt;U65'S2B_MSIL2A_20210401T101559_N030...<pre>array(['S2B_MSIL2A_20210401T101559_N0300_R065_T32TPT_20210401T141913.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0500_R022_T32TPT_20230513T124455.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0300_R022_T32TPT_20210408T132617.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0500_R065_T32TPT_20230519T131855.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0300_R065_T32TPT_20210416T132057.SAFE',\n       'S2B_MSIL2A_20210421T101549_N0300_R065_T32TPT_20210421T134613.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0500_R022_T32TPT_20230512T194503.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0300_R022_T32TPT_20210423T125530.SAFE',\n       'S2B_MSIL2A_20210428T100549_N0300_R022_T32TPT_20210428T134012.SAFE',\n       'S2A_MSIL2A_20210503T101021_N0300_R022_T32TPT_20210503T121741.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0500_R022_T32TPT_20230227T092940.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0300_R022_T32TPT_20210508T145655.SAFE',\n       'S2A_MSIL2A_20210523T101031_N0300_R022_T32TPT_20210523T121333.SAFE',\n       'S2A_MSIL2A_20210526T102021_N0300_R065_T32TPT_20210526T133425.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0500_R022_T32TPT_20230227T082312.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0300_R022_T32TPT_20210528T130836.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0500_R065_T32TPT_20230301T002441.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0300_R065_T32TPT_20210531T140040.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0500_R022_T32TPT_20230313T040411.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0300_R022_T32TPT_20210602T171307.SAFE',\n...\n       'S2B_MSIL2A_20210617T100559_N0500_R022_T32TPT_20230321T221728.SAFE',\n       'S2B_MSIL2A_20210617T100559_N0300_R022_T32TPT_20210617T131219.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0500_R022_T32TPT_20230129T153031.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0300_R022_T32TPT_20210622T121305.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0500_R022_T32TPT_20230318T233859.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0300_R022_T32TPT_20210627T135856.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0500_R022_T32TPT_20230125T130514.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0301_R022_T32TPT_20210702T121353.SAFE',\n       'S2B_MSIL2A_20210710T101559_N0301_R065_T32TPT_20210710T132321.SAFE',\n       'S2A_MSIL2A_20210712T101031_N0301_R022_T32TPT_20210712T132219.SAFE',\n       'S2B_MSIL2A_20210720T101559_N0301_R065_T32TPT_20210720T132207.SAFE',\n       'S2A_MSIL2A_20210722T101031_N0301_R022_T32TPT_20210722T120214.SAFE',\n       'S2B_MSIL2A_20210806T100559_N0301_R022_T32TPT_20210806T150037.SAFE',\n       'S2B_MSIL2A_20210809T101559_N0301_R065_T32TPT_20210814T130157.SAFE',\n       'S2A_MSIL2A_20210811T101031_N0301_R022_T32TPT_20210811T160521.SAFE',\n       'S2A_MSIL2A_20210814T102031_N0301_R065_T32TPT_20210814T132326.SAFE',\n       'S2B_MSIL2A_20210816T100559_N0301_R022_T32TPT_20210816T130848.SAFE',\n       'S2B_MSIL2A_20210819T101559_N0301_R065_T32TPT_20210819T135308.SAFE',\n       'S2A_MSIL2A_20210821T101031_N0301_R022_T32TPT_20210821T120555.SAFE'],\n      dtype='&lt;U65')</pre></li><li>updated(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-06T14:14:42.036Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-06T04:47:25.002Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-06T04:45:55.146Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:product_type()&lt;U7'S2MSI2A'<pre>array('S2MSI2A', dtype='&lt;U7')</pre></li><li>s2:saturated_defective_pixel_percentage()int320<pre>array(0)</pre></li><li>s2:degraded_msi_data_percentage(time)object0 0.1898 0 0.0825 0 0 ... 0 0 0 0 0<pre>array([0, 0.1898, 0, 0.0825, 0, 0, 0.0161, 0, 0, 0, 0.0259, 0, 0, 0,\n       0.0119, 0, 0.0204, 0, 0.0664, 0, 0, 0, 0.0225, 0.0382, 0, 0.0347,\n       0, 0.012, 0, 0.021, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      dtype=object)</pre></li><li>grid:code()&lt;U10'MGRS-32TPT'<pre>array('MGRS-32TPT', dtype='&lt;U10')</pre></li><li>s2:nodata_pixel_percentage(time)object0.002973 9.284004 ... 8.773684<pre>array([0.002973, 9.284004, 8.883444, 0, 3.6e-05, 6e-05, 9.639984,\n       9.534544, 8.656428, 9.141818, 9.115126, 8.715048, 8.926022,\n       0.001904, 8.633243, 8.246658, 0.023792, 0.013444, 9.047445,\n       8.948952, 0.009396, 0.000113, 1.7e-05, 8.783989, 8.390699, 9.33119,\n       9.234183, 8.918451, 8.525366, 9.049369, 8.950624, 0.010153,\n       8.783641, 0.010524, 8.777927, 8.489713, 0.004336, 8.84221,\n       0.003726, 8.610921, 0.000985, 8.773684], dtype=object)</pre></li><li>s2:cloud_shadow_percentage(time)float644.934 0.4877 2.028 ... 2.541 4.589<pre>array([4.934268, 0.487673, 2.027822, 0.962452, 3.386088, 5.310493,\n       0.704442, 2.288749, 3.619292, 4.858182, 0.008787, 0.596837,\n       1.866554, 1.606319, 0.439432, 0.873589, 0.13241 , 0.22497 ,\n       6.900295, 4.897763, 5.058461, 0.094422, 0.031267, 5.442425,\n       4.135132, 2.465164, 2.53105 , 0.682236, 0.868314, 3.576069,\n       2.60878 , 1.147319, 0.079854, 6.248215, 3.624339, 3.247687,\n       1.521886, 5.362982, 1.95151 , 2.662793, 2.541466, 4.588702])</pre></li><li>s2:not_vegetated_percentage(time)float647.727 4.727 2.194 ... 7.388 9.788<pre>array([ 7.727165,  4.727299,  2.194477,  8.085445,  4.191478,  3.285741,\n        9.500962,  7.754106,  5.04821 ,  3.108844,  9.590986,  6.41147 ,\n        0.752953,  3.789353, 10.603052,  5.761432,  8.327983,  8.055834,\n        4.936339,  5.108408,  2.014758, 10.076986,  9.378749,  5.913443,\n        6.891589,  8.868597, 10.756154, 10.553885, 10.917792,  5.873202,\n        7.512838,  9.288875, 11.567761,  8.01588 ,  7.577366,  8.01118 ,\n        8.559394,  7.102291, 10.248062,  5.862629,  7.38764 ,  9.788115])</pre></li><li>s2:datatake_id(time)&lt;U34'GS2B_20210401T101559_021255_N03...<pre>array(['GS2B_20210401T101559_021255_N03.00',\n       'GS2B_20210408T100549_021355_N05.00',\n       'GS2B_20210408T100549_021355_N03.00',\n       'GS2A_20210416T102021_030378_N05.00',\n       'GS2A_20210416T102021_030378_N03.00',\n       'GS2B_20210421T101549_021541_N03.00',\n       'GS2A_20210423T101021_030478_N05.00',\n       'GS2A_20210423T101021_030478_N03.00',\n       'GS2B_20210428T100549_021641_N03.00',\n       'GS2A_20210503T101021_030621_N03.00',\n       'GS2B_20210508T100549_021784_N05.00',\n       'GS2B_20210508T100549_021784_N03.00',\n       'GS2A_20210523T101031_030907_N03.00',\n       'GS2A_20210526T102021_030950_N03.00',\n       'GS2B_20210528T100559_022070_N05.00',\n       'GS2B_20210528T100559_022070_N03.00',\n       'GS2B_20210531T101559_022113_N05.00',\n       'GS2B_20210531T101559_022113_N03.00',\n       'GS2A_20210602T101031_031050_N05.00',\n       'GS2A_20210602T101031_031050_N03.00',\n...\n       'GS2A_20210615T102021_031236_N05.00',\n       'GS2B_20210617T100559_022356_N05.00',\n       'GS2B_20210617T100559_022356_N03.00',\n       'GS2A_20210622T101031_031336_N05.00',\n       'GS2A_20210622T101031_031336_N03.00',\n       'GS2B_20210627T100559_022499_N05.00',\n       'GS2B_20210627T100559_022499_N03.00',\n       'GS2A_20210702T101031_031479_N05.00',\n       'GS2A_20210702T101031_031479_N03.01',\n       'GS2B_20210710T101559_022685_N03.01',\n       'GS2A_20210712T101031_031622_N03.01',\n       'GS2B_20210720T101559_022828_N03.01',\n       'GS2A_20210722T101031_031765_N03.01',\n       'GS2B_20210806T100559_023071_N03.01',\n       'GS2B_20210809T101559_023114_N03.01',\n       'GS2A_20210811T101031_032051_N03.01',\n       'GS2A_20210814T102031_032094_N03.01',\n       'GS2B_20210816T100559_023214_N03.01',\n       'GS2B_20210819T101559_023257_N03.01',\n       'GS2A_20210821T101031_032194_N03.01'], dtype='&lt;U34')</pre></li><li>s2:processing_baseline(time)&lt;U5'03.00' '05.00' ... '03.01' '03.01'<pre>array(['03.00', '05.00', '03.00', '05.00', '03.00', '03.00', '05.00',\n       '03.00', '03.00', '03.00', '05.00', '03.00', '03.00', '03.00',\n       '05.00', '03.00', '05.00', '03.00', '05.00', '03.00', '03.00',\n       '03.00', '05.00', '05.00', '03.00', '05.00', '03.00', '05.00',\n       '03.00', '05.00', '03.01', '03.01', '03.01', '03.01', '03.01',\n       '03.01', '03.01', '03.01', '03.01', '03.01', '03.01', '03.01'],\n      dtype='&lt;U5')</pre></li><li>mgrs:utm_zone()int3232<pre>array(32)</pre></li><li>earthsearch:payload_id(time)&lt;U74'roda-sentinel2/workflow-sentine...<pre>array(['roda-sentinel2/workflow-sentinel2-to-stac/ca1fac60c9640967f45658d79d1523ba',\n       'roda-sentinel2/workflow-sentinel2-to-stac/d1881ee109d59d1df5a0cfc2d85ce430',\n       'roda-sentinel2/workflow-sentinel2-to-stac/f9aa8a3191a8be696d2ddddd9734621c',\n       'roda-sentinel2/workflow-sentinel2-to-stac/57765d545d606d43efe2820f6e0d1159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/1ac3805c5018ad2b5d5989908e95d38d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/21d39a00de8e19a12451c18ba24f832b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/778637c0ddea5fd397ac6695ffe344dd',\n       'roda-sentinel2/workflow-sentinel2-to-stac/29497ba4096ac30ecf99e9603229329b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/03a8dfb01cc4bd0fbcfbb149e25da06f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/c4e5ae224b1aa55e00c6903aa25f65ee',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4f41cddc1225338c2ce7a50a413d05ff',\n       'roda-sentinel2/workflow-sentinel2-to-stac/aa4edf7495a6f12421d1142a8371e750',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4a84c0a1c9ec26453a988d58b3a1e236',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b7752cd2d7341f1cdf30140de86e4992',\n       'roda-sentinel2/workflow-sentinel2-to-stac/8d9d4f770bdb5a0a1a614b2fde70290d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/050ec18e7aef56ed8ce1d702e586175d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/fef3c2c9435cfa8276430d9f2c8fbcdb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ae91b5b1381789ef2f526f90e0d7a89f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/e987ce36035c8be203930ef9d5bcfb55',\n       'roda-sentinel2/workflow-sentinel2-to-stac/64087c1420a6067f46412cfc0215cf07',\n...\n       'roda-sentinel2/workflow-sentinel2-to-stac/1dc42ce3106743876f3fb4adc694ce4d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/af36520f9b99c3e44bf2830bbb217b2b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/5065c12a38660808deee8961e203498a',\n       'roda-sentinel2/workflow-sentinel2-to-stac/6ff82a306f3bf3f579816ae10d063d01',\n       'roda-sentinel2/workflow-sentinel2-to-stac/0081724da96931a83c6f7515f7562943',\n       'roda-sentinel2/workflow-sentinel2-to-stac/737df6bf70bb0f7b565fa5e6ee4129bb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ca09639b46ee5bb24b92fd5b2987cc8b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/82f456e00cccfe29386bd26a12a6e588',\n       'roda-sentinel2/workflow-sentinel2-to-stac/391f5e95d69f9ebd8a98bcc3fc3589eb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cbc04fd5c89f2b587d6dadfc9b9fffbb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/2ee326f71919bb8db20326fefc2dfbb1',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4e6ecb562c70758e02b09233082200a3',\n       'roda-sentinel2/workflow-sentinel2-to-stac/453b45d4b7891c6f3e0afe979aa1538e',\n       'roda-sentinel2/workflow-sentinel2-to-stac/665c266d9bc61d6315aded7c7690b159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/762a8a493aae7d67b31d512131bc22d4',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cebdfaff85f595f644f4d89f380d1a04',\n       'roda-sentinel2/workflow-sentinel2-to-stac/7fd514bec1e5e63f8b389286d09303d5',\n       'roda-sentinel2/workflow-sentinel2-to-stac/9ed53be7f1a72cb5913c128c31eb27b0',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b40cc8ad73175b8ce928a3bc78e9ae28'],\n      dtype='&lt;U74')</pre></li><li>raster:bands(band)objectNone ... [{'nodata': 0, 'data_ty...<pre>array([None, None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint8', 'spatial_resolution': 20}])],\n      dtype=object)</pre></li><li>title(band)&lt;U31'Blue (band 2) - 10m' ... 'Scene...<pre>array(['Blue (band 2) - 10m', 'Green (band 3) - 10m',\n       'Red (band 4) - 10m', 'NIR 1 (band 8) - 10m',\n       'Scene classification map (SCL)'], dtype='&lt;U31')</pre></li><li>gsd(band)object10 10 10 10 None<pre>array([10, 10, 10, 10, None], dtype=object)</pre></li><li>common_name(band)object'blue' 'green' 'red' 'nir' None<pre>array(['blue', 'green', 'red', 'nir', None], dtype=object)</pre></li><li>center_wavelength(band)object0.49 0.56 0.665 0.842 None<pre>array([0.49, 0.56, 0.665, 0.842, None], dtype=object)</pre></li><li>full_width_half_max(band)object0.098 0.045 0.038 0.145 None<pre>array([0.098, 0.045, 0.038, 0.145, None], dtype=object)</pre></li><li>epsg()int3232632<pre>array(32632)</pre></li></ul></li><li>Indexes: (4)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2021-04-01 10:27:39.520000', '2021-04-08 10:17:41.934000',\n               '2021-04-08 10:17:41.935000', '2021-04-16 10:27:35.384000',\n               '2021-04-16 10:27:35.385000', '2021-04-21 10:27:35.793000',\n               '2021-04-23 10:17:40.745000', '2021-04-23 10:17:40.747000',\n               '2021-04-28 10:17:40.197000', '2021-05-03 10:17:42.916000',\n               '2021-05-08 10:17:42.592000', '2021-05-08 10:17:42.594000',\n               '2021-05-23 10:17:45.371000', '2021-05-26 10:27:42.080000',\n               '2021-05-28 10:17:45.418000', '2021-05-28 10:17:45.421000',\n               '2021-05-31 10:27:42.147000', '2021-05-31 10:27:42.149000',\n               '2021-06-02 10:17:45.639000', '2021-06-02 10:17:45.641000',\n               '2021-06-10 10:27:42.429000', '2021-06-15 10:27:41.696000',\n               '2021-06-15 10:27:41.697000', '2021-06-17 10:17:45.721000',\n               '2021-06-17 10:17:45.724000', '2021-06-22 10:17:45.729000',\n               '2021-06-22 10:17:45.732000', '2021-06-27 10:17:45.604000',\n               '2021-06-27 10:17:45.605000', '2021-07-02 10:17:46.959000',\n               '2021-07-02 10:17:46.961000', '2021-07-10 10:27:43.233000',\n               '2021-07-12 10:17:47.866000', '2021-07-20 10:27:43.424000',\n               '2021-07-22 10:17:48.210000', '2021-08-06 10:17:45.959000',\n               '2021-08-09 10:27:42.199000', '2021-08-11 10:17:47.398000',\n               '2021-08-14 10:27:43.878000', '2021-08-16 10:17:44.680000',\n               '2021-08-19 10:27:40.757000', '2021-08-21 10:17:47.043000'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>bandPandasIndex<pre>PandasIndex(Index(['blue', 'green', 'red', 'nir', 'scl'], dtype='object', name='band'))</pre></li><li>xPandasIndex<pre>PandasIndex(Float64Index([675820.0, 675830.0, 675840.0, 675850.0, 675860.0, 675870.0,\n              675880.0, 675890.0, 675900.0, 675910.0,\n              ...\n              685720.0, 685730.0, 685740.0, 685750.0, 685760.0, 685770.0,\n              685780.0, 685790.0, 685800.0, 685810.0],\n             dtype='float64', name='x', length=1000))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5241820.0, 5241810.0, 5241800.0, 5241790.0, 5241780.0, 5241770.0,\n              5241760.0, 5241750.0, 5241740.0, 5241730.0,\n              ...\n              5231920.0, 5231910.0, 5231900.0, 5231890.0, 5231880.0, 5231870.0,\n              5231860.0, 5231850.0, 5231840.0, 5231830.0],\n             dtype='float64', name='y', length=1000))</pre></li></ul></li><li>Attributes: (4)spec :RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0, 5300040.0), resolutions_xy=(10.0, 10.0))crs :epsg:32632transform :| 10.00, 0.00, 600000.00| | 0.00,-10.00, 5300040.00| | 0.00, 0.00, 1.00|resolution :10.0</li></ul> <p>Now we trigger the stacking of these selected bands for our area-of-interest by calling <code>compute()</code>, i.e. we load the data into memory. This may take some minutes but then enables us to analyse and plot the data faster. Alternatively (also for larger datasets), we could continue with the lazy array and in subsequent steps only load what is needed.</p> In\u00a0[40]: Copied! <pre>%%time\ndata = stack_aoi.compute()\n</pre> %%time data = stack_aoi.compute() <pre>CPU times: total: 56.2 s\nWall time: 5min 41s\n</pre> In\u00a0[41]: Copied! <pre>data\n</pre> data Out[41]: <pre>&lt;xarray.DataArray 'stackstac-d35de160ec768a0c51870cec97c80190' (time: 42,\n                                                                band: 5,\n                                                                y: 1000, x: 1000)&gt;\narray([[[[1.3129e+04, 1.2153e+04, 1.2209e+04, ..., 1.8400e+02,\n          1.9900e+02, 1.8600e+02],\n         [1.2824e+04, 1.3097e+04, 1.3017e+04, ..., 1.9100e+02,\n          2.0500e+02, 2.0300e+02],\n         [1.2417e+04, 1.3104e+04, 1.3577e+04, ..., 2.0500e+02,\n          2.0700e+02, 2.0900e+02],\n         ...,\n         [2.1300e+02, 2.3700e+02, 4.7400e+02, ..., 1.5430e+03,\n          1.9900e+03, 2.4620e+03],\n         [2.1100e+02, 1.7800e+02, 2.8200e+02, ..., 1.8520e+03,\n          1.8380e+03, 2.3080e+03],\n         [3.2000e+02, 1.6200e+02, 1.9400e+02, ..., 1.9290e+03,\n          1.5350e+03, 1.5050e+03]],\n\n        [[1.3055e+04, 1.2049e+04, 1.2064e+04, ..., 1.7600e+02,\n          2.1900e+02, 1.9800e+02],\n         [1.2845e+04, 1.3066e+04, 1.2967e+04, ..., 1.8200e+02,\n          2.7200e+02, 2.2100e+02],\n         [1.2360e+04, 1.3113e+04, 1.3719e+04, ..., 2.1500e+02,\n          2.6600e+02, 2.3100e+02],\n...\n         [2.5240e+03, 2.7640e+03, 2.4980e+03, ..., 3.4220e+03,\n          3.8840e+03, 4.8160e+03],\n         [2.6360e+03, 2.5460e+03, 2.4480e+03, ..., 3.0460e+03,\n          3.0180e+03, 3.1860e+03],\n         [2.0360e+03, 2.0800e+03, 2.3320e+03, ..., 3.1400e+03,\n          3.1740e+03, 2.4180e+03]],\n\n        [[3.0000e+00, 3.0000e+00, 3.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [3.0000e+00, 3.0000e+00, 3.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [2.0000e+00, 2.0000e+00, 2.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         ...,\n         [4.0000e+00, 4.0000e+00, 4.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [4.0000e+00, 4.0000e+00, 4.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [4.0000e+00, 4.0000e+00, 4.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00]]]])\nCoordinates: (12/52)\n  * time                                     (time) datetime64[ns] 2021-04-01...\n    id                                       (time) &lt;U24 'S2B_32TPT_20210401_...\n  * band                                     (band) &lt;U12 'blue' ... 'scl'\n  * x                                        (x) float64 6.758e+05 ... 6.858e+05\n  * y                                        (y) float64 5.242e+06 ... 5.232e+06\n    s2:medium_proba_clouds_percentage        (time) float64 1.463 16.5 ... 2.701\n    ...                                       ...\n    title                                    (band) &lt;U31 'Blue (band 2) - 10m...\n    gsd                                      (band) object 10 10 10 10 None\n    common_name                              (band) object 'blue' ... None\n    center_wavelength                        (band) object 0.49 0.56 ... None\n    full_width_half_max                      (band) object 0.098 0.045 ... None\n    epsg                                     int32 32632\nAttributes:\n    spec:        RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0...\n    crs:         epsg:32632\n    transform:   | 10.00, 0.00, 600000.00|\\n| 0.00,-10.00, 5300040.00|\\n| 0.0...\n    resolution:  10.0</pre>xarray.DataArray'stackstac-d35de160ec768a0c51870cec97c80190'<ul><li>time: 42</li><li>band: 5</li><li>y: 1000</li><li>x: 1000</li></ul><ul><li>1.313e+04 1.215e+04 1.221e+04 1.203e+04 1.201e+04 ... 4.0 4.0 4.0 4.0<pre>array([[[[1.3129e+04, 1.2153e+04, 1.2209e+04, ..., 1.8400e+02,\n          1.9900e+02, 1.8600e+02],\n         [1.2824e+04, 1.3097e+04, 1.3017e+04, ..., 1.9100e+02,\n          2.0500e+02, 2.0300e+02],\n         [1.2417e+04, 1.3104e+04, 1.3577e+04, ..., 2.0500e+02,\n          2.0700e+02, 2.0900e+02],\n         ...,\n         [2.1300e+02, 2.3700e+02, 4.7400e+02, ..., 1.5430e+03,\n          1.9900e+03, 2.4620e+03],\n         [2.1100e+02, 1.7800e+02, 2.8200e+02, ..., 1.8520e+03,\n          1.8380e+03, 2.3080e+03],\n         [3.2000e+02, 1.6200e+02, 1.9400e+02, ..., 1.9290e+03,\n          1.5350e+03, 1.5050e+03]],\n\n        [[1.3055e+04, 1.2049e+04, 1.2064e+04, ..., 1.7600e+02,\n          2.1900e+02, 1.9800e+02],\n         [1.2845e+04, 1.3066e+04, 1.2967e+04, ..., 1.8200e+02,\n          2.7200e+02, 2.2100e+02],\n         [1.2360e+04, 1.3113e+04, 1.3719e+04, ..., 2.1500e+02,\n          2.6600e+02, 2.3100e+02],\n...\n         [2.5240e+03, 2.7640e+03, 2.4980e+03, ..., 3.4220e+03,\n          3.8840e+03, 4.8160e+03],\n         [2.6360e+03, 2.5460e+03, 2.4480e+03, ..., 3.0460e+03,\n          3.0180e+03, 3.1860e+03],\n         [2.0360e+03, 2.0800e+03, 2.3320e+03, ..., 3.1400e+03,\n          3.1740e+03, 2.4180e+03]],\n\n        [[3.0000e+00, 3.0000e+00, 3.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [3.0000e+00, 3.0000e+00, 3.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [2.0000e+00, 2.0000e+00, 2.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         ...,\n         [4.0000e+00, 4.0000e+00, 4.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [4.0000e+00, 4.0000e+00, 4.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00],\n         [4.0000e+00, 4.0000e+00, 4.0000e+00, ..., 4.0000e+00,\n          4.0000e+00, 4.0000e+00]]]])</pre></li><li>Coordinates: (52)<ul><li>time(time)datetime64[ns]2021-04-01T10:27:39.520000 ... 2...<pre>array(['2021-04-01T10:27:39.520000000', '2021-04-08T10:17:41.934000000',\n       '2021-04-08T10:17:41.935000000', '2021-04-16T10:27:35.384000000',\n       '2021-04-16T10:27:35.385000000', '2021-04-21T10:27:35.793000000',\n       '2021-04-23T10:17:40.745000000', '2021-04-23T10:17:40.747000000',\n       '2021-04-28T10:17:40.197000000', '2021-05-03T10:17:42.916000000',\n       '2021-05-08T10:17:42.592000000', '2021-05-08T10:17:42.594000000',\n       '2021-05-23T10:17:45.371000000', '2021-05-26T10:27:42.080000000',\n       '2021-05-28T10:17:45.418000000', '2021-05-28T10:17:45.421000000',\n       '2021-05-31T10:27:42.147000000', '2021-05-31T10:27:42.149000000',\n       '2021-06-02T10:17:45.639000000', '2021-06-02T10:17:45.641000000',\n       '2021-06-10T10:27:42.429000000', '2021-06-15T10:27:41.696000000',\n       '2021-06-15T10:27:41.697000000', '2021-06-17T10:17:45.721000000',\n       '2021-06-17T10:17:45.724000000', '2021-06-22T10:17:45.729000000',\n       '2021-06-22T10:17:45.732000000', '2021-06-27T10:17:45.604000000',\n       '2021-06-27T10:17:45.605000000', '2021-07-02T10:17:46.959000000',\n       '2021-07-02T10:17:46.961000000', '2021-07-10T10:27:43.233000000',\n       '2021-07-12T10:17:47.866000000', '2021-07-20T10:27:43.424000000',\n       '2021-07-22T10:17:48.210000000', '2021-08-06T10:17:45.959000000',\n       '2021-08-09T10:27:42.199000000', '2021-08-11T10:17:47.398000000',\n       '2021-08-14T10:27:43.878000000', '2021-08-16T10:17:44.680000000',\n       '2021-08-19T10:27:40.757000000', '2021-08-21T10:17:47.043000000'],\n      dtype='datetime64[ns]')</pre></li><li>id(time)&lt;U24'S2B_32TPT_20210401_0_L2A' ... '...<pre>array(['S2B_32TPT_20210401_0_L2A', 'S2B_32TPT_20210408_2_L2A',\n       'S2B_32TPT_20210408_0_L2A', 'S2A_32TPT_20210416_1_L2A',\n       'S2A_32TPT_20210416_0_L2A', 'S2B_32TPT_20210421_0_L2A',\n       'S2A_32TPT_20210423_2_L2A', 'S2A_32TPT_20210423_0_L2A',\n       'S2B_32TPT_20210428_0_L2A', 'S2A_32TPT_20210503_0_L2A',\n       'S2B_32TPT_20210508_1_L2A', 'S2B_32TPT_20210508_0_L2A',\n       'S2A_32TPT_20210523_0_L2A', 'S2A_32TPT_20210526_0_L2A',\n       'S2B_32TPT_20210528_1_L2A', 'S2B_32TPT_20210528_0_L2A',\n       'S2B_32TPT_20210531_1_L2A', 'S2B_32TPT_20210531_0_L2A',\n       'S2A_32TPT_20210602_1_L2A', 'S2A_32TPT_20210602_0_L2A',\n       'S2B_32TPT_20210610_0_L2A', 'S2A_32TPT_20210615_0_L2A',\n       'S2A_32TPT_20210615_1_L2A', 'S2B_32TPT_20210617_1_L2A',\n       'S2B_32TPT_20210617_0_L2A', 'S2A_32TPT_20210622_1_L2A',\n       'S2A_32TPT_20210622_0_L2A', 'S2B_32TPT_20210627_1_L2A',\n       'S2B_32TPT_20210627_0_L2A', 'S2A_32TPT_20210702_1_L2A',\n       'S2A_32TPT_20210702_0_L2A', 'S2B_32TPT_20210710_0_L2A',\n       'S2A_32TPT_20210712_0_L2A', 'S2B_32TPT_20210720_0_L2A',\n       'S2A_32TPT_20210722_0_L2A', 'S2B_32TPT_20210806_0_L2A',\n       'S2B_32TPT_20210809_0_L2A', 'S2A_32TPT_20210811_0_L2A',\n       'S2A_32TPT_20210814_0_L2A', 'S2B_32TPT_20210816_0_L2A',\n       'S2B_32TPT_20210819_0_L2A', 'S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U24')</pre></li><li>band(band)&lt;U12'blue' 'green' 'red' 'nir' 'scl'<pre>array(['blue', 'green', 'red', 'nir', 'scl'], dtype='&lt;U12')</pre></li><li>x(x)float646.758e+05 6.758e+05 ... 6.858e+05<pre>array([675820., 675830., 675840., ..., 685790., 685800., 685810.])</pre></li><li>y(y)float645.242e+06 5.242e+06 ... 5.232e+06<pre>array([5241820., 5241810., 5241800., ..., 5231850., 5231840., 5231830.])</pre></li><li>s2:medium_proba_clouds_percentage(time)float641.463 16.5 14.33 ... 4.624 2.701<pre>array([ 1.46334 , 16.504081, 14.328757, 10.298105, 16.633525,  7.706635,\n        1.899826,  2.313006,  3.444555, 12.165798,  6.054812,  6.262551,\n        8.735961,  8.490147,  6.304692,  6.609003,  0.209097,  0.926137,\n       12.20682 ,  3.0033  ,  6.494882,  0.308812,  0.057903,  9.846946,\n        2.624075, 10.38753 ,  2.671621,  2.953941,  1.214524, 14.579228,\n        6.361181,  1.917733,  5.046743,  5.886542,  3.49712 ,  5.288142,\n        8.231435,  4.027119,  0.841558, 19.751492,  4.62429 ,  2.701251])</pre></li><li>earthsearch:boa_offset_applied(time)boolFalse True False ... False False<pre>array([False,  True, False,  True, False, False,  True, False, False,\n       False,  True, False, False, False,  True, False,  True, False,\n        True, False, False, False,  True,  True, False,  True, False,\n        True, False,  True, False, False, False, False, False, False,\n       False, False, False, False, False, False])</pre></li><li>processing:software(time)object{'sentinel2-to-stac': '0.1.0'} ....<pre>array([{'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.1'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.1'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'},\n       {'sentinel2-to-stac': '0.1.0'}, {'sentinel2-to-stac': '0.1.0'}],\n      dtype=object)</pre></li><li>proj:epsg()int3232632<pre>array(32632)</pre></li><li>s2:generation_time(time)&lt;U27'2021-04-01T14:19:13.000000Z' .....<pre>array(['2021-04-01T14:19:13.000000Z', '2023-05-13T12:44:55.000000Z',\n       '2021-04-08T13:26:17.000000Z', '2023-05-19T13:18:55.000000Z',\n       '2021-04-16T13:20:57.000000Z', '2021-04-21T13:46:13.000000Z',\n       '2023-05-12T19:45:03.000000Z', '2021-04-23T12:55:30.000000Z',\n       '2021-04-28T13:40:12.000000Z', '2021-05-03T12:17:41.000000Z',\n       '2023-02-27T09:29:40.000000Z', '2021-05-08T14:56:55.000000Z',\n       '2021-05-23T12:13:33.000000Z', '2021-05-26T13:34:25.000000Z',\n       '2023-02-27T08:23:12.000000Z', '2021-05-28T13:08:36.000000Z',\n       '2023-03-01T00:24:41.000000Z', '2021-05-31T14:00:40.000000Z',\n       '2023-03-13T04:04:11.000000Z', '2021-06-02T17:13:07.000000Z',\n       '2021-06-10T13:27:39.000000Z', '2021-06-15T13:16:59.000000Z',\n       '2023-03-21T22:10:50.000000Z', '2023-03-21T22:17:28.000000Z',\n       '2021-06-17T13:12:19.000000Z', '2023-01-29T15:30:31.000000Z',\n       '2021-06-22T12:13:05.000000Z', '2023-03-18T23:38:59.000000Z',\n       '2021-06-27T13:58:56.000000Z', '2023-01-25T13:05:14.000000Z',\n       '2021-07-02T12:13:53.000000Z', '2021-07-10T13:23:21.000000Z',\n       '2021-07-12T13:22:19.000000Z', '2021-07-20T13:22:07.000000Z',\n       '2021-07-22T12:02:14.000000Z', '2021-08-06T15:00:37.000000Z',\n       '2021-08-14T13:01:57.000000Z', '2021-08-11T16:05:21.000000Z',\n       '2021-08-14T13:23:26.000000Z', '2021-08-16T13:08:48.000000Z',\n       '2021-08-19T13:53:08.000000Z', '2021-08-21T12:05:55.000000Z'],\n      dtype='&lt;U27')</pre></li><li>created(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-03T11:38:25.089Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-03T11:19:16.880Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-03T11:12:48.494Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:dark_features_percentage(time)float647.618 3.489 2.558 ... 2.943 2.726<pre>array([7.618114, 3.489247, 2.558084, 5.739467, 4.485189, 2.807637,\n       5.218234, 2.874529, 1.733305, 1.861065, 2.493159, 1.334918,\n       0.416572, 1.220573, 1.255598, 0.764608, 2.384401, 1.463244,\n       0.931227, 1.316954, 1.090162, 0.8525  , 1.352069, 0.733103,\n       1.135998, 1.516507, 1.200472, 1.417252, 0.957145, 1.22749 ,\n       1.375679, 1.512192, 0.560178, 1.777653, 1.21489 , 2.177998,\n       2.577018, 1.8398  , 3.048143, 1.672873, 2.943032, 2.726397])</pre></li><li>s2:datatake_type()&lt;U8'INS-NOBS'<pre>array('INS-NOBS', dtype='&lt;U8')</pre></li><li>s2:water_percentage(time)float641.035 0.3132 ... 0.3022 0.8797<pre>array([1.034616, 0.313168, 0.424308, 0.617732, 0.819981, 0.987005,\n       0.857674, 1.026752, 0.751145, 0.66969 , 0.589903, 0.582099,\n       0.374695, 0.464075, 0.43538 , 0.426498, 0.905694, 0.953868,\n       0.768842, 1.276473, 0.616264, 0.743187, 0.848236, 0.84215 ,\n       1.075244, 0.699068, 0.829451, 0.73024 , 0.757141, 0.394683,\n       0.510065, 0.539884, 0.785743, 0.768489, 1.082259, 0.712835,\n       0.559107, 0.910735, 1.067886, 0.465221, 0.302152, 0.879734])</pre></li><li>eo:cloud_cover(time)float646.149 45.79 45.55 ... 44.97 11.88<pre>array([ 6.149054, 45.791623, 45.550924, 25.692633, 33.007648, 43.998277,\n        3.953265,  4.693773, 35.548758, 39.265612, 15.81282 , 15.84441 ,\n       49.168627, 45.008284, 26.705834, 28.351225,  0.365331,  1.409963,\n       33.068579, 24.769196, 47.15134 ,  1.805315,  1.849951, 26.098341,\n       19.611905, 25.549465, 18.602191, 10.19    ,  9.07656 , 44.637647,\n       35.395256, 16.658998, 10.469738, 24.9592  , 18.490819, 28.947858,\n       27.45625 , 24.8152  ,  3.156408, 49.208091, 44.972815, 11.884551])</pre></li><li>view:sun_azimuth(time)float64161.2 157.5 157.5 ... 158.4 155.0<pre>array([161.22281178, 157.46645516, 157.46542095, 160.80126617,\n       160.80014335, 160.57908911, 156.51420567, 156.51306183,\n       156.0684864 , 155.56597523, 154.95741812, 154.95616171,\n       152.69773459, 156.88007906, 151.8422183 , 151.84086683,\n       156.07844848, 156.07704495, 150.97550979, 150.97415895,\n       154.47087908, 153.7373982 , 153.738758  , 148.65682968,\n       148.65553825, 148.11111459, 148.10983402, 147.7307707 ,\n       147.72954199, 147.54553355, 147.54435203, 152.24342398,\n       147.75100857, 152.92620053, 148.7434059 , 151.42303732,\n       156.18470534, 152.56163454, 157.28195674, 153.75067448,\n       158.41022444, 155.02778045])</pre></li><li>s2:sequence(time)&lt;U1'0' '2' '0' '1' ... '0' '0' '0' '0'<pre>array(['0', '2', '0', '1', '0', '0', '2', '0', '0', '0', '1', '0', '0',\n       '0', '1', '0', '1', '0', '1', '0', '0', '0', '1', '1', '0', '1',\n       '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n       '0', '0', '0'], dtype='&lt;U1')</pre></li><li>s2:datastrip_id(time)&lt;U64'S2B_OPER_MSI_L2A_DS_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_DS_VGS4_20210401T141913_S20210401T102326_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230513T124455_S20210408T101148_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210408T132617_S20210408T101148_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230519T131855_S20210416T102600_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210416T132057_S20210416T102600_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210421T134613_S20210421T102336_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230512T194503_S20210423T101704_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210423T125530_S20210423T101704_N03.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210428T134012_S20210428T101657_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210503T121741_S20210503T101204_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T092940_S20210508T101551_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210508T145655_S20210508T101551_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210523T121333_S20210523T101416_N03.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210526T133425_S20210526T102625_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230227T082312_S20210528T100555_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210528T130836_S20210528T100555_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230301T002441_S20210531T102618_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210531T140040_S20210531T102618_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230313T040411_S20210602T101235_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210602T171307_S20210602T101235_N03.00',\n...\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230321T221728_S20210617T101301_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210617T131219_S20210617T101301_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230129T153031_S20210622T101416_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210622T121305_S20210622T101416_N03.00',\n       'S2B_OPER_MSI_L2A_DS_S2RP_20230318T233859_S20210627T101702_N05.00',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210627T135856_S20210627T101702_N03.00',\n       'S2A_OPER_MSI_L2A_DS_S2RP_20230125T130514_S20210702T101157_N05.00',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210702T121353_S20210702T101157_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210710T132321_S20210710T102312_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210712T132219_S20210712T101027_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210720T132207_S20210720T101617_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210722T120214_S20210722T101419_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210806T150037_S20210806T101207_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS1_20210814T130157_S20210809T102637_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210811T160521_S20210811T101411_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS2_20210814T132326_S20210814T102609_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS2_20210816T130848_S20210816T101300_N03.01',\n       'S2B_OPER_MSI_L2A_DS_VGS4_20210819T135308_S20210819T102635_N03.01',\n       'S2A_OPER_MSI_L2A_DS_VGS4_20210821T120555_S20210821T101418_N03.01'],\n      dtype='&lt;U64')</pre></li><li>mgrs:grid_square()&lt;U2'PT'<pre>array('PT', dtype='&lt;U2')</pre></li><li>instruments()&lt;U3'msi'<pre>array('msi', dtype='&lt;U3')</pre></li><li>s2:thin_cirrus_percentage(time)float640.4854 0.4475 ... 0.004964 3.404<pre>array([4.8540100e-01, 4.4747100e-01, 1.9159610e+00, 2.4920600e-01,\n       8.8436700e-01, 2.2049110e+00, 8.2425000e-02, 7.5833000e-02,\n       1.1564730e+00, 2.1922420e+00, 8.2073200e+00, 7.6982920e+00,\n       1.3726480e+00, 2.5636340e+00, 1.5205705e+01, 1.6151080e+01,\n       4.1297000e-02, 4.4017000e-02, 5.1187000e-01, 8.5886000e-01,\n       1.7029480e+00, 1.1063510e+00, 1.7621680e+00, 1.0222800e-01,\n       2.7094100e-01, 1.3869100e-01, 2.2755900e-01, 4.5100390e+00,\n       4.5370160e+00, 5.0657700e+00, 3.5785940e+00, 9.4725660e+00,\n       3.9136060e+00, 2.2286300e-01, 5.0741000e-02, 8.8339000e-02,\n       8.7864210e+00, 1.1323400e-01, 3.8490000e-03, 8.7228200e-01,\n       4.9640000e-03, 3.4040560e+00])</pre></li><li>s2:vegetation_percentage(time)float6426.34 3.007 3.274 ... 36.39 66.22<pre>array([26.339936,  3.007327,  3.273584, 11.632005, 12.624834, 18.579322,\n       34.706536, 36.717805, 22.266363, 18.495288, 33.440745, 34.884864,\n       14.592455, 19.679427, 29.31349 , 30.193511, 59.83513 , 60.953248,\n       39.538234, 42.743748, 29.41438 , 71.402311, 70.151877, 51.991493,\n       55.270773, 53.398359, 56.250197, 68.942064, 69.730705, 38.577574,\n       43.56502 , 66.226125, 71.196151, 49.621934, 62.576228, 49.759224,\n       54.908115, 55.141944, 78.077263, 32.121503, 36.389408, 66.223991])</pre></li><li>s2:high_proba_clouds_percentage(time)float644.2 28.84 29.31 ... 40.34 5.779<pre>array([4.2003130e+00, 2.8840071e+01, 2.9306206e+01, 1.5145321e+01,\n       1.5489756e+01, 3.4086731e+01, 1.9710150e+00, 2.3049340e+00,\n       3.0947730e+01, 2.4907573e+01, 1.5506890e+00, 1.8835670e+00,\n       3.9060017e+01, 3.3954504e+01, 5.1954370e+00, 5.5911420e+00,\n       1.1493700e-01, 4.3980900e-01, 2.0349889e+01, 2.0907035e+01,\n       3.8953510e+01, 3.9015200e-01, 2.9880000e-02, 1.6149166e+01,\n       1.6716889e+01, 1.5023246e+01, 1.5703011e+01, 2.7260200e+00,\n       3.3250190e+00, 2.4992649e+01, 2.5455481e+01, 5.2686990e+00,\n       1.5093900e+00, 1.8849795e+01, 1.4942957e+01, 2.3571377e+01,\n       1.0438395e+01, 2.0674847e+01, 2.3110010e+00, 2.8584316e+01,\n       4.0343562e+01, 5.7792450e+00])</pre></li><li>s2:unclassified_percentage(time)float645.542 1.249 4.882 ... 4.302 3.051<pre>array([5.542233, 1.249116, 4.882131, 1.901653, 8.202816, 3.343896,\n       0.369042, 4.536037, 5.607506, 5.386119, 0.42674 , 5.226327,\n       5.411838, 6.440622, 1.1914  , 7.262506, 0.095849, 2.831393,\n       0.502138, 3.365285, 5.884814, 2.064657, 0.053503, 0.43447 ,\n       3.136941, 0.979966, 3.44101 , 0.372121, 1.875458, 1.251701,\n       4.816858, 1.852196, 2.070124, 6.984119, 4.280468, 5.570791,\n       3.381409, 4.446889, 1.608293, 5.839212, 4.302198, 3.050546])</pre></li><li>s2:snow_ice_percentage(time)float6440.65 40.93 39.09 ... 1.161 0.858<pre>array([40.654615, 40.934548, 39.088675, 45.368615, 33.28197 , 21.687628,\n       44.689843, 40.108246, 25.425422, 26.355195, 37.636858, 35.119075,\n       27.416307, 21.791345, 30.055815, 26.366636, 27.95321 , 24.107485,\n       13.354345, 16.522175,  8.769823, 12.960622, 16.334337,  8.544581,\n        8.742414,  6.522872,  6.389479,  7.112196,  5.816885,  4.461629,\n        4.215506,  2.77441 ,  3.27045 ,  1.624512,  1.15363 ,  1.572428,\n        1.036823,  0.380159,  0.842441,  2.167679,  1.161288,  0.857967])</pre></li><li>earthsearch:s3_path(time)&lt;U79's3://sentinel-cogs/sentinel-s2-...<pre>array(['s3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210401_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210408_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210416_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210421_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_2_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2A_32TPT_20210423_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/4/S2B_32TPT_20210428_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210503_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210508_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210523_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2A_32TPT_20210526_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210528_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/5/S2B_32TPT_20210531_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210602_0_L2A',\n...\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210617_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2A_32TPT_20210622_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/6/S2B_32TPT_20210627_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_1_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210702_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210710_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210712_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2B_32TPT_20210720_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/7/S2A_32TPT_20210722_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210806_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210809_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210811_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210814_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210816_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2B_32TPT_20210819_0_L2A',\n       's3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2021/8/S2A_32TPT_20210821_0_L2A'],\n      dtype='&lt;U79')</pre></li><li>s2:granule_id(time)&lt;U62'S2B_OPER_MSI_L2A_TL_VGS4_202104...<pre>array(['S2B_OPER_MSI_L2A_TL_VGS4_20210401T141913_A021255_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230513T124455_A021355_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210408T132617_A021355_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230519T131855_A030378_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210416T132057_A030378_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210421T134613_A021541_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230512T194503_A030478_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210423T125530_A030478_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210428T134012_A021641_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210503T121741_A030621_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T092940_A021784_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210508T145655_A021784_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210523T121333_A030907_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210526T133425_A030950_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230227T082312_A022070_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210528T130836_A022070_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230301T002441_A022113_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210531T140040_A022113_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230313T040411_A031050_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210602T171307_A031050_T32TPT_N03.00',\n...\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230321T221728_A022356_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210617T131219_A022356_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230129T153031_A031336_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210622T121305_A031336_T32TPT_N03.00',\n       'S2B_OPER_MSI_L2A_TL_S2RP_20230318T233859_A022499_T32TPT_N05.00',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210627T135856_A022499_T32TPT_N03.00',\n       'S2A_OPER_MSI_L2A_TL_S2RP_20230125T130514_A031479_T32TPT_N05.00',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210702T121353_A031479_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210710T132321_A022685_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210712T132219_A031622_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210720T132207_A022828_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210722T120214_A031765_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210806T150037_A023071_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS1_20210814T130157_A023114_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210811T160521_A032051_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS2_20210814T132326_A032094_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS2_20210816T130848_A023214_T32TPT_N03.01',\n       'S2B_OPER_MSI_L2A_TL_VGS4_20210819T135308_A023257_T32TPT_N03.01',\n       'S2A_OPER_MSI_L2A_TL_VGS4_20210821T120555_A032194_T32TPT_N03.01'],\n      dtype='&lt;U62')</pre></li><li>constellation()&lt;U10'sentinel-2'<pre>array('sentinel-2', dtype='&lt;U10')</pre></li><li>s2:reflectance_conversion_factor(time)float641.004 0.9999 ... 0.9753 0.9761<pre>array([1.00391722, 0.99989016, 0.99989016, 0.99531243, 0.99531243,\n       0.99251188, 0.99141059, 0.99141059, 0.98870861, 0.98609676,\n       0.98359974, 0.98359974, 0.97693835, 0.97577919, 0.97504789,\n       0.97504789, 0.97399904, 0.97399904, 0.97334113, 0.97334113,\n       0.97102521, 0.96985123, 0.96985123, 0.969445  , 0.969445  ,\n       0.96858002, 0.96858002, 0.96794362, 0.96794362, 0.96753822,\n       0.96753822, 0.96737784, 0.96743179, 0.96802315, 0.96826343,\n       0.97121659, 0.97204341, 0.9726335 , 0.97358286, 0.97425016,\n       0.97531576, 0.97606015])</pre></li><li>view:sun_elevation(time)float6445.93 48.03 48.03 ... 53.63 52.36<pre>array([45.92877963, 48.02966446, 48.0295066 , 51.57725751, 51.57711889,\n       53.32274987, 53.3762054 , 53.37603569, 54.97817306, 56.47292004,\n       57.84102398, 57.84083568, 61.10137652, 62.31302901, 61.87597965,\n       61.87575968, 63.010744  , 63.01055231, 62.48509786, 62.4848721 ,\n       63.89999116, 64.08851703, 64.08872077, 63.30105997, 63.30082564,\n       63.24315182, 63.24291386, 63.02832992, 63.02809443, 62.66520394,\n       62.66497104, 62.62975879, 61.51305241, 61.04463203, 59.85139625,\n       56.52628032, 56.4892853 , 55.22451323, 55.10440682, 53.83083839,\n       53.63004967, 52.36396242])</pre></li><li>platform(time)&lt;U11'sentinel-2b' ... 'sentinel-2a'<pre>array(['sentinel-2b', 'sentinel-2b', 'sentinel-2b', 'sentinel-2a',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2a', 'sentinel-2b', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2b', 'sentinel-2a', 'sentinel-2a',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2a', 'sentinel-2b', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a', 'sentinel-2a', 'sentinel-2b',\n       'sentinel-2b', 'sentinel-2a'], dtype='&lt;U11')</pre></li><li>mgrs:latitude_band()&lt;U1'T'<pre>array('T', dtype='&lt;U1')</pre></li><li>s2:product_uri(time)&lt;U65'S2B_MSIL2A_20210401T101559_N030...<pre>array(['S2B_MSIL2A_20210401T101559_N0300_R065_T32TPT_20210401T141913.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0500_R022_T32TPT_20230513T124455.SAFE',\n       'S2B_MSIL2A_20210408T100549_N0300_R022_T32TPT_20210408T132617.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0500_R065_T32TPT_20230519T131855.SAFE',\n       'S2A_MSIL2A_20210416T102021_N0300_R065_T32TPT_20210416T132057.SAFE',\n       'S2B_MSIL2A_20210421T101549_N0300_R065_T32TPT_20210421T134613.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0500_R022_T32TPT_20230512T194503.SAFE',\n       'S2A_MSIL2A_20210423T101021_N0300_R022_T32TPT_20210423T125530.SAFE',\n       'S2B_MSIL2A_20210428T100549_N0300_R022_T32TPT_20210428T134012.SAFE',\n       'S2A_MSIL2A_20210503T101021_N0300_R022_T32TPT_20210503T121741.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0500_R022_T32TPT_20230227T092940.SAFE',\n       'S2B_MSIL2A_20210508T100549_N0300_R022_T32TPT_20210508T145655.SAFE',\n       'S2A_MSIL2A_20210523T101031_N0300_R022_T32TPT_20210523T121333.SAFE',\n       'S2A_MSIL2A_20210526T102021_N0300_R065_T32TPT_20210526T133425.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0500_R022_T32TPT_20230227T082312.SAFE',\n       'S2B_MSIL2A_20210528T100559_N0300_R022_T32TPT_20210528T130836.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0500_R065_T32TPT_20230301T002441.SAFE',\n       'S2B_MSIL2A_20210531T101559_N0300_R065_T32TPT_20210531T140040.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0500_R022_T32TPT_20230313T040411.SAFE',\n       'S2A_MSIL2A_20210602T101031_N0300_R022_T32TPT_20210602T171307.SAFE',\n...\n       'S2B_MSIL2A_20210617T100559_N0500_R022_T32TPT_20230321T221728.SAFE',\n       'S2B_MSIL2A_20210617T100559_N0300_R022_T32TPT_20210617T131219.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0500_R022_T32TPT_20230129T153031.SAFE',\n       'S2A_MSIL2A_20210622T101031_N0300_R022_T32TPT_20210622T121305.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0500_R022_T32TPT_20230318T233859.SAFE',\n       'S2B_MSIL2A_20210627T100559_N0300_R022_T32TPT_20210627T135856.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0500_R022_T32TPT_20230125T130514.SAFE',\n       'S2A_MSIL2A_20210702T101031_N0301_R022_T32TPT_20210702T121353.SAFE',\n       'S2B_MSIL2A_20210710T101559_N0301_R065_T32TPT_20210710T132321.SAFE',\n       'S2A_MSIL2A_20210712T101031_N0301_R022_T32TPT_20210712T132219.SAFE',\n       'S2B_MSIL2A_20210720T101559_N0301_R065_T32TPT_20210720T132207.SAFE',\n       'S2A_MSIL2A_20210722T101031_N0301_R022_T32TPT_20210722T120214.SAFE',\n       'S2B_MSIL2A_20210806T100559_N0301_R022_T32TPT_20210806T150037.SAFE',\n       'S2B_MSIL2A_20210809T101559_N0301_R065_T32TPT_20210814T130157.SAFE',\n       'S2A_MSIL2A_20210811T101031_N0301_R022_T32TPT_20210811T160521.SAFE',\n       'S2A_MSIL2A_20210814T102031_N0301_R065_T32TPT_20210814T132326.SAFE',\n       'S2B_MSIL2A_20210816T100559_N0301_R022_T32TPT_20210816T130848.SAFE',\n       'S2B_MSIL2A_20210819T101559_N0301_R065_T32TPT_20210819T135308.SAFE',\n       'S2A_MSIL2A_20210821T101031_N0301_R022_T32TPT_20210821T120555.SAFE'],\n      dtype='&lt;U65')</pre></li><li>updated(time)&lt;U24'2022-11-06T04:44:05.282Z' ... '...<pre>array(['2022-11-06T04:44:05.282Z', '2023-08-31T07:50:21.366Z',\n       '2022-11-06T04:44:21.881Z', '2023-06-27T15:48:44.610Z',\n       '2022-11-06T04:46:03.520Z', '2022-11-06T04:44:36.763Z',\n       '2023-09-02T22:59:26.724Z', '2022-11-06T04:44:29.648Z',\n       '2022-11-06T04:47:07.787Z', '2022-11-06T04:45:41.579Z',\n       '2023-07-12T22:45:45.419Z', '2022-11-06T04:46:58.082Z',\n       '2022-11-06T14:14:40.862Z', '2022-11-06T14:14:42.036Z',\n       '2023-07-19T01:19:52.871Z', '2022-11-06T04:45:50.443Z',\n       '2023-08-21T02:41:26.150Z', '2022-11-06T04:42:54.266Z',\n       '2023-08-18T18:44:11.699Z', '2022-11-06T04:47:05.944Z',\n       '2022-11-06T04:46:58.841Z', '2022-11-06T04:45:38.682Z',\n       '2023-08-28T20:22:34.463Z', '2023-08-28T19:45:37.020Z',\n       '2022-11-06T04:47:14.932Z', '2023-07-12T22:29:54.601Z',\n       '2022-11-06T04:45:35.210Z', '2023-09-02T00:13:07.819Z',\n       '2022-11-06T04:44:34.328Z', '2023-07-25T03:09:11.888Z',\n       '2022-11-06T13:38:02.731Z', '2022-11-06T04:44:39.696Z',\n       '2022-11-06T04:44:34.118Z', '2022-11-06T04:47:25.362Z',\n       '2022-11-06T04:47:25.002Z', '2022-11-06T13:38:06.945Z',\n       '2022-11-06T13:38:10.031Z', '2022-11-06T04:46:31.465Z',\n       '2022-11-06T04:46:35.572Z', '2022-11-06T04:45:55.146Z',\n       '2022-11-06T04:45:59.733Z', '2022-11-06T04:44:41.651Z'],\n      dtype='&lt;U24')</pre></li><li>s2:product_type()&lt;U7'S2MSI2A'<pre>array('S2MSI2A', dtype='&lt;U7')</pre></li><li>s2:saturated_defective_pixel_percentage()int320<pre>array(0)</pre></li><li>s2:degraded_msi_data_percentage(time)object0 0.1898 0 0.0825 0 0 ... 0 0 0 0 0<pre>array([0, 0.1898, 0, 0.0825, 0, 0, 0.0161, 0, 0, 0, 0.0259, 0, 0, 0,\n       0.0119, 0, 0.0204, 0, 0.0664, 0, 0, 0, 0.0225, 0.0382, 0, 0.0347,\n       0, 0.012, 0, 0.021, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n      dtype=object)</pre></li><li>grid:code()&lt;U10'MGRS-32TPT'<pre>array('MGRS-32TPT', dtype='&lt;U10')</pre></li><li>s2:nodata_pixel_percentage(time)object0.002973 9.284004 ... 8.773684<pre>array([0.002973, 9.284004, 8.883444, 0, 3.6e-05, 6e-05, 9.639984,\n       9.534544, 8.656428, 9.141818, 9.115126, 8.715048, 8.926022,\n       0.001904, 8.633243, 8.246658, 0.023792, 0.013444, 9.047445,\n       8.948952, 0.009396, 0.000113, 1.7e-05, 8.783989, 8.390699, 9.33119,\n       9.234183, 8.918451, 8.525366, 9.049369, 8.950624, 0.010153,\n       8.783641, 0.010524, 8.777927, 8.489713, 0.004336, 8.84221,\n       0.003726, 8.610921, 0.000985, 8.773684], dtype=object)</pre></li><li>s2:cloud_shadow_percentage(time)float644.934 0.4877 2.028 ... 2.541 4.589<pre>array([4.934268, 0.487673, 2.027822, 0.962452, 3.386088, 5.310493,\n       0.704442, 2.288749, 3.619292, 4.858182, 0.008787, 0.596837,\n       1.866554, 1.606319, 0.439432, 0.873589, 0.13241 , 0.22497 ,\n       6.900295, 4.897763, 5.058461, 0.094422, 0.031267, 5.442425,\n       4.135132, 2.465164, 2.53105 , 0.682236, 0.868314, 3.576069,\n       2.60878 , 1.147319, 0.079854, 6.248215, 3.624339, 3.247687,\n       1.521886, 5.362982, 1.95151 , 2.662793, 2.541466, 4.588702])</pre></li><li>s2:not_vegetated_percentage(time)float647.727 4.727 2.194 ... 7.388 9.788<pre>array([ 7.727165,  4.727299,  2.194477,  8.085445,  4.191478,  3.285741,\n        9.500962,  7.754106,  5.04821 ,  3.108844,  9.590986,  6.41147 ,\n        0.752953,  3.789353, 10.603052,  5.761432,  8.327983,  8.055834,\n        4.936339,  5.108408,  2.014758, 10.076986,  9.378749,  5.913443,\n        6.891589,  8.868597, 10.756154, 10.553885, 10.917792,  5.873202,\n        7.512838,  9.288875, 11.567761,  8.01588 ,  7.577366,  8.01118 ,\n        8.559394,  7.102291, 10.248062,  5.862629,  7.38764 ,  9.788115])</pre></li><li>s2:datatake_id(time)&lt;U34'GS2B_20210401T101559_021255_N03...<pre>array(['GS2B_20210401T101559_021255_N03.00',\n       'GS2B_20210408T100549_021355_N05.00',\n       'GS2B_20210408T100549_021355_N03.00',\n       'GS2A_20210416T102021_030378_N05.00',\n       'GS2A_20210416T102021_030378_N03.00',\n       'GS2B_20210421T101549_021541_N03.00',\n       'GS2A_20210423T101021_030478_N05.00',\n       'GS2A_20210423T101021_030478_N03.00',\n       'GS2B_20210428T100549_021641_N03.00',\n       'GS2A_20210503T101021_030621_N03.00',\n       'GS2B_20210508T100549_021784_N05.00',\n       'GS2B_20210508T100549_021784_N03.00',\n       'GS2A_20210523T101031_030907_N03.00',\n       'GS2A_20210526T102021_030950_N03.00',\n       'GS2B_20210528T100559_022070_N05.00',\n       'GS2B_20210528T100559_022070_N03.00',\n       'GS2B_20210531T101559_022113_N05.00',\n       'GS2B_20210531T101559_022113_N03.00',\n       'GS2A_20210602T101031_031050_N05.00',\n       'GS2A_20210602T101031_031050_N03.00',\n...\n       'GS2A_20210615T102021_031236_N05.00',\n       'GS2B_20210617T100559_022356_N05.00',\n       'GS2B_20210617T100559_022356_N03.00',\n       'GS2A_20210622T101031_031336_N05.00',\n       'GS2A_20210622T101031_031336_N03.00',\n       'GS2B_20210627T100559_022499_N05.00',\n       'GS2B_20210627T100559_022499_N03.00',\n       'GS2A_20210702T101031_031479_N05.00',\n       'GS2A_20210702T101031_031479_N03.01',\n       'GS2B_20210710T101559_022685_N03.01',\n       'GS2A_20210712T101031_031622_N03.01',\n       'GS2B_20210720T101559_022828_N03.01',\n       'GS2A_20210722T101031_031765_N03.01',\n       'GS2B_20210806T100559_023071_N03.01',\n       'GS2B_20210809T101559_023114_N03.01',\n       'GS2A_20210811T101031_032051_N03.01',\n       'GS2A_20210814T102031_032094_N03.01',\n       'GS2B_20210816T100559_023214_N03.01',\n       'GS2B_20210819T101559_023257_N03.01',\n       'GS2A_20210821T101031_032194_N03.01'], dtype='&lt;U34')</pre></li><li>s2:processing_baseline(time)&lt;U5'03.00' '05.00' ... '03.01' '03.01'<pre>array(['03.00', '05.00', '03.00', '05.00', '03.00', '03.00', '05.00',\n       '03.00', '03.00', '03.00', '05.00', '03.00', '03.00', '03.00',\n       '05.00', '03.00', '05.00', '03.00', '05.00', '03.00', '03.00',\n       '03.00', '05.00', '05.00', '03.00', '05.00', '03.00', '05.00',\n       '03.00', '05.00', '03.01', '03.01', '03.01', '03.01', '03.01',\n       '03.01', '03.01', '03.01', '03.01', '03.01', '03.01', '03.01'],\n      dtype='&lt;U5')</pre></li><li>mgrs:utm_zone()int3232<pre>array(32)</pre></li><li>earthsearch:payload_id(time)&lt;U74'roda-sentinel2/workflow-sentine...<pre>array(['roda-sentinel2/workflow-sentinel2-to-stac/ca1fac60c9640967f45658d79d1523ba',\n       'roda-sentinel2/workflow-sentinel2-to-stac/d1881ee109d59d1df5a0cfc2d85ce430',\n       'roda-sentinel2/workflow-sentinel2-to-stac/f9aa8a3191a8be696d2ddddd9734621c',\n       'roda-sentinel2/workflow-sentinel2-to-stac/57765d545d606d43efe2820f6e0d1159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/1ac3805c5018ad2b5d5989908e95d38d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/21d39a00de8e19a12451c18ba24f832b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/778637c0ddea5fd397ac6695ffe344dd',\n       'roda-sentinel2/workflow-sentinel2-to-stac/29497ba4096ac30ecf99e9603229329b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/03a8dfb01cc4bd0fbcfbb149e25da06f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/c4e5ae224b1aa55e00c6903aa25f65ee',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4f41cddc1225338c2ce7a50a413d05ff',\n       'roda-sentinel2/workflow-sentinel2-to-stac/aa4edf7495a6f12421d1142a8371e750',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4a84c0a1c9ec26453a988d58b3a1e236',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b7752cd2d7341f1cdf30140de86e4992',\n       'roda-sentinel2/workflow-sentinel2-to-stac/8d9d4f770bdb5a0a1a614b2fde70290d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/050ec18e7aef56ed8ce1d702e586175d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/fef3c2c9435cfa8276430d9f2c8fbcdb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ae91b5b1381789ef2f526f90e0d7a89f',\n       'roda-sentinel2/workflow-sentinel2-to-stac/e987ce36035c8be203930ef9d5bcfb55',\n       'roda-sentinel2/workflow-sentinel2-to-stac/64087c1420a6067f46412cfc0215cf07',\n...\n       'roda-sentinel2/workflow-sentinel2-to-stac/1dc42ce3106743876f3fb4adc694ce4d',\n       'roda-sentinel2/workflow-sentinel2-to-stac/af36520f9b99c3e44bf2830bbb217b2b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/5065c12a38660808deee8961e203498a',\n       'roda-sentinel2/workflow-sentinel2-to-stac/6ff82a306f3bf3f579816ae10d063d01',\n       'roda-sentinel2/workflow-sentinel2-to-stac/0081724da96931a83c6f7515f7562943',\n       'roda-sentinel2/workflow-sentinel2-to-stac/737df6bf70bb0f7b565fa5e6ee4129bb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/ca09639b46ee5bb24b92fd5b2987cc8b',\n       'roda-sentinel2/workflow-sentinel2-to-stac/82f456e00cccfe29386bd26a12a6e588',\n       'roda-sentinel2/workflow-sentinel2-to-stac/391f5e95d69f9ebd8a98bcc3fc3589eb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cbc04fd5c89f2b587d6dadfc9b9fffbb',\n       'roda-sentinel2/workflow-sentinel2-to-stac/2ee326f71919bb8db20326fefc2dfbb1',\n       'roda-sentinel2/workflow-sentinel2-to-stac/4e6ecb562c70758e02b09233082200a3',\n       'roda-sentinel2/workflow-sentinel2-to-stac/453b45d4b7891c6f3e0afe979aa1538e',\n       'roda-sentinel2/workflow-sentinel2-to-stac/665c266d9bc61d6315aded7c7690b159',\n       'roda-sentinel2/workflow-sentinel2-to-stac/762a8a493aae7d67b31d512131bc22d4',\n       'roda-sentinel2/workflow-sentinel2-to-stac/cebdfaff85f595f644f4d89f380d1a04',\n       'roda-sentinel2/workflow-sentinel2-to-stac/7fd514bec1e5e63f8b389286d09303d5',\n       'roda-sentinel2/workflow-sentinel2-to-stac/9ed53be7f1a72cb5913c128c31eb27b0',\n       'roda-sentinel2/workflow-sentinel2-to-stac/b40cc8ad73175b8ce928a3bc78e9ae28'],\n      dtype='&lt;U74')</pre></li><li>raster:bands(band)objectNone ... [{'nodata': 0, 'data_ty...<pre>array([None, None, None, None,\n       list([{'nodata': 0, 'data_type': 'uint8', 'spatial_resolution': 20}])],\n      dtype=object)</pre></li><li>title(band)&lt;U31'Blue (band 2) - 10m' ... 'Scene...<pre>array(['Blue (band 2) - 10m', 'Green (band 3) - 10m',\n       'Red (band 4) - 10m', 'NIR 1 (band 8) - 10m',\n       'Scene classification map (SCL)'], dtype='&lt;U31')</pre></li><li>gsd(band)object10 10 10 10 None<pre>array([10, 10, 10, 10, None], dtype=object)</pre></li><li>common_name(band)object'blue' 'green' 'red' 'nir' None<pre>array(['blue', 'green', 'red', 'nir', None], dtype=object)</pre></li><li>center_wavelength(band)object0.49 0.56 0.665 0.842 None<pre>array([0.49, 0.56, 0.665, 0.842, None], dtype=object)</pre></li><li>full_width_half_max(band)object0.098 0.045 0.038 0.145 None<pre>array([0.098, 0.045, 0.038, 0.145, None], dtype=object)</pre></li><li>epsg()int3232632<pre>array(32632)</pre></li></ul></li><li>Indexes: (4)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2021-04-01 10:27:39.520000', '2021-04-08 10:17:41.934000',\n               '2021-04-08 10:17:41.935000', '2021-04-16 10:27:35.384000',\n               '2021-04-16 10:27:35.385000', '2021-04-21 10:27:35.793000',\n               '2021-04-23 10:17:40.745000', '2021-04-23 10:17:40.747000',\n               '2021-04-28 10:17:40.197000', '2021-05-03 10:17:42.916000',\n               '2021-05-08 10:17:42.592000', '2021-05-08 10:17:42.594000',\n               '2021-05-23 10:17:45.371000', '2021-05-26 10:27:42.080000',\n               '2021-05-28 10:17:45.418000', '2021-05-28 10:17:45.421000',\n               '2021-05-31 10:27:42.147000', '2021-05-31 10:27:42.149000',\n               '2021-06-02 10:17:45.639000', '2021-06-02 10:17:45.641000',\n               '2021-06-10 10:27:42.429000', '2021-06-15 10:27:41.696000',\n               '2021-06-15 10:27:41.697000', '2021-06-17 10:17:45.721000',\n               '2021-06-17 10:17:45.724000', '2021-06-22 10:17:45.729000',\n               '2021-06-22 10:17:45.732000', '2021-06-27 10:17:45.604000',\n               '2021-06-27 10:17:45.605000', '2021-07-02 10:17:46.959000',\n               '2021-07-02 10:17:46.961000', '2021-07-10 10:27:43.233000',\n               '2021-07-12 10:17:47.866000', '2021-07-20 10:27:43.424000',\n               '2021-07-22 10:17:48.210000', '2021-08-06 10:17:45.959000',\n               '2021-08-09 10:27:42.199000', '2021-08-11 10:17:47.398000',\n               '2021-08-14 10:27:43.878000', '2021-08-16 10:17:44.680000',\n               '2021-08-19 10:27:40.757000', '2021-08-21 10:17:47.043000'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>bandPandasIndex<pre>PandasIndex(Index(['blue', 'green', 'red', 'nir', 'scl'], dtype='object', name='band'))</pre></li><li>xPandasIndex<pre>PandasIndex(Float64Index([675820.0, 675830.0, 675840.0, 675850.0, 675860.0, 675870.0,\n              675880.0, 675890.0, 675900.0, 675910.0,\n              ...\n              685720.0, 685730.0, 685740.0, 685750.0, 685760.0, 685770.0,\n              685780.0, 685790.0, 685800.0, 685810.0],\n             dtype='float64', name='x', length=1000))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5241820.0, 5241810.0, 5241800.0, 5241790.0, 5241780.0, 5241770.0,\n              5241760.0, 5241750.0, 5241740.0, 5241730.0,\n              ...\n              5231920.0, 5231910.0, 5231900.0, 5231890.0, 5231880.0, 5231870.0,\n              5231860.0, 5231850.0, 5231840.0, 5231830.0],\n             dtype='float64', name='y', length=1000))</pre></li></ul></li><li>Attributes: (4)spec :RasterSpec(epsg=32632, bounds=(600000.0, 5190240.0, 709800.0, 5300040.0), resolutions_xy=(10.0, 10.0))crs :epsg:32632transform :| 10.00, 0.00, 600000.00| | 0.00,-10.00, 5300040.00| | 0.00, 0.00, 1.00|resolution :10.0</li></ul> <p>Try plotting some data (July 2021 only):</p> In\u00a0[43]: Copied! <pre>data.sel(band='nir').sel(time='2021-07').plot.imshow(col='time')\n</pre> data.sel(band='nir').sel(time='2021-07').plot.imshow(col='time') Out[43]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16ede375840&gt;</pre> In\u00a0[44]: Copied! <pre>import dask.array as da\n\nscl = data.sel(band=[\"scl\"])\n# Sentinel-2 Scene Classification Map: nodata, saturated/defective, dark, cloud shadow, cloud med. prob., cloud high prob., cirrus\ninvalid_data = da.isin(scl, [0, 1, 2, 3, 8, 9, 10])\nvalid_data = data.where(~invalid_data)\n# Show some of the cloud masked data\nvalid_data.sel(band='nir').sel(time='2021-07').plot.imshow(col='time', robust=True)\n</pre> import dask.array as da  scl = data.sel(band=[\"scl\"]) # Sentinel-2 Scene Classification Map: nodata, saturated/defective, dark, cloud shadow, cloud med. prob., cloud high prob., cirrus invalid_data = da.isin(scl, [0, 1, 2, 3, 8, 9, 10]) valid_data = data.where(~invalid_data) # Show some of the cloud masked data valid_data.sel(band='nir').sel(time='2021-07').plot.imshow(col='time', robust=True) Out[44]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16eddff1960&gt;</pre> <p>Let's show cloud-masked RGB true color composites.</p> In\u00a0[45]: Copied! <pre>valid_data.sel(time='2021-07').sel(band=['red', 'green', 'blue']).plot.imshow(col='time', robust=True)\n</pre> valid_data.sel(time='2021-07').sel(band=['red', 'green', 'blue']).plot.imshow(col='time', robust=True) Out[45]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16e84e5bb20&gt;</pre> In\u00a0[46]: Copied! <pre># Compute the NDVI from the NIR (B8) and red (B4) bands\nnir, red = valid_data.sel(band='nir'), valid_data.sel(band='red')\nNDVI = (nir - red) / (nir + red)\n</pre> # Compute the NDVI from the NIR (B8) and red (B4) bands nir, red = valid_data.sel(band='nir'), valid_data.sel(band='red') NDVI = (nir - red) / (nir + red) <p>NDVI is a new DataArray. If you want to select the NDVI raster closest to a specific date and export it as a GeoTIFF, just run this line:</p> In\u00a0[47]: Copied! <pre>NDVI.sel(time='2021-07-15', method='nearest').rio.to_raster(\"C:\\work\\etrainee\\stac_exports/S2-NDVI_midjuly.tif\")\n</pre> NDVI.sel(time='2021-07-15', method='nearest').rio.to_raster(\"C:\\work\\etrainee\\stac_exports/S2-NDVI_midjuly.tif\") <p>Let's plot one month of NDVI rasters.</p> In\u00a0[48]: Copied! <pre>NDVI.sel(time='2021-07').plot.imshow(col='time', cmap=\"PRGn\", robust=True)\n</pre> NDVI.sel(time='2021-07').plot.imshow(col='time', cmap=\"PRGn\", robust=True) Out[48]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16e83c1f010&gt;</pre> In\u00a0[49]: Copied! <pre>NDVI.sel(time='2021-07').interpolate_na(dim='time').plot(col='time', cmap=\"PRGn\", robust=True)\n</pre> NDVI.sel(time='2021-07').interpolate_na(dim='time').plot(col='time', cmap=\"PRGn\", robust=True) Out[49]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16e80ce0be0&gt;</pre> In\u00a0[50]: Copied! <pre>NDVI.resample(time='1M').mean().plot(col='time', cmap=\"PRGn\", vmin=-0.2, vmax=0.8)\n</pre> NDVI.resample(time='1M').mean().plot(col='time', cmap=\"PRGn\", vmin=-0.2, vmax=0.8) Out[50]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16e85458d30&gt;</pre> In\u00a0[51]: Copied! <pre>from xarrayutils import linear_trend\nNDVI_regressed = linear_trend(NDVI, 'time')\n</pre> from xarrayutils import linear_trend NDVI_regressed = linear_trend(NDVI, 'time') <p>Plot maps (with default colormaps) for the resulting slope of the estimated linear trend along with r-value (Pearson correlation coefficient), p-value (for a hypothesis test whose null hypothesis is that the slope is zero), and standard error (of the estimated slope). We use matplotlib syntax and functions to configure the plot.</p> In\u00a0[52]: Copied! <pre>from matplotlib import pyplot as plt\nfig, axes = plt.subplots(2,2, figsize=(15,10), sharex=True, sharey=True)\nNDVI_regressed.slope.plot(robust=True, ax=axes[0,0])\nNDVI_regressed.r_value.plot(robust=True, ax=axes[0,1])\nNDVI_regressed.p_value.plot(robust=True, ax=axes[1,0])\nNDVI_regressed.std_err.plot(robust=True, ax=axes[1,1])\nfor ax, param in zip(axes.flat, ['slope', 'r-value', 'p-value', 'std error']):\n    ax.set_title(f'NDVI trend 04-08/2021: {param}')\nfig.tight_layout()\n</pre> from matplotlib import pyplot as plt fig, axes = plt.subplots(2,2, figsize=(15,10), sharex=True, sharey=True) NDVI_regressed.slope.plot(robust=True, ax=axes[0,0]) NDVI_regressed.r_value.plot(robust=True, ax=axes[0,1]) NDVI_regressed.p_value.plot(robust=True, ax=axes[1,0]) NDVI_regressed.std_err.plot(robust=True, ax=axes[1,1]) for ax, param in zip(axes.flat, ['slope', 'r-value', 'p-value', 'std error']):     ax.set_title(f'NDVI trend 04-08/2021: {param}') fig.tight_layout() <pre>c:\\Users\\Andi\\mambaforge\\envs\\etrainee_m1\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in divide\n  return func(*(_execute_task(a, cache) for a in args))\nc:\\Users\\Andi\\mambaforge\\envs\\etrainee_m1\\lib\\site-packages\\dask\\array\\numpy_compat.py:43: RuntimeWarning: invalid value encountered in divide\n  x = np.divide(x1, x2, out)\nc:\\Users\\Andi\\mambaforge\\envs\\etrainee_m1\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: divide by zero encountered in divide\n  return func(*(_execute_task(a, cache) for a in args))\n</pre> <p>Create an interactive map to inspect the slope of the trend in more detail.</p> In\u00a0[53]: Copied! <pre>import hvplot.xarray\nimport holoviews as hv\nNDVI_regressed.slope.hvplot(x='x', y='y', aspect=1, clim=(-0.02,0.02), cmap=\"PRGn\", title='NDVI slope April - Aug')\n</pre> import hvplot.xarray import holoviews as hv NDVI_regressed.slope.hvplot(x='x', y='y', aspect=1, clim=(-0.02,0.02), cmap=\"PRGn\", title='NDVI slope April - Aug') <pre>c:\\Users\\Andi\\mambaforge\\envs\\etrainee_m1\\lib\\site-packages\\dask\\array\\numpy_compat.py:43: RuntimeWarning: invalid value encountered in divide\n  x = np.divide(x1, x2, out)\nc:\\Users\\Andi\\mambaforge\\envs\\etrainee_m1\\lib\\site-packages\\dask\\core.py:121: RuntimeWarning: invalid value encountered in divide\n  return func(*(_execute_task(a, cache) for a in args))\n</pre> Out[53]: In\u00a0[54]: Copied! <pre>%%time\nNDVI_weekly = NDVI.resample(time='1W').mean()\nNDVI_weekly = NDVI_weekly.chunk(dict(time=-1))                        # rechunk into a single dask array chunk along the time dimension\nNDVI_weekly = NDVI_weekly.interpolate_na(dim='time')\n</pre> %%time NDVI_weekly = NDVI.resample(time='1W').mean() NDVI_weekly = NDVI_weekly.chunk(dict(time=-1))                        # rechunk into a single dask array chunk along the time dimension NDVI_weekly = NDVI_weekly.interpolate_na(dim='time') <pre>CPU times: total: 109 ms\nWall time: 261 ms\n</pre> <p>Now be patient while the movie is created.</p> In\u00a0[55]: Copied! <pre>%%time\nfrom xmovie import Movie\nmov = Movie(NDVI_weekly.chunk({'time':1}), dpi=100, vmin=-0.8, vmax=0.8, cmap=\"PRGn\")\n</pre> %%time from xmovie import Movie mov = Movie(NDVI_weekly.chunk({'time':1}), dpi=100, vmin=-0.8, vmax=0.8, cmap=\"PRGn\") <pre>CPU times: total: 1min 12s\nWall time: 1min 16s\n</pre> <p>Use parallelization by Dask for saving the frames to an MP4 video clip (or a GIF). For this to work we have chunked the input DataArray with a single step along the dimension that represent our frames. Again be patient ... (Code cell doesn't stop running even after video clip is saved!?)</p> In\u00a0[56]: Copied! <pre>%%time\nmov.save(r'C:\\work\\etrainee\\stac_exports\\NDVI_animation.mp4', overwrite_existing=True, parallel=True, framerate=3)\n</pre> %%time mov.save(r'C:\\work\\etrainee\\stac_exports\\NDVI_animation.mp4', overwrite_existing=True, parallel=True, framerate=3)"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#sentinel-2-data-stac-access-and-time-series-processing","title":"Sentinel-2 data STAC access and time series processing\u00b6","text":"<p>This Notebook illustrates how to query and download Sentinel-2 satellite imagery from Amazon Web Service (AWS) using a SpatioTemporal Asset Catalog (STAC). Furthermore, it shows how to work with the data using <code>xarray</code> (which was introduced in a tutorial in theme 1), and how to perform tasks like subsetting, cloud masking, spectral index calculation, temporal aggregation, and trend analysis. A few simple processing steps let you explore the development of the Normalized Difference Vegetation Index (NDVI) in spring and summer, including e.g. the seasonal greening of mountain grasslands.</p> <p>The Notebook heavily relies on the Python packages pystac-client, stackstac, and xarray. These packages are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p> <p>Parts of the Notebook use code from a tutorial provided here.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#query-data-via-stac","title":"Query data via STAC\u00b6","text":"<p>Import packages.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#cloud-masking","title":"Cloud masking\u00b6","text":"<p>We use the scene classification provided by ESA via the SCL band as a simple (but not the best) means to mask clouds and other problematic pixels.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#spectral-index-calculation-ndvi","title":"Spectral index calculation (NDVI)\u00b6","text":"<p>We calculate the Normalized Difference Vegetation Index (NDVI). If you need more indices consider the Spyndex package.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#time-series-operations","title":"Time series operations\u00b6","text":""},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#interpolate-missing-values","title":"Interpolate missing values\u00b6","text":"<p>Let's fill missing values by (linear) interpolation between observations.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#resampling","title":"Resampling\u00b6","text":"<p>We resample/aggregate to one month temporal resolution by computing the mean for each month. Other aggregations (e.g. maximum) can be achieved similarly. Note how we can absolutely define the range of values to be displayed (important for visual comparisons).</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#trend-analysis","title":"Trend analysis\u00b6","text":"<p>The xarrayutils package contains a convenient <code>linear_trend()</code> function. We use this to fit a linear least-squares trendline to our NDVI time series.</p>"},{"location":"module1/02_large_time_series_datasets_in_remote_sensing/T2_Sentinel_STAC_v03.html#create-an-animation","title":"Create an animation\u00b6","text":"<p>Let's create an animation of our NDVI time series using the xmovie package. We resample to weekly means of NDVI and interpolate missing values.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html","title":"Theme 3 - Time series analysis based on classification","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#introduction","title":"Introduction","text":"<p>Studying human-environment interactions with remote sensing very often relies on an investigation of discrete categories of landcover, land surface processes or other phenomena. Hence, classification of such target categories in remote sensing data is a vital task in many information extraction pipelines. Automating this task continues to receive a lot of interest by the Earth observation research community and by the geospatial industry.</p> <p>With the increasing availability of remote sensing time series, such classification tasks are no longer limited to single epochs (single-date classification). More and more classifications are run on time series either (i) in a multitemporal fashion to track categories or discrete objects through time or (ii) exploiting temporal information (on class-specific variations with time) to enhance a classification regarding its robustness and its capability to separate specific classes.</p> <p>Objectives</p> <p>The objectives of this theme are to</p> <ul> <li>understand the principles of automated classification of remote sensing data</li> <li>overview the main approaches to multitemporal classification vs. classification based on temporal information</li> <li>understand how these approaches can be implemented (shown in tutorials for simplified use cases)</li> <li>get an idea of exemplary applications (from the scientific literature)</li> </ul>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#theoretical-background-and-approaches-to-automated-classification-in-remote-sensing","title":"Theoretical background and approaches to automated classification in remote sensing","text":"<p>The general idea behind any data classification is that the data can be grouped into meaningful semantic categories (classes) based on characteristic patterns in the data. The goal of image classification (or point cloud classification) is to assign each spatial unit (pixel or point) a discrete category (or probabilities for several categories in case of fuzzy classifications). In the computer vision community, this task is also known as semantic segmentation or semantic labelling, in contrast to image labelling (e.g. This image contains a dog.) or object detection (resulting usually in bounding boxes around objects in an image, e.g. faces).</p> <p>The basis for any classification is formed by one or more variables (features or predictors) that allow to separate the target classes, ideally without ambiguity. In remote sensing, a feature set used as input to a classification is typically composed of spectral or geometric features or a combination of both:</p> <ul> <li>Spectral features such as spectral band values, spectral indices, texture metrics derived by focal statistics of spectral values</li> <li>Geometric features characterizing the shape or structure of a surface, such as morphometric variables derived from a digital terrain model (e.g. slope, curvature, ...) or the local distribution of points in a 3D point cloud (e.g. eigenvalue ratios, 3D/2D density ratio, ...)</li> </ul> <p></p> <p>Left: Dataset with two classes that are well separable by a straight line in a two-dimensional feature space (dashed linear decision boundary found by linear Support Vector Classification (SVC)). Right: Dataset with two classes that are not well separable by a decision boundary found by linear SVC (dotted line). Separability is better here with a non-linear decision boundary found by SVC with radial basis function (RBF) kernel (dashed line).</p> <p>Remote sensing data (such as satellite images) can be classified through visual interpretation by a human operator, who is drawing object boundaries (such as lake outlines) or linear objects (such as a river network) over an analogue or digital view of the image based on his visual perception, his experience, and certain (more or less systematic) criteria. For such an interpretation, a few features can be visualized in a color image, most intuitively by combining the red, green and blue bands recorded by an imaging sensor but other features (e.g. spectral bands or terrain parameters) can be encoded as RGB color as well. The lines and polygons drawn (vector data) may be converted to raster data if needed. On the one hand, humans are very good in considering complex spatial contexts for object recognition and classification. On the other hand, drawbacks of such a manual classification are subjectivity, a lack of efficiency, repeatability and limitations regarding the systematic consideration of many different features.</p> <p>Alternatively, humans can instruct a computer to categorize spatial units automatically according to certain criteria. These criteria (conditions) can be defined directly via rules or indirectly (in a data-driven manner) by constructing a statistical classifier from the data (e.g. as in the figure above where each data point could be a pixel and the features x<sub>1</sub> and x<sub>2</sub> could be spectral bands). The latter option is often based on machine learning (ML). In unsupervised approaches, the user supplies only the features and chooses a value for certain hyperparameters (depending on the algorithm; e.g., the number of clusters k in k-means clustering). In a supervised classification, the user has to provide additionally the labels for a set of samples (training data) which can then be used by the algorithm to learn classification rules based on the features (a classifier). Once such a classifier has been trained, it can be applied to predict the labels for previously unseen samples based on the feature vector of each sample. Among a variety of ML algorithms, Random Forest (RF; Breiman 2001) and Support Vector Machines (SVM; Cortes and Vapnik) are among the most popular ones of the last two decades (Belgiu and Dr\u0103gu\u0163 2016, Mountrakis et al. 2011). For getting started with the machine learning vocabulary and concepts (along with a Python scikit-learn example) we recommend to read this scikit-learn introduction to ML.</p> <p>Hence, we can distinguish three general approaches to (semi-)automatic classification:</p> <ul> <li>Rule-based classification (e.g. NDSI thresholding to classify snow/no-snow)</li> <li>Unsupervised classification (e.g. clustering NDSI and possibly brightness to classify snow/no-snow)</li> <li>Supervised classification (usually with machine learning approaches, including deep learning)</li> </ul> <p>Often, input data comprising many features is sensed (e.g. by multitemporal or hyperspectral remote sensing) or a large number of features is extracted from the input data (such as spectral indices, time series metrics or morphometric variables, potentially at different scales), and some of the features are actually redundant or irrelevant for the classification. In such cases, feature selection (different techniques to select a subset of features) or some other form of dimensionality reduction (such as principal component analysis (PCA)) is often applied. The main motivation for this step is usually to avoid negative effects known as the curse of dimensionality and/or to reduce the time and computing power needed to train a classifier (and for feature extraction on the full dataset). Learn more about feature selection for multitemporal classification in Module 2.</p> <p>Some external tutorials:</p> <ul> <li>Intermediate Earth Eata Science Textbook Course here by Wasser et al. (2021) - Rule-based classification of a LiDAR canopy height model</li> <li>PyGIS Course - Supervised and unsupervised classification of a single scene with scikit-learn and GeoWombat (ML pipeline, cross-validation, etc.)</li> <li>Supervised classification with Sentinel-2 images (in GEE, with one cloud-free image)</li> </ul> <p>Now let's have a look at how these key concepts for automated classification of (mono-temporal) remote sensing data can be transferred or extended to time series of images and point clouds.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#classification-of-remote-sensing-time-series","title":"Classification of remote sensing time series","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#multitemporal-classification","title":"Multitemporal classification","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#concept","title":"Concept","text":"<p>A multitemporal classification can produce a time series of thematic maps based on monotemporal (single-date) features. This is often followed by spatio-temporal visualization, by post-classification change detection or by another kind of analysis of classes\u2019/objects\u2019 properties (location and area, or temporal-thematic information). We illustrate this approach at the example of snow cover mapping where a snow cover map is produced for each month of 2022.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#tutorial-sentinel-2-snow-cover-classification-in-python","title":"Tutorial: Sentinel-2 snow cover classification in Python","text":"<p>This tutorial introduces a very basic procedure for (binary) snow cover mapping by retreiving and thresholding a time series of the Normalized Difference Snow Index (NDSI). This results in one snow cover map per month (which can be implemented in a straightforward Python workflow using GEE and related tools; however, a higher resolution (such as weekly) could make sense). Moreover, the tutorial shows possibilities for analysis and visualization of the classified time series. At the end of this theme, excercise 1 builds on and extends the tutorial.</p> <p>The examined workflow involves the following steps:</p> <ul> <li>NDSI time series query, monthly aggregation and download</li> <li>Rule-based classification (NDSI thresholding)</li> <li>Calculation of snow covered area (SCA) and plotting of SCA over time</li> <li>Further time series analysis, e.g. of snow cover duration per pixel (map)</li> </ul> <p> </p> <p>Monthly mean Sentinel-2 NDSI (top) and snow cover derived from this (bottom) using a threshold of 0.4 to separate snow-covered and snow-free pixels.</p> <p> </p> <p>Percentage of area covered by snow based on monthly Sentinel-2 NDSI (left) and number of snow covered months per pixel based on the classification (right). Note that the aggregation methods and time periods for this may (in addition to the availability of cloud-free observations) have a substantial impact on the results.</p> <p>To build a more sophisticated ML pipeline on a time series of images instead of a simple rule-based binary classification see also this tutorial from the PyGIS course (v1.2.0; Mann et al. 2022).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#classification-based-on-time-series-features","title":"Classification based on time series features","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#concept_1","title":"Concept","text":"<p>In another approach the classification exploits the temporal \u201cfingerprint\u201d of a spectral feature. In this case, multi-temporal observations of spectral measurements or their derivatives are used to extract time series based metrics as classification features, and these features are used to derive one detailed thematic map. This approach is most commonly applied when the target classes are assumed to be separable by their distinctive seasonal (or management-related) variation and timing of remotely-sensed vegetation vigour and amount (i.e., land surface phenology (LSP)). Time series of satellite derived vegetation indices or biophysical variables are then used as proxies to study LSP or to characterize plants in different phenological phases (Helman 2018, Caparros-Santiago et al. 2021).</p> <p>In a more basic version, a couple of multitemporal, cloud-free images are directly used (\"standard multitemporal features\"). In this case, a few different acquisition dates, where classes are well separable, are selected. For discriminating for example different forest types (with their dominating tree species), such a selection typically includes scenes from different phenological phases (such as green-up or senescence; Fassnacht et al. 2016). For larger study areas covered by multiple satellite orbits, however, cloud-free images of the same date are often unavailable. Hence, the resulting feature sets contain missing values, which are problematic for most machine learning algorithms.</p> <p>To alleviate this problem, time series with all available, cloud-masked imagery from an observation period (e.g. one year) can be aggregated at pixel level to calculate composites for certain time periods. Also referred to  as spectral-temporal metrics, these composites typically contain temporal mean or median values but can also include other statistics, such as standard deviation, minimum and maximum values or selected percentiles (e.g. Pflugmacher et al. 2019).</p> <p>Temporal aggregation (compositing) of such metrics for a long interval, such as multiple months, a growing season or an entire year, reduces the number of features and the data volume for classification. However, aggregation of observations with shorter intervals, capturing the distinct timing of seasonal spectral variation, might perform better for mapping vegetation categories (e.g. 10-day intervals used for crop mapping by Griffiths et al. 2019). The length of adequate aggregation intervals (bins) depends on the application and target classes but also on the density of valid observations in the input time series (e.g. constrained by revisit interval and cloud cover).</p> <p></p> <p>Spectral-temporal metrics extraction at the example of quaterly and monthly median composites (figure by Kollert et al. 2021)/ CC BY-NC-ND 4.0.</p> <p>In addition to such statistical metrics for a defined time interval, ecologically-meaningful metrics related to LSP can be derived from a time series of remotely sensed vegetation indices or biophysical variables (Caparros-Santiago et al. 2021). Such phenological metrics (phenometrics) typically include the start-of-season (SOS), the end-of-season (EOS), and the length-of-the-growing-season (LOS). They have not only been used to study LSP in relation to environmental factors but also to discriminate different landcover classes (such as forest types; Pasquarella et al. 2018, Kollert et al. 2021). All three types of features obtained from a remote sensing time series (standard multitemporal features, spectral-temporal metrics, and phenological metrics) can be fed into a machine learning pipeline for classification.</p> <p></p> <p>Extraction of phenological metrics from an NDVI time series (figure by Kollert et al. 2021/ CC BY-NC-ND 4.0).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#tutorial-image-time-series-classification-in-python-tutorial","title":"Tutorial: Image time series classification in Python tutorial","text":"<p>This E-TRAINEE tutorial on landcover classification shows how you can implement a machine learning workflow with time series metrics as features. For simplicity, we aggregate over the entire growing season (i.e. we do not calculate spectral-temporal metrics for shorter time periods such as months). However, we compute different metrics (mean, standard deviation, minimum and maximum) to capture the temporal variability of the NDVI in each pixel over the growing season.</p> <p></p> <p>Landcover map produced by a supervised classification with seasonal spectral metrics (derived from a Sentinel-2 time series) and manually labelled training data points (indicated by circles).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#application-examples-national-scale-mapping-of-agricultural-practices","title":"Application examples: National-scale mapping of agricultural practices","text":"<p>Classification based on spectral-temporal metrics is particularly useful for mapping and monitoring agricultural landuse in optical satellite image time series. Landuse practices are often not well separable using spectral data from a single point in time but multiple studies have successfully used image time series for this task. The rationale behind this is that landuse events modify the spectral reflectance of the land surface and are, thus, captured by the spectral time series. To learn more about this approach, have a look at these examples from the scientific literature:</p> <p>Stumpf et al. 2018 used Landsat time series together with auxiliary variables in a supervised machine learning classification to map grassland and cropland across Switzerland for each year in the period 2000-2015. The annual land use maps were then categorized into permanent grassland or cropland, respectively, and different conversion regimes. This served as a basis to investigate the soil organic carbon and its change in relation to different agricultural practices. </p> <p>In a follow-up study, Stumpf et al. 2020 calculated Landsat time series metrics to distinguish spatio-temporal grassland management patterns. The resulting management classes describe use intensities of mowing and grazing practices and were subsequently analysed in relation to plant species richness.</p> <p></p> <p>Grassland management classes and their areal proportions within three subregions of characteristic grassland management in Switzerland. Grassland was classified according to prevailing management practices (mowing or grazing) and use intensities (high, moderate, low) (figure by Stumpf et al. 2020/ CC BY-NC-ND 4.0).</p> <p>A comprehensive example for (i) binary classification of annual cropland and (ii) cropping practice classification is provided by Rufin et al. 2019, who derived a large set of features from on year of Landsat imagery and used this as input to Random Forest classifiers. This feature set included best-observation composites for four target days in the year, and spectal temporal metrics for each quarter and the entire year using all clear-sky observations available in the respective period. These metrics were the band-wise minimum, 25th percentile, median, 75th percentile, and maximum, mean spectral reflectance as well as the inter-quartile range, range, standard deviation, skewness and kurtosis of all reflectance values from each period. Additionally, a smoothed and gap-filled time series with equidistant eight-day intervals was produced for the three Tasseled Cap components Brightness, Greenness and Wetness (Crist 1985). An extensive comparison of the classification accuracy obtained with different feature sets (and combinations) was conducted. This revealed that for mapping cropping practices it is highly beneficial to retain the information on intra-annual spectral variation contained in the time series. In specific, Rufin et al. 2019 found that an aggregation of observations into quaterly to near-weekly bins produced suitable classification features resulting in high classification accuracy.</p> <p>Griffiths et al. 2019 produced a national-scale crop and land cover map for Germany using spectral-temporal metrics from Harmonized Landsat Sentinel (HLS) data. The study found gap-filled metrics (composites) for 10-day intervals outperforming monthly or seasonal ones.</p> <p></p> <p>Crop type and landcover map for Germany (for 2016) produced with a time series of 10-day reflectance composites from Harmonized Landsat Sentinel (HLS) data. Reference data was available for three states outlined in magenta (figure by Griffiths et al. 2019/ CC BY-NC-ND 4.0).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#useful-tools-for-remote-sensing-time-series-classification","title":"Useful tools for remote sensing time series classification","text":"<p>For practical implementation of classifications on remote sensing time series have a look at the following lists with tools dedicated to related tasks.</p> <p>Tools for machine learning/deep learning classification:</p> <p>Machine learning classification tools are most commonly used for classification (semantic segmentation) of a single scene but usually you can feed them with spatio-temporal metrics as feature vector to perform a temporally informed supervised classification. A selection of software packages to get started:</p> <ul> <li>Open source GIS and remote sensing software contains ML tools for classification (sometimes as plugins/addons), e.g.:<ul> <li>SAGA GIS (with image classification tools using Random Forest, Support Vector Machine, ...)</li> <li>GRASS GIS r.learn.ml2 addon (needs scikit-learn and pandas installed)</li> <li>QGIS Semi-Automatic Classification Plugin (SCP; needs scikit-learn installed to run a Random Forest classification)</li> </ul> </li> <li>Machine learning and deep learning packages for Python<ul> <li>Scikit-learn - Widely used and well documented ML library with many algorithms and tools</li> <li>Deep Learning: PyTorch, TensorFlow and the Keras API built on top of TensorFlow</li> <li>Pycaret - A wrapper for scikit-learn and other ML packages aimed at simplification and code reduction for ML experiments (and also for ML on time series)</li> <li>Pyspatialml - ML classification and regression modelling for spatial raster data.</li> <li>Mlxtend - ML extensions and utilities compatible with (e.g.) scikit-learn.</li> <li>Optuna - Hyperparameter optimization for different frameworks (incl. scikit-learn, PyTorch, Keras, ...)</li> </ul> </li> <li>Machine learning packages for the R language<ul> <li>\"Official\" overview</li> <li>Overview in E-TRAINEE Module 2</li> <li>mlr3 - A framework for ML regression and classification in the R language, and its extension mlr3spatial that facilitates handling spatial objects (vector and raster data), see e.g. the landcover classification tutorial</li> </ul> </li> </ul> <p>Feature extraction from a time series in Python:</p> <ul> <li>stmetrics package<ul> <li>object-based features and very interesting temporal features for RS time series classification</li> <li>nice tutorial for OBIA classification using the Brasil Data Cube via STAC</li> <li>installation could be a bit difficult on Windows</li> </ul> </li> <li>tsfresh package - Extracts a large number of features from a time series.</li> <li>Feature tools package - Featuretools automatically creates features from temporal and relational datasets.</li> </ul> <p>Time series classification tools:</p> <ul> <li>tslearn - A Python package that provides machine learning tools for the analysis of time series. It includes, e.g., a clustering module that combines k-means clustering with Dynamic Time Warping as a distance metric.</li> <li>Cesium - A package for ML time series analysis, including feature extraction, model building and prediction.</li> <li>tsai - An open-source deep learning package built on top of Pytorch &amp; fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation.</li> </ul>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"Training data is essential for any remote sensing time series classification. True or false?  True False False  What can be the result of classifying a remote sensing time series?  A time series of classified maps. One map with a classification resulting from aggregate characteristics of the entire time series. Several maps describing multiple aspects of a phenomenon (such as landuse categories and landuse intensity levels). A time series of classified maps.&amp;One map with a classification resulting from aggregate characteristics of the entire time series.&amp;Several maps describing multiple aspects of a phenomenon (such as landuse categories and landuse intensity levels).  3)  Which statements about classification based on time series features are correct?  Spectral-temporal metrics used for classification should always be computed as aggregates over an entire season. If we calculate spectral-temporal metrics over relatively short time intervals, we probably get missing values, and these can be a problem for subsequent machine learning steps. For mapping crop types, a classifier trained accurately with spectral-temporal metrics from one season might perform poorly for other seasons if, e.g., the weather and/or agricultural management was different between years. If we calculate spectral-temporal metrics over relatively short time intervals, we probably get missing values, and these can be a problem for subsequent machine learning steps.&amp;For mapping crop types, a classifier trained accurately with spectral-temporal metrics from one season might perform poorly for other seasons if, e.g., the weather and/or agricultural management was different between years."},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#excercise-snow-cover-mapping-interpretation-and-sensitivity-analysis","title":"Excercise: Snow cover mapping - Interpretation and sensitivity analysis","text":"<p>Work through the E-TRAINEE tutorial on Sentinel-2 snow cover classification in Python. Based on this tutorial, try to interpret the spatial patterns of snow cover duration and investigate the sensitivity of such a rule-based classification regarding the classification threshold.</p> <p>Tasks and questions:</p> <ul> <li>The snow cover duration appears to be quite short in some parts of our AOI (given that the village of Obergurgl is located at &gt; 1900 m a.s.l.)? Let's have a look at these areas. Try to get a simple color composite for one date using Google Earth Engine and the tools shown in the tutorial. Maybe this helps with interpretation. Alternatively use (monotemporal) higher resolution data from other sources and display them in Python or in a GIS. What are the surface characteristics of the areas with short snow cover duration? Does this point to potential limitations of the snow classification approach?</li> <li>For the binary classification of NDSI into \"snow\" and \"no snow\", we had chosen an NDSI threshold of 0.4. What if we change this threshold? How does this affect the classification results? Test a range of thresholds (0.2 to 0.6 with step 0.02), and visualize the effects of varying the threshold on the spatio-temporal classification results.</li> </ul> <p>Solution:</p> <p>One possible solution for this excercise is suggested in this notebook (where the excercise is appended to the tutorial). It also includes some conclusions on the classification approach based on the tutorial and the excercise.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/03_time_series_analysis_based_on_classification.html#references","title":"References","text":"<p>Belgiu, M., &amp; Dr\u0103gu\u0163, L. (2016). Random forest in remote sensing: A review of applications and future directions. ISPRS Journal of Photogrammetry and Remote Sensing, 114, 24-31. https://doi.org/10.1016/j.isprsjprs.2016.01.011</p> <p>Breiman, L. (2001). Random forests. Machine Learning, 45, 5-32. https://doi.org/10.1023/A:1010933404324</p> <p>Caparros-Santiago, J. A., Rodriguez-Galiano, V., &amp; Dash, J. (2021). Land surface phenology as indicator of global terrestrial ecosystem dynamics: A systematic review. ISPRS Journal of Photogrammetry and Remote Sensing, 171, 330-347. https://doi.org/10.1016/j.isprsjprs.2020.11.019</p> <p>Cortes, C., &amp; Vapnik, V. (1995). Support-vector networks. Machine Learning, 20, 273-297. https://doi.org/10.1007/BF00994018</p> <p>Crist, E. P. (1985). A TM tasseled cap equivalent transformation for reflectance factor data. Remote Sensing of Environment, 17(3), 301-306. https://doi.org/10.1016/0034-4257(85)90102-6</p> <p>Fassnacht, F. E., Latifi, H., Stere\u0144czak, K., Modzelewska, A., Lefsky, M., Waser, L. T., ... &amp; Ghosh, A. (2016). Review of studies on tree species classification from remotely sensed data. Remote Sensing of Environment, 186, 64-87. https://doi.org/10.1016/j.rse.2016.08.013</p> <p>Griffiths, P., Nendel, C., &amp; Hostert, P. (2019). Intra-annual reflectance composites from Sentinel-2 and Landsat for national-scale crop and land cover mapping. Remote Sensing of Environment, 220, 135-151. https://doi.org/10.1016/j.rse.2018.10.031</p> <p>Helman, D. (2018). Land surface phenology: What do we really \u2018see\u2019from space?. Science of the Total Environment, 618, 665-673. https://doi.org/10.1016/j.scitotenv.2017.07.237</p> <p>Kollert, A., Bremer, M., L\u00f6w, M., &amp; Rutzinger, M. (2021). Exploring the potential of land surface phenology and seasonal cloud free composites of one year of Sentinel-2 imagery for tree species mapping in a mountainous region. International Journal of Applied Earth Observation and Geoinformation, 94, 102208. https://doi.org/10.1016/j.jag.2020.102208</p> <p>Mountrakis, G., Im, J., &amp; Ogole, C. (2011). Support vector machines in remote sensing: A review. ISPRS Journal of Photogrammetry and Remote Sensing, 66(3), 247-259. https://doi.org/10.1016/j.isprsjprs.2010.11.001</p> <p>Pasquarella, V. J., Holden, C. E., &amp; Woodcock, C. E. (2018). Improved mapping of forest type using spectral-temporal Landsat features. Remote Sensing of Environment, 210, 193-207. https://doi.org/10.1016/j.rse.2018.02.064</p> <p>Pflugmacher, D., Rabe, A., Peters, M., &amp; Hostert, P. (2019). Mapping pan-European land cover using Landsat spectral-temporal metrics and the European LUCAS survey. Remote Sensing of Environment, 221, 583-595. https://doi.org/10.1016/j.rse.2018.12.001</p> <p>Rufin, P., Frantz, D., Ernst, S., Rabe, A., Griffiths, P., \u00d6zdo\u011fan, M., &amp; Hostert, P. (2019). Mapping cropping practices on a national scale using intra-annual landsat time series binning. Remote Sensing, 11(3), 232. https://doi.org/10.3390/rs11030232</p> <p>Stumpf, F., Keller, A., Schmidt, K., Mayr, A., Gubler, A., &amp; Schaepman, M. (2018). Spatio-temporal land use dynamics and soil organic carbon in Swiss agroecosystems. Agriculture, Ecosystems &amp; Environment, 258, 129-142. https://doi.org/10.1016/j.agee.2018.02.012</p> <p>Stumpf, F., Schneider, M. K., Keller, A., Mayr, A., Rentschler, T., Meuli, R. G., Schaepman, M., &amp; Liebisch, F. (2020). Spatial monitoring of grassland management using multi-temporal satellite imagery. Ecological Indicators, 113, 106201. https://doi.org/10.1016/j.ecolind.2020.106201</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html","title":"T3 S2 landcover classification","text":"Metadata     title: \"E-TRAINEE Tutorial - Landcover classification based on spectral-temporal metrics\"     description: \"This is a tutorial within the third theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-20     authors: Andreas Mayr <p>Import some of the required packages.</p> In\u00a0[1]: Copied! <pre>import ee, eemont, geemap, wxee         # ee is the GEE Python API, the others are extensions to that\nimport folium\nimport geopandas as gpd\nimport pathlib\n</pre> import ee, eemont, geemap, wxee         # ee is the GEE Python API, the others are extensions to that import folium import geopandas as gpd import pathlib In\u00a0[2]: Copied! <pre>data_dir = pathlib.Path('C:/work/etrainee/gee/T3') / 'data' # Define path on your local system\ndata_dir\n</pre> data_dir = pathlib.Path('C:/work/etrainee/gee/T3') / 'data' # Define path on your local system data_dir Out[2]: <pre>WindowsPath('C:/work/etrainee/gee/T3/data')</pre> <p>We use GeoPandas to import the Geopackage with our Area-of-Interest (AOI). The simplest way to display it in an interactive map is the Geopandas <code>explore()</code> method.</p> In\u00a0[3]: Copied! <pre>my_aoi = gpd.read_file(data_dir / 'aoi.gpkg')\nmy_aoi.explore()\n</pre> my_aoi = gpd.read_file(data_dir / 'aoi.gpkg') my_aoi.explore() Out[3]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook <p>Read the Geopackage with reference data for landcover to a Geopandas Dataframe.</p> <p>The landcover classes are coded with label IDs and we add a new column with the landcover classes named by strings. These classes discriminate different types of (periodically) vegetated areas, water bodies, and \"other\" landcover types (such as barren land, roads, parking lots, buildings, etc. which we do not aim to separate further now).</p> In\u00a0[5]: Copied! <pre>ref_data = gpd.read_file(data_dir / 'ref_data.gpkg')\n\nlandcover_dict={\n    1: \"Grassland\",\n    2: \"Cropland\",\n    3: \"Forest\",\n    4: \"Water\",\n    5: \"Other\"\n}\nref_data[\"landcover_string\"] = ref_data['landcover'].map(landcover_dict)    # Map values of the new column according to the dictionary \nref_data                                                                    # Show the DataFrame\n</pre> ref_data = gpd.read_file(data_dir / 'ref_data.gpkg')  landcover_dict={     1: \"Grassland\",     2: \"Cropland\",     3: \"Forest\",     4: \"Water\",     5: \"Other\" } ref_data[\"landcover_string\"] = ref_data['landcover'].map(landcover_dict)    # Map values of the new column according to the dictionary  ref_data                                                                    # Show the DataFrame Out[5]: landcover geometry landcover_string 0 1 POINT (682379.903 5238921.551) Grassland 1 1 POINT (682521.253 5239272.683) Grassland 2 1 POINT (682951.614 5238795.907) Grassland 3 1 POINT (682490.123 5239846.356) Grassland 4 1 POINT (683024.953 5239605.725) Grassland ... ... ... ... 67 5 POINT (684383.764 5238819.044) Other 68 5 POINT (683702.255 5238405.091) Other 69 5 POINT (683539.871 5238396.677) Other 70 5 POINT (683578.293 5238347.597) Other 71 5 POINT (683478.731 5238432.576) Other <p>72 rows \u00d7 3 columns</p> <p>For plotting we define a colormap associated to the landcover classes in alphabetical order.</p> In\u00a0[6]: Copied! <pre>my_colors = (\"brown\", \"darkgreen\", \"lightgreen\", \"grey\", \"blue\")\n</pre> my_colors = (\"brown\", \"darkgreen\", \"lightgreen\", \"grey\", \"blue\") <p>Create an interactive Leaflet map with the two vector layers (AOI and Landcover), using some additional options for map making based on the Folium package.</p> In\u00a0[7]: Copied! <pre># Show AOI\nm = my_aoi.explore(                                     # Show the AOI in a Folium map object\n    name=\"AOI\",\n    style_kwds={\"fill\": False}                          # Do not fill the polygon\n)\n\n# Add landcover point layer\nref_data.explore(\n    m=m,                                                # Pass the map object\n    column=\"landcover_string\",                          # Column to use for color\n    name=\"Landcover\",                                   # Layer name in the map\n    marker_kwds={\"radius\": 4},                          # Set marker circle size in pixels\n    style_kwds={\"color\": \"black\", \"weight\": 1},         # Set marker stroke color and width in pixels\n    cmap=my_colors,                                     # Use the custom colormap\n    legend=True \n)\n\nfolium.TileLayer('CartoDB positron', control=True).add_to(m)    # Add alternative tiles as background map\nfolium.LayerControl(collapsed=False).add_to(m)                  # Add layer control\nm                                                               # Show map\n</pre> # Show AOI m = my_aoi.explore(                                     # Show the AOI in a Folium map object     name=\"AOI\",     style_kwds={\"fill\": False}                          # Do not fill the polygon )  # Add landcover point layer ref_data.explore(     m=m,                                                # Pass the map object     column=\"landcover_string\",                          # Column to use for color     name=\"Landcover\",                                   # Layer name in the map     marker_kwds={\"radius\": 4},                          # Set marker circle size in pixels     style_kwds={\"color\": \"black\", \"weight\": 1},         # Set marker stroke color and width in pixels     cmap=my_colors,                                     # Use the custom colormap     legend=True  )  folium.TileLayer('CartoDB positron', control=True).add_to(m)    # Add alternative tiles as background map folium.LayerControl(collapsed=False).add_to(m)                  # Add layer control m                                                               # Show map Out[7]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook <p>Authenticate and initialize Google Earth Engine:</p> In\u00a0[8]: Copied! <pre>try:\n        wxee.Initialize()       # Works similarly to ee.Initialize but automatically connects to the high-volume GEE endpoint\nexcept Exception as e:          # If initialize does not work, you probably have to authenticate first\n        ee.Authenticate()\n        wxee.Initialize()\n</pre> try:         wxee.Initialize()       # Works similarly to ee.Initialize but automatically connects to the high-volume GEE endpoint except Exception as e:          # If initialize does not work, you probably have to authenticate first         ee.Authenticate()         wxee.Initialize() <p>From GEE, retrieve an image collection for our AOI and for the year 2019 (we have reference data for the same year), limit the search by a maximum cloudy pixel percentage per scene. We exclude the winter months where snow cover could compromise the spectral time series. The eemont preprocess method automatically masks clouds and shadows, and it scales and offsets the image or image collection. The eemont <code>spectralIndices()</code> method can calculate any index from the Awesome Spectral Indices library but for simplicity we only take the NDVI.</p> In\u00a0[9]: Copied! <pre>aoi_fc = geemap.gdf_to_ee(my_aoi)            # Convert the AOI from GeoPandas GeoDataFrame to ee.FeatureCollection\n\nS2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\\n    .filterDate(\"2019-04-01\",\"2019-10-31\") \\\n    .filterBounds(aoi_fc) \\\n    .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 50) \\\n    .preprocess() \\\n    .spectralIndices(\"NDVI\")\n</pre> aoi_fc = geemap.gdf_to_ee(my_aoi)            # Convert the AOI from GeoPandas GeoDataFrame to ee.FeatureCollection  S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\     .filterDate(\"2019-04-01\",\"2019-10-31\") \\     .filterBounds(aoi_fc) \\     .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 50) \\     .preprocess() \\     .spectralIndices(\"NDVI\") <p>The newer COPERNICUS/S2_SR_HARMONIZED collection results in an error later on (when calling S2_ts.describe()).</p> <p>Create a wxee TimeSeries object to enable xarray operations, such as temporal aggregations. For convenience, we select only the NDVI, omitting all the original bands.</p> In\u00a0[10]: Copied! <pre>S2_ts = S2.select('NDVI').wx.to_time_series()\n</pre> S2_ts = S2.select('NDVI').wx.to_time_series() <p>First, get some info about our time series.</p> In\u00a0[11]: Copied! <pre>S2_ts.describe()\n</pre> S2_ts.describe() <pre>COPERNICUS/S2_SR\n\tImages: 33\n\tStart date: 2019-04-02 10:27:44 UTC\n\tEnd date: 2019-10-26 10:17:45 UTC\n\tMean interval: 6.47 days\n</pre> <p>And plot the timeline to see how the (low-cloudiness) observations are distributed over the observation period.</p> In\u00a0[12]: Copied! <pre>S2_ts.timeline()\n</pre> S2_ts.timeline() In\u00a0[13]: Copied! <pre>S2_seasonal_mean_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.mean(), keep_bandnames=False)    # We do not keep band names only but append the name of the reducer to the band names.\nS2_seasonal_std_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.stdDev(), keep_bandnames=False)\nS2_seasonal_min_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.min(), keep_bandnames=False)\nS2_seasonal_max_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.max(), keep_bandnames=False)\n</pre> S2_seasonal_mean_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.mean(), keep_bandnames=False)    # We do not keep band names only but append the name of the reducer to the band names. S2_seasonal_std_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.stdDev(), keep_bandnames=False) S2_seasonal_min_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.min(), keep_bandnames=False) S2_seasonal_max_ts= S2_ts.aggregate_time(frequency=\"year\", reducer=ee.Reducer.max(), keep_bandnames=False) In\u00a0[14]: Copied! <pre>s_mean_path = data_dir / \"S2_seasonal_mean.nc\"\ns_std_path = data_dir / \"S2_seasonal_std.nc\"\ns_min_path = data_dir / \"S2_seasonal_min.nc\"\ns_max_path = data_dir / \"S2_seasonal_max.nc\"\n\nS2_seasonal_mean_ds = S2_seasonal_mean_ts.wx.to_xarray(path=s_mean_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10)\nS2_seasonal_std_ds = S2_seasonal_std_ts.wx.to_xarray(path=s_std_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10)\nS2_seasonal_min_ds = S2_seasonal_min_ts.wx.to_xarray(path=s_min_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10)\nS2_seasonal_max_ds = S2_seasonal_max_ts.wx.to_xarray(path=s_max_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10)\n</pre> s_mean_path = data_dir / \"S2_seasonal_mean.nc\" s_std_path = data_dir / \"S2_seasonal_std.nc\" s_min_path = data_dir / \"S2_seasonal_min.nc\" s_max_path = data_dir / \"S2_seasonal_max.nc\"  S2_seasonal_mean_ds = S2_seasonal_mean_ts.wx.to_xarray(path=s_mean_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10) S2_seasonal_std_ds = S2_seasonal_std_ts.wx.to_xarray(path=s_std_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10) S2_seasonal_min_ds = S2_seasonal_min_ts.wx.to_xarray(path=s_min_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10) S2_seasonal_max_ds = S2_seasonal_max_ts.wx.to_xarray(path=s_max_path, region = aoi_fc.geometry(), crs='EPSG:25832', scale=10) <pre>Requesting data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Requesting data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Requesting data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Requesting data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <p>Combine all Datasets with seasonal statistics into one Dataset (using their common dimension coordinates). This new Dataset contains all the seasonal statistics (spectral-temporal metrics) but is limited to one time stamp because we aggregated all observations into seasonal metrics.</p> In\u00a0[15]: Copied! <pre>import xarray as xr\n\nS2_seasonal_ds = xr.combine_by_coords([S2_seasonal_mean_ds, S2_seasonal_std_ds, S2_seasonal_min_ds, S2_seasonal_max_ds])\nS2_seasonal_ds\n</pre> import xarray as xr  S2_seasonal_ds = xr.combine_by_coords([S2_seasonal_mean_ds, S2_seasonal_std_ds, S2_seasonal_min_ds, S2_seasonal_max_ds]) S2_seasonal_ds Out[15]: <pre>&lt;xarray.Dataset&gt;\nDimensions:      (time: 1, x: 767, y: 428)\nCoordinates:\n  * time         (time) datetime64[ns] 2019-04-02T10:27:44\n  * x            (x) float64 6.818e+05 6.818e+05 ... 6.895e+05 6.895e+05\n  * y            (y) float64 5.242e+06 5.242e+06 ... 5.237e+06 5.237e+06\n    spatial_ref  int32 0\nData variables:\n    NDVI_max     (time, y, x) float64 0.6992 0.7314 0.7144 ... 0.8441 0.9075\n    NDVI_mean    (time, y, x) float64 0.5153 0.5224 0.5348 ... 0.7466 0.7546\n    NDVI_min     (time, y, x) float64 0.1644 0.1643 0.1623 ... 0.2692 0.3074\n    NDVI_stdDev  (time, y, x) float64 0.147 0.1533 0.1561 ... 0.1084 0.1111\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    _FillValue:              -32768.0\n    scale_factor:            1.0\n    add_offset:              0.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1</li><li>x: 767</li><li>y: 428</li></ul></li><li>Coordinates: (4)<ul><li>time(time)datetime64[ns]2019-04-02T10:27:44<pre>array(['2019-04-02T10:27:44.000000000'], dtype='datetime64[ns]')</pre></li><li>x(x)float646.818e+05 6.818e+05 ... 6.895e+05<pre>array([681825., 681835., 681845., ..., 689465., 689475., 689485.])</pre></li><li>y(y)float645.242e+06 5.242e+06 ... 5.237e+06<pre>array([5241665., 5241655., 5241645., ..., 5237415., 5237405., 5237395.])</pre></li><li>spatial_ref()int320crs_wkt :PROJCS[\"ETRS89 / UTM zone 32N\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"25832\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :ETRS89horizontal_datum_name :European Terrestrial Reference System 1989projected_crs_name :ETRS89 / UTM zone 32Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :9.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"ETRS89 / UTM zone 32N\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"25832\"]]GeoTransform :681820.0 10.0 0.0 5241670.0 0.0 -10.0<pre>array(0)</pre></li></ul></li><li>Data variables: (4)<ul><li>NDVI_max(time, y, x)float640.6992 0.7314 ... 0.8441 0.9075AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[[0.69919377, 0.73142697, 0.71437055, ..., 0.8559322 ,\n         0.85654008, 0.84317825],\n        [0.65101523, 0.7167469 , 0.70149254, ..., 0.78875427,\n         0.79895713, 0.77889237],\n        [0.64624676, 0.66049879, 0.56305506, ..., 0.85332883,\n         0.84915169, 0.83671962],\n        ...,\n        [0.26231386, 0.14564831, 0.21449559, ..., 0.92170819,\n         0.8957529 , 0.8930642 ],\n        [0.17524917, 0.19306931, 0.28106852, ..., 0.8766832 ,\n         0.87547649, 0.89473684],\n        [0.16085627, 0.16450813, 0.27412281, ..., 0.87142857,\n         0.84405236, 0.90745781]]])</pre></li><li>NDVI_mean(time, y, x)float640.5153 0.5224 ... 0.7466 0.7546AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[[0.51531714, 0.5223813 , 0.53478898, ..., 0.60498128,\n         0.59512742, 0.6309808 ],\n        [0.48172513, 0.54111695, 0.52502601, ..., 0.62052049,\n         0.60585851, 0.6114948 ],\n        [0.40717542, 0.43000409, 0.39041016, ..., 0.67340648,\n         0.6620272 , 0.65859652],\n        ...,\n        [0.12588412, 0.08885546, 0.10382371, ..., 0.76243677,\n         0.75564519, 0.76859098],\n        [0.12660089, 0.12725801, 0.16366353, ..., 0.75072947,\n         0.74631246, 0.75494833],\n        [0.12169933, 0.13079258, 0.14550371, ..., 0.74245763,\n         0.74658539, 0.7546435 ]]])</pre></li><li>NDVI_min(time, y, x)float640.1644 0.1643 ... 0.2692 0.3074AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[[0.16440049, 0.16432259, 0.16231833, ..., 0.22355289,\n         0.23814355, 0.35911044],\n        [0.1568456 , 0.14873249, 0.15935829, ..., 0.33333333,\n         0.40208811, 0.41069723],\n        [0.14992928, 0.14870153, 0.15986083, ..., 0.46478873,\n         0.47533849, 0.4537037 ],\n        ...,\n        [0.05005005, 0.00655572, 0.01850049, ..., 0.26995646,\n         0.23511604, 0.28402948],\n        [0.08665402, 0.06890459, 0.08930008, ..., 0.27592955,\n         0.24315953, 0.26521181],\n        [0.09051955, 0.11208791, 0.10704064, ..., 0.25025176,\n         0.26918392, 0.30737999]]])</pre></li><li>NDVI_stdDev(time, y, x)float640.147 0.1533 ... 0.1084 0.1111AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[[0.14696359, 0.15329033, 0.156096  , ..., 0.20795338,\n         0.19588336, 0.13085004],\n        [0.14394609, 0.16750353, 0.14671898, ..., 0.10149418,\n         0.10094479, 0.09359061],\n        [0.12452729, 0.13723628, 0.09984615, ..., 0.08591106,\n         0.08828716, 0.08467714],\n        ...,\n        [0.0410693 , 0.03489703, 0.04835551, ..., 0.13417857,\n         0.13697015, 0.12861676],\n        [0.02006255, 0.02497756, 0.04638278, ..., 0.12623935,\n         0.12259116, 0.12031037],\n        [0.0191513 , 0.0119658 , 0.03284314, ..., 0.11918387,\n         0.10835989, 0.11111289]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2019-04-02 10:27:44'], dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>xPandasIndex<pre>PandasIndex(Float64Index([681825.0, 681835.0, 681845.0, 681855.0, 681865.0, 681875.0,\n              681885.0, 681895.0, 681905.0, 681915.0,\n              ...\n              689395.0, 689405.0, 689415.0, 689425.0, 689435.0, 689445.0,\n              689455.0, 689465.0, 689475.0, 689485.0],\n             dtype='float64', name='x', length=767))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5241665.0, 5241655.0, 5241645.0, 5241635.0, 5241625.0, 5241615.0,\n              5241605.0, 5241595.0, 5241585.0, 5241575.0,\n              ...\n              5237485.0, 5237475.0, 5237465.0, 5237455.0, 5237445.0, 5237435.0,\n              5237425.0, 5237415.0, 5237405.0, 5237395.0],\n             dtype='float64', name='y', length=428))</pre></li></ul></li><li>Attributes: (7)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0</li></ul> In\u00a0[16]: Copied! <pre>import rioxarray\nS2_seasonal_ds.NDVI_mean.rio.to_raster(data_dir / \"NDVI_mean.tif\")\n</pre> import rioxarray S2_seasonal_ds.NDVI_mean.rio.to_raster(data_dir / \"NDVI_mean.tif\") <p>Let's have a look at this template raster.</p> In\u00a0[17]: Copied! <pre>import rasterio\nfrom rasterio.plot import show\n\nraster_template = rasterio.open(data_dir / \"NDVI_mean.tif\")\nprint(raster_template.profile)\nrasterio.plot.show(raster_template, title=\"Mean NDVI\", cmap=\"BrBG\")\n</pre> import rasterio from rasterio.plot import show  raster_template = rasterio.open(data_dir / \"NDVI_mean.tif\") print(raster_template.profile) rasterio.plot.show(raster_template, title=\"Mean NDVI\", cmap=\"BrBG\") <pre>{'driver': 'GTiff', 'dtype': 'float64', 'nodata': -32768.0, 'width': 767, 'height': 428, 'count': 1, 'crs': CRS.from_epsg(25832), 'transform': Affine(10.0, 0.0, 681820.0,\n       0.0, -10.0, 5241670.0), 'blockysize': 1, 'tiled': False, 'interleave': 'band'}\n</pre> Out[17]: <pre>&lt;Axes: title={'center': 'Mean NDVI'}&gt;</pre> <p>Now we can rasterize the reference data with Geopandas and Rasterio (as shown here).</p> In\u00a0[18]: Copied! <pre>from rasterio import features\n\n# Get list of geometries for all features in vector file (ref_data is a GeoDataFrame)\ngeom = [shapes for shapes in ref_data.geometry]\n\n# Create tuples of geometry, value pairs, where value is the attribute value you want to burn\ngeom_value = ((geom,value) for geom, value in zip(ref_data.geometry, ref_data['landcover']))\n\n# Rasterize vector using the shape and coordinate system of the template raster\nref_raster = features.rasterize(geom_value,\n                                out_shape = raster_template.shape,\n                                fill = raster_template.nodata,\n                                out = None,\n                                transform = raster_template.transform,\n                                all_touched = True)\n\n# Plot raster\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, figsize = (5, 5))\nshow(ref_raster, title=\"Reference data\", ax = ax)\nplt.gca().invert_yaxis()        # Get current axes and invert y\n</pre> from rasterio import features  # Get list of geometries for all features in vector file (ref_data is a GeoDataFrame) geom = [shapes for shapes in ref_data.geometry]  # Create tuples of geometry, value pairs, where value is the attribute value you want to burn geom_value = ((geom,value) for geom, value in zip(ref_data.geometry, ref_data['landcover']))  # Rasterize vector using the shape and coordinate system of the template raster ref_raster = features.rasterize(geom_value,                                 out_shape = raster_template.shape,                                 fill = raster_template.nodata,                                 out = None,                                 transform = raster_template.transform,                                 all_touched = True)  # Plot raster import matplotlib.pyplot as plt  fig, ax = plt.subplots(1, figsize = (5, 5)) show(ref_raster, title=\"Reference data\", ax = ax) plt.gca().invert_yaxis()        # Get current axes and invert y <p>The reference samples are only one pixel each and very hard to see. Write the rasterized reference data to disc and load to a GIS to check if everything went fine (... looks perfect;)). Each reference point is one labelled pixel now.</p> In\u00a0[19]: Copied! <pre>with rasterio.open(\n        data_dir / \"rasterized_ref_data.tif\", \"w\",\n        driver = \"GTiff\",\n        transform = raster_template.transform,\n        crs = raster_template.crs,\n        dtype = rasterio.uint8,\n        count = 1,\n        width = raster_template.width,\n        height = raster_template.height) as dst:\n    dst.write(ref_raster, indexes = 1)\n</pre> with rasterio.open(         data_dir / \"rasterized_ref_data.tif\", \"w\",         driver = \"GTiff\",         transform = raster_template.transform,         crs = raster_template.crs,         dtype = rasterio.uint8,         count = 1,         width = raster_template.width,         height = raster_template.height) as dst:     dst.write(ref_raster, indexes = 1) <p>Read the GeoTiff with rasterized reference data to an xarray DataArray.</p> In\u00a0[20]: Copied! <pre>ref_data_da = rioxarray.open_rasterio(data_dir / \"rasterized_ref_data.tif\")\n</pre> ref_data_da = rioxarray.open_rasterio(data_dir / \"rasterized_ref_data.tif\") <p>Add the labels from this reference data to our Dataset as a new variable.</p> In\u00a0[21]: Copied! <pre>S2_seasonal_ds['labels'] = ref_data_da\n</pre> S2_seasonal_ds['labels'] = ref_data_da  <p>Clean up the Dataset: Remove dimensions time and band which have length 1 using <code>squeeze()</code>, and drop coordinates band and spatial_ref using <code>reset_coords()</code>.</p> In\u00a0[22]: Copied! <pre>S2_seasonal_ds = S2_seasonal_ds.squeeze().reset_coords(['time', 'band', 'spatial_ref'], drop=True)\nS2_seasonal_ds\n</pre> S2_seasonal_ds = S2_seasonal_ds.squeeze().reset_coords(['time', 'band', 'spatial_ref'], drop=True) S2_seasonal_ds Out[22]: <pre>&lt;xarray.Dataset&gt;\nDimensions:      (x: 767, y: 428)\nCoordinates:\n  * x            (x) float64 6.818e+05 6.818e+05 ... 6.895e+05 6.895e+05\n  * y            (y) float64 5.242e+06 5.242e+06 ... 5.237e+06 5.237e+06\nData variables:\n    NDVI_max     (y, x) float64 0.6992 0.7314 0.7144 ... 0.8714 0.8441 0.9075\n    NDVI_mean    (y, x) float64 0.5153 0.5224 0.5348 ... 0.7425 0.7466 0.7546\n    NDVI_min     (y, x) float64 0.1644 0.1643 0.1623 ... 0.2503 0.2692 0.3074\n    NDVI_stdDev  (y, x) float64 0.147 0.1533 0.1561 ... 0.1192 0.1084 0.1111\n    labels       (y, x) uint8 ...\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    _FillValue:              -32768.0\n    scale_factor:            1.0\n    add_offset:              0.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>x: 767</li><li>y: 428</li></ul></li><li>Coordinates: (2)<ul><li>x(x)float646.818e+05 6.818e+05 ... 6.895e+05<pre>array([681825., 681835., 681845., ..., 689465., 689475., 689485.])</pre></li><li>y(y)float645.242e+06 5.242e+06 ... 5.237e+06<pre>array([5241665., 5241655., 5241645., ..., 5237415., 5237405., 5237395.])</pre></li></ul></li><li>Data variables: (5)<ul><li>NDVI_max(y, x)float640.6992 0.7314 ... 0.8441 0.9075AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[0.69919377, 0.73142697, 0.71437055, ..., 0.8559322 , 0.85654008,\n        0.84317825],\n       [0.65101523, 0.7167469 , 0.70149254, ..., 0.78875427, 0.79895713,\n        0.77889237],\n       [0.64624676, 0.66049879, 0.56305506, ..., 0.85332883, 0.84915169,\n        0.83671962],\n       ...,\n       [0.26231386, 0.14564831, 0.21449559, ..., 0.92170819, 0.8957529 ,\n        0.8930642 ],\n       [0.17524917, 0.19306931, 0.28106852, ..., 0.8766832 , 0.87547649,\n        0.89473684],\n       [0.16085627, 0.16450813, 0.27412281, ..., 0.87142857, 0.84405236,\n        0.90745781]])</pre></li><li>NDVI_mean(y, x)float640.5153 0.5224 ... 0.7466 0.7546AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[0.51531714, 0.5223813 , 0.53478898, ..., 0.60498128, 0.59512742,\n        0.6309808 ],\n       [0.48172513, 0.54111695, 0.52502601, ..., 0.62052049, 0.60585851,\n        0.6114948 ],\n       [0.40717542, 0.43000409, 0.39041016, ..., 0.67340648, 0.6620272 ,\n        0.65859652],\n       ...,\n       [0.12588412, 0.08885546, 0.10382371, ..., 0.76243677, 0.75564519,\n        0.76859098],\n       [0.12660089, 0.12725801, 0.16366353, ..., 0.75072947, 0.74631246,\n        0.75494833],\n       [0.12169933, 0.13079258, 0.14550371, ..., 0.74245763, 0.74658539,\n        0.7546435 ]])</pre></li><li>NDVI_min(y, x)float640.1644 0.1643 ... 0.2692 0.3074AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[0.16440049, 0.16432259, 0.16231833, ..., 0.22355289, 0.23814355,\n        0.35911044],\n       [0.1568456 , 0.14873249, 0.15935829, ..., 0.33333333, 0.40208811,\n        0.41069723],\n       [0.14992928, 0.14870153, 0.15986083, ..., 0.46478873, 0.47533849,\n        0.4537037 ],\n       ...,\n       [0.05005005, 0.00655572, 0.01850049, ..., 0.26995646, 0.23511604,\n        0.28402948],\n       [0.08665402, 0.06890459, 0.08930008, ..., 0.27592955, 0.24315953,\n        0.26521181],\n       [0.09051955, 0.11208791, 0.10704064, ..., 0.25025176, 0.26918392,\n        0.30737999]])</pre></li><li>NDVI_stdDev(y, x)float640.147 0.1533 ... 0.1084 0.1111AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[0.14696359, 0.15329033, 0.156096  , ..., 0.20795338, 0.19588336,\n        0.13085004],\n       [0.14394609, 0.16750353, 0.14671898, ..., 0.10149418, 0.10094479,\n        0.09359061],\n       [0.12452729, 0.13723628, 0.09984615, ..., 0.08591106, 0.08828716,\n        0.08467714],\n       ...,\n       [0.0410693 , 0.03489703, 0.04835551, ..., 0.13417857, 0.13697015,\n        0.12861676],\n       [0.02006255, 0.02497756, 0.04638278, ..., 0.12623935, 0.12259116,\n        0.12031037],\n       [0.0191513 , 0.0119658 , 0.03284314, ..., 0.11918387, 0.10835989,\n        0.11111289]])</pre></li><li>labels(y, x)uint8...AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0<pre>[328276 values with dtype=uint8]</pre></li></ul></li><li>Indexes: (2)<ul><li>xPandasIndex<pre>PandasIndex(Float64Index([681825.0, 681835.0, 681845.0, 681855.0, 681865.0, 681875.0,\n              681885.0, 681895.0, 681905.0, 681915.0,\n              ...\n              689395.0, 689405.0, 689415.0, 689425.0, 689435.0, 689445.0,\n              689455.0, 689465.0, 689475.0, 689485.0],\n             dtype='float64', name='x', length=767))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5241665.0, 5241655.0, 5241645.0, 5241635.0, 5241625.0, 5241615.0,\n              5241605.0, 5241595.0, 5241585.0, 5241575.0,\n              ...\n              5237485.0, 5237475.0, 5237465.0, 5237455.0, 5237445.0, 5237435.0,\n              5237425.0, 5237415.0, 5237405.0, 5237395.0],\n             dtype='float64', name='y', length=428))</pre></li></ul></li><li>Attributes: (7)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0</li></ul> <p>Maintain only the samples, i.e. pixels where a valid label (&gt; 0) exists, mask out all other pixels (i.e. set them to nan).</p> In\u00a0[23]: Copied! <pre>samples_ds = S2_seasonal_ds.where(S2_seasonal_ds.labels &gt; 0)\n</pre> samples_ds = S2_seasonal_ds.where(S2_seasonal_ds.labels &gt; 0) In\u00a0[24]: Copied! <pre>samples_da = samples_ds.to_array()\nsamples_da_flat = samples_da.stack(samples=('x', 'y'))\nprint(\"Shape and dimensions of samples_da: \", samples_da.shape, samples_da.dims)\nprint(\"Shape and dimensions of samples_da_flat: \", samples_da_flat.shape, samples_da_flat.dims)\n</pre> samples_da = samples_ds.to_array() samples_da_flat = samples_da.stack(samples=('x', 'y')) print(\"Shape and dimensions of samples_da: \", samples_da.shape, samples_da.dims) print(\"Shape and dimensions of samples_da_flat: \", samples_da_flat.shape, samples_da_flat.dims) <pre>Shape and dimensions of samples_da:  (5, 428, 767) ('variable', 'y', 'x')\nShape and dimensions of samples_da_flat:  (5, 328276) ('variable', 'samples')\n</pre> <p>To get our flattened array into the shape of n_samples, n_features (as expected by scikit-learn), we reorder the dimensions using .transpose.</p> In\u00a0[25]: Copied! <pre>samples_da_flat_t = samples_da_flat.transpose()\nsamples_da_flat_t.shape\n</pre> samples_da_flat_t = samples_da_flat.transpose() samples_da_flat_t.shape Out[25]: <pre>(328276, 5)</pre> <p>Most scikit-learn estimators (including Random Forest) do not accept missing values, so we have to remove all NaNs in our samples.</p> In\u00a0[26]: Copied! <pre>samples_da_flat_t = samples_da_flat_t.dropna(dim='samples', how='any')\nsamples_da_flat_t.shape\n</pre> samples_da_flat_t = samples_da_flat_t.dropna(dim='samples', how='any') samples_da_flat_t.shape Out[26]: <pre>(72, 5)</pre> <p>Define the predictors (features; X) and the predicted variable (dependent variable; y) in our samples.</p> In\u00a0[27]: Copied! <pre>X = samples_da_flat_t.sel(variable = ['NDVI_max', 'NDVI_min', 'NDVI_mean', 'NDVI_stdDev'])\ny = samples_da_flat_t.sel(variable = ['labels'])\n</pre> X = samples_da_flat_t.sel(variable = ['NDVI_max', 'NDVI_min', 'NDVI_mean', 'NDVI_stdDev']) y = samples_da_flat_t.sel(variable = ['labels']) In\u00a0[32]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(oob_score=True, random_state=0)    # We use default values for the number of trees and other user-defined parameters of the RF algorithm\nclf = clf.fit(X, y.squeeze(dim='variable'))                     # Without squeeze we get a data conversion warning (change the shape of y to (n_samples,); but it works!?)\nprint(f'Out-of-bag error: {(1 - clf.oob_score_):.3f}')          # Print OOB error rounded to three decimals\n</pre> from sklearn.ensemble import RandomForestClassifier  clf = RandomForestClassifier(oob_score=True, random_state=0)    # We use default values for the number of trees and other user-defined parameters of the RF algorithm clf = clf.fit(X, y.squeeze(dim='variable'))                     # Without squeeze we get a data conversion warning (change the shape of y to (n_samples,); but it works!?) print(f'Out-of-bag error: {(1 - clf.oob_score_):.3f}')          # Print OOB error rounded to three decimals <pre>Out-of-bag error: 0.028\n</pre> <p>As a measure for the prediction error during this training phase, we printed the out-of-bag (OOB) error. The OOB error of a Random Forest is aggregated from those samples of the individual trees that are not used for building the tree by bootstrapping (random sampling with replacement).</p> <p>Let's also have a look at the feature importances provided by the Random Forest algorithm.</p> In\u00a0[33]: Copied! <pre>for i, j in zip(['NDVI_max', 'NDVI_min', 'NDVI_mean', 'NDVI_stdDev'], clf.feature_importances_):\n    print(f'Feature importance of {i}: {j:.3f}')\n</pre> for i, j in zip(['NDVI_max', 'NDVI_min', 'NDVI_mean', 'NDVI_stdDev'], clf.feature_importances_):     print(f'Feature importance of {i}: {j:.3f}') <pre>Feature importance of NDVI_max: 0.257\nFeature importance of NDVI_min: 0.218\nFeature importance of NDVI_mean: 0.215\nFeature importance of NDVI_stdDev: 0.310\n</pre> <p>Basically, it seems that the classifier was fit well to the training data and all four features contribute more or less similarly strong. So let's go on and classify all pixels.</p> In\u00a0[34]: Copied! <pre>predict_da = S2_seasonal_ds.to_array()\npredict_da = predict_da.stack(samples=('x', 'y'))\npredict_da = predict_da.transpose()\nX_predict = predict_da.sel(variable = ['NDVI_max', 'NDVI_min', 'NDVI_mean', 'NDVI_stdDev']).dropna(dim='samples', how='any')\n</pre> predict_da = S2_seasonal_ds.to_array() predict_da = predict_da.stack(samples=('x', 'y')) predict_da = predict_da.transpose() X_predict = predict_da.sel(variable = ['NDVI_max', 'NDVI_min', 'NDVI_mean', 'NDVI_stdDev']).dropna(dim='samples', how='any') <p>Predict all pixels based on their feature vector and the classifier fitted on the samples.</p> In\u00a0[35]: Copied! <pre>labels_predicted = clf.predict(X_predict)\n</pre> labels_predicted = clf.predict(X_predict) <p>Convert the labels from float to integer.</p> In\u00a0[36]: Copied! <pre>import numpy as np\nlabels_predicted = labels_predicted.astype(int)\n</pre> import numpy as np labels_predicted = labels_predicted.astype(int) In\u00a0[37]: Copied! <pre>template_da_1d = predict_da[:, 0]                           # take only one variable\noutput_da_1d = template_da_1d.copy(data=labels_predicted)   # fill template with predicted labels\n</pre> template_da_1d = predict_da[:, 0]                           # take only one variable output_da_1d = template_da_1d.copy(data=labels_predicted)   # fill template with predicted labels <p>Now unstack the output to recover the spatial dimensions. At some point we had swapped the dimensions by <code>transpose()</code>, so we have to undo this here by calling <code>transpose()</code> again.</p> <p>Note that the output has \"inherited\" the metadata of the input data (such as CRS, cell size, etc.; stored as attributes). Unfortunately, it has also \"inherited\" the name of the variable used as template. To be able to rename it to 'landcover_id' we convert the DataArray to a Dataset.</p> In\u00a0[64]: Copied! <pre>output_da_2d = output_da_1d.unstack().transpose()\noutput_ds_2d = output_da_2d.expand_dims('variable').to_dataset(dim='variable')\noutput_ds_2d = output_ds_2d.rename({'NDVI_max': 'landcover_id'})\n</pre> output_da_2d = output_da_1d.unstack().transpose() output_ds_2d = output_da_2d.expand_dims('variable').to_dataset(dim='variable') output_ds_2d = output_ds_2d.rename({'NDVI_max': 'landcover_id'}) <pre>&lt;__array_function__ internals&gt;:200: RuntimeWarning:\n\ninvalid value encountered in cast\n\n</pre> <p>Finally, write the classification output to disk.</p> In\u00a0[70]: Copied! <pre>landcover_out = output_ds_2d['landcover_id']\nlandcover_out.rio.write_crs(raster_template.crs, inplace=True)\nlandcover_out.rio.write_nodata(0)#output_ds_2d.nodatavals[0])\nlandcover_out.rio.to_raster(data_dir / \"landcover_id_v2.tif\")\n</pre> landcover_out = output_ds_2d['landcover_id'] landcover_out.rio.write_crs(raster_template.crs, inplace=True) landcover_out.rio.write_nodata(0)#output_ds_2d.nodatavals[0]) landcover_out.rio.to_raster(data_dir / \"landcover_id_v2.tif\") In\u00a0[71]: Copied! <pre>from matplotlib import pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nmy_colors_by_ids = (\"lightgreen\", \"brown\", \"darkgreen\", \"blue\", \"grey\") # Define colors for landcover IDs\n\nfig, ax = plt.subplots(1, figsize=(10,5))\noutput_ds_2d['landcover_id'].plot.imshow(ax=ax, levels=[1, 2, 3, 4, 5, 6], colors=my_colors_by_ids, cbar_kwargs={'shrink': 0.7})\nref_data.plot(ax=ax, column=\"landcover_string\",                          # Column to use for color\n   cmap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue']),\n    edgecolor='white',\n    legend=True)\nplt.title('Landcover')\nplt.show()\n</pre> from matplotlib import pyplot as plt from matplotlib.colors import ListedColormap  my_colors_by_ids = (\"lightgreen\", \"brown\", \"darkgreen\", \"blue\", \"grey\") # Define colors for landcover IDs  fig, ax = plt.subplots(1, figsize=(10,5)) output_ds_2d['landcover_id'].plot.imshow(ax=ax, levels=[1, 2, 3, 4, 5, 6], colors=my_colors_by_ids, cbar_kwargs={'shrink': 0.7}) ref_data.plot(ax=ax, column=\"landcover_string\",                          # Column to use for color    cmap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue']),     edgecolor='white',     legend=True) plt.title('Landcover') plt.show() In\u00a0[72]: Copied! <pre>from rasterio import features\nfrom shapely.geometry import shape\nimport geopandas as gpd\n\ngeometry_list = []\nvalue_list = []\n\nlc_rast = rioxarray.open_rasterio(data_dir / \"landcover_id.tif\")                        # use rioxarray accessor to open GeoTiff as xarray DataArray\nfor shapedict, value in features.shapes(lc_rast, transform=lc_rast.rio.transform()):    # get a pair of (polygon, value) for each feature found in the image.\n    value_list.append(value)\n    geometry_list.append(shape(shapedict))                      # construct shapely polygons from GeoJSON-like dicts returned by rasterio.features.shapes\n\nlc_dict = {'lc_id': value_list, 'geometry': geometry_list}      # make a dictionary with class ID values and polygon geometries\nlc_gdf = gpd.GeoDataFrame(lc_dict, crs=lc_rast.rio.crs)         # create a GeoDataFrame from the dictionary\nlc_gdf['lc_id'] = lc_gdf['lc_id'].astype(int)                   # landcover class ID to integer\n</pre> from rasterio import features from shapely.geometry import shape import geopandas as gpd  geometry_list = [] value_list = []  lc_rast = rioxarray.open_rasterio(data_dir / \"landcover_id.tif\")                        # use rioxarray accessor to open GeoTiff as xarray DataArray for shapedict, value in features.shapes(lc_rast, transform=lc_rast.rio.transform()):    # get a pair of (polygon, value) for each feature found in the image.     value_list.append(value)     geometry_list.append(shape(shapedict))                      # construct shapely polygons from GeoJSON-like dicts returned by rasterio.features.shapes  lc_dict = {'lc_id': value_list, 'geometry': geometry_list}      # make a dictionary with class ID values and polygon geometries lc_gdf = gpd.GeoDataFrame(lc_dict, crs=lc_rast.rio.crs)         # create a GeoDataFrame from the dictionary lc_gdf['lc_id'] = lc_gdf['lc_id'].astype(int)                   # landcover class ID to integer <p>So far, landcover is coded with IDs. For convenience let's add a column with landcover class names as strings.</p> In\u00a0[75]: Copied! <pre>landcover_dict={1: \"Grassland\", 2: \"Cropland\", 3: \"Forest\", 4: \"Water\", 5: \"Other\"}\nlc_gdf[\"Landcover\"] = lc_gdf['lc_id'].map(landcover_dict)    # Map values of the new column according to the dictionary\n</pre> landcover_dict={1: \"Grassland\", 2: \"Cropland\", 3: \"Forest\", 4: \"Water\", 5: \"Other\"} lc_gdf[\"Landcover\"] = lc_gdf['lc_id'].map(landcover_dict)    # Map values of the new column according to the dictionary In\u00a0[107]: Copied! <pre>from folium import plugins\n\n# Show the AOI in a Folium map object\nmy_aoi = gpd.read_file(data_dir / 'aoi.gpkg')\nm = my_aoi.explore(name=\"AOI\", style_kwds={\"fill\": False})      # Do not fill the polygon\n\n# Add predicted landcover layer (polygonized) to the map object\ncolormap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue'])   # Define a colormap (for alphabetically sorted class names)\nlc_gdf.explore(m=m, column='Landcover', name=\"Landcover predicted\", categorical=True, cmap=colormap)\n\n# Add landcover reference point layer\ncolormap=ListedColormap(['lightgreen', 'brown', 'darkgreen', 'blue', 'grey'])   # Define a colormap (for class IDs)\nref_points = gpd.read_file(data_dir / 'ref_data.gpkg')\nref_points.explore(\n    m=m, column=\"landcover\", name=\"Landcover training samples\",\n    marker_kwds={\"radius\": 4}, style_kwds={\"color\": \"black\", \"weight\": 1},\n    categorical=True, legend=False, cmap=colormap\n    )\n\n# Add alternative background layers\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n                 attr='Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n                 name='ESRI WorldImagery', control=True).add_to(m)\nfolium.TileLayer(tiles='Stamen Terrain',\n                 name='Stamen Terrain', control=True).add_to(m)\n\nfolium.LayerControl(collapsed=False).add_to(m)          # Add layer control\nfolium.plugins.Fullscreen().add_to(m)                   # Add a full screen button (for display in a browser)\nm                                                       # Show map\n</pre> from folium import plugins  # Show the AOI in a Folium map object my_aoi = gpd.read_file(data_dir / 'aoi.gpkg') m = my_aoi.explore(name=\"AOI\", style_kwds={\"fill\": False})      # Do not fill the polygon  # Add predicted landcover layer (polygonized) to the map object colormap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue'])   # Define a colormap (for alphabetically sorted class names) lc_gdf.explore(m=m, column='Landcover', name=\"Landcover predicted\", categorical=True, cmap=colormap)  # Add landcover reference point layer colormap=ListedColormap(['lightgreen', 'brown', 'darkgreen', 'blue', 'grey'])   # Define a colormap (for class IDs) ref_points = gpd.read_file(data_dir / 'ref_data.gpkg') ref_points.explore(     m=m, column=\"landcover\", name=\"Landcover training samples\",     marker_kwds={\"radius\": 4}, style_kwds={\"color\": \"black\", \"weight\": 1},     categorical=True, legend=False, cmap=colormap     )  # Add alternative background layers folium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',                  attr='Tiles \u00a9 Esri \u2014 Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',                  name='ESRI WorldImagery', control=True).add_to(m) folium.TileLayer(tiles='Stamen Terrain',                  name='Stamen Terrain', control=True).add_to(m)  folium.LayerControl(collapsed=False).add_to(m)          # Add layer control folium.plugins.Fullscreen().add_to(m)                   # Add a full screen button (for display in a browser) m                                                       # Show map Out[107]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[82]: Copied! <pre>from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=7, random_state=0, n_init='auto').fit(X)     # fit the kmeans model on the samples only\nclusters_predicted = kmeans.predict(X_predict)                          # apply the model to predict labels for all pixels\nclusters_predicted.shape                                                # the predicted labels are a 1D Numpy array\n</pre> from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=7, random_state=0, n_init='auto').fit(X)     # fit the kmeans model on the samples only clusters_predicted = kmeans.predict(X_predict)                          # apply the model to predict labels for all pixels clusters_predicted.shape                                                # the predicted labels are a 1D Numpy array <pre>c:\\Users\\Andi\\mambaforge\\envs\\etrainee_m1\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n\n</pre> Out[82]: <pre>(328276,)</pre> In\u00a0[83]: Copied! <pre>template_da_1d = predict_da[:, 0]                            # take only one variable\noutput_da_1d = template_da_1d.copy(data=clusters_predicted)  # fill template with predicted labels\n</pre> template_da_1d = predict_da[:, 0]                            # take only one variable output_da_1d = template_da_1d.copy(data=clusters_predicted)  # fill template with predicted labels <p>Now unstack the output to recover the spatial dimensions. At some point we had swapped the dimensions by <code>transpose()</code>, so we have to undo this here by calling <code>transpose()</code> again. Note that the output also has \"inherited\" the metadata of the input data (such as CRS, cell size, etc.; stored as attributes).</p> In\u00a0[84]: Copied! <pre>output_da_2d = output_da_1d.unstack().transpose()\noutput_ds_2d = output_da_2d.expand_dims('variable').to_dataset(dim='variable')\noutput_ds_2d = output_ds_2d.rename({'NDVI_max': 'Clusters'})\n</pre> output_da_2d = output_da_1d.unstack().transpose() output_ds_2d = output_da_2d.expand_dims('variable').to_dataset(dim='variable') output_ds_2d = output_ds_2d.rename({'NDVI_max': 'Clusters'}) <pre>&lt;__array_function__ internals&gt;:200: RuntimeWarning:\n\ninvalid value encountered in cast\n\n</pre> <p>Plot a map with the clusters.</p> In\u00a0[100]: Copied! <pre>output_ds_2d['Clusters'].plot(levels=[1, 2, 3, 4, 5, 6, 7])     # Define levels for a discrete colormap\n</pre> output_ds_2d['Clusters'].plot(levels=[1, 2, 3, 4, 5, 6, 7])     # Define levels for a discrete colormap Out[100]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1b5cb09ba00&gt;</pre> <p>Summarize some of the clusters and assign colors to all clusters, trying to match the result of the supervised classification/visual interpretation of the scene. Plot also the reference data (white circles filled with class-based color).</p> In\u00a0[101]: Copied! <pre>my_colors_by_clusters = (\"darkgreen\", \"darkgreen\", \"darkgreen\", \"brown\", \"grey\", \"lightgreen\", \"blue\") # Define colors for landcover IDs\n\nfig, ax = plt.subplots(1, figsize=(10,5))\noutput_ds_2d['Clusters'].plot.imshow(ax=ax, levels=[1, 2, 3, 4, 5, 6, 7], colors=my_colors_by_clusters, cbar_kwargs={'shrink': 0.7})\nref_data.plot(ax=ax, column=\"landcover_string\",                          # Column to use for color\n   cmap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue']),\n    edgecolor='white',\n    legend=True)\nplt.title('Landcover (unsupervised classification)')\nplt.show()\n</pre> my_colors_by_clusters = (\"darkgreen\", \"darkgreen\", \"darkgreen\", \"brown\", \"grey\", \"lightgreen\", \"blue\") # Define colors for landcover IDs  fig, ax = plt.subplots(1, figsize=(10,5)) output_ds_2d['Clusters'].plot.imshow(ax=ax, levels=[1, 2, 3, 4, 5, 6, 7], colors=my_colors_by_clusters, cbar_kwargs={'shrink': 0.7}) ref_data.plot(ax=ax, column=\"landcover_string\",                          # Column to use for color    cmap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue']),     edgecolor='white',     legend=True) plt.title('Landcover (unsupervised classification)') plt.show() <p>That doesn't look completely bad, given that we used a relatively simple approach without any training data. It seems to be possible to assign class labels to the clusters. We don't do that now, but take the results of this unsupervised and the supervised classification as an indicator for the good predictive power of spectral-temporal metrics (such as minimum, maximum. mean and standard deviation of the NDVI) for landcover classification.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#landcover-classification-based-on-spectral-temporal-metrics","title":"Landcover classification based on spectral-temporal metrics\u00b6","text":"<p>This Notebook shows how to use spectral-temporal metrics as features for a supervised landcover classification. A simple approach for unsupervised classification is tried additionaly towards the end of the Notebook. We will look at the accuracy of the supervised landcover classification in theme 6 of the E-TRAINEE Module 1.</p> <p>We use the geemap, wxee and eemont packages to derive the spectral-temporal metrics from a Sentinel-2 satellite image time series in the Google Earth Engine (GEE). The subsequent machine learning part is accomplished on your local machine using the scikit-learn library. Xarray is used for raster time series handling. These and other Python packages used in this tutorial are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p> <p>Data:</p> <ul> <li>We will download the Sentinel-2 time series metrics from the Google Earth Engine.</li> <li>Two geopackages with an area-of-interest polygon and a reference data point layer, respectively, are provided for download on the course data repository in the theme 3 folder:<ul> <li><code>/T3/aoi.gpkg</code> - A rectangular area in the Inn valley (Austria), covering urban structures, infrastructure, forest, grassland and cropland.</li> <li><code>/T3/ref_data.gpkg</code> - A set of 72 points purposely placed and labelled according to landcover categories, as interpreted by visual inspection of a (0.2-m resolution) aerial orthophoto from 2019 (i.e., the same year as the satellite imagery is from; orthophto provided by the Office of the Government of Tyrol, Geoinfomation Division).</li> </ul> </li> </ul>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#area-of-interest-aoi-and-reference-data-set","title":"Area-of-interest (AOI) and reference data set\u00b6","text":"<p>Download the AOI and the reference data from the course data repository and save them on your local drive. Set the directory where you store the data.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#sentinel-2-time-series-metrics","title":"Sentinel-2 time series metrics\u00b6","text":"<p>For the landcover classification we need some features (variables) that discriminate the target classes (landcover categories). Therefore, we retrieve a set of spectral-temporal metrics from the Google Earth Engine (GEE) using the geemap, wxee and eemont packages.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#feature-extraction","title":"Feature extraction\u00b6","text":"<p>To create spectral-temporal metrics that can be used as classification features, we perform a temporal aggregation (calculate pixel-level seasonal statistics) of our time series in the GEE. We define the frequency as year but in fact it will be an aggregation over the growing season as our time series covers only the months April to October 2019.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#downloading-features-from-gee","title":"Downloading features from GEE\u00b6","text":"<p>The nice thing is that so far all the big computations on the time series data happened in the cloud (GEE). Now use wxee to download the results of the time series aggregation to xarray Datasets. Note that wxee, however, is not designed to download larger areas and high resolution - in such cases consider downloading to a Google Drive using ee or to a local drive using geemap.</p> <p>Either define a path where a NetCDF file will be written to, or load the Dataset into memory only. You can set the scale parameter to control the resolution. See the documentation for more options.</p> <p>The requests and downloads may take some minutes.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#data-preprocessing","title":"Data preprocessing\u00b6","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#rasterize-reference-data","title":"Rasterize reference data\u00b6","text":"<p>We will use the entire reference dataset for training a supervised machine learning classifier. We do not hold out a part of the samples for validation (testing the classification performance) but in theme 6 we will label new samples for this purpose.</p> <p>To make the reference data (training data) compatible with the spectral-temporal features, we rasterize the sample points using a template raster. First, create this template raster by writing one variable (extracted from the S2 imagery) to a GeoTiff using <code>rioxarray</code>.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#flatten-array-stack-dimensions","title":"Flatten array (stack dimensions)\u00b6","text":"<p>Our samples data set is a raster with shape (n_features, n_cells_y, n_cells_x). Scikit-learn, however, expects it to be of shape (n_samples, n_features). In other words, the Scikit-learn algorithm doesn't care how the samples (pixels, each with feature values and a label) are ordered in 2D space. Therefore, we have to re-shape our data, i.e. flatten the x and y dimensions. We use xarray methods (<code>stack</code>) for this operation. This makes it possible to keep track of the coordinate labels ('x' and 'y') along the next steps and re-shape back to our array (in 2D raster space) later without loosing information.</p> <p>Convert Dataset to DataArray and stack x and y dimensions, call new dimension \"samples\". This is a multi-index object, which can be unstacked into x and y later.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#random-forest-classification-using-scikit-learn","title":"Random Forest classification using scikit-learn\u00b6","text":"<p>We use scikit-learn to fit a Random Forest (RF) classifier (Breiman 2001) to the samples and apply this classifier to the entire AOI covered by our spectral temporal feature rasters (We fix the <code>random_state</code> for reproducible results across executions).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#classify-all-pixels-of-our-aoi","title":"Classify all pixels of our AOI\u00b6","text":"<p>The steps above were done for the samples only (recall that we have masked all unsampled pixels using xarray's <code>where()</code>). Now we prepare the full rasters with spectral-temporal features for landcover classification across the entire AOI. Could probably be simplified by re-ordering the processing steps.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#un-flatten-the-classification-output","title":"Un-flatten the classification output\u00b6","text":"<p>To be able to un-flatten (unstack) the spatial dimensions x and y of our predicted labels we use the flattened predict_da DataArray as a template. The original DataArray has the shape n_samples by n_features, we need a template with shape n_samples, so take only one of the variables. then fill this template with the predicted labels.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#plot-the-results-in-a-static-map","title":"Plot the results in a static map\u00b6","text":"<p>We use the plotting capabilities of Pandas and xarray with matplotlib as plotting backend to create a static map with the reference data and the classification output (predicted labels for each pixel).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#interactive-map-of-the-results","title":"Interactive map of the results\u00b6","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#vectorize-landcover","title":"Vectorize landcover\u00b6","text":"<p>For plotting an interactive Leaflet map using the Folium package, we vectorize the landcover classes of our classification raster with <code>rasterio.features.shapes</code> and <code>shapely.geometry.shape</code>. This converts a 2D <code>xarray.DataArray</code> into a <code>geopandas.GeoDataFrame</code>.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#plot-an-interactive-map","title":"Plot an interactive map\u00b6","text":"<p>Finally, use <code>folium</code> to plot a map and be able to compare the validation samples and the prediction in spatial context. This may also help to see deficiencies of the classification missed by the random validation samples (there is, e.g., no cropland in the northwestern corner).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#unsupervised-classification-by-kmeans-clustering","title":"Unsupervised classification by kmeans clustering\u00b6","text":"<p>After the supervised landcover classification with the Random Forest classifier, we try also an unsupervised classification with the KMeans algorithm. Let's see how well this classic and relatively simple method can divide the spectral-temporal feature vectors into meaningful clusters. We simply set the number of clusters to seven and hope that they will (at least to some degree) correspond to our target classes defined above if we merge some of the clusters.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#un-flatten-the-clustering-output","title":"Un-flatten the clustering output\u00b6","text":"<p>The following procedure is the same as before with the supervised classification output.</p> <p>To be able to un-flatten (unstack) the spatial dimensions x and y of our predicted clusters we use the flattened predict_da DataArray as a template. The original DataArray has the shape n_samples by n_features, we need a template with shape n_samples, so take only one of the variables. Then fill this template with the predicted clusters.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_landcover_classification.html#remaining-problems-and-outlook","title":"Remaining problems and outlook\u00b6","text":"<p>Have a look at the northwestern corner of our AOI. Strange location for cropland ...!? What is the correct label for this area? What happened here? We will investigate this area in the hands-on part of theme 4.</p> <p>Note that this is a very much simplified analysis and many aspects could be expanded, such as the accuracy analysis, the feature extraction, the discriminated categories or the training data. Please see Module 1 Theme 6 of the E-TRAINEE course for more details on accuracy analysis. There, we will also perform an accuracy analysis for the landcover map produced by supervised classification.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification.html","title":"T3 S2 snow classification","text":"Metadata     title: \"E-TRAINEE Tutorial - Sentinel-2 snow cover time series classification in Python\"     description: \"This is a tutorial within the third theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-20     authors: Andreas Mayr <p>Import packages, then authenticate and initialize Google Earth Engine.</p> In\u00a0[1]: Copied! <pre>import ee, eemont, geemap, wxee\n</pre> import ee, eemont, geemap, wxee In\u00a0[4]: Copied! <pre>try:\n        wxee.Initialize()\nexcept Exception as e:          # If initialize does not work, you probably have to authenticate first\n        ee.Authenticate()\n        wxee.Initialize()\n</pre> try:         wxee.Initialize() except Exception as e:          # If initialize does not work, you probably have to authenticate first         ee.Authenticate()         wxee.Initialize() In\u00a0[5]: Copied! <pre>AOI = ee.Geometry.BBoxFromQuery(\"Obergurgl, Austria\", user_agent = \"RS_course\")\n</pre> AOI = ee.Geometry.BBoxFromQuery(\"Obergurgl, Austria\", user_agent = \"RS_course\") <p>The <code>user_agent</code> argument must be specified: This is a string describing the name of the app that is using a geocoding service.</p> <p>The <code>geemap</code> package can directly display the <code>ee.Geometry.BBox</code> object with our AOI in an interactive map.</p> In\u00a0[6]: Copied! <pre>Map = geemap.Map()\nMap.addLayer(AOI,{'color':'blue'}, 'Nominatim')  # Use Nominatim, the Open Street Map geocoding service\nMap.centerObject(AOI, 10)\nMap\n</pre> Map = geemap.Map() Map.addLayer(AOI,{'color':'blue'}, 'Nominatim')  # Use Nominatim, the Open Street Map geocoding service Map.centerObject(AOI, 10) Map Out[6]: <pre>Map(center=[46.87037509744985, 11.02749840000902], controls=(WidgetControl(options=['position', 'transparent_b\u2026</pre> <p>Retrieve an image collection from GEE, preprocess it and calculate the NDSI.</p> In\u00a0[7]: Copied! <pre>S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\\n    .filterDate(\"2022-01-01\",\"2022-06-30\") \\\n    .filterBounds(AOI) \\\n    .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\\n    .preprocess() \\\n    .spectralIndices(\"NDSI\") \\\n    .select(\"NDSI\")\n</pre> S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\     .filterDate(\"2022-01-01\",\"2022-06-30\") \\     .filterBounds(AOI) \\     .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\     .preprocess() \\     .spectralIndices(\"NDSI\") \\     .select(\"NDSI\") <p>We have filtered by a date range and by bounds, i.e. queried imagery for the year 2022 and for our AOI. The eemont <code>preprocess()</code> method automatically masks clouds and shadows and scales and offsets the image or image collection. The eemont <code>spectralIndices()</code> method can calculate any index from the Awesome Spectral Indices library. For snow classification we will only use the NDSI, no original bands.</p> <p>Create a wxee <code>TimeSeries</code> object from the image collection to to add more functionality, such as temporal aggregations.</p> In\u00a0[8]: Copied! <pre>S2_ts = S2.wx.to_time_series()\n</pre> S2_ts = S2.wx.to_time_series() <p>First, get some info about our time series.</p> In\u00a0[9]: Copied! <pre>S2_ts.describe()\n</pre> S2_ts.describe() <pre>COPERNICUS/S2_SR\n\tImages: 68\n\tStart date: 2022-01-01 10:27:43 UTC\n\tEnd date: 2022-06-27 10:18:12 UTC\n\tMean interval: 2.64 days\n</pre> <p>And plot the timeline to see how the (low-cloudiness) observations are distributed over the observation period.</p> In\u00a0[10]: Copied! <pre>S2_ts.wx.to_time_series().timeline()\n</pre> S2_ts.wx.to_time_series().timeline() <p>Perform a temporal aggregation (calculate monthly medians) of our time series in the GEE.</p> In\u00a0[11]: Copied! <pre>S2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median())\n</pre> S2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median()) <p>Now use wxee to get the result of the time series aggregation to an xarray <code>Dataset</code> (i.e. download the aggregated rasters). You could also download multiple images as GeoTIFFs. Wxee, however, is not designed to download larger areas and high resolution - in such cases consider downloading to a Google Drive using <code>ee</code> or to a local drive using <code>geemap</code>. In our case 12 months with 20 m resolution throw a GEE memory error (\"User memory exceeded\"). Therefore, we split the request and download simply into two parts with 6 months each.</p> <p>Either define a path where a NetCDF file will be written to, or load the <code>Dataset</code> into memory. You can set the <code>scale</code> parameter to control the resolution. See the wxee documentation for more options.</p> <p>The requests and downloads may take some minutes.</p> In\u00a0[12]: Copied! <pre>S2_ds_1 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20)\n</pre> S2_ds_1 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20) <pre>Requesting data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <p>We repeat the data query and the download for the second part of the year.</p> In\u00a0[13]: Copied! <pre>S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\\n    .filterDate(\"2022-07-01\",\"2022-12-31\") \\\n    .filterBounds(AOI) \\\n    .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\\n    .preprocess() \\\n    .spectralIndices(\"NDSI\") \\\n    .select(\"NDSI\")                                                                     # Query S2 NDSI from GEE\nS2_ts = S2.wx.to_time_series()                                                          # Build wxee time series object\nS2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median())     # Reduce NDSI to monthly median\nS2_ds_2 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20)          # Download monthly NDSI\n</pre> S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\     .filterDate(\"2022-07-01\",\"2022-12-31\") \\     .filterBounds(AOI) \\     .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\     .preprocess() \\     .spectralIndices(\"NDSI\") \\     .select(\"NDSI\")                                                                     # Query S2 NDSI from GEE S2_ts = S2.wx.to_time_series()                                                          # Build wxee time series object S2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median())     # Reduce NDSI to monthly median S2_ds_2 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20)          # Download monthly NDSI <pre>Requesting data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <p>Concatenate the two xarray Datasets (six months of monthly NDSI each) along the time dimension and check the final Dataset we obtained.</p> In\u00a0[14]: Copied! <pre>import xarray as xr\nS2_ds = xr.concat([S2_ds_1, S2_ds_2], dim='time')\nS2_ds\n</pre> import xarray as xr S2_ds = xr.concat([S2_ds_1, S2_ds_2], dim='time') S2_ds Out[14]: <pre>&lt;xarray.Dataset&gt;\nDimensions:      (time: 12, x: 159, y: 227)\nCoordinates:\n  * time         (time) datetime64[ns] 2022-01-01T10:27:57 ... 2022-12-07T10:...\n  * x            (x) float64 6.529e+05 6.53e+05 6.53e+05 ... 6.561e+05 6.561e+05\n  * y            (y) float64 5.195e+06 5.195e+06 ... 5.191e+06 5.19e+06\n    spatial_ref  int32 0\nData variables:\n    NDSI         (time, y, x) float64 0.8335 0.8201 0.7924 ... 0.7509 0.6929\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    _FillValue:              -32768.0\n    scale_factor:            1.0\n    add_offset:              0.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 12</li><li>x: 159</li><li>y: 227</li></ul></li><li>Coordinates: (4)<ul><li>time(time)datetime64[ns]2022-01-01T10:27:57 ... 2022-12-...<pre>array(['2022-01-01T10:27:57.000000000', '2022-02-05T10:27:49.000000000',\n       '2022-03-02T10:27:58.000000000', '2022-04-06T10:27:52.000000000',\n       '2022-05-11T10:27:59.000000000', '2022-06-10T10:28:04.000000000',\n       '2022-07-02T10:18:04.000000000', '2022-08-04T10:28:00.000000000',\n       '2022-09-13T10:27:57.000000000', '2022-10-03T10:27:53.000000000',\n       '2022-11-14T10:17:47.000000000', '2022-12-07T10:27:56.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>x(x)float646.529e+05 6.53e+05 ... 6.561e+05<pre>array([652930., 652950., 652970., 652990., 653010., 653030., 653050., 653070.,\n       653090., 653110., 653130., 653150., 653170., 653190., 653210., 653230.,\n       653250., 653270., 653290., 653310., 653330., 653350., 653370., 653390.,\n       653410., 653430., 653450., 653470., 653490., 653510., 653530., 653550.,\n       653570., 653590., 653610., 653630., 653650., 653670., 653690., 653710.,\n       653730., 653750., 653770., 653790., 653810., 653830., 653850., 653870.,\n       653890., 653910., 653930., 653950., 653970., 653990., 654010., 654030.,\n       654050., 654070., 654090., 654110., 654130., 654150., 654170., 654190.,\n       654210., 654230., 654250., 654270., 654290., 654310., 654330., 654350.,\n       654370., 654390., 654410., 654430., 654450., 654470., 654490., 654510.,\n       654530., 654550., 654570., 654590., 654610., 654630., 654650., 654670.,\n       654690., 654710., 654730., 654750., 654770., 654790., 654810., 654830.,\n       654850., 654870., 654890., 654910., 654930., 654950., 654970., 654990.,\n       655010., 655030., 655050., 655070., 655090., 655110., 655130., 655150.,\n       655170., 655190., 655210., 655230., 655250., 655270., 655290., 655310.,\n       655330., 655350., 655370., 655390., 655410., 655430., 655450., 655470.,\n       655490., 655510., 655530., 655550., 655570., 655590., 655610., 655630.,\n       655650., 655670., 655690., 655710., 655730., 655750., 655770., 655790.,\n       655810., 655830., 655850., 655870., 655890., 655910., 655930., 655950.,\n       655970., 655990., 656010., 656030., 656050., 656070., 656090.])</pre></li><li>y(y)float645.195e+06 5.195e+06 ... 5.19e+06<pre>array([5195010., 5194990., 5194970., ..., 5190530., 5190510., 5190490.])</pre></li><li>spatial_ref()int320crs_wkt :PROJCS[\"ETRS89 / UTM zone 32N\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"25832\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :ETRS89horizontal_datum_name :European Terrestrial Reference System 1989projected_crs_name :ETRS89 / UTM zone 32Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :9.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"ETRS89 / UTM zone 32N\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"25832\"]]GeoTransform :652920.0 20.0 0.0 5195020.0 0.0 -20.0<pre>array(0)</pre></li></ul></li><li>Data variables: (1)<ul><li>NDSI(time, y, x)float640.8335 0.8201 ... 0.7509 0.6929AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[[ 0.83347258,  0.82005118,  0.79238057, ...,  0.76307165,\n          0.79166117,  0.83129174],\n        [ 0.83494638,  0.83383798,  0.81916169, ...,  0.83061446,\n          0.78532727,  0.82393969],\n        [ 0.82457756,  0.82274569,  0.8174314 , ...,  0.82036485,\n          0.81009338,  0.80782382],\n        ...,\n        [ 0.79291798,  0.81264701,  0.88137514, ...,  0.75945815,\n          0.7544387 ,  0.76867828],\n        [ 0.80431842,  0.815004  ,  0.87471994, ...,  0.75349023,\n          0.75221907,  0.76332348],\n        [ 0.82485224,  0.82809334,  0.85753318, ...,  0.75150405,\n          0.76232545,  0.7784285 ]],\n\n       [[ 0.59440865,  0.61101965,  0.6239223 , ...,  0.55676812,\n          0.61030832,  0.69755538],\n        [ 0.57851684,  0.59208392,  0.6037291 , ...,  0.71875742,\n          0.6890166 ,  0.7293551 ],\n        [ 0.56505542,  0.56604171,  0.57175732, ...,  0.72653908,\n          0.73424765,  0.70586033],\n...\n        [ 0.70584748,  0.70437338,  0.6624305 , ...,  0.61629527,\n          0.6305422 ,  0.64171007],\n        [ 0.69312367,  0.68990647,  0.68672063, ...,  0.63631509,\n          0.62112878,  0.64997948],\n        [ 0.70005901,  0.68807203,  0.71435407, ...,  0.64093476,\n          0.6264804 ,  0.62620483]],\n\n       [[ 0.65609963,  0.62680896,  0.62837559, ...,  0.53004512,\n          0.57358725,  0.69331777],\n        [ 0.63700406,  0.68713632,  0.69208331, ...,  0.70432215,\n          0.67729935,  0.71781428],\n        [ 0.65343804,  0.64356819,  0.68929443, ...,  0.70064971,\n          0.70965125,  0.6897312 ],\n        ...,\n        [ 0.69632761,  0.72107541,  0.70840846, ...,  0.70166119,\n          0.71203673,  0.66643397],\n        [ 0.71461496,  0.74168118,  0.7283801 , ...,  0.68993478,\n          0.70489904,  0.6400744 ],\n        [ 0.72569184,  0.72088827,  0.73755618, ...,  0.72458002,\n          0.75094874,  0.69290814]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2022-01-01 10:27:57', '2022-02-05 10:27:49',\n               '2022-03-02 10:27:58', '2022-04-06 10:27:52',\n               '2022-05-11 10:27:59', '2022-06-10 10:28:04',\n               '2022-07-02 10:18:04', '2022-08-04 10:28:00',\n               '2022-09-13 10:27:57', '2022-10-03 10:27:53',\n               '2022-11-14 10:17:47', '2022-12-07 10:27:56'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>xPandasIndex<pre>PandasIndex(Float64Index([652930.0, 652950.0, 652970.0, 652990.0, 653010.0, 653030.0,\n              653050.0, 653070.0, 653090.0, 653110.0,\n              ...\n              655910.0, 655930.0, 655950.0, 655970.0, 655990.0, 656010.0,\n              656030.0, 656050.0, 656070.0, 656090.0],\n             dtype='float64', name='x', length=159))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5195010.0, 5194990.0, 5194970.0, 5194950.0, 5194930.0, 5194910.0,\n              5194890.0, 5194870.0, 5194850.0, 5194830.0,\n              ...\n              5190670.0, 5190650.0, 5190630.0, 5190610.0, 5190590.0, 5190570.0,\n              5190550.0, 5190530.0, 5190510.0, 5190490.0],\n             dtype='float64', name='y', length=227))</pre></li></ul></li><li>Attributes: (7)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0</li></ul> <p>Note that we have only downloaded the monthly medians of the NDSI, no other bands.</p> In\u00a0[15]: Copied! <pre>S2_ds.NDSI.plot(col=\"time\", col_wrap=6, cmap='coolwarm_r', center=0.4, aspect=1, robust=True)\n</pre> S2_ds.NDSI.plot(col=\"time\", col_wrap=6, cmap='coolwarm_r', center=0.4, aspect=1, robust=True) Out[15]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x19467f33190&gt;</pre> In\u00a0[24]: Copied! <pre>import xarray as xr\nfrom matplotlib.colors import ListedColormap\nsnow = xr.where(S2_ds.NDSI&gt;0.4, 1, 0)\nsnow.attrs = S2_ds.attrs        # Copy the attributes of the S2 Dataset (CRS, spatial res., ...)\ncmap = ListedColormap(['peru', 'lightblue'])\nsnow.plot(col='time', col_wrap=6, cbar_kwargs={'label': 'Snow cover', 'ticks': [0, 1]}, cmap=cmap)\n</pre> import xarray as xr from matplotlib.colors import ListedColormap snow = xr.where(S2_ds.NDSI&gt;0.4, 1, 0) snow.attrs = S2_ds.attrs        # Copy the attributes of the S2 Dataset (CRS, spatial res., ...) cmap = ListedColormap(['peru', 'lightblue']) snow.plot(col='time', col_wrap=6, cbar_kwargs={'label': 'Snow cover', 'ticks': [0, 1]}, cmap=cmap) Out[24]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x1947756ffd0&gt;</pre> <p>Let's take this time series of snow/no-snow maps to quantify the percentage of our area-of-interest that is snow covered for each month.</p> In\u00a0[17]: Copied! <pre>import numpy as np\nsnow_percentage = np.nanmean(snow.values, axis=(1,2))*100\n</pre> import numpy as np snow_percentage = np.nanmean(snow.values, axis=(1,2))*100 <p>... and plot this.</p> In\u00a0[18]: Copied! <pre>import matplotlib.pyplot as plt\nplt.figure(figsize=(5,3))\nplt.plot(snow.time, snow_percentage)\nplt.title(\"Percentage of area covered by snow\\nbased on monthly Sentinel-2 NDSI\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Snow covered area [%]\")\n</pre> import matplotlib.pyplot as plt plt.figure(figsize=(5,3)) plt.plot(snow.time, snow_percentage) plt.title(\"Percentage of area covered by snow\\nbased on monthly Sentinel-2 NDSI\") plt.xticks(rotation=45) plt.ylabel(\"Snow covered area [%]\") Out[18]: <pre>Text(0, 0.5, 'Snow covered area [%]')</pre> In\u00a0[19]: Copied! <pre>snow.sum(dim='time').plot(cbar_kwargs={'label': 'Snow cover duration [months]'})\nplt.title('Snow cover duration 2022')\n</pre> snow.sum(dim='time').plot(cbar_kwargs={'label': 'Snow cover duration [months]'}) plt.title('Snow cover duration 2022') Out[19]: <pre>Text(0.5, 1.0, 'Snow cover duration 2022')</pre>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification.html#sentinel-2-snow-cover-time-series-classification-in-python","title":"Sentinel-2 snow cover time series classification in Python\u00b6","text":"<p>This Notebook shows how to implement a simple snow classification workflow on a Sentinel-2 (S-2) satellite image time series.</p> <p>For a small area-of-interest (AOI) in the Alps (Obergurgl, Austria), an S-2 Normalized Difference Snow Index (NDSI) time series is downloaded from the Google Earth Engine (GEE) and classified per-scene into snow-covered and snow-free areas, using the widely used NDSI threshold of 0.4. The results of this simple rule-based classification are analysed and visualized regarding the duration of snow cover (months) per pixel and percentage of snow-covered area within the AOI per month.</p> <p>For Google Earth Engine (GEE) and <code>xarray</code> based raster time series processing we use the geemap, wxee and eemont packages, which are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification.html#construct-an-area-of-interest-aoi","title":"Construct an area-of-interest (AOI)\u00b6","text":"<p>We want a area around Obergurgl (Tyrol, Austria) as our area-of-interest (AOI). Using <code>eemont</code>, we pass a query for this place via the <code>geopy</code> package to a geocoding service. We just provide a string representing a place that is geocoded. A Bounding Box (<code>ee.Geometry.BBox</code>) for a geocoded place can be constructed from such a query using the <code>ee.Geometry.BBoxFromQuery</code> constructor (extended through <code>eemont</code>). Depending on the geocoding service used (e.g., ArcGIS, Baidu, Google Maps, IGN France, Nominatim, etc.), the resulting Bounding Boxes will differ. In this example, the method is convenient because we don't have to enter any coordinates for our AOI or import a file.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification.html#aggregate-and-download-time-series","title":"Aggregate and download time series\u00b6","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification.html#show-ndsi-maps","title":"Show NDSI maps\u00b6","text":"<p>Display the monthly NDSI maps using <code>xarray</code>'s built-in plotting functions:</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification.html#snow-classification-for-each-month","title":"Snow classification for each month\u00b6","text":"<p>Now we map binary snow cover simply by thresholding the NDSI with the widely used threshold 0.4 (which we also took to center the colormap in the plot above). This results in a time series of monthly snow/no-snow maps (encoded as 1/0).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification.html#snow-cover-duration-per-pixel","title":"Snow cover duration per pixel\u00b6","text":"<p>Finally calculate how many months every pixel is snow covered.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html","title":"T3 S2 snow classification  excercise","text":"Metadata     title: \"E-TRAINEE Tutorial - Sentinel-2 snow cover time series classification in Python (tutorial with excercise)\"     description: \"This is a tutorial with excercise within the third theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-21     authors: Andreas Mayr <p>Import packages, then authenticate and initialize Google Earth Engine.</p> In\u00a0[1]: Copied! <pre>import ee, eemont, geemap, wxee\n</pre> import ee, eemont, geemap, wxee In\u00a0[2]: Copied! <pre>try:\n        wxee.Initialize()\nexcept Exception as e:          # If initialize does not work, you probably have to authenticate first\n        ee.Authenticate()\n        wxee.Initialize()\n</pre> try:         wxee.Initialize() except Exception as e:          # If initialize does not work, you probably have to authenticate first         ee.Authenticate()         wxee.Initialize() In\u00a0[3]: Copied! <pre>AOI = ee.Geometry.BBoxFromQuery(\"Obergurgl, Austria\", user_agent = \"RS_course\")\n</pre> AOI = ee.Geometry.BBoxFromQuery(\"Obergurgl, Austria\", user_agent = \"RS_course\") <p>The <code>user_agent</code> argument must be specified: This is a string describing the name of the app that is using a geocoding service.</p> <p>The <code>geemap</code> package can directly display the <code>ee.Geometry.BBox</code> object with our AOI in an interactive map.</p> In\u00a0[4]: Copied! <pre>Map = geemap.Map()\nMap.addLayer(AOI,{'color':'blue'},'Nominatim')  # Open Street Map geocoding service\nMap.centerObject(AOI,10)\nMap\n</pre> Map = geemap.Map() Map.addLayer(AOI,{'color':'blue'},'Nominatim')  # Open Street Map geocoding service Map.centerObject(AOI,10) Map Out[4]: <pre>Map(center=[46.87037509744985, 11.02749840000902], controls=(WidgetControl(options=['position', 'transparent_b\u2026</pre> <p>Retrieve an image collection from GEE, preprocess it and calculate the NDSI.</p> In\u00a0[5]: Copied! <pre>S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\\n    .filterDate(\"2022-01-01\",\"2022-06-30\") \\\n    .filterBounds(AOI) \\\n    .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\\n    .preprocess() \\\n    .spectralIndices(\"NDSI\") \\\n    .select(\"NDSI\")\n</pre> S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\     .filterDate(\"2022-01-01\",\"2022-06-30\") \\     .filterBounds(AOI) \\     .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\     .preprocess() \\     .spectralIndices(\"NDSI\") \\     .select(\"NDSI\") <p>We have filtered by a date range and by bounds, i.e. queried imagery for the year 2022 and for our AOI. The eemont <code>preprocess()</code> method automatically masks clouds and shadows and scales and offsets the image or image collection. The eemont <code>spectralIndices()</code> method can calculate any index from the Awesome Spectral Indices library. For snow classification we will only use the NDSI, no original bands.</p> <p>Create a wxee <code>TimeSeries</code> object from the image collection to to add more functionality, such as temporal aggregations.</p> In\u00a0[6]: Copied! <pre>S2_ts = S2.wx.to_time_series()\n</pre> S2_ts = S2.wx.to_time_series() <p>First, get some info about our time series.</p> In\u00a0[7]: Copied! <pre>S2_ts.describe()\n</pre> S2_ts.describe() <pre>COPERNICUS/S2_SR\n\tImages: 68\n\tStart date: 2022-01-01 10:27:43 UTC\n\tEnd date: 2022-06-27 10:18:12 UTC\n\tMean interval: 2.64 days\n</pre> <p>And plot the timeline to see how the (low-cloudiness) observations are distributed over the observation period.</p> In\u00a0[8]: Copied! <pre>S2_ts.wx.to_time_series().timeline()\n</pre> S2_ts.wx.to_time_series().timeline() <p>Perform a temporal aggregation (calculate monthly medians) of our time series in the GEE.</p> In\u00a0[9]: Copied! <pre>S2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median())\n</pre> S2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median()) <p>Now use wxee to get the result of the time series aggregation to an xarray <code>Dataset</code> (i.e. download the aggregated rasters). You could also download multiple images as GeoTIFFs. Wxee, however, is not designed to download larger areas and high resolution - in such cases consider downloading to a Google Drive using <code>ee</code> or to a local drive using <code>geemap</code>. In our case 12 months with 20 m resolution throw a GEE memory error (\"User memory exceeded\"). Therefore, we split the request and download simply into two parts with 6 months each.</p> <p>Either define a path where a NetCDF file will be written to, or load the <code>Dataset</code> into memory. You can set the <code>scale</code> parameter to control the resolution. See the wxee documentation for more options.</p> <p>The requests and downloads may take some minutes.</p> In\u00a0[10]: Copied! <pre>S2_ds_1 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20)\n</pre> S2_ds_1 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20) <pre>Requesting data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <p>We repeat the data query and download for the second part of the year.</p> In\u00a0[11]: Copied! <pre>S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\\n    .filterDate(\"2022-07-01\",\"2022-12-31\") \\\n    .filterBounds(AOI) \\\n    .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\\n    .preprocess() \\\n    .spectralIndices(\"NDSI\") \\\n    .select(\"NDSI\")                                                                     # Query S2 NDSI from GEE\nS2_ts = S2.wx.to_time_series()                                                          # Build wxee time series object\nS2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median())     # Reduce NDSI to monthly median\nS2_ds_2 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20)          # Download monthly NDSI\n</pre> S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\     .filterDate(\"2022-07-01\",\"2022-12-31\") \\     .filterBounds(AOI) \\     .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30) \\     .preprocess() \\     .spectralIndices(\"NDSI\") \\     .select(\"NDSI\")                                                                     # Query S2 NDSI from GEE S2_ts = S2.wx.to_time_series()                                                          # Build wxee time series object S2_monthly_ts= S2_ts.aggregate_time(frequency=\"month\", reducer=ee.Reducer.median())     # Reduce NDSI to monthly median S2_ds_2 = S2_monthly_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=20)          # Download monthly NDSI <pre>Requesting data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <p>Concatenate the two xarray Datasets (six months of monthly NDSI each) along the time dimension and check the final Dataset we obtained.</p> In\u00a0[12]: Copied! <pre>import xarray as xr\nS2_ds = xr.concat([S2_ds_1, S2_ds_2], dim='time')\nS2_ds\n</pre> import xarray as xr S2_ds = xr.concat([S2_ds_1, S2_ds_2], dim='time') S2_ds Out[12]: <pre>&lt;xarray.Dataset&gt;\nDimensions:      (time: 12, x: 159, y: 227)\nCoordinates:\n  * time         (time) datetime64[ns] 2022-01-01T10:27:57 ... 2022-12-07T10:...\n  * x            (x) float64 6.529e+05 6.53e+05 6.53e+05 ... 6.561e+05 6.561e+05\n  * y            (y) float64 5.195e+06 5.195e+06 ... 5.191e+06 5.19e+06\n    spatial_ref  int32 0\nData variables:\n    NDSI         (time, y, x) float64 0.8335 0.8201 0.7924 ... 0.7509 0.6929\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    _FillValue:              -32768.0\n    scale_factor:            1.0\n    add_offset:              0.0</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 12</li><li>x: 159</li><li>y: 227</li></ul></li><li>Coordinates: (4)<ul><li>time(time)datetime64[ns]2022-01-01T10:27:57 ... 2022-12-...<pre>array(['2022-01-01T10:27:57.000000000', '2022-02-05T10:27:49.000000000',\n       '2022-03-02T10:27:58.000000000', '2022-04-06T10:27:52.000000000',\n       '2022-05-11T10:27:59.000000000', '2022-06-10T10:28:04.000000000',\n       '2022-07-02T10:18:04.000000000', '2022-08-04T10:28:00.000000000',\n       '2022-09-13T10:27:57.000000000', '2022-10-03T10:27:53.000000000',\n       '2022-11-14T10:17:47.000000000', '2022-12-07T10:27:56.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>x(x)float646.529e+05 6.53e+05 ... 6.561e+05<pre>array([652930., 652950., 652970., 652990., 653010., 653030., 653050., 653070.,\n       653090., 653110., 653130., 653150., 653170., 653190., 653210., 653230.,\n       653250., 653270., 653290., 653310., 653330., 653350., 653370., 653390.,\n       653410., 653430., 653450., 653470., 653490., 653510., 653530., 653550.,\n       653570., 653590., 653610., 653630., 653650., 653670., 653690., 653710.,\n       653730., 653750., 653770., 653790., 653810., 653830., 653850., 653870.,\n       653890., 653910., 653930., 653950., 653970., 653990., 654010., 654030.,\n       654050., 654070., 654090., 654110., 654130., 654150., 654170., 654190.,\n       654210., 654230., 654250., 654270., 654290., 654310., 654330., 654350.,\n       654370., 654390., 654410., 654430., 654450., 654470., 654490., 654510.,\n       654530., 654550., 654570., 654590., 654610., 654630., 654650., 654670.,\n       654690., 654710., 654730., 654750., 654770., 654790., 654810., 654830.,\n       654850., 654870., 654890., 654910., 654930., 654950., 654970., 654990.,\n       655010., 655030., 655050., 655070., 655090., 655110., 655130., 655150.,\n       655170., 655190., 655210., 655230., 655250., 655270., 655290., 655310.,\n       655330., 655350., 655370., 655390., 655410., 655430., 655450., 655470.,\n       655490., 655510., 655530., 655550., 655570., 655590., 655610., 655630.,\n       655650., 655670., 655690., 655710., 655730., 655750., 655770., 655790.,\n       655810., 655830., 655850., 655870., 655890., 655910., 655930., 655950.,\n       655970., 655990., 656010., 656030., 656050., 656070., 656090.])</pre></li><li>y(y)float645.195e+06 5.195e+06 ... 5.19e+06<pre>array([5195010., 5194990., 5194970., ..., 5190530., 5190510., 5190490.])</pre></li><li>spatial_ref()int320crs_wkt :PROJCS[\"ETRS89 / UTM zone 32N\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"25832\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :ETRS89horizontal_datum_name :European Terrestrial Reference System 1989projected_crs_name :ETRS89 / UTM zone 32Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :9.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"ETRS89 / UTM zone 32N\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"25832\"]]GeoTransform :652920.0 20.0 0.0 5195020.0 0.0 -20.0<pre>array(0)</pre></li></ul></li><li>Data variables: (1)<ul><li>NDSI(time, y, x)float640.8335 0.8201 ... 0.7509 0.6929AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0<pre>array([[[ 0.83347258,  0.82005118,  0.79238057, ...,  0.76307165,\n          0.79166117,  0.83129174],\n        [ 0.83494638,  0.83383798,  0.81916169, ...,  0.83061446,\n          0.78532727,  0.82393969],\n        [ 0.82457756,  0.82274569,  0.8174314 , ...,  0.82036485,\n          0.81009338,  0.80782382],\n        ...,\n        [ 0.79291798,  0.81264701,  0.88137514, ...,  0.75945815,\n          0.7544387 ,  0.76867828],\n        [ 0.80431842,  0.815004  ,  0.87471994, ...,  0.75349023,\n          0.75221907,  0.76332348],\n        [ 0.82485224,  0.82809334,  0.85753318, ...,  0.75150405,\n          0.76232545,  0.7784285 ]],\n\n       [[ 0.59440865,  0.61101965,  0.6239223 , ...,  0.55676812,\n          0.61030832,  0.69755538],\n        [ 0.57851684,  0.59208392,  0.6037291 , ...,  0.71875742,\n          0.6890166 ,  0.7293551 ],\n        [ 0.56505542,  0.56604171,  0.57175732, ...,  0.72653908,\n          0.73424765,  0.70586033],\n...\n        [ 0.70584748,  0.70437338,  0.6624305 , ...,  0.61629527,\n          0.6305422 ,  0.64171007],\n        [ 0.69312367,  0.68990647,  0.68672063, ...,  0.63631509,\n          0.62112878,  0.64997948],\n        [ 0.70005901,  0.68807203,  0.71435407, ...,  0.64093476,\n          0.6264804 ,  0.62620483]],\n\n       [[ 0.65609963,  0.62680896,  0.62837559, ...,  0.53004512,\n          0.57358725,  0.69331777],\n        [ 0.63700406,  0.68713632,  0.69208331, ...,  0.70432215,\n          0.67729935,  0.71781428],\n        [ 0.65343804,  0.64356819,  0.68929443, ...,  0.70064971,\n          0.70965125,  0.6897312 ],\n        ...,\n        [ 0.69632761,  0.72107541,  0.70840846, ...,  0.70166119,\n          0.71203673,  0.66643397],\n        [ 0.71461496,  0.74168118,  0.7283801 , ...,  0.68993478,\n          0.70489904,  0.6400744 ],\n        [ 0.72569184,  0.72088827,  0.73755618, ...,  0.72458002,\n          0.75094874,  0.69290814]]])</pre></li></ul></li><li>Indexes: (3)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2022-01-01 10:27:57', '2022-02-05 10:27:49',\n               '2022-03-02 10:27:58', '2022-04-06 10:27:52',\n               '2022-05-11 10:27:59', '2022-06-10 10:28:04',\n               '2022-07-02 10:18:04', '2022-08-04 10:28:00',\n               '2022-09-13 10:27:57', '2022-10-03 10:27:53',\n               '2022-11-14 10:17:47', '2022-12-07 10:27:56'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li><li>xPandasIndex<pre>PandasIndex(Float64Index([652930.0, 652950.0, 652970.0, 652990.0, 653010.0, 653030.0,\n              653050.0, 653070.0, 653090.0, 653110.0,\n              ...\n              655910.0, 655930.0, 655950.0, 655970.0, 655990.0, 656010.0,\n              656030.0, 656050.0, 656070.0, 656090.0],\n             dtype='float64', name='x', length=159))</pre></li><li>yPandasIndex<pre>PandasIndex(Float64Index([5195010.0, 5194990.0, 5194970.0, 5194950.0, 5194930.0, 5194910.0,\n              5194890.0, 5194870.0, 5194850.0, 5194830.0,\n              ...\n              5190670.0, 5190650.0, 5190630.0, 5190610.0, 5190590.0, 5190570.0,\n              5190550.0, 5190530.0, 5190510.0, 5190490.0],\n             dtype='float64', name='y', length=227))</pre></li></ul></li><li>Attributes: (7)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :-32768.0scale_factor :1.0add_offset :0.0</li></ul> <p>Note that we have only downloaded the monthly medians of the NDSI, no other bands.</p> In\u00a0[13]: Copied! <pre>S2_ds.NDSI.plot(col=\"time\", col_wrap=6, cmap='coolwarm_r', center=0.4, aspect=1, robust=True)\n</pre> S2_ds.NDSI.plot(col=\"time\", col_wrap=6, cmap='coolwarm_r', center=0.4, aspect=1, robust=True) Out[13]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x217e8593be0&gt;</pre> In\u00a0[14]: Copied! <pre>import xarray as xr\nsnow = xr.where(S2_ds.NDSI&gt;0.4, 1, 0)\nsnow.attrs = S2_ds.attrs                                                # Copy the attributes of the S2 Dataset (CRS, spatial res., ...)\nsnow.plot(col='time', col_wrap=6, cbar_kwargs={'label': 'Snow cover'}, cmap='coolwarm_r')\n</pre> import xarray as xr snow = xr.where(S2_ds.NDSI&gt;0.4, 1, 0) snow.attrs = S2_ds.attrs                                                # Copy the attributes of the S2 Dataset (CRS, spatial res., ...) snow.plot(col='time', col_wrap=6, cbar_kwargs={'label': 'Snow cover'}, cmap='coolwarm_r') Out[14]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x217ed38e1a0&gt;</pre> <p>Let's take this time series of snow/no-snow maps to quantify the percentage of our area-of-interest that is snow covered for each month.</p> In\u00a0[15]: Copied! <pre>import numpy as np\nsnow_percentage = np.nanmean(snow.values, axis=(1,2))*100\n</pre> import numpy as np snow_percentage = np.nanmean(snow.values, axis=(1,2))*100 <p>... and plot this.</p> In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nplt.figure(figsize=(5,3))\nplt.plot(snow.time, snow_percentage)\nplt.title(\"Percentage of area covered by snow\\nbased on monthly Sentinel-2 NDSI\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Snow covered area [%]\")\n</pre> import matplotlib.pyplot as plt plt.figure(figsize=(5,3)) plt.plot(snow.time, snow_percentage) plt.title(\"Percentage of area covered by snow\\nbased on monthly Sentinel-2 NDSI\") plt.xticks(rotation=45) plt.ylabel(\"Snow covered area [%]\") Out[16]: <pre>Text(0, 0.5, 'Snow covered area [%]')</pre> In\u00a0[17]: Copied! <pre>snow.sum(dim='time').plot(cbar_kwargs={'label': 'Snow cover duration [months]'})\n</pre> snow.sum(dim='time').plot(cbar_kwargs={'label': 'Snow cover duration [months]'}) Out[17]: <pre>&lt;matplotlib.collections.QuadMesh at 0x217f04a3c70&gt;</pre> In\u00a0[18]: Copied! <pre>S2_RGB = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\\n    .filterBounds(AOI) \\\n    .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 10) \\\n    .closest('2022-08-01') \\\n    .preprocess() \\\n    .select(['B4', 'B3', 'B2'])                                                         # Query only certain bands from GEE\nS2_RGB_ts = S2_RGB.wx.to_time_series()                                                  # Build wxee time series object (although we expect only one observation)\nS2_RGB_ds = S2_RGB_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=10)            # Download to xarray dataset, now with 10 m resolution\n</pre> S2_RGB = ee.ImageCollection(\"COPERNICUS/S2_SR\") \\     .filterBounds(AOI) \\     .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 10) \\     .closest('2022-08-01') \\     .preprocess() \\     .select(['B4', 'B3', 'B2'])                                                         # Query only certain bands from GEE S2_RGB_ts = S2_RGB.wx.to_time_series()                                                  # Build wxee time series object (although we expect only one observation) S2_RGB_ds = S2_RGB_ts.wx.to_xarray(region = AOI, crs='EPSG:25832', scale=10)            # Download to xarray dataset, now with 10 m resolution <pre>Requesting data:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <p>Note that this request has unexpectedly found two images of the same day. The reason is that our AOI is located on a tile overlap and, thus, gets imagery from both tiles.</p> In\u00a0[19]: Copied! <pre>S2_RGB_ds['time'].values\n</pre> S2_RGB_ds['time'].values Out[19]: <pre>array(['2022-08-09T10:27:53.000000000', '2022-08-09T10:28:07.000000000'],\n      dtype='datetime64[ns]')</pre> <p>We simply take the per-pixel mean values of the two images along the time dimension and plot the RGB bands as a true color composite. Try to assess the landcover in the areas with relatively short snow cover duration. How could this impact the NDSI values?</p> In\u00a0[55]: Copied! <pre>S2_RGB_ds.to_array().mean(dim='time').plot.imshow(robust=True)\nplt.title('Sentinel-2 true color image summer 2022')\n</pre> S2_RGB_ds.to_array().mean(dim='time').plot.imshow(robust=True) plt.title('Sentinel-2 true color image summer 2022') Out[55]: <pre>Text(0.5, 1.0, 'Sentinel-2 true color image summer 2022')</pre> In\u00a0[21]: Copied! <pre>t_var = xr.Variable('threshold', np.arange(0.2, 0.62, 0.02))            # Create a variable for the tresholds\nsnow_sensitivity = xr.concat([xr.where(S2_ds.NDSI&gt;t, 1, 0) for t in np.arange(0.2, 0.62, 0.02)], dim=t_var)\nsnow_sensitivity.attrs = S2_ds.attrs                                    # Copy attributes (CRS, spatial resolution, ...)\n</pre> t_var = xr.Variable('threshold', np.arange(0.2, 0.62, 0.02))            # Create a variable for the tresholds snow_sensitivity = xr.concat([xr.where(S2_ds.NDSI&gt;t, 1, 0) for t in np.arange(0.2, 0.62, 0.02)], dim=t_var) snow_sensitivity.attrs = S2_ds.attrs                                    # Copy attributes (CRS, spatial resolution, ...) In\u00a0[22]: Copied! <pre>snow_sensitivity.mean(dim='threshold').plot(col='time', col_wrap=6, cbar_kwargs={'label': 'Snow cover probability [%]'})\n</pre> snow_sensitivity.mean(dim='threshold').plot(col='time', col_wrap=6, cbar_kwargs={'label': 'Snow cover probability [%]'}) Out[22]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x217eece5bd0&gt;</pre> In\u00a0[39]: Copied! <pre>snow_sensitivity.var(dim='threshold').plot(col='time', col_wrap=6, cmap='Reds', cbar_kwargs={'label': 'Snow classification sensitivity to threshold'})\n</pre> snow_sensitivity.var(dim='threshold').plot(col='time', col_wrap=6, cmap='Reds', cbar_kwargs={'label': 'Snow classification sensitivity to threshold'}) Out[39]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x2178726ff40&gt;</pre> In\u00a0[40]: Copied! <pre>snow_percentage_sens = np.nanmean(snow_sensitivity.values, axis=(2,3))*100\nsnow_percentage_sens.shape\n</pre> snow_percentage_sens = np.nanmean(snow_sensitivity.values, axis=(2,3))*100 snow_percentage_sens.shape Out[40]: <pre>(21, 12)</pre> In\u00a0[41]: Copied! <pre>import pandas as pd\n\ndf = pd.DataFrame(snow_percentage_sens, index=snow_sensitivity.threshold)               # each month in a separate column, lines indexed with the thresholds\ndf = df.melt(var_name='Month', value_name='Snow covered area [%]', ignore_index=False)  # melt converts the table to long-form (months are indicated by a column now)\ndf['Month'] = df['Month']+1                                                             # otherwise the months start with zero\ndf\n</pre> import pandas as pd  df = pd.DataFrame(snow_percentage_sens, index=snow_sensitivity.threshold)               # each month in a separate column, lines indexed with the thresholds df = df.melt(var_name='Month', value_name='Snow covered area [%]', ignore_index=False)  # melt converts the table to long-form (months are indicated by a column now) df['Month'] = df['Month']+1                                                             # otherwise the months start with zero df Out[41]: Month Snow covered area [%] 0.20 1 99.174355 0.22 1 99.063530 0.24 1 98.961017 0.26 1 98.833569 0.28 1 98.706120 ... ... ... 0.52 12 89.682210 0.54 12 87.878536 0.56 12 85.384978 0.58 12 82.292965 0.60 12 77.405591 <p>252 rows \u00d7 2 columns</p> <p>Plot the mean and the standard deviation (light blue band) for the classifications of each month.</p> In\u00a0[56]: Copied! <pre>import seaborn as sns\nsns.lineplot(data=df, x='Month', y='Snow covered area [%]', errorbar='sd')\n</pre> import seaborn as sns sns.lineplot(data=df, x='Month', y='Snow covered area [%]', errorbar='sd') Out[56]: <pre>&lt;Axes: xlabel='Month', ylabel='Snow covered area [%]'&gt;</pre> In\u00a0[57]: Copied! <pre>sns.lineplot(data=df, x='Month', y='Snow covered area [%]', hue=df.index)\nplt.legend(title='NDSI threshold')\n</pre> sns.lineplot(data=df, x='Month', y='Snow covered area [%]', hue=df.index) plt.legend(title='NDSI threshold') Out[57]: <pre>&lt;matplotlib.legend.Legend at 0x21780f22bf0&gt;</pre>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#sentinel-2-snow-cover-time-series-classification-in-python-tutorial-with-excercise","title":"Sentinel-2 snow cover time series classification in Python (Tutorial with excercise)\u00b6","text":"<p>This Notebook shows how to implement a simple snow classification workflow on a Sentinel-2 (S-2) satellite image time series.</p> <p>For a small area-of-interest (AOI) in the Alps (Obergurgl, Austria), an S-2 Normalized Difference Snow Index (NDSI) time series is downloaded from the Google Earth Engine (GEE) and classified per-scene into snow-covered and snow-free areas, using the widely used NDSI threshold of 0.4. The results of this simple rule-based classification are analysed and visualized regarding the duration of snow cover (months) per pixel and percentage of snow-covered area within the AOI per month.</p> <p>For Google Earth Engine (GEE) and <code>xarray</code> based raster time series processing we use the geemap, wxee and eemont packages, which are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p> <p>This is an extended version of the Notebook including a suggestion to solve the related ETRAINEE excercise.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#construct-an-area-of-interest-aoi","title":"Construct an area-of-interest (AOI)\u00b6","text":"<p>We want a area around Obergurgl (Tyrol, Austria) as our area-of-interest (AOI). Using <code>eemont</code>, we pass a query for this place via the <code>geopy</code> package to a geocoding service. We just provide a string representing a place that is geocoded. A Bounding Box (<code>ee.Geometry.BBox</code>) for a geocoded place can be constructed from such a query using the <code>ee.Geometry.BBoxFromQuery</code> constructor (extended through <code>eemont</code>). Depending on the geocoding service used (e.g., ArcGIS, Baidu, Google Maps, IGN France, Nominatim, etc.), the resulting Bounding Boxes will differ. In this example, the method is convenient because we don't have to enter any coordinates for our AOI or import a file.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#aggregate-and-download-time-series","title":"Aggregate and download time series\u00b6","text":""},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#show-ndsi-maps","title":"Show NDSI maps\u00b6","text":"<p>Display the monthly NDSI maps using <code>xarray</code>'s built-in plotting functions:</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#snow-classification-for-each-month","title":"Snow classification for each month\u00b6","text":"<p>Now we map binary snow cover simply by thresholding the NDSI with the widely used threshold 0.4 (which we also took to center the colormap in the plot above). This results in a time series of monthly snow/no-snow maps (encoded as 1/0).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#snow-cover-duration-per-pixel","title":"Snow cover duration per pixel\u00b6","text":"<p>Finally calculate how many months every pixel is snow covered.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#excercise-snow-cover-mapping-interpretation-and-sensitivity-analysis","title":"Excercise: Snow cover mapping - Interpretation and sensitivity analysis\u00b6","text":"<p>Tasks and questions:</p> <ul> <li>The snow cover duration appears to be quite short in some parts of our AOI (given that the village of Obergurgl is located at &gt; 1900 m a.s.l.)? Let's have a look at these areas. Try to get a simple color composite for one date using Google Earth Engine and the tools shown in the tutorial. Maybe this helps with interpretation. Alternatively use (monotemporal) higher resolution data from other sources and display them in Python or in a GIS. What are the surface characteristics of the areas with short snow cover duration? Does this point to potential limitations of the snow classification approach?</li> <li>For the binary classification of NDSI into \"snow\" and \"no snow\", we had chosen an NDSI threshold of 0.4. What if we change this threshold? How does this affect the classification results? Test a range of thresholds (0.2 to 0.6 with step 0.02), and visualize the effects of varying the threshold on the spatio-temporal classification results.</li> </ul>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#rgb-composite-for-interpretation-of-snow-cover-duration-patterns","title":"RGB composite for interpretation of snow cover duration patterns\u00b6","text":"<p>Why is the snow cover duration apparently so short in some parts of our AOI? Let's have a look at these areas. Maybe a simple color composite from one date helps with interpretation.</p> <p>As our downloaded time series contains only the NDSI, we need a new query: We search for the closest image to a certain date matching also some other criteria (bounds, cloud cover percentage).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#classification-threshold-sensitivity-analysis","title":"Classification threshold sensitivity analysis\u00b6","text":"<p>For the binary classification of NDSI into \"snow\" and \"no snow\", we had chosen an NDSI threshold of 0.4. What if we change this threshold? How does this affect the classification results?</p> <p>Let's construct a new dataset <code>snow_sensitivity</code> with a new dimension called 'threshold', where we test a range of different values for the threshold parameter (0.2 to 0.6 with step 0.02).</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#spatio-temporal-sensitivity-of-snow-classification","title":"Spatio-temporal sensitivity of snow classification\u00b6","text":"<p>For each pixel, we compute the average of all the binary snow classification outputs with different thresholds. Assuming that each of these threshold is equally likely to produce a correct classification, we call this average value \"snow probability\" and plot it in a map for each month. Moreover, we compute the variance of all classification outputs (referring to as \"snow classification sensitivity to threshold\"). Thereby, we can see where and when the choice of the classification threshold is most critical.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#sensitivity-of-snow-covered-area-to-classification-threshold","title":"Sensitivity of snow covered area to classification threshold\u00b6","text":"<p>Finally, we want to see how the choice of classification thresholds affects the snow covered area determined over the course of the year.</p>"},{"location":"module1/03_time_series_analysis_based_on_classification/T3_S2_snow_classification__excercise.html#conclusions","title":"Conclusions\u00b6","text":"<p>A simple thresholding approach applied on Sentinel-2 Normalized Difference Snow Index (NDSI) rasters was, in general, shown to be capable of mapping snow cover. Classification of a time series of monthly composites is straightforward and can plausibly track the seasonal evolution of snow covered area.</p> <p>For some parts of the AOI, the workflow shown in this notebook indicates a short snow cover duration. These areas are mainly covered by forest. This points to difficulties of the approach to map snow cover reliably in forested areas.</p> <p>The impact of NDSI threshold choice varies with space and time, depending on the snow cover conditions. Completely snow-free or completely snow-covered pixels show sufficient contrast in an NDSI raster and can be classified unambiguously (with a range of thresholds). A patchy snow cover (i) in the melting period and after the first snowfalls in autumn or (ii) in steep and rocky terrain is more ambiguous (mixed pixel problem) and sensitive to the classification threshold.</p> <p>The approach works without any reference data as long as it is not being validated (no training of a statistical classifier). Note that this example is a binary classification based on only one classification feature which provides a good contrast (i.e. separability) for the target classes. Lots of experience from scientific studies is available on the suitability of the feature (NDSI) and the threshold for the problem at hand. For more complex classification tasks (more classes, poor separability, more features, ...) we need more sophisticated approaches to find appropriate decision boundaries (classification rules), for instance using machine learning.</p>"},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html","title":"Theme 4 - Trajectory-based analysis","text":""},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#introduction","title":"Introduction","text":"<p>Objectives</p> <p>The objectives of this theme are to</p> <ul> <li>overview different ways how land surface change can be studied with remote sensing</li> <li>understand the concept of trajectories of a remotely sensed variable along the time dimension</li> <li>overview the main approaches to an analysis of such trajectories</li> <li>understand how a trajectory-based analysis can be implemented (shown in tutorials for simplified use cases)</li> <li>get an idea of exemplary applications of a trajectory-based view on remote sensing time series (from the scientific literature)</li> </ul> <p>Concept</p> <p>For many applications related to the study of human-environment interactions it is crucial to find out when and where changes occur on the land surface, and to characterize these changes in terms of their magnitude, type or direction, as well as spatial and temporal patterns. In this context, temporally dense time series of remote sensing data offer new possibilities for the detection and analysis of land change. Compared to temporally coarser time series (or multi-temporal data with a few observations only), they not only increase the chance that even short-term changes are captured and precisely located in space and time, they also enhance the possibility to separate different components of variation of a signal (thereby overcoming partly the problem of rather low signal to noise ratios). </p> <p>Time series of remote sensing data essentially provide for every spatial unit (e.g. pixel) a series of observations, with each observation containing the value of a variable at a specific point in time. The fluctuation of these values with time can be seen as a trajectory in variable space (feature space). This may become clearer when we plot the observations for one spatial unit as points, connect them with lines and add a resampling or smoothing operation to the time series (to remove noise or high frequency parts of the signal).</p> <p> </p> <p>Example of a spectral trajectory. The detectability of changes is depending on how the original time series is acquired and processed (e.g. resampled or smoothed). The yearly maxima of a vegetation index indicate a change in the year 2019, whereas this is not well visible if observations are available only every four years, i.e. the temporal resolution of observations and aggregated time series is important.</p> <p>The development of observed values with time (trajectory) often follows a distinct pattern, Such patterns may be a constant increase, a constant decrease, or a succession of the two types in different phases of the development (e.g., increase followed by decrease). The transition between such phases can be gradual or abrupt (slow or sudden change of the direction of slope of a trend fitted locally to the trajectory). The observed value may also drop or jump suddenly between two (or a few) observations (breaks).</p> <p>With the increasing availability of dense remote sensing time series, a number of methods have been established to disentangle various components of a spectral trajectory (such as trends, seasonality, noise and short-term change). Before detailing more sophisticated trajectory-based analysis, however, let's have a look at more traditional and general approaches to remote sensing change detection and analysis.</p>"},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#approaches-and-methods","title":"Approaches and methods","text":""},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#change-detection-in-multitemporal-remote-sensing-data","title":"Change detection in multitemporal remote sensing data","text":"<p>Following the definition of Singh (1989), change detection is \"the process of identifying differences in the state of an object or phenomenon by observing it at different times\". The classic way of change detection in remote sensing is a pairwise comparison of data acquired at different points in time. If corresponding spatial units (e.g. pixels) in these data differ from each other (e.g. the difference of a continuous variable exceeding a threshold), the spatial unit is assumed to have changed. A key challenge is to discriminate \"real\" change against noise (random variation) or systematic errors in the data. Therefore, preprocessing steps (such as filtering/de-noising/smoothing and data harmonization) are typically applied prior to any change detection. Over the years, many algorithms have been developed for finding differences in multitemporal data and deciding if these correspond to change. Tewkesbury et al. (2015) suggest to distinguish change detection techniques primarily by the unit of analysis and by the comparison method.</p> <p>Moreover, a general categorization of change detection approaches is often made into pre-classification change detection vs. post-classification change detection. Pre-classification change detection involves a comparison of continuous variables (such as surface reflectance or a vegetation index; e.g. by layer arithmetic), whereas in post-classification change detection the data from each observation is first classified into discrete categories (such as landcover classes) and then these categories are compared.</p> <p>While the detection of change is an important step, in most cases also the nature of change (such as magnitude, direction, timing, speed, thematic categories (from-to change information) or aggregated values (e.g. affected area or volumes)) are of interest. Often, the nature of change can be characterized as either abrupt or gradual (in relative terms) but it may also be something in between, e.g. a sequence of small abrupt events looking like a gradual development (Woodcock et al. 2020). Following Woodcock et al. (2020), change can be transitional or conditional. Transitional changes typically refer to a complete transition of the surface type (e.g. from forest to parking lot), whereas conditional changes represent a change in the condition of the surface (e.g. seasonal green-up of a forest). For detailed reviews of change detection methods see Singh (1989), Tewkesbury et al. (2015) and Ban &amp; Youssif (2016). A starting point for 3D change detection is provided in review articles by Qin et al. (2016) and Xiao et al. (2023).</p> <p>Classic change detection techniques by pairwise comparison of images are to some extent possible with basic raster processing tools (raster calculator, reclassify, ...) available in most GIS and remote sensing software suites (e.g. QGIS). For more specific algorithms you may need plugins or implementations in a programming language. Code and links to traditional and deep learning algorithms for change detection in Python and other languages are collected, for instance, in the repositories by Chen Hongruixuan (2023), Wenhwu (2023), and Min Zhang-Whu (2023).</p> <p>Learn more on theoretical backgrounds and practical implementations in the dedicated sections of the E-TRAINEE Modules 2 and 3, which focus on change detection and analysis in optical satellite data and in 3D point clouds, respectively.</p>"},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#analysis-and-decomposition-of-spectral-trajectories","title":"Analysis and decomposition of spectral trajectories","text":"<p>As time series of remote sensing data are expanding the possibilities for land change analysis, Woodcock et al. (2020) have postulated a paradigm shift away from bi-temporal change detection towards more continuous monitoring of change. This methodological trend enhances the detection of subtle changes, the precise recognition of their timing, and the characterization of the nature of change (e.g. gradual vs. abrupt). Especially the imagery from Landsat and Sentinel-2 permit users to conduct analyses at spatial and temporal scales appropriate for many ecological topics (Kennedy et al. 2014).</p> <p>Monitoring ecosystem disturbance and recovery</p> <p>Among the many applications of such analyses, one of the most prominent ones is studying ecosystem disturbance and recovery, especially of forests (e.g., Kennedy et al. 2007, White et al. 2017, Zhu et al 2020).  To identify and categorize vegetation cover disturbance by open-pit mining and assess the status of subsequent vegetation recovery, Yang et al. (2018) classified different types of Landsat NDVI trajectories. In this context, the LandTrendr software (Kennedy et al. 2010) was developed for segmenting spectral-temporal time series. It is capable of capturing and describing the characteristic trajectories of different forest states, with shorter disturbance events and longer-term processes of recovery in forested ecosystems (see also Module 2). The LandTrendr algorithm has also been ported from its original IDL implementation to the Google Earth Engine platform (Kennedy et al. 2018), for details see the documentation at the LT-GEE website.</p> <p> </p> <p>Spectral trajectories for different forest states (left) and an illustration of different change metrics (such as magnitude (m<sub>1</sub>, m<sub>2</sub>) and persistence (p<sub>1</sub>, p<sub>2</sub>); right; figure by Wulder et al. 2019/ CC BY 4.0).</p> <p>Another widely used option for landcover change detection is the search for breakpoints in remote sensing trajectories, e.g. with the Breaks For Additive Season and Trend (BFAST) algorithm (Verbesselt et al. 2010). BFAST is an unsupervised time series change detection algorithm specialised in detecting multiple breakpoints within a multi-year time series. It involves an iterative process with multiple steps for decomposing a time series into trend, seasonal and error components, and for detecting breakpoints in each of the components. Variants developed on this basis include the faster and more flexible BFASTlite (Masili\u016bnas et al. 2021), BFASTmonitor (for near-real time detection of a single break; Verbesselt et al. 2012), and a Google Earth Engine implementation of BFASTmonitor (Hamunyela et al. 2020).</p> <p>A comprehensive overview of algorithms for change detection and characterization in satellite image time series is provided in Module 2. A single change detection method often turns out to be not reliable enough and, depending on the situation (e.g. change type, data availability, etc.), different algorithms will be most suitable. This motivates the development of multi-algorithm ensembles to increase change detection accuracy. Xu et al. (2022) took an ensemble approach for land cover change detection and combined different BFAST outputs in a Random Forest classification. Bullock et al. (2020) found that using a combination of break detection algorithms based on fundamentally different approaches can considerably improve the robustness of land change monitoring. In their study, they applied two tests to detect breaks and another test to identify falsely identified breaks.</p> <p>Seasonality parameters/phenology</p> <p>Another application where satellite image time series are invaluable, is the study of distinctive seasonal variation and timing of vegetation vigour and amount (i.e., land surface phenology (LSP)). Here, time series of satellite derived vegetation indices or biophysical variables are used as proxies to study LSP and to investigate potential trends in LSP-related parameters and their drivers (Henebry and de Boers 2013, Helman 2018, Caparros-Santiago et al. 2021).</p> <p>The TIMESAT software (Eklundh &amp; J\u00f6nsson 2016, Eklundh 2023) was developed to derive a number of seasonality parameters from satellite image time series. A smoothing method (either asymmetric Gaussian fits, double-logistic fits, or Savitzky-Golay filtering) is applied to the time series of each pixel. A range of parameters can then be extracted from the smoothed trajectories for each season. TIMESAT is typically employed for phenological studies, and the upcoming release (version 4) is also used for the Copernicus HR-VPP (high-resolution vegetation phenology and productivity) product suite.</p> <p> </p> <p>Some of the seasonality parameters generated in TIMESAT: (a) beginning of season, (b) end of season, (c) length of season, (d) base value, (e) time of middle of season, (f) maximum value, (g) amplitude, (h) small integrated value, (h+i) large integrated value. (figure by Eklundh 2023/ CC BY-NC-ND 2.5 SE).</p>"},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#spatial-segmentation-based-on-similarity-of-trajectories","title":"Spatial segmentation based on similarity of trajectories","text":"<p>Striving to delineate areas with a homogeneous development through time, remote sensing data can be segmented spatially according to time series similarity (Maus et al. 2016, Belgiu &amp; Csilik 2018, Anders et al. 2021). In this approach, trajectories of a spectral band or index or another variable (such as topographic change measured by photogrammetry or laser scanning) are grouped (i) by a clustering algorithm applied to the time series (Kuschnerus et al. 2021) or (ii) by spatial segmentation according to a similarity measure used to compare trajectories of adjacent spatial units (Anders et al. 2021). The result are spatial objects with relatively homogeneous development through time (\"objects-by-change\"; Anders et al. 2020), and these objects can help to interpret the dataset with respect to underlying processes and drivers of change. This approach is contained in detail in module 3 of this course!</p>"},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#research-examples","title":"Research examples","text":"<p>Some examples of multi-temporal change detection and trajectory-based time series analysis for investigating human-environment interactions:</p> <p>Global land cover change</p> <p>Song et al. (2018) analysed 35 years of multi-sensor satellite image data to map global land-change dynamics. For each year, vegetation continuous fields (VCF, i.e. percent cover of tree canopy, short vegetation and bare ground) at the local growing season peak were derived per 0.05\u00b0 \u00d7 0.05\u00b0 pixel. A comprehensive analysis of VCF changes revealed that 60% of all land changes were associated with direct human activities and 40% with indirect drivers such as climate change. VCF change and uncertainty layers are published here.</p> <p>Similarly, Radwan et al. (2021) provide a global perspective on land cover trajectories and transitions. This was achieved by an analysis of the European Space Agency\u2019s Climate Change Initiative-Land Cover (ESA-CCI-LC27) dataset with 300 m spatial resolution and global coverage annually from 1992 to 2018.</p> <p></p> <p>Areal development of different landcover types per continent between 1992 and 2018, relative to the initial area. Error bars for the 95% confidence interval are provided for the continent showing greatest change in each plot (figure by Radwan et al. 2021/ CC BY 4.0)</p> <p> </p> <p>Global LC transitions (from-to change) between 1992 and 2018, expressed in percentage terms relative to the total global LC area that changed over this period (figure by Radwan et al. 2021/ CC BY 4.0).</p> <p>Coastal change</p> <p>Murray et al. (2019) used satellite image time series analysis to map and study the global extent of and change in tidal flats over the course of 33 years (1984\u20132016). This analysis was later expanded until 2019 by Murray et al. (2022). The dataset can be explored in this Google Earth Engine app. Luijendijk et al. (2018) used data from the Landsat archive to study directions and rates of shoreline change at sandy beaches of the World.</p> <p></p> <p>Global hotspots of beach erosion and accretion. The red (green) circles indicate erosion (accretion) for the four shoreline change rate categories (depicted in the legend). The bar plots to the right and at the bottom present the relative occurrence of eroding (accreting) sandy shorelines per degree latitude and longitude, respectively. The numbers presented in the main plot represent the average change rate for all sandy shorelines per continent (figure by Luijendijk et al. (2018)/ CC BY 4.0).</p>"},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"1) Correct or false: A trajectory-based analysis of remote sensing time series (RSTS) \u2026  \u2026 looks at the spatial variation at each moment by computing a variable\u2019s differences between neighbouring spatial units (e.g., pixels). \u2026 analyses how a variable fluctuates in a spatial unit with time. \u2026 analyses how a variable fluctuates in a spatial unit with time.  2) Correct or false: For an analysis of spectral-temporal trajectories in RSTS, it is usually best to \u2026  \u2026 use the original data with high temporal resolution to ensure maximum accuracy. \u2026 resample, aggregate, or smooth the time series to identify changes at the desired temporal scale and ignore higher-frequency fluctuations. \u2026 strive for a good trade-off between high temporal resolution and filtering of undesired parts of the signal. \u2026 resample, aggregate, or smooth the time series to identify changes at the desired temporal scale and ignore higher-frequency fluctuations.&amp;\u2026 strive for a good trade-off between high temporal resolution and filtering of undesired parts of the signal.  3) What are main differences between classic change detection with bi-temporal data and change analysis in remote sensing time series (RSTS)?  RSTS enhance the possibilities for a detection of subtle changes, for a precise recognition of their timing, and for the characterization of the nature of change (e.g., gradual vs. abrupt). Bi-temporal change detection can deal with heterogeneous input data and needs no preprocessing (such as harmonization) of this data. Change detection and analysis with RSTS requires more specialized algorithms and software. RSTS enhance the possibilities for a detection of subtle changes, for a precise recognition of their timing, and for the characterization of the nature of change (e.g., gradual vs. abrupt).&amp;Change detection and analysis with RSTS requires more specialized algorithms and software."},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#tutorial-forest-disturbance-assessment-in-python-and-gee","title":"Tutorial: Forest disturbance assessment in Python and GEE","text":"<p>In this Jupyter Notebook we compare Landsat 8 NDVI time series (spectral-temporal trajectory) for a disturbed forest location and for an undisturbed location to assess the timing of forest disturbance. You will learn how to extract, process and visualize spectral-temporal profiles (trajectories) for defined locations using Python and the Google Earth Engine (GEE). In this case the process and timing of forest damage on the mountain above Innsbruck (Tyrol, Austria) is well-known: a large avalanche in January 2019; see this image by the Avalanche Warning Service Tyrol; more photographs and background information here: in German and in English.</p> <p> </p> <p>Avalanche zone north of Innsbruck (Austria) with forest damaged in January 2019 (images by A. Mayr, 23 June 2019/ CC BY-SA 4.0).</p> <p> </p> <p>Landsat 8 NDVI time series of a disturbed (blue) and an undisturbed (orange) forest site.</p> <p> </p> <p>Monthly (top), three monthly (center) and annual (bottom) maximum Landsat 8 NDVI time series of a disturbed (blue) and an undisturbed (orange) forest site.</p>"},{"location":"module1/04_trajectory-based_analysis/04_trajectory_based_analysis.html#references","title":"References","text":"<p>Ban, Y., Yousif, O. (2016). Change Detection Techniques: A Review. In: Ban, Y. (eds). Multitemporal Remote Sensing. Remote Sensing and Digital Image Processing, vol 20. Springer, Cham. https://doi.org/10.1007/978-3-319-47037-5_2</p> <p>Bullock, E. L., Woodcock, C. E., &amp; Holden, C. E. (2020). Improved change monitoring using an ensemble of time series algorithms. Remote Sensing of Environment, 238, 111165. https://doi.org/10.1016/j.rse.2019.04.018</p> <p>Caparros-Santiago, J. A., Rodriguez-Galiano, V., &amp; Dash, J. (2021). Land surface phenology as indicator of global terrestrial ecosystem dynamics: A systematic review. ISPRS Journal of Photogrammetry and Remote Sensing, 171, 330-347. https://doi.org/10.1016/j.isprsjprs.2020.11.019</p> <p>Eklundh, L., J\u00f6nsson, P. (2016). TIMESAT for Processing Time-Series Data from Satellite Sensors for Land Surface Monitoring. In: Ban, Y. (eds) Multitemporal Remote Sensing. Remote Sensing and Digital Image Processing, vol 20. Springer, Cham. https://doi.org/10.1007/978-3-319-47037-5_9</p> <p>Hamunyela, E., Rosca, S., Mirt, A., Engle, E., Herold, M., Gieseke, F., &amp; Verbesselt, J. (2020). Implementation of BFASTmonitor algorithm on google earth engine to support large-area and sub-annual change monitoring using earth observation data. Remote Sensing, 12(18), 2953. https://doi.org/10.3390/rs12182953</p> <p>Helman, D. (2018). Land surface phenology: What do we really \u2018see\u2019 from space? Science of the Total Environment, 618, 665-673. https://doi.org/10.1016/j.scitotenv.2017.07.237</p> <p>Henebry, G. M., de Beurs, K. M. (2013). Remote Sensing of Land Surface Phenology: A Prospectus. In Schwartz, M. D. (Ed.), Phenology: An Integrative Environmental Science, Springer, Dordrecht, Netherlands. https://doi.org/10.1007/978-94-007-6925-0_21</p> <p>Kennedy, R. E., Cohen, W. B., &amp; Schroeder, T. A. (2007). Trajectory-based change detection for automated characterization of forest disturbance dynamics. Remote Sensing of Environment, 110(3), 370-386. https://doi.org/10.1016/j.rse.2007.03.010</p> <p>Kennedy, R. E., Andr\u00e9fou\u00ebt, S., Cohen, W. B., G\u00f3mez, C., Griffiths, P., Hais, M., ... &amp; Zhu, Z. (2014). Bringing an ecological view of change to Landsat\u2010based remote sensing. Frontiers in Ecology and the Environment, 12(6), 339-346. https://doi.org/10.1890/130066</p> <p>Kennedy, R.E., Yang, Z., Gorelick, N., Braaten, J., Cavalcante, L., Cohen, W.B., Healey, S. (2018). Implementation of the LandTrendr Algorithm on Google Earth Engine. Remote Sensing. 10, 691. https://doi.org/10.3390/rs10050691</p> <p>Luijendijk, A., Hagenaars, G., Ranasinghe, R. et al. (2018). The State of the World\u2019s Beaches. Scientific Reports 8, 6641. https://doi.org/10.1038/s41598-018-24630-6</p> <p>Masili\u016bnas, D., Tsendbazar, N. E., Herold, M., &amp; Verbesselt, J. (2021). BFAST Lite: A lightweight break detection method for time series analysis. Remote Sensing, 13(16), 3308. https://doi.org/10.3390/rs13163308</p> <p>Murray, N. J., Phinn, S. R., DeWitt, M., Ferrari, R., Johnston, R., Lyons, M. B., ... &amp; Fuller, R. A. (2019). The global distribution and trajectory of tidal flats. Nature, 565(7738), 222-225. https://doi.org/10.1038/s41586-018-0805-8</p> <p>Radwan, T. M., Blackburn, G. A., Whyatt, J. D., &amp; Atkinson, P. M. (2021). Global land cover trajectories and transitions. Scientific Reports, 11(1), 1-16. https://doi.org/10.1038/s41598-021-92256-2</p> <p>Singh, A. (1989). Review article digital change detection techniques using remotely-sensed data. International Journal of Remote Sensing, 10(6), 989-1003. https://doi.org/10.1080/01431168908903939</p> <p>Song, X.P., Hansen, M.C., Stehman, S.V., Potapov, P.V., Tyukavina, A., Vermote, E.F., &amp; Townshend, J.R. (2018). Global land change 1982-2016. Nature 560, 639\u2013643. https://doi.org/10.1038/s41586-018-0411-9</p> <p>Tewkesbury, A. P., Comber, A. J., Tate, N. J., Lamb, A., &amp; Fisher, P. F. (2015). A critical synthesis of remotely sensed optical image change detection techniques. Remote Sensing of Environment, 160, 1-14. https://doi.org/10.1016/j.rse.2015.01.006</p> <p>Verbesselt, J., Hyndman, R., Newnham, G., &amp; Culvenor, D. (2010). Detecting trend and seasonal changes in satellite image time series. Remote Sensing of Environment, 114(1), 106-115. https://doi.org/10.1016/j.rse.2009.08.014</p> <p>Verbesselt, J., Zeileis, A., &amp; Herold, M. (2012). Near real-time disturbance detection using satellite image time series. Remote Sensing of Environment, 123, 98-108. https://doi.org/10.1016/j.rse.2012.02.022</p> <p>White, J. C., Wulder, M. A., Hermosilla, T., Coops, N. C., &amp; Hobart, G. W. (2017). A nationwide annual characterization of 25 years of forest disturbance and recovery for Canada using Landsat time series. Remote Sensing of Environment, 194, 303-321. https://doi.org/10.1016/j.rse.2017.03.035</p> <p>Wulder, M. A., Loveland, T. R., Roy, D. P., Crawford, C. J., Masek, J. G., Woodcock, C. E., ... &amp; Zhu, Z. (2019). Current status of Landsat program, science, and applications. Remote Sensing of Environment, 225, 127-147. https://doi.org/10.1016/j.rse.2019.02.015</p> <p>Xiao, W., Cao, H., Tang, M., Zhang, Z., &amp; Chen, N. (2023). 3D urban object change detection from aerial and terrestrial point clouds: A review. International Journal of Applied Earth Observation and Geoinformation, 118, 103258. https://doi.org/10.1016/j.jag.2023.103258</p> <p>Xu, L., Herold, M., Tsendbazar, N. E., Masili\u016bnas, D., Li, L., Lesiv, M., ... &amp; Verbesselt, J. (2022). Time series analysis for global land cover change monitoring: A comparison across sensors. Remote Sensing of Environment, 271, 112905. https://doi.org/10.1016/j.rse.2022.112905</p> <p>Yang, Z., Li, J., Zipper, C. E., Shen, Y., Miao, H., &amp; Donovan, P. F. (2018). Identification of the disturbance and trajectory types in mining areas using multitemporal remote sensing images. Science of the Total Environment, 644, 916-927. https://doi.org/10.1016/j.scitotenv.2018.06.341</p> <p>Zhu, Z., Zhang, J., Yang, Z., Aljaddani, A. H., Cohen, W. B., Qiu, S., &amp; Zhou, C. (2020). Continuous monitoring of land disturbance based on Landsat time series. Remote Sensing of Environment, 238, 111116. https://doi.org/10.1016/j.rse.2019.03.009</p>"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html","title":"T4 GEE NDVI time series points","text":"Metadata     title: \"E-TRAINEE Tutorial - Forest disturbance assessment with Python and the GEE\"     description: \"This is a tutorial within the fourth theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-21     authors: Andreas Mayr In\u00a0[10]: Copied! <pre>import ee, eemont, geemap         # ee is the GEE Python API\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n</pre> import ee, eemont, geemap         # ee is the GEE Python API import pandas as pd import numpy as np import seaborn as sns from matplotlib import pyplot as plt <p>Authenticate and initialize Earth Engine.</p> In\u00a0[11]: Copied! <pre>try:\n        ee.Initialize()\nexcept Exception as e:          # If initialize does not work, you probably have to authenticate first\n        ee.Authenticate()\n        ee.Initialize()\n</pre> try:         ee.Initialize() except Exception as e:          # If initialize does not work, you probably have to authenticate first         ee.Authenticate()         ee.Initialize() <p>We want to compare the NDVI time series (spectral-temporal profile) for two points-of-interest: a disturbed forest location and an undisturbed location. Create a GEE FeatureCollection by defining locations as geographic coordinates (copied from QGIS) and adding a buffer around each point.</p> In\u00a0[12]: Copied! <pre>lat = [47.2989635, 47.300451]   # two latitude values, one for each point\nlon = [11.4038744, 11.417014]   # two longitude values, one for each point\nmypoints = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([lon[0],lat[0]]).buffer(50),{'point_ID':1}),   # a location with disturbed forest\n    ee.Feature(ee.Geometry.Point([lon[1],lat[1]]).buffer(50),{'point_ID':2})    # an undisturbed location\n])\n</pre> lat = [47.2989635, 47.300451]   # two latitude values, one for each point lon = [11.4038744, 11.417014]   # two longitude values, one for each point mypoints = ee.FeatureCollection([     ee.Feature(ee.Geometry.Point([lon[0],lat[0]]).buffer(50),{'point_ID':1}),   # a location with disturbed forest     ee.Feature(ee.Geometry.Point([lon[1],lat[1]]).buffer(50),{'point_ID':2})    # an undisturbed location ]) <p>Let's use the geemap package to display our locations on a map.</p> In\u00a0[13]: Copied! <pre>Map = geemap.Map()\nMap.addLayer(mypoints,{'color':'blue'})\nMap.centerObject(mypoints,14)       # we provide a zoom level here (14)\nMap\n</pre> Map = geemap.Map() Map.addLayer(mypoints,{'color':'blue'}) Map.centerObject(mypoints,14)       # we provide a zoom level here (14) Map Out[13]: <pre>Map(center=[47.299707488940804, 11.410444109531477], controls=(WidgetControl(options=['position', 'transparent\u2026</pre> In\u00a0[14]: Copied! <pre>L8 = (ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')         # or 'LANDSAT/LC08/C02/T1_L2' for Landsat Collection 2\n      .filterBounds(mypoints)\n      .maskClouds()\n      .scaleAndOffset()\n      .spectralIndices(['NDVI', 'EVI']))\n</pre> L8 = (ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')         # or 'LANDSAT/LC08/C02/T1_L2' for Landsat Collection 2       .filterBounds(mypoints)       .maskClouds()       .scaleAndOffset()       .spectralIndices(['NDVI', 'EVI'])) <p>The eemont package has a function for extracting spectral time series for regions. As regions we use the FeatureCollection containing our points with a buffer. We aggregate the pixels inside the buffer by taking the mean value at each time stamp.</p> In\u00a0[15]: Copied! <pre>L8_point_ts = L8.getTimeSeriesByRegions(collection = mypoints,\n                              bands = ['NDVI', 'EVI'],\n                              reducer = ee.Reducer.mean(),\n                              scale = 30)\n</pre> L8_point_ts = L8.getTimeSeriesByRegions(collection = mypoints,                               bands = ['NDVI', 'EVI'],                               reducer = ee.Reducer.mean(),                               scale = 30) In\u00a0[16]: Copied! <pre>tsPandas = geemap.ee_to_pandas(L8_point_ts)\ntsPandas\n</pre> tsPandas = geemap.ee_to_pandas(L8_point_ts) tsPandas Out[16]: date point_ID EVI reducer NDVI 0 2013-03-24T10:00:18 1 -9999.000000 mean -9999.000000 1 2013-03-24T10:00:18 2 -9999.000000 mean -9999.000000 2 2013-04-13T09:59:46 1 -9999.000000 mean -9999.000000 3 2013-04-13T09:59:46 2 0.385702 mean 0.697527 4 2013-05-15T09:59:56 1 -9999.000000 mean -9999.000000 ... ... ... ... ... ... 673 2021-11-20T10:04:29 2 0.375316 mean 0.770483 674 2021-12-06T10:04:30 1 -9999.000000 mean -9999.000000 675 2021-12-06T10:04:30 2 -9999.000000 mean -9999.000000 676 2021-12-22T10:04:27 1 -0.021050 mean -0.004391 677 2021-12-22T10:04:27 2 0.456888 mean 0.785765 <p>678 rows \u00d7 5 columns</p> <p>Replace all -9999 values by NaN (NoData, 'not-a-number'), and convert the date from string to a date data type.</p> In\u00a0[17]: Copied! <pre>tsPandas[tsPandas == -9999] = np.nan\ntsPandas['date'] = pd.to_datetime(tsPandas['date'],infer_datetime_format = True)\ntsPandas\n</pre> tsPandas[tsPandas == -9999] = np.nan tsPandas['date'] = pd.to_datetime(tsPandas['date'],infer_datetime_format = True) tsPandas Out[17]: date point_ID EVI reducer NDVI 0 2013-03-24 10:00:18 1 NaN mean NaN 1 2013-03-24 10:00:18 2 NaN mean NaN 2 2013-04-13 09:59:46 1 NaN mean NaN 3 2013-04-13 09:59:46 2 0.385702 mean 0.697527 4 2013-05-15 09:59:56 1 NaN mean NaN ... ... ... ... ... ... 673 2021-11-20 10:04:29 2 0.375316 mean 0.770483 674 2021-12-06 10:04:30 1 NaN mean NaN 675 2021-12-06 10:04:30 2 NaN mean NaN 676 2021-12-22 10:04:27 1 -0.021050 mean -0.004391 677 2021-12-22 10:04:27 2 0.456888 mean 0.785765 <p>678 rows \u00d7 5 columns</p> In\u00a0[18]: Copied! <pre>plt.figure(figsize = (10,3))\nsns.lineplot(data = tsPandas,\n             x = 'date',\n             y = 'NDVI',\n             palette = ['red', 'grey'],\n             hue = 'point_ID',\n             alpha = 0.5)\n</pre> plt.figure(figsize = (10,3)) sns.lineplot(data = tsPandas,              x = 'date',              y = 'NDVI',              palette = ['red', 'grey'],              hue = 'point_ID',              alpha = 0.5) Out[18]: <pre>&lt;Axes: xlabel='date', ylabel='NDVI'&gt;</pre> <p>Or each spectral-temporal profile in a separate panel:</p> In\u00a0[19]: Copied! <pre>myplot = sns.FacetGrid(tsPandas,row = 'point_ID',height = 3,aspect = 4)\nmyplot.map_dataframe(sns.lineplot,x = 'date',y = 'NDVI')\nmyplot.add_legend()\n</pre> myplot = sns.FacetGrid(tsPandas,row = 'point_ID',height = 3,aspect = 4) myplot.map_dataframe(sns.lineplot,x = 'date',y = 'NDVI') myplot.add_legend() Out[19]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x211cac72290&gt;</pre> In\u00a0[20]: Copied! <pre>tsPandas_indexed = tsPandas.set_index('date')\ntsPandas_indexed\n</pre> tsPandas_indexed = tsPandas.set_index('date') tsPandas_indexed Out[20]: point_ID EVI reducer NDVI date 2013-03-24 10:00:18 1 NaN mean NaN 2013-03-24 10:00:18 2 NaN mean NaN 2013-04-13 09:59:46 1 NaN mean NaN 2013-04-13 09:59:46 2 0.385702 mean 0.697527 2013-05-15 09:59:56 1 NaN mean NaN ... ... ... ... ... 2021-11-20 10:04:29 2 0.375316 mean 0.770483 2021-12-06 10:04:30 1 NaN mean NaN 2021-12-06 10:04:30 2 NaN mean NaN 2021-12-22 10:04:27 1 -0.021050 mean -0.004391 2021-12-22 10:04:27 2 0.456888 mean 0.785765 <p>678 rows \u00d7 4 columns</p> <p>Next, we want the NDVI data of each point in a separate column.</p> In\u00a0[21]: Copied! <pre>tsPandas_pv = pd.pivot_table(tsPandas, index='date', values='NDVI', columns='point_ID')\ntsPandas_pv\n</pre> tsPandas_pv = pd.pivot_table(tsPandas, index='date', values='NDVI', columns='point_ID') tsPandas_pv Out[21]: point_ID 1 2 date 2013-04-13 09:59:46 NaN 0.697527 2013-06-07 10:06:10 0.876574 0.865738 2013-06-16 09:59:55 0.757573 0.766580 2013-06-23 10:06:04 0.891951 0.806887 2013-07-02 09:59:56 0.881142 0.875385 ... ... ... 2021-09-10 09:58:14 0.732096 0.853265 2021-10-28 09:58:24 0.560447 0.759319 2021-11-20 10:04:29 0.465900 0.770483 2021-12-22 10:04:27 -0.004391 0.785765 2021-12-31 09:58:12 0.192541 0.899323 <p>159 rows \u00d7 2 columns</p> <p>Now we can compute aggregate statistics (such as the mean, maximum, minimum, median) over defined time periods. This will smooth the time series and make it easier to grasp certain patterns from the plots.</p> In\u00a0[22]: Copied! <pre>tsPandas_monthly = tsPandas_pv.resample('M').max()\ntsPandas_3monthly = tsPandas_pv.resample('3M').max()\ntsPandas_yearly = tsPandas_pv.resample('Y').max()\n</pre> tsPandas_monthly = tsPandas_pv.resample('M').max() tsPandas_3monthly = tsPandas_pv.resample('3M').max() tsPandas_yearly = tsPandas_pv.resample('Y').max() In\u00a0[23]: Copied! <pre>fig, axes = plt.subplots(3,1, figsize=(10, 5), sharex=True)\nsns_plot = sns.lineplot(data = tsPandas_monthly, ax=axes[0])\nsns_plot = sns.lineplot(data = tsPandas_3monthly, ax=axes[1])\nsns_plot = sns.lineplot(data = tsPandas_yearly, ax=axes[2])\n</pre> fig, axes = plt.subplots(3,1, figsize=(10, 5), sharex=True) sns_plot = sns.lineplot(data = tsPandas_monthly, ax=axes[0]) sns_plot = sns.lineplot(data = tsPandas_3monthly, ax=axes[1]) sns_plot = sns.lineplot(data = tsPandas_yearly, ax=axes[2]) <p>Of course we can also plot the time series with matplotlib.</p> In\u00a0[24]: Copied! <pre>plt.figure(figsize=(10,5))\nplt.plot_date(tsPandas_pv.index, tsPandas_pv)\nplt.title(\"Landsat 8 NDVI time series forest sites\")\nplt.ylabel(\"NDVI\")\nplt.legend(['Point 1', 'Point 2'])\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(10,5)) plt.plot_date(tsPandas_pv.index, tsPandas_pv) plt.title(\"Landsat 8 NDVI time series forest sites\") plt.ylabel(\"NDVI\") plt.legend(['Point 1', 'Point 2']) plt.grid(True) plt.show()"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#forest-disturbance-assessment-with-python-and-the-gee","title":"Forest disturbance assessment with Python and the GEE\u00b6","text":"<p>We want to compare the NDVI time series (spectral-temporal profile) for a disturbed forest location and for an undisturbed location to assess the timing of forest disturbance.</p> <p>This notebook introduces possibilities for working with Python, the Google Earth Engine (GEE) and additional (helper) packages. You will learn how to extract spectral-temporal profiles for defined locations using the following packages:</p> <ul> <li>ee - The GEE Python API.</li> <li>geemap - A Python package described in Wu (2020). The geemap tools facilitate interactive mapping but also some other tasks with GEE.</li> <li>eemont - This package adds utility methods for different Earth Engine Objects which are friendly with Python method chaining (Montero 2021). Thereby, the complexity of the code needed for processing tasks like cloud masking and spectral index calculation is reduced.</li> <li>pandas and seaborn are used to process and plot the point-based time series</li> </ul> <p>These packages are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p> <p>Note: You will need a Google account with GEE activated (Sign up here, if you do not already have one.).</p>"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#define-points-of-interest","title":"Define points-of-interest\u00b6","text":"<p>Start with importing the required packages.</p>"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#extract-time-series-for-regions","title":"Extract time series for regions\u00b6","text":""},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#query-gee-image-collection","title":"Query GEE image collection\u00b6","text":"<p>Query the Landsat 8 image collection from the Google Earth Engine centered around our points-of-interest (a FeatureCollection), and preprocess the image collection. We also calculate two different vegetation indices. The eemont package provides functions to make this easy.</p>"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#convert-time-series-to-pandas","title":"Convert time series to Pandas\u00b6","text":"<p>To utilize the plotting and analysis functionality of Pandas, we convert the time series retrieved as a feature collection to a Pandas DataFrame.</p>"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#visualization","title":"Visualization\u00b6","text":"<p>We use <code>seaborn</code> (imported as sns in the beginning) to visualize our time series.</p>"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#temporal-aggregations","title":"Temporal aggregations\u00b6","text":"<p>To perform temporal aggregations (such as computing monthly means) we need a DataFrame with DatetimeIndex. So we set the date column as index.</p>"},{"location":"module1/04_trajectory-based_analysis/T4_GEE_NDVI_time_series_points.html#interpretation","title":"Interpretation\u00b6","text":"<p>Looking at the NDVI time series for the two points, we see a seasonal cycle and some variation between different years. The variation is stronger for the yearly minima than for the yearly maxima, maybe because the minima are affected by snow and Landsat 8 did not always capture the (relatively short-lived) situations with a lot of snow on the canopy.</p> <p>However, what is apparent in all these spectral trajectories is that point 1 (the disturbed forest site) has an unusually low NDVI in summer 2019, compared to the previous years, whereas the NDVI of point 2 (the undisturbed site) maintains constantly high annual maxima. In the years 2020 and 2021, the annual NDVI maxima of point 1 have increased again. This pattern reflects the destruction of forest in winter 2019 (by an avalanche as we know in this case) with subsequent establishment or expansion of some vegetation in the following years (probably forbs and shrubs).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html","title":"Theme 5 - Spatio-temporal data fusion","text":""},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#introduction","title":"Introduction","text":"<p>In the first themes (mainly themes 1 and 2) you have learned about the characteristics of remote sensing time series (RSTS) from different sources. In each excercise or practical tutorial we have used data from a single source (mostly Sentinel-2). For capturing environmental dynamics, you may, however, also have identified some limitations related to the data characteristics, such as the native spatial, spectral or temporal resolution or data gaps due to cloud cover. This theme deals with possibilities of addressing such limitations by spatio-temporal data fusion.</p> <p>Objectives</p> <p>The objectives of the theoretical part of this theme are to</p> <ul> <li>Understand why spatio-temporal data fusion is needed for analysing human-environment interactions</li> <li>Learn about important theoretical concepts of data fusion and different approaches</li> <li>Overview some popular data type combinations and typical or innovative methods to fuse RSTS data</li> </ul> <p>In the practical parts of the theme, one possible way to combine multi-modal data for observing human-environment interactions is illustrated in a tutorial Notebook. There, you will learn how to jointly analyse Sentinel-1 and Sentinel-2 data for monitoring the extent of a water surface at the example of the Grand Ethiopian Rennaissance Dam (GERD). Afterwards there is an excercise, where this approach is used as a basis for flood mapping.</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#theory-and-concepts-for-spatio-temporal-fusion-of-remote-sensing-time-series","title":"Theory and concepts for spatio-temporal fusion of remote sensing time series","text":""},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#motivation-and-general-concept","title":"Motivation and general concept","text":"<p>In remote sensing applications there are very often high expectations on monitoring phenomena with challenging characteristics, such as</p> <ul> <li>Small magnitudes of change (compared to the noise in our data or its resolution)</li> <li>Small object size (Can you detect colonization by individual trees in 30-m pixels?)</li> <li>Rapid changes or fluctuations compared to the frequency of observations</li> <li>Slow, gradual changes or trends over multiple decades (which are usually not covered by RSTS from a single system)</li> <li>Similarity of different phenomena in a remotely sensed feature space (e.g., similar spectral change due to a construction site and due to a landslide)</li> </ul> <p>In many cases analysis of RSTS from a single source just does not contain enough information, either in terms of detail (resolution in any domain) or in terms of extent (spatially or temporally, i.e., the area covered is too small or the time series is too short). Given such issues we often get disappointed when we see how even sophisticated RSTS methods really perform, as the results may contain too much spatial, temporal or semantic uncertainty to make any reliable interpretation and to ground our decisions on it. Sometimes only a combination of data from different systems provides the desired results.</p> <p>Therefore, we can try to perform a spatio-temporal fusion of data from different systems (sensors and platforms) that are resolving different phenomena or aspects of them (spatially, spectrally, and temporally). As a broad definition for data fusion we may use the one proposed by Steinberg et al. (1999):</p> <p>\"Data fusion is the process of combining data to refine state estimates and predictions.\"</p> <p>Following Schmitt and Zhu 2016, such a data fusion has typically one of the following two goals:</p> <p>\"To estimate the state of a target or object from multiple sensors, if it is not possible to carry out the estimate from one sensor or data type alone\"</p> <p>\"Improve the estimate of this target state by the exploitation of redundant and complementary information\"</p> <p>That means, the general concept of data fusion is to jointly exploit the information contained in two or more datasets from different acquisition systems in order to produce a result that could not be obtained (at this spatial, temporal or semantic quality) from a single data source. While these aspects are applicable to any data fusion task, fusing RSTS is somewhat special: We must make sure that the datasets are in a common resolution and reference frame in space and time. This will often require a matching and co-registration step and a resampling (Schmitt and Zhu 2016), which is often a challenge on its own. At least in the geospatial domain, data integration is often used synonymously with data fusion.</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#data-fusion-at-different-levels","title":"Data fusion at different levels","text":"<p>In the following, we will look at how this process of spatio-temporal data fusion can look like and which data is typically combined. The topic is quite large, but we try to give a concise overview of popular combinations and typical or innovative approaches to fuse spatio-temporal data. First of all, multi-modal data can be fused at different levels (see e.g. Schmitt and Zhu 2016 or Hu et al. 2021), and fusion approaches are often characterized as low-, mid- or high-level fusion or categorized into these three categories:</p> <ol> <li>Observation-level fusion (a.k.a. signal-level fusion)</li> <li>Feature-level fusion</li> <li>Decision-level fusion</li> </ol> <p>What does this mean? An observation-level (low-level) data fusion is typically performed for remote sensing time series from relatively similar systems (e.g., optical satellite sensors, such as Landsat 7/8/9, Sentinel-2, MODIS, \u2026). A key requirement here is often, that an assimilation is done for the datasets (e.g., rescaling or normalization, a resampling, a spatial transformation to a common reference system and an accurate co-registration, ...; after their \"normal\" pre-processing (e.g., atmospheric correction etc.)). Then, the actual fusion is only a relatively simple combination of the values from multiple datasets (such as spectral reflectance in corresponding bands). The fused data can then be used for further analysis.</p> <p>In a feature-level (mid-level) data fusion, the observations are not directly combined; instead they are used to derive features, such as vegetation indices (from multi-spectral imagery) or geometric/morphometric features (from raster DEMs or 3D point clouds, e.g., slope gradient, roughness, planarity, curvature, eigenvalue-based features, height above ground, etc.). These features are then fused, for example by a machine learning algorithm to predict a category (classification) or a target quantity (regression).</p> <p>In a decision-level (high-level) data fusion, each dataset from a specific system is first used to produce a 'decision' (higher-level information, such as a classification or a modelled target quantity). In the actual fusion step, these decisions are then merged into spatially or temporally more complete product or they are aggregated (summarized) into an ensemble output (e.g. as the median value or the mode of multiple model outputs). Such higher-level fusion approaches are typically applied for datasets from more diverse sensors.</p> <p> </p> <p>Data fusion levels (modified from Schmitt and Zhu 2016).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#fusion-approaches-for-remote-sensing-time-series","title":"Fusion approaches for remote sensing time series","text":"<p>With remote sensing time series (RSTS) data we have (at least) two general options for the fusion step (regardless of the fusion level):</p> <ol> <li>We can either maintain individual data values (observations, features or decisons) from multiple systems and merge them into a denser time series (hopefully with less gaps in space and time), or</li> <li> <p>We aggregate values from multiple systems:</p> <ul> <li>We may use observations (such as spectral band reflectance) or derived higher-level features (such as vegetation indices or morphometric/geometric features from raster DEMs or 3D point clouds (e.g., slope gradient, roughness or height above ground)) as input for a classifier.</li> <li>At decision-level we may take an ensemble from multiple decisions (e.g. majority vote from multiple classifiers or median value from multiple regression models).</li> </ul> </li> </ol> <p>The ideal data fusion level depends on the analysis task and the characteristics of the datasets (also relative to each other).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#popular-data-combinations-for-spatio-temporal-fusion","title":"Popular data combinations for spatio-temporal fusion","text":""},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#data-from-different-multispectral-satellite-systems","title":"Data from different multispectral satellite systems","text":"<p>Image time series from different multispectral satellite systems are typically fused to optimize the spatial and/or temporal resolution, compared to a single source time series. Such RSTS types are among the most frequently combined, e.g.:</p> <ul> <li>Imagery from different Landsats make up an unprecedentedly long and consistent time series, which is used in numerous studies (mostly Landsat 4-9, including earlier ones is a bit more difficult). For detecting forest-cover change Fortin et al. (2020) presented an interesting example of fusing a large and diverse set of data collections (back to Landsat 1) via post-classification data fusion (BULC approach and CART classifiers), including also a modification of the minimum mapping unit.</li> <li>Data from Landsat and Sentinel-2 satellites are similar in many aspects but they still require adjustments and homogenization for optimum fusion results (e.g., Gan et al. 2021). For Landsat 8 OLI and Sentinel-2 MSI data, you can readily use the surface reflectance products of the Harmonized Landsat and Sentinel-2 (HLS) archive at (Landsat-native) 30-m resolution (Claverie et al. 2018, Franch et al. 2019).</li> <li>Sentinel-2 and MODIS, or (similarly) Landsat and MODIS (e.g. Gevaert and Garc\u00eda-Haro 2015, Gao et al. 2017, Moreno-Mart\u00ednez et al. 2020), or Sentinel-2 and Sentinel-3 (Wang and Atkinson 2018) - These pairs are particularly attractive as they combine relatively high spatial and temporal resolutions, and suitable fusion methods are the subject of ongoing research.</li> <li>Planet (CubeSat), Landsat and MODIS data - While the Planet CubeSat system offers unprecedented spatio-temporal observing capacity, the relatively low radiometric quality and cross-sensor inconsistencies challenge its use. Houborg and McCabe 2018 presented a multi-scale machine-learning method to radiometrically rectify multi-date CubeSat data into L8 consistent VNIR data.</li> </ul> <p> </p> <p>Monthly RGB composites (for April 2010) of original Landsat reflectance (top) and of the smoothed and gap filled reflectance estimates by the HIghly Scalable Temporal Adaptive Reflectance Fusion Model (HISTARFM) algorithm (bottom). HISTARFM combines multispectral images of different sensors (Landsat and MODIS) to reduce noise and produce monthly gap free high resolution (30\u202fm) observations (figure by Moreno-Mart\u00ednez et al. 2020 / CC BY-NC-ND 4.0).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#data-from-active-and-passive-sensing-techniques","title":"Data from active and passive sensing techniques","text":"<p>A major limitation of passively sensed imagery (e.g., from Sentinel-2) is that the objects of interest are frequently obstructed by clouds, resulting in many gaps in a time series. Actively sensed data (such as Sentinel-1 data acquired with synthetic aperture radar (SAR)) can reduce this problem and a fusion of optical and SAR data might provide you with a substantively improved temporal resolution and completeness. See the E-TRAINEE tutorial below, where you learn how to implement such an approach for monitoring the extent of a water surface.</p> <p>Other approaches using such a combination of optical and SAR data have been presented in the context of agricultural monitoring, e.g., for grassland mowing detection (De Vroey et al. 2022) and for crop types and crop sequences monitoring (Blickensd\u00f6rfer et al. 2022). </p> <p> </p> <p>Example for a crop type monitoring workflow based on optical and SAR time series in combination with variables describing local and seasonal environmental conditions (figure by Blickensd\u00f6rfer et al. 2022 / CC BY 4.0).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#3d-point-clouds-from-different-systems","title":"3D Point clouds from different systems","text":"<p>3D point clouds aacquired with different platforms and sensors may be combined to provide a more complete representation of a three-dimensional scene. This can be especially attractive if complex terrain, vegetation and unfavourable viewing angles degrade the geometric quality or leave gaps in a single source point cloud. Given a sufficiently good co-registration of multiple point clouds, a simple way to fuse them would be to merge them and keep the sum of all points. Other methods, however, might better combine the strengths of multiple systems.</p> <p>In this context, Zieher et al. (2018) presented an approach for fusing point clouds from terrestrial laser scanning (TLS), uncrewed aerial vehicle (UAV) photogrammetry and terrestrial photogrammetry (TerrPh), respectively, thereby combining the advantages of the different measurement principles for landslide monitoring. At the core of this approach is a rule-based selection of points that are kept, depending on local morphometric features computed from eigenvectors, elevation and the agreement of the individual point clouds. The rules tested in the study were:</p> <ol> <li>\u2019Maximum geometric curvature\u2019: data of the sensor with significantly higher geometric curvature are kept</li> <li>\u2019Maximum planarity\u2019: data of the sensor with significantly higher planarity are kept</li> <li>\u2019Minimum elevation\u2019: data of the sensor located significantly below the other sensors\u2019 data are kept</li> <li>\u2019Sensor correspondence\u2019: only data within voxels are kept where two or more sensors are represented by at least 20 points</li> </ol> <p>It was found that, depending on the subsequent analysis (e.g., extraction of morphological features such as edges or derivation of displacement vectors vs. topographic change detection) and the data quality requirements (outlier and error filtering vs. completeness of the scene), different rules are suitable for point cloud fusion. Details on these rules, their rationale, and the advantages and disadvantages found can be read in Zieher et al. (2018). The suggested workflow was applied for two years and a subsequent bi-temporal displacement vector analysis but it could also be rolled out to more observations and, thereby, offers a systematic and reproducible method to conduct a monitoring over multiple years or at a higher frequency.</p> <p> </p> <p>Left: Data acquisition principles of the considered platforms/sensors including UAV (a), TerrPh (b) and TLS (c). Right: Rule-based point cloud fusion workflow (figures by Zieher et al. 2018/ CC BY 4.0).</p> <p> </p> <p>Left: Planimetric view of the point cloud fusion results, with color-coded decisions for the applied fusion rules \u2019maximum geometric curvature\u2019 (a), \u2019maximum planarity\u2019 (b), \u2019minimum elevation\u2019 (c), and \u2019sensor correspondence\u2019 (d). Right: Oblique view of a subset of the point cloud fusion results (figures by Zieher et al. 2018/ CC BY 4.0).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#other-combinations","title":"Other combinations","text":"<p>In addition to the fusion approaches outlined above, there are plenty of studies where remote sensing time series are fused with other geospatial data (not remotely sensed or higher-level information from interpreted remote sensing data). Such datasets can be raster or vector data, including digital elevation models, climatological or meteorological data (e.g. Stumpf et al. 2018 or Blickensd\u00f6rfer et al. 2022), or cadastral data and building footprints (e.g. Uhl and Leyk 2020). Such auxilliary datasets can be very helpful in combination with remote sensing time series, also if they are not multi-temporal.</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"1) The purpose of data fusion is typically ...  to reduce the data volume by condensing the information content. to highlight differences in the results obtained with different platforms, sensors, and analysis methods. to combine data from different sources and thereby reveal information that would not be clearly visible in single-source data. to combine data from different sources and thereby reveal information that would not be clearly visible in single-source data.  2) Data fusion can be performed at different levels, namely ...  observation level signal level feature level decision level observation level&amp;signal level&amp;feature level&amp;decision level  3) A fusion of remote sensing time series ...  may be used to improve the spatial and temporal resolution of an analysis. is sometimes needed for long-term monitoring tasks where the observation period must be extended beyond the operating time of a single system. is a typical strategy for streamlining the preprocessing steps. may be used to improve the spatial and temporal resolution of an analysis.&amp;is sometimes needed for long-term monitoring tasks where the observation period must be extended beyond the operating time of a single system."},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#tutorial-sentinel-1-2-surface-water-monitoring","title":"Tutorial: Sentinel-1/-2 surface water monitoring","text":"<p>If you are finished with the theoretical part of this theme, you are ready to try a data fusion approach to monitoring the extent of a water surface.</p> <p>Case study</p> <p>With the Grand Ethiopian Renaissance Dam (GERD) being constructed on the Blue Nile river in Ethiopia, filling of a large water reservoir for hydroelectric power production started in the year 2020. We will try to observe the surface water extent in the GERD reservoir area during the first filling phase in 2020. As the filling makes quite rapid progress during the rainy season (when clouds tend to obstruct the view for optical satellite sensors), we want to combine data from Sentinel-1 and -2 to keep track of the lake extent dynamics.</p> <p>Approach</p> <p>On the one hand, we will get a time series of Normalized Difference Water Index (NDWI) computed from Sentinel-2 optical bands in the Google Earth Engine (GEE). On the other hand, we will use a time series of Sentinel-1 Synthetic Aperture Radar (SAR) data from the GEE, more specifically this is C-band SAR Ground Range Detected (GRD) backscatter intensity. Learn more about SAR characteristics and Sentinel-1 (in general and in the GEE) here and here.</p> <p>We will not directly fuse data from S-1 and S-2, instead we first apply a simple classification of water and non-water and then fuse the classified water time series. As water contrasts well with other surface types in both S-1 and S-2 features and the changes taking place are large, we can expect such a relatively simple approach to work reasonably well (but let's see ...). The goal is to get something like the figure below that informs us about the progress of reservoir filling with time.</p> <p>Tutorial Notebook</p> <p>You can find an interactive tutorial where you can try the approach in this Notebook. There, you can learn how to achieve the following outputs:</p> <p> </p> <p>Percentage of area covered by water based on data from Sentinel-1 and -2.</p> <p> </p> <p>Water surface classification based on Sentinel-2 and -1, respectively, needs different (user-defined) classification thresholds, here shown for one scene each.</p> <p> </p> <p>Fused water surface time series from the S-2/-1 classification with water pixels shown in blue and non-water pixels shown in light brown. The white NoData pixels results from (masked) clouds in S-2 imagery.</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#excercise-flood-mapping-with-sentinel-1-and-sentinel-2-data","title":"Excercise: Flood mapping with Sentinel-1 and Sentinel-2 data","text":"<p>The Sentinel-1/-2 surface water monitoring approach presented in the Notebook above cannot only be used for monitoring the progress of resevoir filling, it could also be used for mapping the inundated areas in a river flood event. This might be important for authorities, humanitarian aid organizations etc. who need rapid estimates of the affected areas and the damage. Do a short internet search on possible use cases for flood mapping. Maybe a major flooding event has been reported on the media recently? Select an event according to your interest and choose area and time period accordingly (a few weeks should be enough).</p> <ul> <li>Try to replicate the approach from the tutorial Notebook for this use case (with the small adaptations, e.g. it probably makes sense to download data at higher spatial resolution but for a shorter time period).</li> <li>Briefly discuss the data fusion: Did it work well? Any obvious problems? Does the fusion of S-1 and S-2 data improve the analysis compared to using a single data set?</li> <li>Extract only the areas flooded by the event, beyond the \"normal\" river area. (Hint: Use a pre-flood water map as a baseline and intersect this with a peak flood water map.)</li> <li>Make a qualitative assessment of the inundation (describe regarding space and time) and its likely consequences (settlements, infrastructure and agricultural area affected?). (Hint: For such an interpretation you will need at least some background layer ...)</li> <li>Beyond this practical excercise, think about other geospatial data: What else could be combined with the satellite data and what kind of analyses could be useful then? Briefly discuss this! </li> </ul>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#more-practical-material","title":"More practical material","text":"<p>Another interesting excercise is provided in this tutorial by the Worldbank. It presents a data fusion approach for assessing change in built-up land cover in Nepal, involving</p> <ul> <li>Nighttime lights (monthly composites) data from the Visible Infrared Imaging Radiometer Suite (VIIRS) Day-Night Band (DNB)</li> <li>Sentinel-2 daytime visible band images (monthly composites)</li> <li>Global Human Settlement Layer for training data</li> </ul>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#further-reading","title":"Further reading","text":"<p>To learn more about recent developments and trends in spatio-temporal data fusion the following articles are recommended:</p> <p>Ghamisi, P., Rasti, B., Yokoya, N., Wang, Q., Hofle, B., Bruzzone, L., ... &amp; Benediktsson, J. A. (2019). Multisource and multitemporal data fusion in remote sensing: A comprehensive review of the state of the art. IEEE Geoscience and Remote Sensing Magazine, 7(1), 6-39. https://doi.org/10.1109/MGRS.2018.2890023</p> <p>Karagiannopoulou, A., Tsertou, A., Tsimiklis, G., &amp; Amditis, A. (2022). Data fusion in earth observation and the role of citizen as a sensor: A scoping review of applications, methods and future trends. Remote Sensing, 14(5), 1263. https://doi.org/10.3390/rs14051263</p> <p>Li, J., Hong, D., Gao, L., Yao, J., Zheng, K., Zhang, B., &amp; Chanussot, J. (2022). Deep learning in multimodal remote sensing data fusion: A comprehensive review. International Journal of Applied Earth Observation and Geoinformation, 112, 102926. https://doi.org/10.1016/j.jag.2022.102926</p> <p> </p> <p>Data Fusion in Earth Observation and Citicen Science (figure by Karagiannopoulou et al. 2022 / CC BY 4.0).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/05_spatio-temporal_data_fusion.html#references-cited-above","title":"References (cited above)","text":"<p>Blickensd\u00f6rfer, L., Schwieder, M., Pflugmacher, D., Nendel, C., Erasmi, S., &amp; Hostert, P. (2022). Mapping of crop types and crop sequences with combined time series of Sentinel-1, Sentinel-2 and Landsat 8 data for Germany. Remote Sensing of Environment, 269, 112831. https://doi.org/10.1016/j.rse.2021.112831</p> <p>Franch, B., Vermote, E., Skakun, S., Roger, J. C., Masek, J., Ju, J., ... &amp; Santamaria-Artigas, A. (2019). A method for Landsat and Sentinel 2 (HLS) BRDF normalization. Remote Sensing, 11(6), 632. https://doi.org/10.3390/rs11060632</p> <p>De Vroey, M., de Vendictis, L., Zavagli, M., Bontemps, S., Heymans, D., Radoux, J., ... &amp; Defourny, P. (2022). Mowing detection using Sentinel-1 and Sentinel-2 time series for large scale grassland monitoring. Remote Sensing of Environment, 280, 113145. https://doi.org/10.1016/j.rse.2022.113145</p> <p>Gan, W., Albanwan, H., &amp; Qin, R. (2021). Radiometric normalization of multitemporal Landsat and Sentinel-2 images using a reference MODIS product through spatiotemporal filtering. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 14, 4000-4013. https://doi.org/10.1109/JSTARS.2021.3069855</p> <p>Li, J., Hong, D., Gao, L., Yao, J., Zheng, K., Zhang, B., &amp; Chanussot, J. (2022). Deep learning in multimodal remote sensing data fusion: A comprehensive review. International Journal of Applied Earth Observation and Geoinformation, 112, 102926. https://doi.org/10.1016/j.jag.2022.102926</p> <p>Moreno-Mart\u00ednez, \u00c1., Izquierdo-Verdiguier, E., Maneta, M. P., Camps-Valls, G., Robinson, N., Mu\u00f1oz-Mar\u00ed, J., ... &amp; Running, S. W. (2020). Multispectral high resolution sensor fusion for smoothing and gap-filling in the cloud. Remote Sensing of Environment, 247, 111901. https://doi.org/10.1016/j.rse.2020.111901</p> <p>Schmitt, M., &amp; Zhu, X. X. (2016). Data fusion and remote sensing: An ever-growing relationship. IEEE Geoscience and Remote Sensing Magazine, 4(4), 6-23. https://doi.org/10.1109/MGRS.2016.2561021</p> <p>Steinberg, A. N., Bowman, C. L., &amp; White, F. E. (1999). Revisions to the JDL data fusion model. In Sensor fusion: Architectures, algorithms, and applications III (Vol. 3719, pp. 430-441). SPIE. https://doi.org/10.1117/12.341367, https://apps.dtic.mil/sti/pdfs/ADA391479.pdf</p> <p>Uhl, J. H., &amp; Leyk, S. (2020). Towards a novel backdating strategy for creating built-up land time series data using contemporary spatial constraints. Remote Sensing of Environment, 238, 111197. https://doi.org/10.1016/j.rse.2019.05.016</p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html","title":"T5 water surface data fusion","text":"Metadata     title: \"E-TRAINEE Tutorial - Sentinel-1/-2 surface water monitoring\"     description: \"This is a tutorial within the fifth theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-09-12     authors: Andreas Mayr <p>Import packages, then authenticate and initialize Google Earth Engine.</p> In\u00a0[3]: Copied! <pre>import ee, eemont, wxee, geemap\n</pre> import ee, eemont, wxee, geemap In\u00a0[4]: Copied! <pre>try:\n        wxee.Initialize()\nexcept Exception as e:          # If initialize does not work, you probably have to authenticate first\n        ee.Authenticate()\n        wxee.Initialize()\n</pre> try:         wxee.Initialize() except Exception as e:          # If initialize does not work, you probably have to authenticate first         ee.Authenticate()         wxee.Initialize() In\u00a0[5]: Copied! <pre>bbox = ee.Geometry.BBox(35.023041,10.756414, 35.430908,11.263265)\npoi = ee.Geometry.Point(35.226974,11.009612)\n</pre> bbox = ee.Geometry.BBox(35.023041,10.756414, 35.430908,11.263265) poi = ee.Geometry.Point(35.226974,11.009612) <p>See the area on a map. To speed things up, we do not cover the entire reservoir area - feel free to change this.</p> In\u00a0[6]: Copied! <pre>bbox_gdf = geemap.ee_to_geopandas(ee.FeatureCollection(bbox)).set_crs(crs='EPSG:4326')\nbbox_gdf.explore()\n</pre> bbox_gdf = geemap.ee_to_geopandas(ee.FeatureCollection(bbox)).set_crs(crs='EPSG:4326') bbox_gdf.explore() Out[6]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[7]: Copied! <pre>S2 = (ee.ImageCollection(\"COPERNICUS/S2_SR\")\n      .filterBounds(poi)      # use the point-of-interest in the center of our AOI here\n      .filterDate(\"2020-05-01\",\"2020-10-31\")\n      .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30)\n      .preprocess().spectralIndices(\"NDWI\").select(\"NDWI\"))\nprint(len(S2))                # number of images found\n</pre> S2 = (ee.ImageCollection(\"COPERNICUS/S2_SR\")       .filterBounds(poi)      # use the point-of-interest in the center of our AOI here       .filterDate(\"2020-05-01\",\"2020-10-31\")       .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", 30)       .preprocess().spectralIndices(\"NDWI\").select(\"NDWI\")) print(len(S2))                # number of images found <pre>14\n</pre> <p>Convert GEE ImageCollection to wxee TimeSeries and have a look at the temporal distribution of the available images matching our search criteria (max. cloud cover etc.).</p> In\u00a0[8]: Copied! <pre>S2_ts = S2.wx.to_time_series()\nS2_ts.timeline()\n</pre> S2_ts = S2.wx.to_time_series() S2_ts.timeline() <p>Download the wxee TimeSeries (load into memory as an xarray Dataset). We set the <code>scale</code> parameter to get a cell size of 100 m.</p> In\u00a0[\u00a0]: Copied! <pre>S2_ds = S2_ts.wx.to_xarray(region=bbox, scale=100)\n</pre> S2_ds = S2_ts.wx.to_xarray(region=bbox, scale=100) <p>Show time series of NDWI maps.</p> In\u00a0[10]: Copied! <pre>from matplotlib import colormaps\n\ncustom_cmap = colormaps.get_cmap('PuOr')    # Let's modify an existing colormap\ncustom_cmap.set_bad(color='black')          # Show NaNs in black\nS2_ds.NDWI.plot(col='time', col_wrap=7, cmap=custom_cmap)\n</pre> from matplotlib import colormaps  custom_cmap = colormaps.get_cmap('PuOr')    # Let's modify an existing colormap custom_cmap.set_bad(color='black')          # Show NaNs in black S2_ds.NDWI.plot(col='time', col_wrap=7, cmap=custom_cmap) Out[10]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16d6a57d5d0&gt;</pre> <p>With water surfaces contrasting quite well with surrounding land surfaces, we can see a lake having emerged within the investigated time period. However, looking at the timeline plotted above the temporal resolution of our S-2 time series is quite heterogeneous (we have excluded imagery with 30% cloudy pixels or more). The largest gap is between 23 June and 12 August. In the time series of NDWI maps we see that filling of the reservoir mostly took place during this period (apparently the rainy season). Further data gaps in the downloaded imagery are shown in black, most likely as a result of cloud masking (and possibly a tile border in the very south).</p> In\u00a0[11]: Copied! <pre>S1 = (ee.ImageCollection(\"COPERNICUS/S1_GRD\")\n      .filterBounds(poi)\n      .filterDate(\"2020-05-01\",\"2020-10-31\")\n      .select(\"VV\"))\nprint(len(S1))                # number of images found\n</pre> S1 = (ee.ImageCollection(\"COPERNICUS/S1_GRD\")       .filterBounds(poi)       .filterDate(\"2020-05-01\",\"2020-10-31\")       .select(\"VV\")) print(len(S1))                # number of images found <pre>14\n</pre> <p>Convert GEE ImageCollection to wxee TimeSeries. The temporal distribution of the available images is more homogeneous as we don't have the cloud cover restriction.</p> In\u00a0[12]: Copied! <pre>S1_ts = S1.wx.to_time_series()\nS1_ts.timeline()\n</pre> S1_ts = S1.wx.to_time_series() S1_ts.timeline() <p>Download the wxee TimeSeries (load into memory as an xarray Dataset). We set the <code>scale</code> parameter to get a cell size of 100 m.</p> In\u00a0[13]: Copied! <pre>S1_ds = S1_ts.wx.to_xarray(region=bbox, scale=100)\n</pre> S1_ds = S1_ts.wx.to_xarray(region=bbox, scale=100) <pre>Requesting data:   0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> <pre>Downloading data:   0%|          | 0/14 [00:00&lt;?, ?it/s]</pre> <p>Show the VV polarization band backscatter intensity time series (unit is decibels).</p> In\u00a0[14]: Copied! <pre>S1_ds.VV.plot(col='time', col_wrap=7, robust=True)\n</pre> S1_ds.VV.plot(col='time', col_wrap=7, robust=True) Out[14]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16d06533280&gt;</pre> In\u00a0[15]: Copied! <pre>from matplotlib import pyplot as plt\n\nfig, axes = plt.subplots(1,4, figsize=(12,2.5))\n\nS2_ds.NDWI.sel(time='2020-09-06').plot(cmap=custom_cmap, ax=axes[0])\naxes[0].set_title('S-2 (2020-09-06)')\n\nS2_ds.NDWI.sel(time='2020-09-06').plot.hist(bins=100, ax=axes[1]);\naxes[1].axvline(-0.15, color='grey', linestyle='dashed', linewidth=1)\naxes[1].set_title('S-2 (2020-09-06)')\n\nS1_ds.VV.sel(time='2020-09-07').plot(robust=True, ax=axes[2])\naxes[2].set_title('S-1 (2020-09-07)')\n\nS1_ds.VV.sel(time='2020-09-07').plot.hist(bins=100, ax=axes[3]);\naxes[3].axvline(-17.0, color='grey', linestyle='dashed', linewidth=1)\naxes[3].set_title('S-1 (2020-09-07)');\n\nfig.tight_layout()\n</pre> from matplotlib import pyplot as plt  fig, axes = plt.subplots(1,4, figsize=(12,2.5))  S2_ds.NDWI.sel(time='2020-09-06').plot(cmap=custom_cmap, ax=axes[0]) axes[0].set_title('S-2 (2020-09-06)')  S2_ds.NDWI.sel(time='2020-09-06').plot.hist(bins=100, ax=axes[1]); axes[1].axvline(-0.15, color='grey', linestyle='dashed', linewidth=1) axes[1].set_title('S-2 (2020-09-06)')  S1_ds.VV.sel(time='2020-09-07').plot(robust=True, ax=axes[2]) axes[2].set_title('S-1 (2020-09-07)')  S1_ds.VV.sel(time='2020-09-07').plot.hist(bins=100, ax=axes[3]); axes[3].axvline(-17.0, color='grey', linestyle='dashed', linewidth=1) axes[3].set_title('S-1 (2020-09-07)');  fig.tight_layout() <p>Classify water in NDWI time series with the chosen threshold.</p> In\u00a0[16]: Copied! <pre>import numpy as np\nimport xarray as xr\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\n# In a new DataArray, set all pixels to 1 if their NDWI &gt;= -0.15, keep the NDWI value for all other pixels\nS2_water = xr.where(S2_ds.NDWI &gt;= -0.15, 1, S2_ds.NDWI)\n\n# Set all pixels to 1 if their NDWI &lt; -0.15 and if they are not NaN, keep values otherwise\nS2_water = xr.where((S2_water &lt; -0.15) &amp; (np.isnan(S2_water)==False), 0, S2_water)\n\nS2_water.attrs = S2_ds.attrs                # Copy the attributes of the S2 Dataset (CRS, spatial res., ...)\n\n# Let' plot\ncmap = ListedColormap(['peru', 'blue'])\nS2_water.plot(col='time', col_wrap=7, cbar_kwargs={'label': 'Water surface (Sentinel-2)', 'ticks': [0, 1]}, cmap=cmap)\n</pre> import numpy as np import xarray as xr from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt  # In a new DataArray, set all pixels to 1 if their NDWI &gt;= -0.15, keep the NDWI value for all other pixels S2_water = xr.where(S2_ds.NDWI &gt;= -0.15, 1, S2_ds.NDWI)  # Set all pixels to 1 if their NDWI &lt; -0.15 and if they are not NaN, keep values otherwise S2_water = xr.where((S2_water &lt; -0.15) &amp; (np.isnan(S2_water)==False), 0, S2_water)  S2_water.attrs = S2_ds.attrs                # Copy the attributes of the S2 Dataset (CRS, spatial res., ...)  # Let' plot cmap = ListedColormap(['peru', 'blue']) S2_water.plot(col='time', col_wrap=7, cbar_kwargs={'label': 'Water surface (Sentinel-2)', 'ticks': [0, 1]}, cmap=cmap) Out[16]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16d06333730&gt;</pre> <p>Classify water in VV time series with the chosen threshold.</p> In\u00a0[17]: Copied! <pre># In a new DataArray, set all pixels to 0 if their VV &gt;= -17.0, keep the VV value for all other pixels\nS1_water = xr.where(S1_ds.VV &gt;= -17.0, 0, S1_ds.VV)\n\n# Set all pixels to 1 if their VV &lt; -17.0 and if they are not NaN, keep values otherwise\nS1_water = xr.where((S1_water &lt; -17.0) &amp; (np.isnan(S1_water)==False), 1, S1_water)\n\nS1_water.attrs = S1_ds.attrs                # Copy the attributes of the S2 Dataset (CRS, spatial res., ...)\n\n# Let' plot\ncmap = ListedColormap(['peru', 'blue'])\nS1_water.plot(col='time', col_wrap=7, cbar_kwargs={'label': 'Water surface (Sentinel-1)', 'ticks': [0, 1]}, cmap=cmap)\n</pre> # In a new DataArray, set all pixels to 0 if their VV &gt;= -17.0, keep the VV value for all other pixels S1_water = xr.where(S1_ds.VV &gt;= -17.0, 0, S1_ds.VV)  # Set all pixels to 1 if their VV &lt; -17.0 and if they are not NaN, keep values otherwise S1_water = xr.where((S1_water &lt; -17.0) &amp; (np.isnan(S1_water)==False), 1, S1_water)  S1_water.attrs = S1_ds.attrs                # Copy the attributes of the S2 Dataset (CRS, spatial res., ...)  # Let' plot cmap = ListedColormap(['peru', 'blue']) S1_water.plot(col='time', col_wrap=7, cbar_kwargs={'label': 'Water surface (Sentinel-1)', 'ticks': [0, 1]}, cmap=cmap) Out[17]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16d1b47d330&gt;</pre> In\u00a0[18]: Copied! <pre>S_water = xr.concat([S2_water, S1_water], dim='time').sortby('time')\n\ncmap = ListedColormap(['peru', 'blue'])\nS_water.plot(col='time', col_wrap=10, cbar_kwargs={'label': 'Water surface (Sentinel-1 &amp; -2)', 'ticks': [0, 1]}, cmap=cmap)\n</pre> S_water = xr.concat([S2_water, S1_water], dim='time').sortby('time')  cmap = ListedColormap(['peru', 'blue']) S_water.plot(col='time', col_wrap=10, cbar_kwargs={'label': 'Water surface (Sentinel-1 &amp; -2)', 'ticks': [0, 1]}, cmap=cmap) Out[18]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16d1eb12980&gt;</pre> In\u00a0[19]: Copied! <pre>S_water_filled = S_water.interpolate_na(dim='time', method='nearest')   # fill with nearest neighbour\nS_water_filled = S_water_filled.ffill(dim='time')                       # forward fill (requires the bottleneck package to be installed)\nS_water_filled = S_water_filled.bfill(dim='time')                       # backward fill\nS_water_filled.plot(col='time', col_wrap=10, cbar_kwargs={'label': 'Water surface (Sentinel-1 &amp; -2)', 'ticks': [0, 1]}, cmap=cmap)\n</pre> S_water_filled = S_water.interpolate_na(dim='time', method='nearest')   # fill with nearest neighbour S_water_filled = S_water_filled.ffill(dim='time')                       # forward fill (requires the bottleneck package to be installed) S_water_filled = S_water_filled.bfill(dim='time')                       # backward fill S_water_filled.plot(col='time', col_wrap=10, cbar_kwargs={'label': 'Water surface (Sentinel-1 &amp; -2)', 'ticks': [0, 1]}, cmap=cmap) Out[19]: <pre>&lt;xarray.plot.facetgrid.FacetGrid at 0x16d2c2863b0&gt;</pre> <p>For comparison, add surface water extent derived from S-2 and S-1 alone. First, fill missing values.</p> In\u00a0[20]: Copied! <pre># S-1\nS1_water_filled = S1_water.interpolate_na(dim='time', method='nearest')\nS1_water_filled = S1_water_filled.ffill(dim='time')\nS1_water_filled = S1_water_filled.bfill(dim='time')\n\n# S-2\nS2_water_filled = S2_water.interpolate_na(dim='time', method='nearest')\nS2_water_filled = S2_water_filled.ffill(dim='time')\nS2_water_filled = S2_water_filled.bfill(dim='time')\n</pre> # S-1 S1_water_filled = S1_water.interpolate_na(dim='time', method='nearest') S1_water_filled = S1_water_filled.ffill(dim='time') S1_water_filled = S1_water_filled.bfill(dim='time')  # S-2 S2_water_filled = S2_water.interpolate_na(dim='time', method='nearest') S2_water_filled = S2_water_filled.ffill(dim='time') S2_water_filled = S2_water_filled.bfill(dim='time') <p>Calculate how much of the scene (percentage) is covered by a water surface and plot this.</p> In\u00a0[21]: Copied! <pre>water_percentage = np.nanmean(S_water_filled.values, axis=(1,2))*100\nS2_water_percentage = np.nanmean(S2_water_filled.values, axis=(1,2))*100\nS1_water_percentage = np.nanmean(S1_water_filled.values, axis=(1,2))*100\n\nplt.figure(figsize=(5,3))\nplt.plot(S_water_filled.time, water_percentage, label='S-1 and S-2', color='grey')\nplt.scatter(S1_water_filled.time, S1_water_percentage, color='blue', alpha=0.4, label='S-1')\nplt.scatter(S2_water_filled.time, S2_water_percentage, color='red', alpha=0.4, label='S-2')\nplt.legend()\nplt.title(\"Percentage of area covered by water\\nbased on data from Sentinel-1 and -2\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Water surface extent [%]\")\n</pre> water_percentage = np.nanmean(S_water_filled.values, axis=(1,2))*100 S2_water_percentage = np.nanmean(S2_water_filled.values, axis=(1,2))*100 S1_water_percentage = np.nanmean(S1_water_filled.values, axis=(1,2))*100  plt.figure(figsize=(5,3)) plt.plot(S_water_filled.time, water_percentage, label='S-1 and S-2', color='grey') plt.scatter(S1_water_filled.time, S1_water_percentage, color='blue', alpha=0.4, label='S-1') plt.scatter(S2_water_filled.time, S2_water_percentage, color='red', alpha=0.4, label='S-2') plt.legend() plt.title(\"Percentage of area covered by water\\nbased on data from Sentinel-1 and -2\") plt.xticks(rotation=45) plt.ylabel(\"Water surface extent [%]\") Out[21]: <pre>Text(0, 0.5, 'Water surface extent [%]')</pre>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#sentinel-1-2-surface-water-monitoring","title":"Sentinel-1/-2 surface water monitoring\u00b6","text":"<p>One of the key requirements for monitoring Earth surface processes by remote sensing is an adequate temporal resolution. In satellite remote sensing this is often limited by the satellites' revisit interval and (for optical imagery) by cloud cover. This Notebook shows how to combine data from two different sensor constellations, namely Sentinel-1 (S-1) and Sentinel-2 (S-2), for monitoring the spatial extent of surface water over a few months.</p> <p>Case study</p> <p>With the Grand Ethiopian Renaissance Dam (GERD) being constructed on the Blue Nile river in Ethiopia, filling of a large water reservoir for hydroelectric power production started in the year 2020. We will try to observe the surface water extent in the GERD reservoir area during the first filling phase in 2020. As the filling makes quite rapid progress during the rainy season (when clouds tend to obstruct the view for optical satellite sensors), we want to combine data from Sentinel-1 and -2 to keep track of the lake extent dynamics.</p> <p>Approach</p> <p>On the one hand, we will get a time series of Normalized Difference Water Index (NDWI) computed from Sentinel-2 optical bands in the Google Earth Engine (GEE). On the other hand, we will use a time series of Sentinel-1 Synthetic Aperture Radar (SAR) data from the GEE, more specifically this is C-band SAR Ground Range Detected (GRD) backscatter intensity. Learn more about SAR characteristics and Sentinel-1 (in general and in the GEE) here and here.</p> <p>Importantly, NDWI and SAR backscatter intensity are two very different quantities, so the datasets cannot easily be combined as they are. Hence, we will not directly fuse data from S-2 and S-1 and we will not try any normalization or similar (as it is, e.g., done to harmonize Sentinel-2 and Landsat spectral bands), instead we first apply a simple classification of water and non-water and then fuse the classified water time series. As water contrasts well with other surface types in both S-2 and S-1 features and the changes taking place are large, we can expect such a relatively simple approach to work reasonably well (but let's see ...).</p> <p>Software environment</p> <p>For GEE and <code>xarray</code> based raster time series processing we use the packages geemap, wxee and eemont, which are contained in the requirements file provided for the course. Please see the instructions on the software page for setting up a Conda environment based on this file.</p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#area-of-interest","title":"Area-of-interest\u00b6","text":"<p>Define a bounding box with our area-of-interest (AOI) and a point (the center of the bounding box; both obtained from bboxfinder).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#data-search-and-preprocessing","title":"Data search and preprocessing\u00b6","text":""},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#sentinel-2","title":"Sentinel-2\u00b6","text":"<p>First, we query Sentinel-2 optical satellite imagery for the time period and area we are interested in, conditioned by cloudy pixel percentage. We mask clouds and scale the reflectance values (using eemont's convenient <code>preprocess()</code> method), compute the Normalized Difference Water Index (NDWI; using Awesome Spectral Indices backing eemont), and select the NDWI as a band for download and further analysis.</p> <p>We use the NDWI for open water features (McFeeters 1996), not to be confused with the NDWI for vegetation water content estimation by Gao (1996)00067-3). Alternatively you might try, e.g., the Sentinel-2 Water Index (SWI) by Jiang et al. (2021).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#sentinel-1","title":"Sentinel-1\u00b6","text":"<p>To overcome the apparent limitations of the optical image time series (but still use its information content), let's go for a data fusion approach including Sentinel-1 data, which is known to be much more robust towards clouds. We search and request Sentinel-1 C-band SAR data (Ground Range Detected, log-scaled), mostly similar to the procedure we used for S2. For simplicity, we select only the VV band (i.e., single co-polarization, vertical transmit/vertical receive).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#water-surface-classification","title":"Water surface classification\u00b6","text":"<p>According to the map time series plots of NDWI and VV, both these features seem to be well suitable to discriminate surface water from surrounding land surface. Therefore, let's try a very simple classification approach, where we select a threshold for each of the two features and use this to reclassify pixels into 'water' and 'non-water'.</p> <p>Plot the histograms for one S-2 scene and one S-1 scene where the lake is well visible. The thresholds we use in the next step are shown as dashed lines in the histograms. Of course, these values can be debated or optimized (by try-and-error or by data-driven approaches, such as clustering) but for now we just go on with them and reclassify the raster time series in the next step.</p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#combine-the-two-water-surface-time-series","title":"Combine the two water surface time series\u00b6","text":"<p>Finally, we can combine the two two time series (post-classification data fusion) by concatenating them along the time dimension and sorting the new DataArray by time. An important prerequisite for this is that they \"fit\" in space and in time, i.e. in terms of CRS, grid system (bounds and cell size), and temporal reference system (calendar, time zone etc.). Moreover, the classification into common semantic target categories (water/no-water) is advantageous for most of the further analyses. </p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#development-of-water-surface-extent-over-time","title":"Development of water surface extent over time\u00b6","text":"<p>Now let's use this fused time series to observe the development of water surface extent. First, we eliminate NaN pixels by a sequence of gap filling operations along the time dimension: fill with nearest (temporal) neighbour, forward fill, then backward fill (Tipp: plot the time series after each step to see why multiple steps are applied).</p>"},{"location":"module1/05_spatio_temporal_data_fusion/T5_water_surface_data_fusion.html#conclusion","title":"Conclusion\u00b6","text":"<p>In our small case study, the (actual, i.e. usable) temporal resolution of S-2 is severely downgraded by cloud cover, especially in the time period most critical for our analysis. The temporal resolution of S-1 is much more robust in this respect (thanks to the active sensing principle and radar penetrating the clouds) but still benefits from an addition of other data. Hence, we see how the temporal resolution of a remotely sensed time series can be strongly enhanced by a fusion of data from two different sensors.</p> <p>In this case, a data fusion at a lower level (such as harmonization of Landsat and Sentinel-2 optical bands or indices) would have been difficult as the sensors used here measure completely different physical quantities and, thus, cannot directly be put together. A classification of each of the two time series prior to fusing them seemed more appropriate and gave reasonable results, although the classification results might be optimized to provide a more harmonic fused time series (look at the plot above where S-1 and S-2 based water extents seem to be (systematically) diverging).</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html","title":"Theme 6 - Reference data, validation and accuracy assessment","text":""},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#introduction","title":"Introduction","text":"<p>As we have seen in the previous themes of this module, remote sensing time series analysis is a powerful tool for monitoring changes in the environment over time. When we ground our interpretations and decisions in such an analysis, however, it is essential to validate the analysis results and ensure their reliability. In this theme we will have a look at how this can be done and what data we might use as a reference.</p> <p>Objectives</p> <p>The objective of this theme is to understand important principles of validation in remote sensing analyses. You will get an idea of potential reference data sources and learn which metrics are appropriate for assessing the accuracy of regressions, classifications and change detection. A particular focus is on critical issues in acquiring and using reference data for time series analysis in remote sensing. Finally, you will gain practical experience with assessing the accuracy of a classification in an excercise.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#principles-of-validation-and-accuracy-assessment-in-remote-sensing","title":"Principles of validation and accuracy assessment in remote sensing","text":""},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#aims-of-validation-and-accuracy-assessment","title":"Aims of validation and accuracy assessment","text":"<p>A rigorous validation and accuracy assessment should be a key element of any remote sensing analysis as it serves to</p> <ul> <li>Judge the performance of a method: We want to know how good it works, i.e. how accurate it is.</li> <li>Learn in which situations a method tends to deliver reliable results and in which situations uncertainty remains high (e.g. by assessing the spatial or temporal distribution of errors or uncertainty).</li> <li>Identify strengths and weaknesses of a method for a certain application (e.g. ability to predict a certain class, effects of data acquisition scenarios and error propagation).</li> <li>Help with improving the method and/or the input data it uses.</li> <li>Make it possible to compare different methods subjectively and by scientific standards (thereby helping ourselves or others to choose the most suitable method for applied use).</li> <li>Evaluate a method's potential for transferability and scalability (to some degree).</li> <li>Provide you and others with confidence (or scepticism) for any interpretation, conclusions and decisions based on the data analysis. An accuracy assessment is needed to judge if and how far such further steps make sense.</li> </ul>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#methodological-principles","title":"Methodological principles","text":"<p>'External' accuracy estimates</p> <p>Validation and accuracy analysis in remote sensing is typically based on a comparison of analysis results against reference data. Reference data are measurements or observations of the target variable collected in-situ (i.e. directly at the site) or through remote sensing. In many cases reference data serves two different purposes:</p> <ul> <li>Model building (often by training a supervised classifier or regression model through statistical learning; see theme 3)</li> <li>Validation and accuracy analysis (i.e. checking if the information obtained via our analysis is correct (to a required degree) and quantify the errors of this analysis)</li> </ul> <p>In this theme, we focus on the second purpose, and hence can (more specifically) refer to validation data. Typically, the validation data covers only a fraction of the space (and time) under investigation but is of higher quality (more accurate) than the classification or regression result. As the validation data and the calculation of accuracy metrics is separated from the classification or regression part (main analysis), we refer to the outcomes of this approach as 'external' accuracy estimates.</p> <p>Some key principles of validation and accuracy analysis are</p> <ul> <li>Transparency: Clearly describe the methods and data used.</li> <li>Independence of validation data<ul> <li>Use data for validation that has not been used for building your model (e.g. for training a machine learning classifier):<ul> <li>Often an initial reference data set is split into a training data set and a validation (or 'test') data set. Then, the training data is used for building the model (training), while the validation data is excluded from this (put aside) and subsequently used for accuracy assessment.</li> <li>Such a split can be random with a defined ratio (e.g., 70% of the data for training, 30% of the data for validation), or by spatial subsets of your study area (i.e. use one part of the area for training and the other one for validation).</li> <li>Looking not only at the (in-sample) training error but also at the (out-of-sample) test error is important to evaluate how good the model generalizes, i.e. avoids overfitting (to the training data). In most cases we want a model that does not only fit well to the data it was trained on but also accurately predicts cases not seen during training.</li> <li>Using only one hold-out test set from a single (random) split is straightforward. However, the resulting accuracy estimate lacks robustness because, if the proceedure is repeated with different random splits, the variance of estimated accuracy is large. More robust estimates and confidence intervals (or variance) for classification accuracy can be obtained by resampling frameworks, typically using bootstrapping or cross-validation, into the classification and accuracy assessment (further reading in Module 2 and in Lyons et al. 2018).</li> <li>Cross-validation (CV) - In the basic version, k-fold CV, the training set is split into k smaller sets of data (called folds), and for each fold a model is trained on the other (k - 1) folds and then validated on the current fold.</li> <li>Cross validation can be used to assess the performance of a (final) model, but also for tuning hyperparameters of a machine learning model</li> <li>If you have multi-temporal reference data, you can also make a temporal split (e.g. use one year for training, the other years for validation or set up a temporal cross-validation with years as folds).</li> </ul> </li> <li>As an alternative to splitting the initial reference dataset into training data and validation data, you might collect new validation data (in a separate campaign after collecting training data) but beware of a potential operator bias if you already know the predicted results at this time.</li> </ul> </li> <li>Reporting: It is often advisable to present your accuracy assessment in different ways, graphically and with different statistical measures (accuracy metrics) for summary. This allows you and others to judge the results critically and from different perspectives. Commonly used accuracy metrics are detailed below.</li> </ul> <p> <p>Two possibilities of (randomly) splitting a dataset for training and validation (testing). a) Single split (here as a 60/40 split). b) Cross-validation (CV) with k-folds (here with k = 5) and averaging the output of k runs for modelling and validation. </p> <p>'Internal' accuracy estimates</p> <p>'Internal' accuracy estimates, in contrast, are indicators of reliability computed without the use of external validation data. Instead, they are based on knowledge and understanding of the data observation and information extraction processes, on certain assumptions made, and on statistics. Examples are</p> <ul> <li>Probability of a classification based on ensemble classifiers<ul> <li>Ensemble models can be constructed by combining multiple components (such as trees in a Random Forest, a model used in different variations (e.g. with different thresholds), or a set of models with different approaches).</li> <li>Outputs of ensemble components are usually aggregated (i) into a 'majority vote', i.e. the most frequently predicted class per predicted unit (pixel, point, or segment), or (ii) by averaging the components' probabilistic predictions.</li> <li>Additionally, the fraction of components predicting this class can be reported as relative probability of the class being the best prediction (Note that 'best' prediction and 'probability' must be seen in the context of model setups tested (algorithm, parameters, parameters' range, features, training data, defined classes).</li> </ul> </li> <li>An error budget of measurements can be constructed from known or estimated error components as an estimate for analysis uncertainty. It is also important to consider how the error components of single epochs can propagate into multitemporal derivatives (e.g., in change analysis; see Module 3)</li> <li>Level-of-detection and confidence intervals for topographic change<ul> <li>Topographic change detection studies often propagate the uncertainty (error budget) of multitemporal input data (3D point clouds or raster digital terrain models (DTMs)) and potentially other parameters (such as local surface roughness) into a global or spatially varying level-of-detection (LOD) at a defined confidence interval (e.g. Lane et al. 2003, Wheaton et al. 2010, Lague et al. 2013). This LOD indicates the minimum magnitude of change that can be detected; smaller changes are then excluded from further analysis as they could also result from measurement errors.</li> <li>For 3D point clouds from topographic laser scanning, different error sources can be considered to determine such an LOD (including registration errors, local surface roughness, and 3D positional errors per point as a function of the scanning geometry; see e.g. Mayr et al. 2020).</li> <li>For terrestrial laser scanning time series (4D point clouds) the temporal resolution can additionally be considered for investigating the LOD (Kromer et al. 2017, Anders et al. 2019). </li> </ul> </li> <li>Learn more about uncertainty in 3D change analysis and how it can be quantified in Module 3</li> <li>Learn more about uncertainty of satellite data products in Module 2</li> </ul> <p> <p>Left: Example for a classification probability raster, derived for the landcover map produced in theme 3 by Random Forest classification based on spectral-temporal metrics. For every pixel, the map shows the average probability of all trees for the most likely class (i.e. the majority vote of the ensemble). Thereby, it indicates where the classifier is certain about a predicted class and where it is less certain. Right: Distribution of classification probabilities per landcover class. </p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#freely-available-reference-data-sets","title":"Freely available reference data sets","text":"<p>When looking for reference data, you can search public archives hosted by governmental authorities. These increasingly open the access to regional and national geodata acquired covering a range of different topics. In addition, you might find reference data for your (satellite-based) analysis in collections and platforms of a range of (international) initiatives dedicated to collecting, harmonizing and provisioning of reference data (see also Module 2). Examples are</p> <ul> <li>The Radiant MLHub by the Radiant Earth Foundation allows anyone to access, store, register, and share open training datasets and models for machine learning in Earth observation. The datasets cover applications ranging from crops, wildfire, flood, building footprints, and tropical storms to land cover, and they can be useful also for validation of your models (not only training) if you split the data appropriately.</li> <li>World Cereal - The WorldCereal Reference Data Module (RDM) is a reference data store containing labelled data which can be used for crop and irrigation related model training and validation (with varying accessibility and licences of datasets).</li> <li>Land Use/Cover Area frame Survey (LUCAS) - Since 2006, the LUCAS initiative has collected tri-annual in-situ observations of land cover, land use and environmental parameters along with photographs at hundreds of thousands of unique locations across the European Union. The datasets have been used for supervised landcover classification (e.g. Pflugmacher et al. 2019, Weigand et al. 2020) and harmonized into a consolidated, openly accessible database by d'Andrimont et al. (2020).</li> </ul> <p> <p>Schematic overview of the LUCAS initiative and harmonisation of datasets (figure by d'Andrimont et al. 2020/ CC BY 4.0). </p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#acquiring-reference-data","title":"Acquiring reference data","text":""},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#acquisition-of-reference-data-in-the-field","title":"Acquisition of reference data in the field","text":"<p>One possibility of getting reference data is visiting sample locations in the field (in-situ) and</p> <ul> <li>making (visual) observations and categorizing these into discrete semantic categories (such as landcover, landform or status of the land surface) or into categorized values of a continuous variable (e.g. plant species cover values in percent in percent of a certain area)</li> <li>performing measurements of a (bio-)physical variable (such as temperature, snow depth, vegetation height, surface elevation, ...) with appropriate measurement devices</li> <li>collecting samples of soil, rock, plants etc. for subsequent analysis in the lab (if the properties of interest cannot be estimated in the field)</li> </ul> <p>A wide variety of measurement devices is used for such purposes, ranging from simple tools like a ruler to more complex instruments like field spectrometers. For time series observations, sensors connected to a data logger or a data transmission system can be used.</p> <p>Thorough planning is crucial to any field data acquisition campaign, so include this planning stage in your time schedule. To avoid observer/surveyor bias, data at each sample location should be acquired by a standardized procedure defined in a manual before the field campaign. In the planning stage of a field campaign consider also</p> <ul> <li>safety-related issues (e.g. when working in areas with rough terrain or extreme climate)</li> <li>privacy-related issues (e.g. permission to collect and publish data)</li> <li>accessibility (permission, time needed, terrain, physical ability and hazard management)</li> </ul> <p>A key issue of field observations and measurements is the recording of their position, which can often be achieved by (differentially corrected) Global Navigation Satellite System (GNSSS) measurements. To facilitate re-locating the exact locations for repeat measurements, consider marking them physically (in addition to recording the GNSS position). In general, a thorough documentation following a pre-designed protocol is important, and (geotagged) photographs of the sample site are recommended. Often, a mobile GIS (e.g. QField, a mobile app version of QGIS) is an ideal tool for these tasks.</p> <p> <p>QField app on a smartphone used for the collection of observations and metadata and for navigation to sample locations. </p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#remotely-sensed-reference-data","title":"Remotely sensed reference data","text":"<p>Furthermore, reference data is frequently collected through remote sensing:</p> <ul> <li>This typically involves input data of higher (spatial) resolution and/or alternative analysis methods with presumably superior accuracy (e.g. visual interpretation and manual mapping). Module 1 Theme 3 of this course includes a tutorial where the landcover data was collected by placing and labelling points in an aerial orthophoto (0.2 m resolution), and this data was used for training a classifier on Sentinel-2 data (20 m resolution). Similarly, we could use a part of this data or new data acquired in the same way for validation of the classification results.</li> <li>When we generate reference data by visual interpretation and manual labelling, we have different options:<ul> <li>Labelling entire scenes</li> <li>Labelling individual points or pixels</li> <li>Labelling groups of points or pixels<ul> <li>Contiguous patches that are homogeneous according to certain features and labelled with one label per patch<ul> <li>Derived by segmentation (e.g Kollert et al. 2021)</li> <li>Delineated by visual interpretation</li> </ul> </li> <li>Contiguous patches defined by a (random) sampling strategy and labelled on a per pixel basis (e.g. random points with a buffer or rectangular image chips)</li> </ul> </li> </ul> </li> <li>Critically judge the quality of your reference data: Depending on the target classes and their visual separability in the remote sensing data interpreted to create a reference, such an approach may be quite accurate or not.</li> </ul> <p>Tools for reference data acquisition and for validation</p> <p>Various tools have been developed to improve the efficiency of reference data collection based on remote sensing data and for using such data for validation, e.g.:</p> <ul> <li>TimeSync - A tool for human interpretation of Landsat time series. It is made to calibrate and validate a temporal segmentation of Landsat time series and a trajectory-based detection of land change Cohen et al. (2010).</li> <li>Computer Vision Annotation Tool (CVAT) - A free, online, interactive video and image annotation tool for computer vision.</li> <li>R tool for reference data collection presented in Exercise 2 in Module 2</li> <li>Standard GIS packages (and plugins) contain various tools for accuracy assessment of raster-based remote sensing classifications<ul> <li>Digitizing tools to create points or polygons as reference (and optionally rasterize them)</li> <li>Tools for placing sample points randomly or on a regular grid</li> <li>QGIS AcATaMa plugin (Llano 2022) - A comprehensive plugin for accuracy assessment of thematic maps with many options for sample design, response design  following key recommendations in the scientific literature (e.g. in Stehman and Foody 2019)</li> <li>QGIS Semi-automatic Classification Plugin (SCP) - Amongst many other tools for satellite remote sensing, this plugin by Congedo (2021) contains a tool for accuracy assesssment, see the documentation and this tutorial.</li> <li>GRASS GIS features the r.kappa module for creating a confusion matrix and deriving accuracy metrics.</li> <li>SAGA GIS has two tools for accuracy assessment of a classification raster: The tool Confusion Matrix (Polygons / Grid)) taking the reference data as polygons, and the tool Confusion Matrix (Two Grids) taking the reference data as a raster.</li> </ul> </li> </ul>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#sampling-strategies","title":"Sampling strategies","text":"<p>When collecting your own reference data, where do you sample? I.e. which locations do you choose for measurements, to collect sample material for analysis in the lab, or for placing manually labelled points in images?</p> <p>In general we should aim for samples that are representative for the problem at hand and for the study area where the analysis is applied. Therefore, try to cover the variability of your target variable (if known) or the variables (features) used for analysis as good as possible with a given number of samples. As a guideline, 50 sample units (locations) per thematic class (or more for complex scenarios, i.e. with large areas or many classes) are recommended by Congalton (2015).</p> <p>Popular sampling strategies include:</p> <ul> <li>User defined sampling - A simple approach is to subjectively select locations to sample (not recommended in general).</li> <li>Sampling on a regular grid - Sample points are placed in regular spatial intervals to cover the study area homogeneously</li> <li>Random sampling  - A defined number of samples is placed in the study area.</li> <li>Stratified (random) sampling - Prior to sampling, the study area is divided into sub-units (strata), where samples are then placed (randomly). A stratification can be achieved by categories of a single variable (e.g. elevation bands or categories of an available land cover map) or by clustering several variables (clusters then used as strata). If you already have a (pre-)classification of your data and need validation data (no training data), you could also use the target classes as strata.</li> <li>A stratified sampling can be designed to either place a fixed, equal number of samples per stratum or a variable number of samples per stratum. The latter option typically defines the number of samples as proportionate to the stratum size.</li> <li>Sometimes you have to constrain the area for sampling by auxiliary variables (e.g. accessibility of terrain, distance from road network, ...).</li> </ul> <p>For a multitemporal analysis consider if you can collect multitemporal samples and if this is needed. Then, an additional question is how to distribute the samples in time. Do we place the samples randomly in time or with a defined number of samples per fixed time interval?</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#citizen-science","title":"Citizen science","text":"<p>Collecting reference data in the field or by manually labelling (remote sensing) imagery is often relatively easy for humans. Depending on the specific task, even untrained persons can do this quite reliably. However, labelling a large amount of digital samples or visiting a lot of sampling locations spread across large areas is costly in terms of time and labour.</p> <p>TO improve the efficiency of data collection (amongst others), crowdsourcing and digital citizen science approaches have emerged (Kullenberg and Kasperowski 2016, See et al. 2016, Liu et al. 2021), and are also used in remote sensing (Dell'Acqua 2015, Fritz et al. 2017a, Purdy et al. 2023). The idea is to spread relatively simple, yet repetitive tasks over a large group of volunteers, thereby gathering a large set of urgently needed reference data. These volunteers usually perform well defined tasks, like image labelling or making and reporting geolocalized field observations, typically using smartphones and/or dedicated web applications. Motivating a sufficent number of people for engaging in such a citizen science initiative is a key issue. Therefore, the usefulness and value of the collected datasets for a higher-level goal (serving sustainable development, environmental protection, food security, solving socio-economic problems, contributing to the progress of basic science) must be well communicated. Moreover, some citizen science apps are designed like a game (e.g. with points earned for the tasks and participant rankings being listed) to let participants have fun while performing the task. Citizen Science is also seen as contributing to Open Science aims and principles (Wagenknecht et al. 2021).</p> <p>Examples for citizen science projects and platforms in remote sensing are</p> <ul> <li>Zooniverse - A large platform hosting many citizen science projects from various domains, such as Beavers from Space or ForestEyes (Dallacqua et al. 2020).</li> <li>LandSense - A project that aims to bring together various citizen science data sets on land use and land cover.</li> <li>Geo-Wiki (Fritz et al. 2012, Fritz et al. 2017b)</li> <li>LACO-Wiki, 'The Land Cover Validation Platform' (See et al. 2015).</li> <li>FotoQuest Go</li> <li>MountaiNow - A platform for sharing mountain observations (e.g. in the context of glacier and permafrost monitoring).</li> <li>IceWatch - A program collecting sea ice observations.</li> <li>eu-citizen.science - A platform that aims to become the central knowledge-sharing hub for citizen science in Europe.</li> </ul>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#accuracy-metrics","title":"Accuracy metrics","text":"<p>Depending on the type of data that you compare against reference data, different sets of accuracy metrics can be used. We will first look at ways to assess categorical data (discrete values or labels of a classification), then at measures for assessing continuous variables measured by a sensor or predicted by a model with one or more input variables.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#classification","title":"Classification","text":"<p>The confusion matrix</p> <p>A basic, yet quite useful way of presenting the output of a classification model (predicted labels) compared with values at validation samples (reference labels) is the confusion matrix (also known as cross-tabulation matrix or error matrix). This is a cross-tabulation of how many classification units (pixels, points or polygons) were predicted for a certain class vs. how many actually belong to this class (according to the reference data). The matrix allows us to see if a classifier confuses two classes (hence the name), i.e. mislabels one class as another.</p> <p>The following figure illustrates this for a binary classification. The two classes ('positive' and 'negative') could be, for example, 'snow' and 'no snow'.</p> <p> <p>Confusion matrix for binary classification. </p> <p>Here, we display the predicted values along the rows, the reference values along the columns. Note, however, that the matrix may be transposed (i.e. the axes swapped), as there is no uniform convention for this layout; so check the documentation of the software package you are using.</p> <p>Classification metrics</p> <p>The confusion matrix itself is already very informative, especially if colourized as a heatmap (i.e. colorized by the (relative) number of samples in a matrix cell). Moreover, it forms the basis of different accuracy metrics , e.g.:</p> <ul> <li> <p>Overall accuracy: The fraction of correctly classified samples (TP + TN) out of all reference samples, a value ranging from 0 to 1 (or from 0% to 100%).</p> </li> <li> <p>Precision (a.k.a. user's accuracy or correctness)</p> <pre><code>precision = TP / TP + FP\n</code></pre> </li> <li> <p>Recall (a.k.a. producers's accuracy or completeness or sensitivity)</p> <pre><code>recall = TP / TP + FN\n</code></pre> </li> <li> <p>F1 score, the harmonic mean of precision and recall (equal contributions of the two measures)</p> <pre><code>F1 score = 2 * (precision * recall) / (precision + recall)\n</code></pre> <p>As the different types of mis-classification are not for all applications equally important, measures with different a weighting of precision and recall have been proposed (see e.g. here).</p> </li> <li> <p>Error of omission and error of commission</p> <pre><code>error of omission = 1 - recall\n\nerror of commission = 1 - precision\n</code></pre> </li> </ul> <p>To understand the performance of a model, it is often a good idea to look at a few different metrics as well as the confusion matrix itself. The 'Kappa' metric, despite being widely used to assess remote sensing classifications, is critized as providing redundant or misleading information for practical decision making and is, therefore, no longer recommended (Pontius and Millones 2011, Foody 2020).</p> <p>Most metrics range from 0 to 1, with 1 being the best rating. We recommend to look up details on interpretation as well as advantages and drawbacks of the metrics or options to calculate with specific settings (especially in the multilabel case), e.g. in Olson and Delen 2008, Congalton and Green 2019, Stehman et al. 2019, Hand et al. 2012. Also the documentation of software packages often provides details on the implementation and advice on the use of available accuracy metrics, e.g. in Python scikit-learn.</p> <p>Confusion matrix with more than two categories</p> <p>The next figure illustrates a confusion matrix for more than two classes. If all samples are predicted correctly, they are aligned along the diagonal (top left to lower right). Here, we also sum up the number of samples per class.</p> <p>In such multiclass scenarios, accuracy metrics can be calculated for each class separately or for all classes. Class-specific accuracy metrics provide a more differentiated view and often a better identification of potential problems with a specific class. To summarize the performance for all classes, a metric can be averaged over all classes, with different possibilities how this is done in case of imbalanced datasets (weighting etc.; further reading in the scikit-learn documentation).</p> <p> </p> <p>Left: Schematic confusion matrix for multiple classes (classes A, B, and C). Right: Confusion matrix for the landcover map produced in the classification tutorial of theme 3. Here, the confusion matrix is normalized over the predicted conditions, i.e. the displayed values indicate the fraction of predictions per class being correct or misclassified as a certain other class.</p> <p>In the matrix above, n<sub>ij</sub> is the number of samples labelled as class i in the reference and predicted as class j by the model. n<sub>i+</sub> is the total number of samples actually being class i, n<sub>+i</sub> is the total number of samples predicted as class i, and n is the total number of samples. </p> <p>How would you calculate precision and recall for class A? Write down the two formulas.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#regression","title":"Regression","text":"<p>For a comparison of continuous variables, such as a variable estimated through a regression vs. an observed (reference) variable, we need different measures than for categorical data. In addition to a visual inspection of (i) predicted values plotted against actual values (reference data) or (ii) residuals plotted against predicted values, these are a few metrics commonly used to assess the performance of regression:</p> <ul> <li>R<sup>2</sup> - The 'coefficient of determination' R<sup>2</sup> provides a measure for the goodness of fit (model output vs. reference) and indicates the proportion of variance (of a target variable y) that is explained by the model based on the independent variables. Ideally, R<sup>2</sup> yields 1.0 (or 100%) but, unlike most other metrics, the R<sup>2</sup> can be negative for badly performing models.</li> <li>Mean error (ME) - The average of the errors (or 'residuals' if computed on the training set), i.e. the differences between observed (reference) values and estimated values. An accurate prediction has a small ME. However, large positive and negative errors can balance each other out and lead to a small ME as well. A large ME points to systematic errors into one direction (bias).</li> <li>Mean squared error (MSE) - The MSE is the average of the squared errors. Squaring the errors avoids that positive and negative ones cancel each other out. Squaring both the errors and their units, however, makes them difficult to interpret.</li> <li>Root mean squared error (RMSE) - The RMSE is the square root of the MSE and, thus, has the same unit as the estimated quantity. As a result of squaring each error, both MSE and RMSE put more weight to large errors than to small ones and, therefore, are sensitive to outliers (more than, e.g., the MAE). If occasional large outliers in the prediction of a model are acceptable for an application or cannot be avoided, RMSE and MSE are the wrong metrics. If you want to penalize large errors strongly, RMSE may be suitable.</li> <li>Mean absolute error (MAE) - The MAE is the average of the absolute (i.e. non-negative) errors. It avoids the effect of positive and negative errors balancing each other out.</li> </ul> <p>Of these metrics, only R<sup>2</sup> is on a relative scale and is, thereby, suitable for direct comparisons of regression performance with different datasets. The other metrics depend on the dataset and problem at hand and on the scale and unit of the estimated quantity and are, thus, often harder to assess (What is a good MSE?). However, keep in mind, that also metrics on a relative (or normalized) scale (like R<sup>2</sup> or recall of a classification) must be seen in context, and depending on the application requirements and our expectation on model performance. For some applications we may be excited about an R<sup>2</sup> of 0.7, for others we need an R<sup>2</sup> &gt; 0.9 to make use of the model. It is often a good idea to compute R<sup>2</sup>, ME and MAE as a basis for evaluation.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#challenges-of-accuracy-analysis","title":"Challenges of accuracy analysis","text":"<p>Acquisition of reference data is in most cases constrained by practical aspects (e.g. site accessibility) and costs (time and money). So there is usually a tradeoff between quality and size of the reference data set, and this tradeoff needs to be balanced. In other words:</p> <ul> <li>How can we obtain reliable data for a sufficient number of samples?</li> <li>Where do we collect reference samples to get unbiased, statistically defendable accuracy estimates?</li> </ul> <p>Moreover, when validating analysis results by comparison against external, independent data (e.g. from in-situ observations), we must consider the quality of this validation data. Can we justify the assumption that this is the 'best available measurement'? Can we quantify the uncertainty of this validation data itself?</p> <p>For a validation of time series analyses we face additional challenges that need to be considered:</p> <ul> <li>How can we ensure that the samples are not only representative for our area of interest but also for our time period under investigation or for certain points in time?</li> <li>For retrospective analyses we often do not have specifically collected in-situ data and must rely on archive data.</li> <li>Archive data often does not match the points in time and area that we cover by our (main) analysis. Therefore, we might have to perform an accuracy analysis for areas and time periods matching the available reference data and then assume that the outcomes are valid also for our main analysis.</li> <li>Repeat visits of a site for multi-temporal sampling are often difficult to organize practically. Often we need to balance a tradeoff between spatial and temporal coverage.</li> <li>Permanently installed in-situ or close-range sensing systems collecting time series data autonomously at high spatial and temporal resolution can be useful. An example are webcams or soil temperature loggers used to validate satellite-based snow cover products.</li> </ul>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"Principles of validation and accuracy assessment \u2013 Which of the following statements is correct?  Accuracy assessment of a remote sensing analysis is needed to judge if we can use the results as a basis for further interpretation and/or decision making. To ensure consistency we should use the same data for accuracy assessment that we used for training our model. Accuracy assessment of a remote sensing analysis is needed to judge if we can use the results as a basis for further interpretation and/or decision making.  Which accuracy metrics are suited for classification?  Root mean squared error User\u2019s accuracy Producer\u2019s accuracy Recall User\u2019s accuracy&amp;Producer\u2019s accuracy&amp;Recall  Which accuracy metrics are suitable for regression?  Mean absolute error F1 score R\u00b2 Mean absolute error&amp;R\u00b2  True or false: Overall accuracy or the Kappa value are the best metric to look at, when we want to learn how to improve a method.  True False False"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#exercise-assessment-of-landcover-classification-accuracy","title":"Exercise: Assessment of landcover classification accuracy","text":"<p>In the image time series classification tutorial of theme 3, we produced a landcover classification by a machine learning approach with time series metrics as features. The resulting landcover map of this tutorial is provided in the course data repository.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#task","title":"Task","text":"<p>The task in this excercise is to perform an accuracy assessment for this landcover map (using Python and/or a GIS package). These are the main steps:</p> <ul> <li>Place random points as validation samples. To get an equal number of samples (e.g. 10) for each class use e.g.<ul> <li>QGIS and the tool Random Points in Polygons or</li> <li>Python and the GeoPandas <code>sample_points()</code>) method</li> </ul> </li> <li>Label samples (QGIS editing functionality recommended)</li> <li>Compute confusion matrix and accuracy metrics, using e.g. one of these:<ul> <li>Python scikit-learn with its <code>sklearn.metrics</code> module (Tipp: <code>classification_report</code> and <code>ConfusionMatrixDisplay</code> functions)</li> <li>QGIS with the AcaTaMa plugin or the Semi-Automatic Classification plugin</li> <li>SAGA GIS or GRASS GIS (as standalone software or within QGIS)</li> </ul> </li> <li>Discuss the results of this accuracy analysis:<ul> <li>Which classes have been classified well and which classes not?</li> <li>How do you see this in the computed metrics?</li> <li>Which classes get confused frequently? Any ideas why? How could this be improved?</li> <li>Limitations of this accuracy analysis?</li> </ul> </li> </ul>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#solution","title":"Solution","text":"<p>A Python solution is provided in this notebook. The notebook demonstrates how to create validation data by stratified random sampling of points and manual labelling of these points in an orthophoto (in QGIS), then creates a confusion matrix and a tabular classification report from this validation data and the classified map.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/06_reference_data_validation_accuracy_assessment.html#references","title":"References","text":"<p>Anders, K., Lindenbergh, R. C., Vos, S. E., Mara, H., De Vries, S., &amp; H\u00f6fle, B. (2019). High-frequency 3D geomorphic observation using hourly terrestrial laser scanning data of a sandy beach. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 4(2/W5), 317-324. https://doi.org/10.5194/isprs-annals-IV-2-W5-317-2019</p> <p>Cohen, W. B., Yang, Z., &amp; Kennedy, R. (2010). Detecting trends in forest disturbance and recovery using yearly Landsat time series: 2. TimeSync\u2014Tools for calibration and validation. Remote Sensing of Environment, 114(12), 2911-2924. https://doi.org/10.1016/j.rse.2010.07.010</p> <p>Congalton R.G. (2015). Assessing positional and thematic accuracies of maps generated from remotely sensed data. In: Thenkabail P.S. (ed.). Remotely sensed data characterisation, classification, and accuracies. CRC Press, Boca Raton, 712 p. https://doi.org/10.1201/b19294 </p> <p>Congalton , R. G., &amp; Green, K. (2019). Assessing the accuracy of remotely sensed data: principles and practices. 3rd edition, CRC Press, Boca Raton. https://doi.org/10.1201/9780429052729</p> <p>Congedo, L. (2021). Semi-Automatic Classification Plugin: A Python tool for the download and processing of remote sensing images in QGIS. Journal of Open Source Software, 6(64), 3172, https://doi.org/10.21105/joss.03172</p> <p>Dallaqua, F. B. J. R., Faria, F. A., &amp; Fazenda, \u00c1. L. (2020). Building data sets for rainforest deforestation detection through a citizen science project. IEEE Geoscience and Remote Sensing Letters, 19, 1-5. https://doi.org/10.1109/LGRS.2020.3032098</p> <p>Dell\u2019Acqua, F. (2015). Crowdsourcing and Remote Sensing: Combining Two Views of Planet Earth. In: Thenkabail P.S. (ed.). Remotely sensed data characterisation, classification, and accuracies. CRC Press, Boca Raton, 712 p. https://doi.org/10.1201/b19294</p> <p>d\u2019Andrimont, R., Yordanov, M., Martinez-Sanchez, L., Eiselt, B., Palmieri, A., Dominici, P., ... &amp; van der Velde, M. (2020). Harmonised LUCAS in-situ land cover and use database for field surveys from 2006 to 2018 in the European Union. Scientific data, 7(1), 352. https://doi.org/10.1038/s41597-020-00675-z</p> <p>Foody, G. M. (2020). Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification. Remote Sensing of Environment, 239, 111630. https://doi.org/10.1016/j.rse.2019.111630</p> <p>Fritz, S., McCallum, I., Schill, C., Perger, C., See, L., Schepaschenko, D., ... &amp; Obersteiner, M. (2012). Geo-Wiki: An online platform for improving global land cover. Environmental Modelling &amp; Software, 31, 110-123. https://doi.org/10.1016/j.envsoft.2011.11.015</p> <p>Fritz, S., Fonte, C. C., &amp; See, L. (2017). The role of citizen science in earth observation. Remote Sensing, 9(4), 357. https://doi.org/10.3390/rs9040357</p> <p>Fritz, S., See, L., Perger, C., McCallum, I., Schill, C., Schepaschenko, D., ... &amp; Obersteiner, M. (2017b). A global dataset of crowdsourced land cover and land use reference data. Scientific data, 4(1), 1-8. https://doi.org/10.1038/sdata.2017.75</p> <p>Hand, D. J. (2012). Assessing the performance of classification methods. International Statistical Review, 80(3), 400-414. https://doi.org/10.1111/j.1751-5823.2012.00183.x</p> <p>Kollert, A., Bremer, M., L\u00f6w, M., &amp; Rutzinger, M. (2021). Exploring the potential of land surface phenology and seasonal cloud free composites of one year of Sentinel-2 imagery for tree species mapping in a mountainous region. International Journal of Applied Earth Observation and Geoinformation, 94, 102208. https://doi.org/10.1016/j.jag.2020.102208</p> <p>Kromer, R. A., Abell\u00e1n, A., Hutchinson, D. J., Lato, M., Chanut, M. A., Dubois, L., &amp; Jaboyedoff, M. (2017). Automated terrestrial laser scanning with near-real-time change detection\u2013monitoring of the S\u00e9chilienne landslide. Earth Surface Dynamics, 5(2), 293-310. https://doi.org/10.5194/esurf-5-293-2017</p> <p>Kullenberg, C., &amp; Kasperowski, D. (2016). What is citizen science? \u2013 A scientometric meta-analysis. PloS one, 11(1), e0147152. https://doi.org/10.1371/journal.pone.0147152</p> <p>Lague, D., Brodu, N., &amp; Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (NZ). ISPRS Journal of Photogrammetry and Remote Sensing, 82, 10-26. https://doi.org/10.1016/j.isprsjprs.2013.04.009</p> <p>Lane, S.N., Westaway, R.M., Murray HD., (2003). Estimation of erosion and deposition volumes in a large, gravel\u2010bed, braided river using synoptic remote sensing. Earth Surface Processes and Landforms, 28(3), 249-271. https://doi.org/10.1002/esp.483</p> <p>Lyons, M. B., Keith, D. A., Phinn, S. R., Mason, T. J., &amp; Elith, J. (2018). A comparison of resampling methods for remote sensing classification and accuracy assessment. Remote Sensing of Environment, 208, 145-153. https://doi.org/10.1016/j.rse.2018.02.026</p> <p>Llano, X. (2022), SMByC-IDEAM. AcATaMa - QGIS plugin for Accuracy Assessment of Thematic Maps, version v23.4. https://github.com/SMByC/AcATaMa</p> <p>Liu, H. Y., D\u00f6rler, D., Heigl, F., &amp; Grossberndt, S. (2021). Citizen science platforms. In: Vohland et al. (Eds.): The Science of Citizen Science, Springer, Cham, 439-459. https://doi.org/10.1007/978-3-030-58278-4_22</p> <p>Mayr, A., Bremer, M., &amp; Rutzinger, M. (2020). 3D point errors and change detection accuracy of unmanned aerial vehicle laser scanning data. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2, 765-772. https://doi.org/10.5194/isprs-annals-V-2-2020-765-2020</p> <p>Olson, D.L. and Delen D. (2008). Performance evaluation for predictive modeling. Chapter 9 in: Advanced Data Mining Techniques Springer, Berlin, Germany. https://doi.org/10.1007/978-3-540-76917-0</p> <p>Pflugmacher, D., Rabe, A., Peters, M., &amp; Hostert, P. (2019). Mapping pan-European land cover using Landsat spectral-temporal metrics and the European LUCAS survey. Remote Sensing of Environment, 221, 583-595. https://doi.org/10.1016/j.rse.2018.12.001</p> <p>Pontius Jr, R. G., &amp; Millones, M. (2011). Death to Kappa: birth of quantity disagreement and allocation disagreement for accuracy assessment. International Journal of Remote Sensing, 32(15), 4407-4429. https://doi.org/10.1080/01431161.2011.552923</p> <p>Purdy, L. M., Sang, Z., Beaubien, E., &amp; Hamann, A. (2023). Validating remotely sensed land surface phenology with leaf out records from a citizen science network. International Journal of Applied Earth Observation and Geoinformation, 116, 103148. https://doi.org/10.1016/j.jag.2022.103148</p> <p>See, L., Perger, C., Hofer, M., Weichselbaum, J., Dresel, C., &amp; Fritz, S. (2015). LACO-WIKI: an open access online portal for land cover validation. ISPRS Ann Photogramm Remote Sens Spatial Inf Sci, 2, 167-171.  https://doi.org/10.5194/isprsannals-II-3-W5-167-2015</p> <p>See, L., Mooney, P., Foody, G., Bastin, L., Comber, A., Estima, J., ... &amp; Rutzinger, M. (2016). Crowdsourcing, citizen science or volunteered geographic information? The current state of crowdsourced geographic information. ISPRS International Journal of Geo-Information, 5(5), 55. https://doi.org/10.3390/ijgi5050055</p> <p>Stehman, S. V., &amp; Foody, G. M. (2019). Key issues in rigorous accuracy assessment of land cover products. Remote Sensing of Environment, 231, 111199. https://doi.org/10.1016/j.rse.2019.05.018</p> <p>Weigand, M., Staab, J., Wurm, M., &amp; Taubenb\u00f6ck, H. (2020). Spatial and semantic effects of LUCAS samples on fully automated land use/land cover classification in high-resolution Sentinel-2 data. International Journal of Applied Earth Observation and Geoinformation, 88, 102065. https://doi.org/10.1016/j.jag.2020.102065</p> <p>Wheaton, J. M., Brasington, J., Darby, S. E., &amp; Sear, D. A. (2010). Accounting for uncertainty in DEMs from repeat topographic surveys: improved sediment budgets. Earth Surface Processes and Landforms, 35(2), 136-156. https://doi.org/10.1002/esp.1886</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html","title":"T6 S2 landcover classification accuracy solution","text":"Metadata     title: \"E-TRAINEE Tutorial - Accuracy assessment of a landcover classification\"     description: \"This is a tutorial within the sixth theme of Module 1 of the E-TRAINEE course.\"     lastUpdate: 2023-07-21     authors: Andreas Mayr <p>Set the directory where data is stored. Save the AOI and the landcover map there.</p> In\u00a0[1]: Copied! <pre>import pathlib\ndata_dir = pathlib.Path('C:/work/etrainee/gee/T3') / 'data' # Define path on your local system\ndata_dir\n</pre> import pathlib data_dir = pathlib.Path('C:/work/etrainee/gee/T3') / 'data' # Define path on your local system data_dir Out[1]: <pre>WindowsPath('C:/work/etrainee/gee/T3/data')</pre> In\u00a0[3]: Copied! <pre>import rioxarray\nfrom rasterio import features\nfrom shapely.geometry import shape\nimport geopandas as gpd\n\ngeometry_list = []\nvalue_list = []\n\nlc_rast = rioxarray.open_rasterio(data_dir / \"landcover_id.tif\")    # use rioxarray accessor to open GeoTiff as xarray DataArray\nfor shapedict, value in features.shapes(lc_rast, transform=lc_rast.rio.transform()):    # get a pair of (polygon, value) for each feature found in the image.\n    value_list.append(value)\n    geometry_list.append(shape(shapedict))  # construct shapely polygons from GeoJSON-like dicts returned by rasterio.features.shapes\n\nlc_dict = {'lc_id': value_list, 'geometry': geometry_list}\nlc_gdf = gpd.GeoDataFrame(lc_dict, crs=lc_rast.rio.crs)\nlc_gdf['lc_id'] = lc_gdf['lc_id'].astype(int)\nlc_gdf\n</pre> import rioxarray from rasterio import features from shapely.geometry import shape import geopandas as gpd  geometry_list = [] value_list = []  lc_rast = rioxarray.open_rasterio(data_dir / \"landcover_id.tif\")    # use rioxarray accessor to open GeoTiff as xarray DataArray for shapedict, value in features.shapes(lc_rast, transform=lc_rast.rio.transform()):    # get a pair of (polygon, value) for each feature found in the image.     value_list.append(value)     geometry_list.append(shape(shapedict))  # construct shapely polygons from GeoJSON-like dicts returned by rasterio.features.shapes  lc_dict = {'lc_id': value_list, 'geometry': geometry_list} lc_gdf = gpd.GeoDataFrame(lc_dict, crs=lc_rast.rio.crs) lc_gdf['lc_id'] = lc_gdf['lc_id'].astype(int) lc_gdf Out[3]: lc_id geometry 0 2 POLYGON ((681940.000 5241670.000, 681940.000 5... 1 3 POLYGON ((682160.000 5241670.000, 682160.000 5... 2 3 POLYGON ((682180.000 5241670.000, 682180.000 5... 3 3 POLYGON ((682620.000 5241670.000, 682620.000 5... 4 3 POLYGON ((683910.000 5241670.000, 683910.000 5... ... ... ... 16056 1 POLYGON ((688760.000 5237430.000, 688760.000 5... 16057 3 POLYGON ((689470.000 5238730.000, 689470.000 5... 16058 3 POLYGON ((688830.000 5237400.000, 688830.000 5... 16059 3 POLYGON ((689300.000 5237400.000, 689300.000 5... 16060 3 POLYGON ((689480.000 5237570.000, 689480.000 5... <p>16061 rows \u00d7 2 columns</p> In\u00a0[6]: Copied! <pre>lc_gdf = lc_gdf.dissolve(by='lc_id', as_index=False)    # Do not use landcover ID as index, keep it as a column\n</pre> lc_gdf = lc_gdf.dissolve(by='lc_id', as_index=False)    # Do not use landcover ID as index, keep it as a column <p>So far, landcover is coded with IDs. For convenience let's add a column with landcover class names as strings.</p> In\u00a0[7]: Copied! <pre>landcover_dict={1: \"Grassland\", 2: \"Cropland\", 3: \"Forest\", 4: \"Water\", 5: \"Other\"}\nlc_gdf[\"Landcover\"] = lc_gdf['lc_id'].map(landcover_dict)    # Map values of the new column according to the dictionary\nlc_gdf\n</pre> landcover_dict={1: \"Grassland\", 2: \"Cropland\", 3: \"Forest\", 4: \"Water\", 5: \"Other\"} lc_gdf[\"Landcover\"] = lc_gdf['lc_id'].map(landcover_dict)    # Map values of the new column according to the dictionary lc_gdf Out[7]: lc_id geometry Landcover 0 1 MULTIPOLYGON (((682050.000 5237400.000, 682060... Grassland 1 2 MULTIPOLYGON (((682030.000 5237400.000, 682040... Cropland 2 3 MULTIPOLYGON (((682060.000 5237390.000, 682060... Forest 3 4 MULTIPOLYGON (((681930.000 5237400.000, 681930... Water 4 5 MULTIPOLYGON (((681950.000 5237390.000, 681950... Other In\u00a0[5]: Copied! <pre>vali_points = lc_gdf.sample_points(5)                   # Randomly sample a fixed number of validation points in each polygon\nvali_points = vali_points.explode(index_parts=False)    # Split mulipart points to single part points\nvali_points.to_file(data_dir / \"vali_points.gpkg\")      # Write validation points to Geopackage\n</pre> vali_points = lc_gdf.sample_points(5)                   # Randomly sample a fixed number of validation points in each polygon vali_points = vali_points.explode(index_parts=False)    # Split mulipart points to single part points vali_points.to_file(data_dir / \"vali_points.gpkg\")      # Write validation points to Geopackage In\u00a0[4]: Copied! <pre>vali_points = gpd.read_file(data_dir / \"vali_points.gpkg\")          # Read Geopackage with validation points\nvali_points = vali_points.sjoin(lc_gdf, how='inner')                # Spatial join of points and predicted landcover polygons\nvali_points = vali_points.rename(columns = {'lc_id': 'Predicted'})  # Let's call the column with predicted landcover ID simply 'Predicted'\nvali_points = vali_points.drop(columns='index_right')               # Drop the second index column (of the joined layer) - we don't need it\nvali_points.head(3)\n</pre> vali_points = gpd.read_file(data_dir / \"vali_points.gpkg\")          # Read Geopackage with validation points vali_points = vali_points.sjoin(lc_gdf, how='inner')                # Spatial join of points and predicted landcover polygons vali_points = vali_points.rename(columns = {'lc_id': 'Predicted'})  # Let's call the column with predicted landcover ID simply 'Predicted' vali_points = vali_points.drop(columns='index_right')               # Drop the second index column (of the joined layer) - we don't need it vali_points.head(3) Out[4]: Reference geometry Predicted 0 3 POINT (683370.570 5240412.204) 3 11 1 POINT (686932.053 5241450.139) 3 1 3 POINT (688202.511 5237762.774) 1 In\u00a0[12]: Copied! <pre>from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n</pre> from sklearn.metrics import ConfusionMatrixDisplay, classification_report <p>Define the columns in our GeoDataFrame to use as reference (\"true\") and predicted labels, respectively. As these columns contain class IDs, we also provide a list of class names to use in the confusion matrix.</p> In\u00a0[13]: Copied! <pre>y_true = vali_points['Reference']\ny_pred = vali_points['Predicted']\nclass_names = ['Grassland', 'Cropland', 'Forest', 'Water', 'Other']\nConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=class_names, normalize=None, cmap='Blues')\n</pre> y_true = vali_points['Reference'] y_pred = vali_points['Predicted'] class_names = ['Grassland', 'Cropland', 'Forest', 'Water', 'Other'] ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=class_names, normalize=None, cmap='Blues') Out[13]: <pre>&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2b5e7a424c0&gt;</pre> <p>This confusion matrix reports the number of samples labelled as a class in the reference (\"True label\") and in the prediction, respectively. There would also be options to report ratios instead of counts, i.e. normalize the confusion matrix over the true (rows), predicted (columns) conditions or over all the population. Correctly predicted samples align along the diagonal (starting top-left), off-diagonal samples are misclassified.</p> <p>For instance, reading along the first column, of four samples predicted as grassland only one is correct (top left corner), one is actually cropland and two are forest (in the manually labelled validation data). Reading along the third row, of four samples actually being forest, two samples were misclassified as grassland and only two were recognized correctly. You will likely get slightly different results, due to randomization in model building and in sampling.</p> <p>Moreover, scikit-learn provides a convenient function to generate a tabular classification report with the main metrics per class and overall. For details on the metrics and for individually calculating a specific metric see the documentation of scikit-learn.</p> In\u00a0[14]: Copied! <pre>print(classification_report(y_true, y_pred, target_names=class_names))\n</pre> print(classification_report(y_true, y_pred, target_names=class_names)) <pre>              precision    recall  f1-score   support\n\n   Grassland       0.25      0.50      0.33         2\n    Cropland       1.00      0.56      0.71         9\n      Forest       0.33      0.50      0.40         4\n       Water       0.80      1.00      0.89         4\n       Other       1.00      0.83      0.91         6\n\n    accuracy                           0.68        25\n   macro avg       0.68      0.68      0.65        25\nweighted avg       0.80      0.68      0.71        25\n\n</pre> In\u00a0[9]: Copied! <pre>from matplotlib.colors import ListedColormap\nimport folium\nfrom folium import plugins\n\n# Show the predicted landcover layer (polygonized) in a Folium map object\ncolormap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue'])   # Define a colormap (for alphabetically sorted class names)\nm = lc_gdf.explore(column='Landcover', name=\"Landcover predicted\", categorical=True, cmap=colormap)\n\n# Add landcover validation point layer\ncolormap=ListedColormap(['lightgreen', 'brown', 'darkgreen', 'blue', 'grey'])   # Define a colormap (for class IDs)\nvali_points.explore(\n    m=m, column=\"Reference\", name=\"Landcover validation samples\",\n    marker_kwds={\"radius\": 4}, style_kwds={\"color\": \"black\", \"weight\": 1},\n    categorical=True, legend=False, cmap=colormap\n    )\n\n# Add alternative background layers\nfolium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n                 attr='Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',\n                 name='ESRI WorldImagery', control=True).add_to(m)\nfolium.TileLayer(tiles='Stamen Terrain',\n                 name='Stamen Terrain', control=True).add_to(m)\n\nfolium.LayerControl(collapsed=False).add_to(m)          # Add layer control\nfolium.plugins.Fullscreen().add_to(m)                   # Add a full screen button (for display in a browser)\nm                                                       # Show map\n</pre> from matplotlib.colors import ListedColormap import folium from folium import plugins  # Show the predicted landcover layer (polygonized) in a Folium map object colormap=ListedColormap(['brown', 'darkgreen', 'lightgreen', 'grey', 'blue'])   # Define a colormap (for alphabetically sorted class names) m = lc_gdf.explore(column='Landcover', name=\"Landcover predicted\", categorical=True, cmap=colormap)  # Add landcover validation point layer colormap=ListedColormap(['lightgreen', 'brown', 'darkgreen', 'blue', 'grey'])   # Define a colormap (for class IDs) vali_points.explore(     m=m, column=\"Reference\", name=\"Landcover validation samples\",     marker_kwds={\"radius\": 4}, style_kwds={\"color\": \"black\", \"weight\": 1},     categorical=True, legend=False, cmap=colormap     )  # Add alternative background layers folium.TileLayer(tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',                  attr='Tiles \u00a9 Esri \u2014 Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community',                  name='ESRI WorldImagery', control=True).add_to(m) folium.TileLayer(tiles='Stamen Terrain',                  name='Stamen Terrain', control=True).add_to(m)  folium.LayerControl(collapsed=False).add_to(m)          # Add layer control folium.plugins.Fullscreen().add_to(m)                   # Add a full screen button (for display in a browser) m                                                       # Show map Out[9]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#accuracy-assessment-of-a-landcover-classification","title":"Accuracy assessment of a landcover classification\u00b6","text":"<p>This Python Notebook demonstrates a procedure for assessing the accuracy of a classification map by comparing a landcover map against a set of manually labelled validation samples. The landcover map has been classified by a Random Forest (trained with a set of labelled points) with a few spectral-temporal metrics as features (see theme 3). The validation samples are points placed by stratified random sampling (predicted landcover as strata) with an equal number of points allocated to each stratum. The main results are:</p> <ul> <li>a confusion matrix</li> <li>a tabular classification report with classification accuracy metrics</li> </ul> <p>To have all required packages installed, we recommend to set up a Conda environment based on the requirements file provided for the course. If you have not yet done this, please see the instructions on the software page.</p> <p>Data:</p> <p>The course data repository contains a landcover map (<code>landcover_id.tif</code>) resulting from the classification workflow shown in theme 3. Either assess your own map or take this one.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#accuracy-assessment","title":"Accuracy assessment\u00b6","text":"<p>In this section, we will perform an accuracy assessment based on newly created reference data ('validation data' as it is only used for validation, not for training). We do a stratified random sampling to define points to be labelled manually as reference data. We will simply allocate an equal number of samples in each landcover class.</p> <p>The first thing we need is a set of polygons representing the landcover classes to constrain the placement of random points.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#vectorize-landcover","title":"Vectorize landcover\u00b6","text":"<p>We vectorize the landcover classes of our classification raster with <code>rasterio.features.shapes</code> and <code>shapely.geometry.shape</code>. This converts a 2D <code>xarray.DataArray</code> into a <code>geopandas.GeoDataFrame</code>.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#dissolve-landcover-polygons","title":"Dissolve landcover polygons\u00b6","text":"<p>As we want the landcover classes to guide the sampling, the next step is to dissolve the &gt; 10k polygons of our landcover GeoDataFrame by landcover ID (i.e., column 'lc_id') to obtain one multipart polygon for each landcover class.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#stratified-random-sampling","title":"Stratified random sampling\u00b6","text":"<p>In the next step, we randomly place a fixed number of sample points per landcover class for validation (i.e. stratified sampling with equal allocation; for varying numbers of points per class see here). For simplicity, only 5 samples per class are created but note that more samples are recommended in practice (e.g. 50 samples per class, depending on the complexity of the classification problem).  We write the samples to disc (as Geopackage).</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#manually-label-sample-points-in-qgis","title":"Manually label sample points in QGIS\u00b6","text":"<p>The next step is to label the randomly sampled points. We make use of the convenient editing capabilities in QGIS.</p> <p>Open QGIS and follow these steps:</p> <ul> <li>Open the file vali_points.gpkg and enable editing for this layer (click the pen symbol).</li> <li>Create a new attribute column named 'Reference' with data type integer (yellow column symbol in the attribute table or CTRL+W shortcut).</li> <li>In the 'Symbology' tab of the 'Layer properties' menu, set colors indicative of the landcover label in the 'Reference' attribute column (helps to see which samples you already labelled).</li> <li>Go through the sample points one by one and label them, i.e. as a 'Reference' attribute type the landcover ID you consider appropriate (based on your visual interpretation of the orthophoto).</li> <li>Don't forget to save your edits.</li> </ul> <p>Where did you have problems to clearly identify the true landcover class of a point? Apart from buildings, parking lots and roads, which other situations did not fit well into one of the first four defined target classes?</p> <p>Labelling points in QGIS. The point displayed in magenta has not yet been labelled (background: orthophoto 2019 WMS layer, provided by the Department of Geoinformation, Office of the Tyrolean State Government).</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#accuracy-assessment","title":"Accuracy assessment\u00b6","text":""},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#assign-predicted-landcover-to-sample-points","title":"Assign predicted landcover to sample points\u00b6","text":"<p>We continue in Python again and load the labelled sample points as GeoDataFrame, then add the predicted landcover IDs at the sample locations.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#confusion-matrix-and-accuracy-metrics","title":"Confusion matrix and accuracy metrics\u00b6","text":"<p>The <code>sklearn.metrics</code> module has many functions for scores and accuracy metrics as well as for a classification report and for the confusion matrix.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#disclaimer","title":"Disclaimer\u00b6","text":"<p>Keep in mind that this accuracy assessment demo is based on very few samples (\"support\" column in the report) and, in practice, we would acquire more validation data. Also remember that during orthophoto interpretation we might have had doubts how to label a validation sample. And last but not least, the classification is based on a simple set of only four features derived from the Sentinel-2 time series, and NDVI was the only spectral index we used.</p>"},{"location":"module1/06_reference_data_validation_accuracy_assessment/T6_S2_landcover_classification_accuracy_solution.html#plot-an-interactive-map","title":"Plot an interactive map\u00b6","text":"<p>Finally, plot a Leaflet map to compare the validation samples and the prediction in spatial context. This may also help to see deficiencies of the classification missed by the random validation samples (there is, e.g., no cropland in the northwestern corner).</p>"},{"location":"module1/toolbox_intro/ETRAINEE_intro_overview.html","title":"Toolbox overview","text":"<p>Before starting the course let's \"unpack our toolbox\" to ensure that we have the necessary digital working environment ready. This means we introduce the different software components used in E-TRAINEE Module 1 (and partly also in other Modules). Thereby, we cover installation and some basic methods of</p> <ul> <li>Visual Studio Code (code editor)</li> <li>Conda (package management system)</li> <li>Jupyter Notebooks (interactive computing), with some Python fundamentals (programming language)</li> <li>GeoPython - A quickstart to geographic data handling in Python: Vector data with GeoPandas, raster data with rasterio, and multidimensional raster data with xarray.</li> </ul>"},{"location":"module1/toolbox_intro/ETRAINEE_intro_overview.html#background-and-objective","title":"Background and objective","text":"<p>The objective of this introduction to the E-TRAINEE course is to provide you with the software knowledge and skills needed to start with the practical parts of E-TRAINEE Module 1. For working on these practical parts, there are often a couple of similar software tools and varieties that may be suitable for a specific task, e.g. (our choice in bold):</p> <ul> <li>Program code can be run by executing either \"normal\" scripts (e.g. <code>*.py</code> files) or code cells within interactive Jupyter Notebooks (<code>*.ipynb</code>) with text explanations as well as graphics and other output in between.</li> <li>Jupyter Notebook documents can be edited and run in various web-based or desktop applications, such as JupyterLab, JupyterHub, Jupyter Desktop, or Visual Studio Code, etc..</li> <li>Data can be processed with a variety of graphical user interface software or command line tools or by scripting in a programming language such as R, Python, JavaScript, or Julia.</li> <li>GeoPython: For handling geographic data in Python, we focus on the packages GeoPandas, rasterio and xarray. There are other packages available but some of them introduce unnecessary complexity, are no longer well-maintained, or are tailored to rather specific tasks.</li> </ul> <p>Out of this variety, we selected a set of tools that makes up a tested and proven digital working environment. We hope that, by using a such a recommended, uniform environment for all course participants, you will (i) encounter less software-related problems, (ii) get more helpful, more specific instructions, (iii) learn an approach that enables you to set up and customize your working environment also for follow-up tasks (such as your MSc thesis, where you may need to install additional or different packages). A concise, step-by-step guide will show you how to make the components of this environment interact, so you don't need to search the endless resources of the web, and you have a condensed resource to look things up in case you forget something.</p> <p>In addition to setting up such a working environment, you will learn some of the most useful methods of geodata handling and visualization in Python. This will be helpful when working on the practical parts of E-TRAINEE Module 1, where many of the more advanced workflows build upon these tools and methods.</p>"},{"location":"module1/toolbox_intro/ETRAINEE_intro_overview.html#next-vscode","title":"Next: VSCode","text":"<p>Get started with the Visual Studio Code source-code editor here</p>"},{"location":"module1/toolbox_intro/conda.html","title":"Conda","text":"<p>To install the general-purpose programming language Python and to manage its versions along with all packages extending it (make it useful for our specific purposes) we use the package management system Conda. Conda quickly installs, runs, and updates packages and their dependencies. Conda easily creates, saves, loads, and switches between environments on your local computer. It was created for Python programs but it can package and distribute software for any language.</p> <p> </p>"},{"location":"module1/toolbox_intro/conda.html#conda-distributions","title":"Conda distributions","text":"<p>Anaconda is a popular distribution of Conda, which has lots of software and extensions preinstalled (e.g., Python with numerous packages, ...). While this may sound nice you will probably not need all this software and, instead, you might want to install other packages or specific versions.</p> <p>Miniconda is a free minimal installer that includes only conda, Python, the packages they both depend on, and a small number of other useful packages. More packages can be installed from thousands of packages available by default in Anaconda\u2019s public repo, or from other channels, like conda-forge or bioconda.</p> <p>Miniforge is comparable to Miniconda but it has conda-forge as the default channel to install packages from and it has Mamba installed in the base environment (more on this later).</p>"},{"location":"module1/toolbox_intro/conda.html#should-i-install-miniforge","title":"Should I install Miniforge?","text":"<ul> <li>If you have anaconda or miniconda or miniforge already installed and it works for you: Just keep your installation.</li> <li>If you encounter problems with an existing installation, uninstall and install Miniforge as described below.</li> <li>If you do not yet have a Conda distribution installed, we recommend Miniforge.</li> </ul>"},{"location":"module1/toolbox_intro/conda.html#miniforge-installation","title":"Miniforge installation","text":"<p>Go to the Miniforge download website and download the installer fitting your operating system (Windows, Linux, Mac OS and) and architecture (most probably <code>x86_64</code> is the right one).</p> <ul> <li>On Windows just download and execute the installer manually (double-click the <code>.exe</code> file), follow the instructions on the screen and accept the default settings (recommendations). Avoid installing in a directory with special characters and spaces in the name.</li> <li>If you are on Linux or Mac Os, follow the procedure described here.</li> </ul>"},{"location":"module1/toolbox_intro/conda.html#manage-packages-and-environments","title":"Manage packages and environments","text":"<p>Open the Miniforge prompt (in Windows type \"miniforge prompt\" into the search bar, then add this app to the task bar for convenience). You will see a black window with a command line where you can use the standard commands for your system (e.g. Windows) plus the commands for Conda (and Mamba if you installed Miniforge).</p> <p>To see all your Python installations, type <code>where Python</code> and hit enter. Now you are probably shown multiple paths where Python is installed and this can easily cause confusion about which Python is actually used to run code or to install extensions (packages) for. Conda is made exactly to avoid this confusion and to easily maintain control over your Python installations and packages.</p> <p>What are Python packages? -  A Python package is a collection of modules, which, in turn, are essentially Python or C scripts that contain published functionality. There are Python packages for data input, data analysis, data visualization, etc. Each package offers a unique toolset and may have its own unique syntax. The Python Standard Library contains a general-purpose set of packages shipped with every Python installation. Many additional ('third-party' or 'external') packages have been published and can be installed as needed.</p> <p>Package management is useful because you may want to update a package for one of your projects, but keep it at the same version in other projects to ensure that they continue to run as expected. With Conda, we can manage packages in different environments, as an installation happens only in the currently active environment (indicated in brackets at the beginning of the prompt and indicated by an asterisk (*) when you list all environemnts whith <code>conda env list</code>). We can switch between environments with <code>conda activate &lt;your_env_name&gt;</code> Initially, there is only the 'base' environment but we can create others. When you install packages (or another Python version) for a specific project, it is recommended that you do this in a fresh environment (not in the base environment).</p> <p>When one of your environments becomes \u201cbroken\u201d or obsolete, you can simply delete it with <code>conda remove -n &lt;your_env_name&gt; --all</code>. This will delete the corresponding folder and all packages in it (Yes, an environment is just a folder on your system!).</p>"},{"location":"module1/toolbox_intro/conda.html#mamba","title":"Mamba","text":"<p>Solving the dependencies for complex environments with many different packages can take a long time (and sometimes fail) with conda. Mamba was developed to improve this. Mamba is a drop-in replacement and uses the same commands and configuration options as Conda, i.e., you can swap almost all commands between Conda and Mamba (see below).</p> <p>Miniforge has Mamba pre-installed in the base environment.</p> <p>If you have a Conda installed with another distribution (e.g. Anaconda), you can</p> <pre><code>conda install mamba -n base -c conda-forge\n</code></pre> <p>and then use mamba to install other packages.</p>"},{"location":"module1/toolbox_intro/conda.html#basic-use","title":"Basic use","text":"<p>Let's try this! Create a new environment called 'geopython' which contains an installation of Python 3.10.</p> <pre><code>mamba create --name geopython python=3.10\n</code></pre> <p>Activate this 'geopython' environment.</p> <pre><code>mamba activate geopython\n</code></pre> <p>Check which packages and versions are installed in this environment.</p> <pre><code>mamba list\n</code></pre> <p>Now install some packages to this active environment. Note that we only specify some packages but many others (such as pandas, numpy and scipy) are also installed automatically because they are required for the specified ones to work properly.</p> <pre><code>mamba install ipykernel geopandas rasterio xarray rioxarray\n</code></pre> <p>Check which environments we have. The active one is marked by an asterisk (*).</p> <pre><code>mamba env list\n</code></pre> <p>Just for testing, we could create another environment called 'geopython_v2' which is identical to the 'geopython' environment.</p> <pre><code>mamba create -n geopython_v2 --clone geopython\n</code></pre> <p>Delete the 'geopython_v2' environment and all packages in it.</p> <pre><code>mamba remove -n geopython_v2 --all\n</code></pre> <p>See also this Conda cheat sheet for a list of useful commands. Read more about Conda, packages and environments in the conda documentation.</p> <p>Unfortunately, not all packages are available from a conda channel (such as conda-forge). In this case (and only in this case) you should install the package either from the Python Package Index (PyPi) via <code>pip</code> within a Conda environment (see also the recommendations here). More on the differences between Conda and pip and why Conda is the recommended way is explained here.</p>"},{"location":"module1/toolbox_intro/conda.html#set-up-the-e-trainee-course-environment","title":"Set up the E-TRAINEE course environment","text":"<p>To set up an environment with Python and the packages required for the course, there are two options:</p> <p>Option 1: Use the YAML requirements file provided for the course. This file defines the environment with all required packages (versions fixed, builds not fixed for cross-platform compatibility). Just download the Module 1 requirements file from the E-TRAINEE GitHub, save it in your working directory and run this command:</p> <pre><code>conda env create -f etrainee_m1.yml --name etrainee_m1\n</code></pre> <p>You can also open the YAML file in a text editor and have a look at its content. Here, you find also requirements files for the other E-TRAINEE modules and one for the entire course.</p> <p>Option 2: Run the following commands (in the Conda/Miniforge prompt or in a VSCode terminal with Conda recognized):</p> <pre><code>mamba create -n etrainee_m1 python=3.10\nmamba activate etrainee_m1\nmamba install ipykernel earthengine-api eemont geemap pygis wxee scikit-learn stackstac xarrayutils hvplot datashader xmovie laspy vaex seaborn\n</code></pre> <p>Note: This will install &gt;480 packages, with a total download volume of &gt;780 MB.</p> <p>If you need to install additional packages, you can usually do this with <code>mamba install &lt;package_name&gt;</code>. If a specific package is not available on the Conda channels (e.g., conda-forge), you might have to use <code>pip install &lt;package_name&gt;</code> or install from source.</p>"},{"location":"module1/toolbox_intro/conda.html#conda-and-python-in-vscode","title":"Conda and Python in VSCode","text":""},{"location":"module1/toolbox_intro/conda.html#conda-environments-in-vscode","title":"Conda environments in VSCode","text":"<p>You can also use Conda in a terminal (prompt) in Visual Studio Code (instead of the Miniforge prompt) to manage packages and environments.  And, importantly, you will be able to select one of your environments as a 'kernel' which is used to run Python code. </p> <p>If Conda is not recognized in VSCode, open VSCode by entering <code>`code</code> in the Miniforge prompt. For other solutions see e.g. this article.</p>"},{"location":"module1/toolbox_intro/conda.html#creating-and-running-a-python-script","title":"Creating and running a Python script","text":"<p>In the upper left menu of VScode, go to File - New File ... to create a new Python file (.py) and save it in your working directory ('my_script.py'). Alternatively, use the shortcut CTRL + Shift + P\" to show and run commands for VSCode at the top of your screen and then type/select \"Python: New Python File\". Write a very simple script that prints \"Hello world!\".</p> <p>You have at least two options to control the environment/kernel used to run the script:</p> <ul> <li>In the VSCode terminal, activate the environment you want to use. Type the file name of the script into the terminal and hit enter (make sure you are in the same folder as this script or type also the path to the script).</li> <li><code>CTRL + Shift + P</code> and type <code>Python: Select Interpreter</code>. Run the script via the play button in the upper right. This opens an interactive window, where you can also choose the `kernel' among your Python installations (for the next run).</li> </ul> <p>Actually, the environment doesn't matter now for this simple script but as we are going to do more specific things we will need an environment with the right packages installed. More on Python and VSCode is explained here.</p>"},{"location":"module1/toolbox_intro/conda.html#next-jupyter-notebooks","title":"Next: Jupyter Notebooks","text":"<p>Continue with Jupyter Notebooks for interactive computing and workflow documentation here.</p>"},{"location":"module1/toolbox_intro/geopandas.html","title":"Vector data with GeoPandas","text":"In\u00a0[2]: Copied! <pre>from pathlib import Path\ndata_dir = Path(f'F:/OneDrive - uibk.ac.at/FE_WS2324/data')\n</pre> from pathlib import Path data_dir = Path(f'F:/OneDrive - uibk.ac.at/FE_WS2324/data') In\u00a0[3]: Copied! <pre>import geopandas as gpd\ntirol = gpd.read_file(data_dir / \"tirol\" / \"Tirol_BEV_VGD_250_LAM.shp\")\n</pre> import geopandas as gpd tirol = gpd.read_file(data_dir / \"tirol\" / \"Tirol_BEV_VGD_250_LAM.shp\") <p>Let's see how the first two features (lines in the table) of the <code>tirol</code> GeoDataFrame look like.</p> In\u00a0[4]: Copied! <pre>tirol.head(2)\n</pre> tirol.head(2) Out[4]: ST_KZ FL MERIDIAN GKZ BKZ FA_NR BL_KZ KG_NR KG PG PB FA GB_KZ GB VA_NR VA BL ST geometry 0 1 36336202 31 70733 707 82 7 85213 Untertilliach Untertilliach Lienz Kitzb\u00fchel Lienz 850 Lienz 85 Lienz Tirol \u00d6sterreich POLYGON ((351284.617 315719.819, 351918.758 31... 1 1 9147234 31 70713 707 82 7 85204 Hollbruck Kartitsch Lienz Kitzb\u00fchel Lienz 850 Lienz 85 Lienz Tirol \u00d6sterreich POLYGON ((333260.458 316083.047, 333446.094 31... In\u00a0[5]: Copied! <pre>tirol.plot(\"PB\", legend=True)\n</pre> tirol.plot(\"PB\", legend=True) Out[5]: <pre>&lt;Axes: &gt;</pre> <p>This simple plot allows us to quickly inspect the data and there are possibilities to improve it using <code>matplotlib</code> and other plotting packages. It shows coordinates as axis labels, but what is the coordinate reference system (CRS)?</p> In\u00a0[6]: Copied! <pre>print(tirol.crs)\nprint(\"\\n More detailed information on the CRS: \\n\")\ntirol.crs\n</pre> print(tirol.crs) print(\"\\n More detailed information on the CRS: \\n\") tirol.crs <pre>EPSG:31287\n\n More detailed information on the CRS: \n\n</pre> Out[6]: <pre>&lt;Projected CRS: EPSG:31287&gt;\nName: MGI / Austria Lambert\nAxis Info [cartesian]:\n- X[north]: Northing (metre)\n- Y[east]: Easting (metre)\nArea of Use:\n- name: Austria.\n- bounds: (9.53, 46.4, 17.17, 49.02)\nCoordinate Operation:\n- name: Austria Lambert\n- method: Lambert Conic Conformal (2SP)\nDatum: Militar-Geographische Institut\n- Ellipsoid: Bessel 1841\n- Prime Meridian: Greenwich</pre> In\u00a0[7]: Copied! <pre>tirol[\"area\"] = tirol.area/1000000  # Map units are metres (as shown in the CRS metadata), convert area from m\u00b2 to km\u00b2\n</pre> tirol[\"area\"] = tirol.area/1000000  # Map units are metres (as shown in the CRS metadata), convert area from m\u00b2 to km\u00b2 In\u00a0[8]: Copied! <pre>from matplotlib import pyplot as plt\ntirol.plot(\"area\", legend=True)\nplt.title('Area of municipalities of Tirol [km\u00b2]')\n</pre> from matplotlib import pyplot as plt tirol.plot(\"area\", legend=True) plt.title('Area of municipalities of Tirol [km\u00b2]') Out[8]: <pre>Text(0.5, 1.0, 'Area of municipalities of Tirol [km\u00b2]')</pre> In\u00a0[9]: Copied! <pre>ibk = tirol[tirol[\"PG\"] == \"Innsbruck\"]\n</pre> ibk = tirol[tirol[\"PG\"] == \"Innsbruck\"] In\u00a0[10]: Copied! <pre>ibk.explore()\n</pre> ibk.explore() Out[10]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook In\u00a0[11]: Copied! <pre>print(ibk.crs)      # Show only the EPSG code\nibk_utm = ibk.to_crs(\"EPSG:32632\")  # Re-project to EPSG:32632\nibk_utm.crs         # Show CRS information after re-projection\n</pre> print(ibk.crs)      # Show only the EPSG code ibk_utm = ibk.to_crs(\"EPSG:32632\")  # Re-project to EPSG:32632 ibk_utm.crs         # Show CRS information after re-projection <pre>EPSG:31287\n</pre> Out[11]: <pre>&lt;Projected CRS: EPSG:32632&gt;\nName: WGS 84 / UTM zone 32N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 6\u00b0E and 12\u00b0E, northern hemisphere between equator and 84\u00b0N, onshore and offshore. Algeria. Austria. Cameroon. Denmark. Equatorial Guinea. France. Gabon. Germany. Italy. Libya. Liechtenstein. Monaco. Netherlands. Niger. Nigeria. Norway. Sao Tome and Principe. Svalbard. Sweden. Switzerland. Tunisia. Vatican City State.\n- bounds: (6.0, 0.0, 12.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 32N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich</pre> In\u00a0[12]: Copied! <pre>ibk_utm.to_file(data_dir / \"tirol\" / \"innsbruck.gpkg\", driver='GPKG')\n</pre> ibk_utm.to_file(data_dir / \"tirol\" / \"innsbruck.gpkg\", driver='GPKG') In\u00a0[19]: Copied! <pre># Dissolve by districts ('PB' attribute) and sum up the area of their municipalities\ndistricts = tirol.dissolve(by='PB', aggfunc={'area': 'sum'})\ndistricts   # Show the GeoDataFrame\n</pre> # Dissolve by districts ('PB' attribute) and sum up the area of their municipalities districts = tirol.dissolve(by='PB', aggfunc={'area': 'sum'}) districts   # Show the GeoDataFrame Out[19]: geometry area PB Imst POLYGON ((206586.407 334773.410, 206168.211 33... 1723.983146 Innsbruck-Land POLYGON ((240943.882 343263.997, 240591.673 34... 1989.285665 Innsbruck-Stadt POLYGON ((255095.061 372623.401, 255169.732 37... 104.712177 Kitzb\u00fchel POLYGON ((315095.968 396867.394, 315132.915 39... 1161.741353 Kufstein POLYGON ((299074.104 381374.166, 298682.327 38... 969.229330 Landeck POLYGON ((193058.209 332835.454, 192867.887 33... 1594.594778 Lienz POLYGON ((339653.906 307289.062, 339367.216 30... 2019.128342 Reutte MULTIPOLYGON (((181326.148 373897.774, 180786.... 1235.533926 Schwaz POLYGON ((288645.345 346813.812, 288430.744 34... 1841.607396 In\u00a0[20]: Copied! <pre>from matplotlib import pyplot as plt\nimport contextily as cx\nfrom matplotlib_scalebar.scalebar import ScaleBar   # Needs to be installed (mamba install matplotlib-scalebar)\n\ndistricts_wm = districts.to_crs(epsg=3857)          # Convert the CRS of our data to match the CRS used typically by web map tiles (Web Mercator)\nax = districts_wm.plot(\n    column='area',                          # Colorize the polygons by values in this column\n    cmap='cividis',                         # Select a colourmap\n    legend_kwds={'label': 'Area [km\u00b2]'},\n    legend=True,\n    alpha=0.8);\nax.set_title('Size of districts in Tirol')\ncx.add_basemap(ax)                          # Add a basemap from an xyz tiles provider (default is OSM)\nax.add_artist(ScaleBar(dx=1))               # Add a scalebar\n</pre> from matplotlib import pyplot as plt import contextily as cx from matplotlib_scalebar.scalebar import ScaleBar   # Needs to be installed (mamba install matplotlib-scalebar)  districts_wm = districts.to_crs(epsg=3857)          # Convert the CRS of our data to match the CRS used typically by web map tiles (Web Mercator) ax = districts_wm.plot(     column='area',                          # Colorize the polygons by values in this column     cmap='cividis',                         # Select a colourmap     legend_kwds={'label': 'Area [km\u00b2]'},     legend=True,     alpha=0.8); ax.set_title('Size of districts in Tirol') cx.add_basemap(ax)                          # Add a basemap from an xyz tiles provider (default is OSM) ax.add_artist(ScaleBar(dx=1))               # Add a scalebar Out[20]: <pre>&lt;matplotlib_scalebar.scalebar.ScaleBar at 0x22a54125ab0&gt;</pre>"},{"location":"module1/toolbox_intro/geopandas.html#vector-data-geopandas","title":"Vector data: Geopandas\u00b6","text":"<p>Geopandas is the primary package we use for geographic vector data. It extends the data types used by <code>pandas</code> to allow spatial operations on geometric types (performed by the <code>shapely</code> package internally).</p> <p>We use one of the administrative boundaries files (provided by the office of the government of Land Tirol in Austria under CC BY 4.0 Deed license), more specifically we download this file and unzip it into a sub-directory \"tirol\" within our data directory.</p> <p>We use the pathlib package to set the path to our data directory (where we store sample data) as a variable <code>data_dir</code>.</p>"},{"location":"module1/toolbox_intro/geopandas.html#reading-vector-data","title":"Reading vector data\u00b6","text":"<p>Let's read one of the three Shapefiles contained in the directory into a <code>geopandas.GeoDataFrame</code>. Almost any vector-based spatial data format containing both geometry and attribute data (such as GeoJSON or Geopackage) is recognizesd by <code>geopandas.read_file()</code> and would work in the same way.</p>"},{"location":"module1/toolbox_intro/geopandas.html#simple-maps","title":"Simple maps\u00b6","text":"<p>In the table we have a number of attributes (with many of the abbreviated attribute names being difficult to interpret) and, importantly, we have a geometry column on the very right end of the table. Let's plot these geometries in a map and colorize the polygons by the \"PB\" attribute (the B stands probably for \"Bezirk\", which is a regional-level Austrian administrative unit (~district)).</p>"},{"location":"module1/toolbox_intro/geopandas.html#geometric-operations-on-vector-data","title":"Geometric operations on vector data\u00b6","text":"<p>We can also use geopandas to compute the area of the polygons and store it in a new attibute column called \"area\".</p>"},{"location":"module1/toolbox_intro/geopandas.html#subsets","title":"Subsets\u00b6","text":"<p>For a new GeoDataFrame called \"ibk\", let's select a subset of the \"tirol\" GeoDataFrame containing only the municipal area of the city of Innsbruck. We do this by a conditional expression on the municipality column (which is called \"PG\"). If you want to make a subset already during import of a vector dataset, look up the selection methods here.</p>"},{"location":"module1/toolbox_intro/geopandas.html#interactive-maps","title":"Interactive maps\u00b6","text":"<p>To have get a better view of the data, including a web-based background layer and the possibilit to zoom and pan, we use Geopandas' built-in method <code>GeoDataFrame.explore()</code> to create an interactive Leaflet map. This works via the Folium library internally, which has a number of built-in tilesets from OpenStreetMap, Mapbox, etc, and supports custom tilesets. Vector attributes are shown on mouse-over. Later in the course we will customize such maps a bit.</p>"},{"location":"module1/toolbox_intro/geopandas.html#re-projection","title":"Re-projection\u00b6","text":"<p>Our data is in a national CRS, which is often incompatible with the rest of our data (acquired from global scale satellite data archives or by GNSS measurements in the field, for instance) which is typically in a different CRS. But don't worry, we can easily re-project the geometries of our data to another CRS, thus making it compatible for joint visualization and analysis with other data such as satellite imagery. The easiest way to define a CRS is via an EPSG code (which can be looked up at https://epsg.io/).</p>"},{"location":"module1/toolbox_intro/geopandas.html#writing-vector-data","title":"Writing vector data\u00b6","text":"<p>To write a GeoDataFrame back to file use <code>GeoDataFrame.to_file()</code>. The default file format is Shapefile, but you can specify your own with the driver keyword.</p>"},{"location":"module1/toolbox_intro/geopandas.html#dissolve-polygons","title":"Dissolve polygons\u00b6","text":"<p>If we want to do an analysis at a coarser level, e.g., for districts, the municipality polygons can be dissolved by the district column ('PB') and geopandas can, e.g., sum up the area of all municipalities in a district (more aggregation options are available, such as mean, minimum or maximum, ...; see the documentation).</p>"},{"location":"module1/toolbox_intro/geopandas.html#customize-maps","title":"Customize maps\u00b6","text":"<p>Plot the district polygons and customize the map a bit (scalebar, colormap, ...), now using <code>matplotlib</code>'s object-oriented interface where the plot of the districts is an axes object. Adding a scalebar is easy with the matplotlib-scalebar package, if we are using a projected coordinate system with meters as units: Just set <code>dx = 1</code>. With the <code>contextily</code> pacakge we can add a basemap from an xyz tiles provider to the axes object.</p>"},{"location":"module1/toolbox_intro/geopandas.html#more-vector-data-processing","title":"More vector data processing\u00b6","text":"<p>Much more can be done with GeoPandas, such as spatial joins, buffers and spatial intersections.</p>"},{"location":"module1/toolbox_intro/jupyter.html","title":"Jupyter Notebooks","text":"In\u00a0[\u00a0]: Copied! <pre>print(\"Hello world!\")\n</pre> print(\"Hello world!\") <p>Try skipping this cell and run the next one first. What happens?</p> In\u00a0[\u00a0]: Copied! <pre>my_string = \"Hello world!\"      # don't run this cell, run the next one first\n</pre> my_string = \"Hello world!\"      # don't run this cell, run the next one first In\u00a0[\u00a0]: Copied! <pre>print(my_string)                # this only works if we have run the cell above already (try)\n</pre> print(my_string)                # this only works if we have run the cell above already (try) In\u00a0[\u00a0]: Copied! <pre>for n in range(3):\n    print(\"Hello world!\")\n    print(f\"This is iteration number {n}\")\nprint(\"Finished!\")\n</pre> for n in range(3):     print(\"Hello world!\")     print(f\"This is iteration number {n}\") print(\"Finished!\") <p>This example shows several things:</p> <ul> <li>The colon <code>:</code> denotes the beginning of a definition (here of the repeated code under the for loop).</li> <li>Python defaults to counting from 0 rather than from 1.</li> <li>Function calls in Python always use parentheses: <code>print()</code></li> <li>Code blocks are identified through indentations.</li> <li>We can supply a variable to a string, in this case we supply the current value of <code>n</code> to an f-string (a string preceeded by the letter \"f\").</li> </ul> <p>For logical decisions, Python has <code>if</code> statements.</p> In\u00a0[\u00a0]: Copied! <pre>if n &gt; 2:\n    print(\"n is greater than 2!\")\nelse:\n    print(\"n is not greater than 2!\")\n</pre> if n &gt; 2:     print(\"n is greater than 2!\") else:     print(\"n is not greater than 2!\") <p>The <code>while</code> statement can be used to repeat code as long as a condition is met (conditional loop).</p> In\u00a0[\u00a0]: Copied! <pre>m = 0\nwhile m &lt; 3:\n    print(f\"This is iteration number {m}.\")\n    m += 1\nprint(\"Now the condition is no longer met.\")\n</pre> m = 0 while m &lt; 3:     print(f\"This is iteration number {m}.\")     m += 1 print(\"Now the condition is no longer met.\") In\u00a0[8]: Copied! <pre>import pandas\n</pre> import pandas <p>Now, we can access any function from the package, by typing pandas.*. For example, create a Series object (a labelled 1-dimensional array):</p> In\u00a0[\u00a0]: Copied! <pre>pandas.Series([1, 3, 5])\n</pre> pandas.Series([1, 3, 5]) <p>For many packages it is common practice to import a package under a different name which is shorter to type, for example:</p> In\u00a0[10]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd <p>We can now access all of the functions in pandas using <code>pd.*</code>, instead of <code>pandas.*</code>. Let's try working on the series.</p> In\u00a0[\u00a0]: Copied! <pre>a = pd.Series([1, 3, 5])\nb = 3                     # What is different if b = 3.0 (instead of 3)?\nc = a + b\nc\n</pre> a = pd.Series([1, 3, 5]) b = 3                     # What is different if b = 3.0 (instead of 3)? c = a + b c <p>While importing packages or their submodules under a short name is somehow a matter of taste, you should maybe not do this too excessively. Sometimes it is preferable to have a longer but also more informative line of code (you won't remember all packages and submodules imported in the beginning of a long script). For widely used packages, however, it is more or less a convention to abbreviate them and you will also get used to these short names. Examples used in the course are:</p> <pre>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport xarray as xr\nimport seaborn as sns\nimport matplotlib.pyplot as plt         # Matplotlib's pyplot interface (a submodule of thematplotlib package)\nfrom matplotlib import pyplot as plt    # Alternative way to import only the pyplot submodule\n</pre> In\u00a0[6]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import matplotlib.pyplot as plt In\u00a0[23]: Copied! <pre>theta = np.linspace(0 , 20, 100)            # Create some data\nsintheta = np.sin(theta)\n\nplt.plot(theta, sintheta, label='y = sin(x)', color='purple')\nplt.grid()\nplt.legend()\nplt.xlabel('Theta')\nplt.ylabel('Sintheta')\nplt.title('This is a great title', fontsize=14)\n</pre> theta = np.linspace(0 , 20, 100)            # Create some data sintheta = np.sin(theta)  plt.plot(theta, sintheta, label='y = sin(x)', color='purple') plt.grid() plt.legend() plt.xlabel('Theta') plt.ylabel('Sintheta') plt.title('This is a great title', fontsize=14) Out[23]: <pre>Text(0.5, 1.0, 'This is a great title')</pre> <p>For this plot we used the functional (<code>pyplot</code>) interface of <code>matplotlib</code>. Various function calls add elements (like a title or a legend) to the same figure and or modify this figure. While we can create rather simple plots quickly and easily with this approach, the possibilities for customization and complexity in a figure are somehow limited.</p> <p>The object-oriented interface of <code>matplotlib</code> provides more advanced options and is suited to generate even very complex and specific figures. If you want to learn more about the two different plotting approaches of <code>matplotlib</code> read the documentation or the blog posts here and here. In the course we will use also other, more specific plotting libraries (e.g., to visualize geographic data or for statistical visualizations) which are built on top of <code>matplotlib</code>.</p> <p>Now you should be able to use Jupyter Notebooks for interactive Python coding. For more detailed information on Jupyter Notebooks please see the user documentation. For a quickstart to geographic data in Python continue here.</p>"},{"location":"module1/toolbox_intro/jupyter.html#jupyter-notebooks-and-a-few-python-basics","title":"Jupyter Notebooks and a few python basics\u00b6","text":"<p>Here, you can learn how to use a Jupyter Notebook for interactive computing and for documenting a workflow.</p> <p>At this opportunity we will also have a look at a few selected Python basics.</p>"},{"location":"module1/toolbox_intro/jupyter.html#jupyter-notebooks","title":"Jupyter Notebooks\u00b6","text":"<p>Program code can be run by executing \"normal\" scripts (e.g. <code>*.py</code> files), as we did before. Especially for experimenting or learning, a great alternative is to run pieces of code interactively within Jupyter Notebooks. The Jupyter Notebook format (<code>*.ipynb</code>, short for \u201cIPython notebook\u201d) can contain code cells along with text explanations as well as graphics and other output in between. This makes the Notebooks also a great format for documenting your work and for explaining it to others. The E-TRAINEE course and many other tutorials make extensive use of such Notebooks.</p> <p>Although Jupyter Notebooks are plain text files, it makes not much sense to view or edit them in a normal plain text editor (in contrast to <code>*.py</code> files for instance). They are made to be edited and run in dedicated web-based or desktop applications that can process and display them properly, such as JupyterLab, JupyterHub, Jupyter Desktop, or Visual Studio Code. We suggest the latter, amongst others because VSCode is also a good editor for \"normal\" Python scripts.</p>"},{"location":"module1/toolbox_intro/jupyter.html#create-a-new-notebook","title":"Create a new Notebook\u00b6","text":"<p>In VSCode, use the shortcut <code>CTRL + Shift + P</code> to show and run commands for VSCode at the top of your screen and then type/select \"Create: New Jupyter Notebook\". Don't forget to save your Notebook if you want to continue with it.</p> <p>There are two different \"modes\" when you are in Jupyter Notebook: Navigation mode and cell editing mode. You can switch between the two modes with Enter and Esc, respectively. A key feature of Notebooks is that they are usually divided into different cells.</p>"},{"location":"module1/toolbox_intro/jupyter.html#code-cells","title":"Code cells\u00b6","text":"<p>As the name suggests, code cells are used to edit and write new code in a programming language such as Python, Julia or R. When a code cell is executed, its code is sent to the kernel associated with the Notebook. If the kernel can process it (right kernel for the language, required packages installed, no bugs in the code, ...), the results that are returned from that computation are displayed in the Notebook as the cell's output. Such output can be text, tables or figures.</p> <p>Your new Notebook will contain one first code cell and look more or less like this:</p> <p>There are (amongst others) options to</p> <ul> <li>Select the Kernel in the upper right corner</li> <li>Add a code cell or Add a Markdown cell in the upper left corner (appears also below/between existing cells in the document if you are pointing there with your mouse)</li> <li>Run a single code cell (play button left of the cell) or run all code cells (play button above the document)</li> <li>Select the language mode of the cell in the bottom right corner (default is Markdown or Python)</li> </ul> <p>If you click into a cell you get additional options, such as for rendering a Markdown cell (stop editing; hook in the upper right corner of the cell), for running code, or to delete the cell.</p>"},{"location":"module1/toolbox_intro/jupyter.html#text-cells","title":"Text cells\u00b6","text":"<p>For writing descriptive text, we can insert Markdown cells. Markdown is a simple and easy-to-use markup language that allows us to format these text cells (e.g., with headings, emphasis, lists of bullet points, etc.). Actuallly, the E-TRAINEE course documents are more or less based on Jupyter Notebooks and pure Markdown documents (with some HTML syntax for more advanced formatting in between).</p> <p>Try to create a simple example with a few code and text cells! Have a short look at this guide to the Markdown syntax and try to create text with two heading levels and parts of the text highlighted in bold or italic.</p>"},{"location":"module1/toolbox_intro/jupyter.html#open-an-existing-notebook","title":"Open an existing Notebook\u00b6","text":"<p>What you are viewing here (if you are on the course website) is actually a Jupyter Notebook converted to HTML and rendered in your browser. To be able to use it interactively (modify and execute cells), you have to download it as <code>*.ipynb</code> file from GitHub (download button at the upper right of this Notebook) and open it in VSCode (or JupiterLab).</p>"},{"location":"module1/toolbox_intro/jupyter.html#try-some-interactive-python","title":"Try some interactive Python\u00b6","text":"<p>Once you have downloaded this Notebook, you can use it interactively, that means run code cells, see their output, modify the code (if you want) and run it again to see what changes. Let's try this!</p> <p>Along this way we learn or repeat also some Python basics. However, this is by far not a comprehensive crash course and if you are new to Python or need a more thorough refresher of your coding skills, please have a look at the official Python tutorial.</p>"},{"location":"module1/toolbox_intro/jupyter.html#print-messages","title":"Print messages\u00b6","text":""},{"location":"module1/toolbox_intro/jupyter.html#loops-and-flow-control","title":"Loops and flow control\u00b6","text":"<p>For repeated execution of a task, <code>for</code> loops are important.</p>"},{"location":"module1/toolbox_intro/jupyter.html#loading-packages","title":"Loading packages\u00b6","text":"<p>To make the functions defined in an installed Python package available to us, we must load the package using the <code>import</code> keyword. Let's load the <code>pandas</code> package, which is popular for working with tabular data.</p>"},{"location":"module1/toolbox_intro/jupyter.html#some-basic-plotting-with-matplotlib","title":"Some basic plotting with Matplotlib\u00b6","text":"<p>A nice feature of Jupyter Notebooks is that they can contain also graphical output, such as figures generated by the <code>matplotlib</code> library.</p>"},{"location":"module1/toolbox_intro/rasterio.html","title":"Raster data with rasterio","text":"In\u00a0[51]: Copied! <pre>import rasterio\nfrom pathlib import Path\ndata_dir = Path('F:/OneDrive - uibk.ac.at/FE_WS2324/data')      # Set your local path to the data\nb04 = rasterio.open(data_dir / 'S2' / 'T32TPT_20201122T102339_B04_10m.jp2')     # This is the red band\nb08 = rasterio.open(data_dir / 'S2' / 'T32TPT_20201122T102339_B08_10m.jp2')     # This is the NIR band\n</pre> import rasterio from pathlib import Path data_dir = Path('F:/OneDrive - uibk.ac.at/FE_WS2324/data')      # Set your local path to the data b04 = rasterio.open(data_dir / 'S2' / 'T32TPT_20201122T102339_B04_10m.jp2')     # This is the red band b08 = rasterio.open(data_dir / 'S2' / 'T32TPT_20201122T102339_B08_10m.jp2')     # This is the NIR band <p>Once <code>rasterio.open()</code> has established a file connection, we can access two types of data from the of the opened dataset object:</p> <ul> <li>the <code>numpy</code> array containing the raster values (in a single band raster one value per pixel (raster cell))</li> <li>and a dictionary with the metadata.</li> </ul> <p>Have a look at georeferencing attributes and other metadata stored with the Band 4 raster, namely</p> <ul> <li>'Geotransform': Affine transformation matrix that maps pixel locations from image space ((col, row) coordinates) to georeferenced (projected or geographic) coordinates (basically you can see the spatial resolution/pixel size and the upper left corner coordinates, more details here)</li> <li>Coordinate reference system (CRS) where spatial positions relate to</li> <li>Shape and bounding box</li> <li>Number of bands in the image</li> <li>Data type used to store the data</li> </ul> In\u00a0[20]: Copied! <pre>print(b04.transform)    # Affine transformation matrix\nprint(b04.crs)          # Coordinate reference system EPSG code\nprint(b04.shape)        # Shape of b04 (number of pixels in both dimensions)\nprint(b04.bounds)       # Bounding box\nprint(b04.count)        # Band count\nprint(b04.dtypes)       # Data type\n</pre> print(b04.transform)    # Affine transformation matrix print(b04.crs)          # Coordinate reference system EPSG code print(b04.shape)        # Shape of b04 (number of pixels in both dimensions) print(b04.bounds)       # Bounding box print(b04.count)        # Band count print(b04.dtypes)       # Data type <pre>| 10.00, 0.00, 600000.00|\n| 0.00,-10.00, 5300040.00|\n| 0.00, 0.00, 1.00|\nEPSG:32632\n1\n(10980, 10980)\nBoundingBox(left=600000.0, bottom=5190240.0, right=709800.0, top=5300040.0)\n('uint16',)\n</pre> <p>... or simply print all the metadata.</p> In\u00a0[21]: Copied! <pre>print(b04.meta)\n</pre> print(b04.meta) <pre>{'driver': 'JP2OpenJPEG', 'dtype': 'uint16', 'nodata': None, 'width': 10980, 'height': 10980, 'count': 1, 'crs': CRS.from_epsg(32632), 'transform': Affine(10.0, 0.0, 600000.0,\n       0.0, -10.0, 5300040.0)}\n</pre> <p>Read the bands, which returns a <code>numpy.ndarray</code>. As seen in the metadata, this contains unsigned integer values but we want the pixel values as float, so that the NDVI computed in a later step will be float as well. Therefore, we read the bands data and convert the data type. We tell rasterio to read the band 1 in this raster file although this is actually not needed here because it's a single band file.</p> <p>Why do the bands contain integers - reflectance should be between 0 and 1? That's because the values have been scaled by a factor of 10000 to store the same information in smaller files.</p> In\u00a0[22]: Copied! <pre>a04 = b04.read(1).astype(float)\na08 = b08.read(1).astype(float)\n</pre> a04 = b04.read(1).astype(float) a08 = b08.read(1).astype(float) <p>For the bands opened as numpy arrays, we can perform various array operations, for instance calculate basic statistics.</p> In\u00a0[23]: Copied! <pre>print(a04.min())\nprint(a04.max())\nprint(a04.mean())\n</pre> print(a04.min()) print(a04.max()) print(a04.mean()) <pre>0.0\n16080.0\n1473.3225049269909\n</pre> In\u00a0[24]: Copied! <pre>from rasterio.plot import show\nshow(b04, title='Band 4')\n</pre> from rasterio.plot import show show(b04, title='Band 4') Out[24]: <pre>&lt;Axes: title={'center': 'Band 4'}&gt;</pre> In\u00a0[41]: Copied! <pre>ndvi = (a08-a04)/(a08+a04)\n</pre> ndvi = (a08-a04)/(a08+a04) <pre>C:\\Users\\Andi\\AppData\\Local\\Temp\\ipykernel_2500\\2912796907.py:1: RuntimeWarning: invalid value encountered in divide\n  ndvi = (a08-a04)/(a08+a04)\n</pre> In\u00a0[53]: Copied! <pre>with rasterio.open(\n    data_dir / 'S2' / 'ndvi.tif',       # output file\n    mode=\"w\",                           # write mode\n    driver=\"GTiff\",                     # Specify the driver (here GeoTiff)\n    height=b04.shape[0],                # number of rows of the raster\n    width=b04.shape[1],                 # number of columns of the raster\n    count=1,                            # a count of the raster bands (NDVI is only one band)\n    dtype=a04.dtype,                    # data type (same as for a04, i.e. the float version of the array)\n    crs=b04.crs,                        # write the same CRS as one of the source bands has\n    transform=b04.transform             # write the same transform as one of the source bands has\n) as new_dataset:\n    new_dataset.write(ndvi, 1)           # write NDVI data to band at location 1\n</pre> with rasterio.open(     data_dir / 'S2' / 'ndvi.tif',       # output file     mode=\"w\",                           # write mode     driver=\"GTiff\",                     # Specify the driver (here GeoTiff)     height=b04.shape[0],                # number of rows of the raster     width=b04.shape[1],                 # number of columns of the raster     count=1,                            # a count of the raster bands (NDVI is only one band)     dtype=a04.dtype,                    # data type (same as for a04, i.e. the float version of the array)     crs=b04.crs,                        # write the same CRS as one of the source bands has     transform=b04.transform             # write the same transform as one of the source bands has ) as new_dataset:     new_dataset.write(ndvi, 1)           # write NDVI data to band at location 1 In\u00a0[49]: Copied! <pre># Band 4\nprint(b04.closed)\nb04.close()\nprint(b04.closed)\n\n# Band 8\nprint(b08.closed)\nb08.close()\nprint(b08.closed)\n</pre> # Band 4 print(b04.closed) b04.close() print(b04.closed)  # Band 8 print(b08.closed) b08.close() print(b08.closed) <pre>False\nTrue\nFalse\nTrue\n</pre> In\u00a0[61]: Copied! <pre>import matplotlib.pyplot as plt\nwith rasterio.open(data_dir / 'S2' / 'ndvi.tif') as src:\n    plt.imshow(src.read(1))\n</pre> import matplotlib.pyplot as plt with rasterio.open(data_dir / 'S2' / 'ndvi.tif') as src:     plt.imshow(src.read(1)) <p>... and with rasterio's built-in plotting methods. What's the difference?</p> In\u00a0[62]: Copied! <pre>from rasterio.plot import show\nwith rasterio.open(data_dir / 'S2' / 'ndvi.tif') as src:\n    show(src.read(), transform=src.transform)       # Use the transform attribute to get CRS coordinates.\n</pre> from rasterio.plot import show with rasterio.open(data_dir / 'S2' / 'ndvi.tif') as src:     show(src.read(), transform=src.transform)       # Use the transform attribute to get CRS coordinates. <p>An advantage of rasterio's <code>plot()</code> is that we can use the transform attribute to easily get CRS coordinates. Unfortunately, it does not support a colorbar - but matplotlib's <code>imshow()</code> does so. To get both the proper coordinates and the colorbar, we use a trick: Plot with the two methods on the same axes.</p> <p>We can also try another one of the pre-defined colormaps. And we plot a histogram in the same figure, next to the NDVI map.</p> In\u00a0[88]: Copied! <pre>from rasterio.plot import show_hist\n\nwith rasterio.open(data_dir / 'S2' / 'ndvi.tif') as src:\n\n    # Read the raster data\n    data = src.read(1)\n    \n    # Create a figure with two subplots (axes)\n    fig, ax = plt.subplots(1, 2, figsize=(10,4))\n\n    # Use matplotlib's imshow() to plot the data as image to the first subplot, define a named colormap\n    image = ax[0].imshow(data, cmap='PRGn')                 # Only needed for the colorbar\n    fig.colorbar(image, label='NDVI', ax=ax[0])             # Add a colorbar to the image in the first subplot\n\n    # Use rasterio's show() to plot the data as image to the first subplot AGAIN, define the geotransform\n    show(data, cmap='PRGn', transform=src.transform, ax=ax[0])\n    \n    # Plot the data as a histogram to the second subplot\n    show_hist(src, bins=50, label='NDVI', ax=ax[1])\n    \n    # Make it look nicer (prevent that labels of different axes overlap each other)\n    fig.tight_layout()\n</pre> from rasterio.plot import show_hist  with rasterio.open(data_dir / 'S2' / 'ndvi.tif') as src:      # Read the raster data     data = src.read(1)          # Create a figure with two subplots (axes)     fig, ax = plt.subplots(1, 2, figsize=(10,4))      # Use matplotlib's imshow() to plot the data as image to the first subplot, define a named colormap     image = ax[0].imshow(data, cmap='PRGn')                 # Only needed for the colorbar     fig.colorbar(image, label='NDVI', ax=ax[0])             # Add a colorbar to the image in the first subplot      # Use rasterio's show() to plot the data as image to the first subplot AGAIN, define the geotransform     show(data, cmap='PRGn', transform=src.transform, ax=ax[0])          # Plot the data as a histogram to the second subplot     show_hist(src, bins=50, label='NDVI', ax=ax[1])          # Make it look nicer (prevent that labels of different axes overlap each other)     fig.tight_layout() In\u00a0[218]: Copied! <pre>import numpy as np\n\nin_file = data_dir / 'S2' / '20180726T102019_20180726T102150_T32TPS.tif' # Specify the path and the filename of an image\n\n# Define what parts of the value distribution are clipped\nlower_pct = 2       # The smallest percentile that will be clipped (try e.g. 10)\nupper_pct = 98      # The largest percentile that will be clipped (try e.g. 99)\n\n# Define a percentile clip function\ndef pct_clip(array,pct=[lower_pct,upper_pct]):\n    array_min, array_max = np.nanpercentile(array,pct[0]), np.nanpercentile(array,pct[1])\n    clip = (array - array_min) / (array_max - array_min)\n    clip[clip&gt;1]=1\n    clip[clip&lt;0]=0\n    return clip\n\nwith rasterio.open(in_file) as src:\n\n    # Read RGB bands, convert to float, apply scale factor, then apply percentile clip\n    red = pct_clip(src.read(3).astype(float)/10000)\n    green = pct_clip(src.read(2).astype(float)/10000)\n    blue = pct_clip(src.read(1).astype(float)/10000)\n\n    # Copy metadata of source file to use as destination file metadata\n    dst_meta = src.meta.copy()\n    dst_meta.update({'count': 3, 'dtype': 'float32'})   # Update only the band count and the data type\n\n    # Write the RGB bands to a 3-band GeoTiff\n    with rasterio.open(\n        data_dir / 's2' / 'rgb.tif',\n        'w',\n        **dst_meta\n        ) as dst:\n        dst.write(red, 1)       # Write red band raster to first band of the file\n        dst.write(green, 2)     # Write green band raster to second band of the file\n        dst.write(blue, 3)      # Write blue band raster to third band of the file\n</pre> import numpy as np  in_file = data_dir / 'S2' / '20180726T102019_20180726T102150_T32TPS.tif' # Specify the path and the filename of an image  # Define what parts of the value distribution are clipped lower_pct = 2       # The smallest percentile that will be clipped (try e.g. 10) upper_pct = 98      # The largest percentile that will be clipped (try e.g. 99)  # Define a percentile clip function def pct_clip(array,pct=[lower_pct,upper_pct]):     array_min, array_max = np.nanpercentile(array,pct[0]), np.nanpercentile(array,pct[1])     clip = (array - array_min) / (array_max - array_min)     clip[clip&gt;1]=1     clip[clip&lt;0]=0     return clip  with rasterio.open(in_file) as src:      # Read RGB bands, convert to float, apply scale factor, then apply percentile clip     red = pct_clip(src.read(3).astype(float)/10000)     green = pct_clip(src.read(2).astype(float)/10000)     blue = pct_clip(src.read(1).astype(float)/10000)      # Copy metadata of source file to use as destination file metadata     dst_meta = src.meta.copy()     dst_meta.update({'count': 3, 'dtype': 'float32'})   # Update only the band count and the data type      # Write the RGB bands to a 3-band GeoTiff     with rasterio.open(         data_dir / 's2' / 'rgb.tif',         'w',         **dst_meta         ) as dst:         dst.write(red, 1)       # Write red band raster to first band of the file         dst.write(green, 2)     # Write green band raster to second band of the file         dst.write(blue, 3)      # Write blue band raster to third band of the file <p>Now we can read this RGB image and show it as true-color image. Black areas are NoData (the result of cloud masking). The image shows a small area around the village of Obergurgl (Austria).</p> In\u00a0[202]: Copied! <pre>with rasterio.open(data_dir / 's2' / 'rgb.tif') as src:\n    show(src.read(), transform=src.transform)\n</pre> with rasterio.open(data_dir / 's2' / 'rgb.tif') as src:     show(src.read(), transform=src.transform) <p>And finally we plot the histogram fro the three bands.</p> In\u00a0[208]: Copied! <pre>with rasterio.open(data_dir / 's2' / 'rgb.tif') as src:\n    show_hist(src, bins=50, histtype='stepfilled', lw=0.0, stacked=False, alpha=0.3, label=('red','green','blue'))\n</pre> with rasterio.open(data_dir / 's2' / 'rgb.tif') as src:     show_hist(src, bins=50, histtype='stepfilled', lw=0.0, stacked=False, alpha=0.3, label=('red','green','blue')) <p>We have seen that rasterio can handle files with multiple bands and we can flexibly work with these bands. However, this comes with some complexity of the required code and when we introduce time as an additional dimension it becomes rather impractical. In the next tutorials we explore how the <code>xarray</code> package helps with multiple bands and then even with multispectral time series.</p>"},{"location":"module1/toolbox_intro/rasterio.html#raster-data-handling-with-rasterio","title":"Raster data handling with rasterio\u00b6","text":"<p>In this Notebook we open and explore two spectral bands from the Sentinel-2 satellite, calculate the NDVI and save the NDVI raster to a GeoTiff. Along this way, we touch a couple of issues related to raster handling using the rasterio package. Towards the end of the Notebook, we have a look at how files with multiple bands can be handled with rasterio.</p>"},{"location":"module1/toolbox_intro/rasterio.html#read-sentinel-2-bands","title":"Read Sentinel-2 bands\u00b6","text":"<p>Define the bands to open with rasterio. Rasterio's <code>open()</code> function returns an opened dataset object.</p>"},{"location":"module1/toolbox_intro/rasterio.html#plotting","title":"Plotting\u00b6","text":"<p>Plot one band using rasterio's built-in <code>show()</code> method.</p>"},{"location":"module1/toolbox_intro/rasterio.html#calculate-the-ndvi","title":"Calculate the NDVI\u00b6","text":"<p>Now we calculate the normalized difference vegetation index (NDVI) from the float bands.</p>"},{"location":"module1/toolbox_intro/rasterio.html#write-the-ndvi-to-a-geotiff","title":"Write the NDVI to a GeoTiff\u00b6","text":""},{"location":"module1/toolbox_intro/rasterio.html#close-datasets","title":"Close datasets\u00b6","text":"<p>As Python <code>file</code> objects can, Rasterio datasets can manage the entry into and exit from runtime contexts created using a <code>with</code> statement. This initiates a context manager that ensures that files are automatically closed after use, no matter what exceptions may be raised within the block. We have used such a context manager in the previous cell for writing the NDVI raster to a file. For the input datasets we did not do this, so they are still open and we should close them (read here why).</p>"},{"location":"module1/toolbox_intro/rasterio.html#plotting-raster-data","title":"Plotting raster data\u00b6","text":"<p>Try plotting the NDVI raster with matplotlib ...</p>"},{"location":"module1/toolbox_intro/rasterio.html#imagery-with-multiple-bands","title":"Imagery with multiple bands\u00b6","text":"<p>So far we have worked on single band rasters for reading, writing and plotting. Image file formats, however, can store multiple spectral bands in one file. In the following cell we read a small subset from a Sentinel-2 satellite image. Clouds and cloud shadows have been masked, i.e. they are set to zero (the NoData value of this image). Six of the bands available in Sentinel-2 datasets are stored in a GeoTiff, the first three bands are blue, green and red.</p> <p>Rasterio can show color composites but for this we need to write the the bands that we want to display into a new 3-band file with values ranging from 0 to 1. To enhance the contrast in the image, we apply a percentile clip (you may experiment with these clip values and see how this effects the plot in the following cells!).</p>"},{"location":"module1/toolbox_intro/vscode.html","title":"Visual Studio Code","text":"<p>To write, modify and execute Python code, we use Visual Studio Code (VSCode).</p>"},{"location":"module1/toolbox_intro/vscode.html#what-is-it","title":"What is it?","text":"<p>VSCode is a popular source-code editor by Microsoft. It is available for Windows, Linux and macOS and comes with the following features (and more):</p> <ul> <li>Support for many popular programming and markup languages: C++, Fortran, JavaScript, Python, Julia, Markdown, HTML, ... (built-in or via extensions)</li> <li>Syntax highlighting, bracket matching, code folding</li> <li>Intelligent code completion (auto-completion popups while typing)</li> <li>Support for debugging</li> <li>Version control (needs link to a version control system such as Git)</li> <li>Possibilities to change the theme, keyboard shortcuts, preferences, and install extensions that add functionality</li> <li>Telemetry: Collects usage data and sends it to Microsoft (can be disabled)</li> </ul> <p>Is it open-source?</p> <p>The source code for VSCode is fully open-source (and released under a MIT license), but the binaries that you download from Microsoft are not. With VSCodium you could also download a truly open-source binary distribution of VSCode, which is freely licensed and has telemetry disabled. This is basically the same as but more convenient than building from source by yourself.</p>"},{"location":"module1/toolbox_intro/vscode.html#installation","title":"Installation","text":"<p>Download and execute the installer for VSCode from here.</p> <p>Open VSCode and install the Python and Jupyter extensions for VSCode, see here.</p>"},{"location":"module1/toolbox_intro/vscode.html#command-line-essentials","title":"Command line essentials","text":"<p>In VSCode, there is a command line (\"terminal\", \"prompt\", \"shell\"), usually at the bottom. This can be a Windows PowerShell or the \"classic\" cmd command prompt or \"Bash\" (Unix shell). For a comprehensive overview of commands see the Windows Commands Reference or the Guide to Unix commands. Some commands related to navigation in your directory (folder) structure will be useful (here for Windows; Unix commands in italic if they differ):</p> Command Operation dir (Unix: ls) List files and subdirectories in the current directory. tree Displays the directory structure of a path or of the disk. cd FOLDER_NAME \"Change directory\": Move to a folder within your current folder. cd PATH_TO_YOUR_FOLDER_AND_NAME Move to a folder at any path. cd .. Move to parent directory (i.e. one level up). <p>Use these commands to move to your working directory, where you save your scripts (Hint: Try the 'Tab' key to auto-complete folder or file names).</p> <p>Some other useful commands are related to file handling:</p> Command Operation mkdir DIRECTORY Creates a directory or subdirectory rd DIRECTORY /s Deletes a directory including all its files. copy FILE FOLDER Copies one or more files from one location to another. rename SRC DES Renames files or directories. del FILE (Unix: rm FILE) Delete one or more files."},{"location":"module1/toolbox_intro/vscode.html#next-conda","title":"Next: Conda","text":"<p>Continue with Python installation and package management using Conda here.</p>"},{"location":"module1/toolbox_intro/xarray.html","title":"Raster data with xarray","text":"In\u00a0[1]: Copied! <pre>import rioxarray\nimport xarray\nfrom pathlib import Path\n</pre> import rioxarray import xarray from pathlib import Path In\u00a0[2]: Copied! <pre># Open a GeoTiff from a local directory using the rio accessor\ndata_dir = Path('F:/data/etrainee/gee')        # Define the path to the s2 directory with the Sentinel-2 image subsets on your local system\nin_file = data_dir / 's2' / '20180726T102019_20180726T102150_T32TPS.tif' # Specify the path and the filename of an image\nxds = rioxarray.open_rasterio(in_file, masked=True)     # Read the mask (NoData in the GeoTiff) and set values to NaN (for proper interpretation by xarray)\n</pre> # Open a GeoTiff from a local directory using the rio accessor data_dir = Path('F:/data/etrainee/gee')        # Define the path to the s2 directory with the Sentinel-2 image subsets on your local system in_file = data_dir / 's2' / '20180726T102019_20180726T102150_T32TPS.tif' # Specify the path and the filename of an image xds = rioxarray.open_rasterio(in_file, masked=True)     # Read the mask (NoData in the GeoTiff) and set values to NaN (for proper interpretation by xarray) <p>Print some information about the xarray.DataArray called 'xds':</p> In\u00a0[6]: Copied! <pre>xds\n</pre> xds Out[6]: <pre>&lt;xarray.DataArray (band: 6, y: 201, x: 251)&gt;\n[302706 values with dtype=float32]\nCoordinates:\n  * band         (band) int32 1 2 3 4 5 6\n  * x            (x) float64 6.516e+05 6.516e+05 ... 6.566e+05 6.566e+05\n  * y            (y) float64 5.194e+06 5.194e+06 5.194e+06 ... 5.19e+06 5.19e+06\n    spatial_ref  int32 0\nAttributes:\n    AREA_OR_POINT:           Area\n    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n    TIFFTAG_XRESOLUTION:     1\n    TIFFTAG_YRESOLUTION:     1\n    scale_factor:            1.0\n    add_offset:              0.0</pre>xarray.DataArray<ul><li>band: 6</li><li>y: 201</li><li>x: 251</li></ul><ul><li>...<pre>[302706 values with dtype=float32]</pre></li><li>Coordinates: (4)<ul><li>band(band)int321 2 3 4 5 6<pre>array([1, 2, 3, 4, 5, 6])</pre></li><li>x(x)float646.516e+05 6.516e+05 ... 6.566e+05<pre>array([651590., 651610., 651630., ..., 656550., 656570., 656590.])</pre></li><li>y(y)float645.194e+06 5.194e+06 ... 5.19e+06<pre>array([5193750., 5193730., 5193710., ..., 5189790., 5189770., 5189750.])</pre></li><li>spatial_ref()int320crs_wkt :PROJCS[\"WGS 84 / UTM zone 32N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32632\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 32Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :9.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 32N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32632\"]]GeoTransform :651580.0 20.0 0.0 5193760.0 0.0 -20.0<pre>array(0)</pre></li></ul></li><li>Indexes: (3)<ul><li>bandPandasIndex<pre>PandasIndex(Index([1, 2, 3, 4, 5, 6], dtype='int32', name='band'))</pre></li><li>xPandasIndex<pre>PandasIndex(Index([651590.0, 651610.0, 651630.0, 651650.0, 651670.0, 651690.0, 651710.0,\n       651730.0, 651750.0, 651770.0,\n       ...\n       656410.0, 656430.0, 656450.0, 656470.0, 656490.0, 656510.0, 656530.0,\n       656550.0, 656570.0, 656590.0],\n      dtype='float64', name='x', length=251))</pre></li><li>yPandasIndex<pre>PandasIndex(Index([5193750.0, 5193730.0, 5193710.0, 5193690.0, 5193670.0, 5193650.0,\n       5193630.0, 5193610.0, 5193590.0, 5193570.0,\n       ...\n       5189930.0, 5189910.0, 5189890.0, 5189870.0, 5189850.0, 5189830.0,\n       5189810.0, 5189790.0, 5189770.0, 5189750.0],\n      dtype='float64', name='y', length=201))</pre></li></ul></li><li>Attributes: (6)AREA_OR_POINT :AreaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1scale_factor :1.0add_offset :0.0</li></ul> <p>Or more specifically:</p> In\u00a0[7]: Copied! <pre>print(xds.shape)\nprint(xds.dims)\nprint(xds.coords)\n</pre> print(xds.shape) print(xds.dims) print(xds.coords) <pre>(6, 201, 251)\n('band', 'y', 'x')\nCoordinates:\n  * band         (band) int32 1 2 3 4 5 6\n  * x            (x) float64 6.516e+05 6.516e+05 ... 6.566e+05 6.566e+05\n  * y            (y) float64 5.194e+06 5.194e+06 5.194e+06 ... 5.19e+06 5.19e+06\n    spatial_ref  int32 0\n</pre> <p>... and some descriptive statistics for one band (using xarray's built-in methods, but numpy could be used as well):</p> In\u00a0[8]: Copied! <pre># Extract a band to a new DataArray (for conveniance)\nband_1 =xds.sel(band=1)\nprint('min: ', band_1.min().values)     # Print only the value(s) of the array; its only one value for one band and one time stamp \nprint('max: ', band_1.max().values)\nprint('mean: ', band_1.mean().values)\nprint('std: ', band_1.std().values)\n</pre> # Extract a band to a new DataArray (for conveniance) band_1 =xds.sel(band=1) print('min: ', band_1.min().values)     # Print only the value(s) of the array; its only one value for one band and one time stamp  print('max: ', band_1.max().values) print('mean: ', band_1.mean().values) print('std: ', band_1.std().values) <pre>min:  138.0\nmax:  6651.0\nmean:  666.3916\nstd:  358.31906\n</pre> <p>We can use the extended capabilities of rioxarray to query more metadata, such as coordinate reference system (CRS), bounding coordinates, dimensions, spatial resolution, or defined NoData value:</p> In\u00a0[9]: Copied! <pre>print(xds.rio.crs)\nprint(xds.rio.bounds())\nprint(xds.rio.width)\nprint(xds.rio.height)\nprint(xds.rio.resolution())\nprint(xds.rio.nodata)\n</pre> print(xds.rio.crs) print(xds.rio.bounds()) print(xds.rio.width) print(xds.rio.height) print(xds.rio.resolution()) print(xds.rio.nodata) <pre>EPSG:32632\n(651580.0, 5189740.0, 656600.0, 5193760.0)\n251\n201\n(20.0, -20.0)\nnan\n</pre> In\u00a0[10]: Copied! <pre>band_1 = xds.sel(band=1)            # Select a band by its label (or by its index with '.isel()'), as already shown above\nband_1.plot.imshow(robust=True)     # The 'robust' option stretches the colormap range to 2nd and 98th percentiles.\n</pre> band_1 = xds.sel(band=1)            # Select a band by its label (or by its index with '.isel()'), as already shown above band_1.plot.imshow(robust=True)     # The 'robust' option stretches the colormap range to 2nd and 98th percentiles. Out[10]: <pre>&lt;matplotlib.image.AxesImage at 0x28681981960&gt;</pre> <p>Now let's create a histogram for this band. We use xarray.plot.hist(), for more advanced options there are the xhistogram and the boost-histogram packages. You might try different estimators to determine the optimum number of bins (see here; default would be 10 bins; 'auto' takes the maximum of the Sturges and Freedman Diaconis estimators).</p> In\u00a0[11]: Copied! <pre>xds.sel(band=1).plot.hist(bins='auto'); # The semicolon tells Jupyter Notebook not to display the output of that line of code (i.e. the bin values; but it will show the histogram).\n</pre> xds.sel(band=1).plot.hist(bins='auto'); # The semicolon tells Jupyter Notebook not to display the output of that line of code (i.e. the bin values; but it will show the histogram). <p>Similarly, we can plot 3-band combinations as true-color or false-color (color-infrared, CIR) image by selecting the bands to plot as RGB:</p> In\u00a0[12]: Copied! <pre>xds.sel(band=[3, 2, 1]).plot.imshow(robust=True)\n</pre> xds.sel(band=[3, 2, 1]).plot.imshow(robust=True) Out[12]: <pre>&lt;matplotlib.image.AxesImage at 0x28682744100&gt;</pre> <pre>c:\\Users\\Andi\\mambaforge\\envs\\geopython\\lib\\site-packages\\matplotlib\\cm.py:494: RuntimeWarning: invalid value encountered in cast\n  xx = (xx * 255).astype(np.uint8)\n</pre> In\u00a0[13]: Copied! <pre>xds.sel(band=[4, 3, 2]).plot.imshow(robust=True)\n</pre> xds.sel(band=[4, 3, 2]).plot.imshow(robust=True) Out[13]: <pre>&lt;matplotlib.image.AxesImage at 0x286827ca470&gt;</pre> <p>Instead of using <code>robust=True</code> we could also constrain the color stretch to the respective quantiles using the <code>vmin</code> and <code>vmax</code> keyword arguments. Try also other quantiles or explicit values and see how this affects the image display.</p> In\u00a0[14]: Copied! <pre>xds.sel(band=[3, 2, 1]).plot.imshow(\n    vmin=xds.sel(band=[3, 2, 1]).quantile(0.02),\n    vmax=xds.sel(band=[3, 2, 1]).quantile(0.98))\n</pre> xds.sel(band=[3, 2, 1]).plot.imshow(     vmin=xds.sel(band=[3, 2, 1]).quantile(0.02),     vmax=xds.sel(band=[3, 2, 1]).quantile(0.98)) Out[14]: <pre>&lt;matplotlib.image.AxesImage at 0x28682618ac0&gt;</pre> <pre>c:\\Users\\Andi\\mambaforge\\envs\\geopython\\lib\\site-packages\\matplotlib\\cm.py:494: RuntimeWarning: invalid value encountered in cast\n  xx = (xx * 255).astype(np.uint8)\n</pre> <p>Now you know how to work with <code>xarray</code> on a single multispectral image and you are ready to expand this by the time dimension. Several tutorials of E-TRAINEE Module 1 use <code>xarray</code> to handle satellite image time series.</p>"},{"location":"module1/toolbox_intro/xarray.html#multi-dimensional-arrays-xarray","title":"Multi-dimensional arrays: xarray\u00b6","text":"<p>This short tutorial provides an introduction to raster handling with Python using the xarray package (Hoyer and Hamman 2017) for multi-dimensional data. Xarray's data structures (<code>DataArray</code> and <code>Dataset</code>, see here) complement NumPy-like arrays by labels in the form of dimensions, coordinates and attributes, thereby encoding information about how the array values are absolutely related to space, time etc. The labels can also be used to access and analyse the array values. These properties and functionality are very useful if you work with time series of remote sensing images or other raster data. Many of the visualizations in this tutorial could be improved with a few lines of code (e.g. to add axes labels and plot titles) or by using specialized plotting packages, but here we try to keep the code simple.</p> <p>Xarray Dataset structure. Spectral bands can be treated as data variables, time as a dimension in addition to the spatial dimensions (modified from the xarray User Guide).</p> <p>Data</p> <p>As a sample dataset, we use a a single GeoTiff file with six bands. This imagery was acquired by one of the two Sentinel-2 satellites and covers a relatively small area around the village of Obergurgl, located in the Central Alps of Tyrol (Austria). This imagery has been queried from the Google Earth Engine (GEE), then cloud-masked and exported from GEE.</p>"},{"location":"module1/toolbox_intro/xarray.html#getting-started-with-xarrays-data-structures-and-basic-tools","title":"Getting started with xarray's data structures and basic tools\u00b6","text":""},{"location":"module1/toolbox_intro/xarray.html#loading-and-handling-single-images","title":"Loading and handling single images\u00b6","text":"<p>In this tutorial, we start with importing and exploring an individual image. So we load a single GeoTiff file with six bands. A good way to do this is using rioxarray, a geospatial xarray extension powered by rasterio. Rioxarray extends xarray by improved support for geospatial metadata, as rasterio does for numpy arrays.</p>"},{"location":"module1/toolbox_intro/xarray.html#plotting-dataarrays","title":"Plotting DataArrays\u00b6","text":"<p>Creating a new DataArray with only one selected band is straightforward and plotting this is easy as well (matplotlib must be installed in the active conda environment). Note the white NoData areas due to cloud masking:</p>"},{"location":"module2/module2.html","title":"E-TRAINEE Module 2: Satellite Multispectral Images Time Series Analysis","text":"<p>This Module aims to equip you with an in-depth understanding of satellite multispectral imaging, a potent tool that affords a unique perspective for observing and analyzing Earth\u2019s surface. Through the capture of images across various wavelengths, satellite multispectral imaging allows for the identification and quantification of a broad spectrum of physical and biological phenomena.</p> <p>Module 2 provides a diverse array of pre-processing and specific analysis methods, all thanks to the many opportunities afforded by multitemporal analysis of regularly captured, freely available satellite data. These methods pave the way for a plethora of applications in environmental studies. The selection of data and methods hinges on the study\u2019s scale \u2014 whether local, regional, or global \u2014 and the required time interval to accurately capture phenomena of interest, such as the effects of hurricanes, climate change, or the peak of the growing season, in either inter-annual or intra-annual data. Thus, in this module, you will:</p> <ul> <li>grasp the fundamentals of multispectral imaging</li> <li>discover major sources and unique characteristics of Earth observation data</li> <li>understand how temporal aspect of satellite data can be used in different analyses on various scales</li> <li>learn the essential steps to prepare your images for analysis, including different corrections and maskings, as well as time series specific methods such as data harmonization, normalization, compositing, and gap filling</li> <li>dive into various approaches to the classification problems including machine learning algorithms and feature selection</li> <li>explore different approaches to detecting changes and disturbances in vegetation using change detection algorithms and methods</li> <li>apply the theoretical knowledge in the practical exercises to produce your own reference and image datasets and use them in classification and change detection problems</li> <li>conduct case study analyses with multispectral time series in different use cases</li> </ul>"},{"location":"module2/module2.html#structure","title":"Structure","text":"<p>This module is structured into the following themes:</p> <ul> <li>Principles of multispectral imaging </li> <li>Temporal information in satellite data </li> <li>Image processing </li> <li>Multitemporal classification </li> <li>Vegetation change and disturbance detection </li> <li>Case study: Monitoring tundra grasslands (Karkonosze/Krkono\u0161e Mountains)</li> <li>Case study: Effects of pollution in Ore Mountains</li> <li>Case study: Forest disturbance detection (Tatra Mountains)</li> </ul>"},{"location":"module2/module2.html#prerequisites-to-perform-this-module","title":"Prerequisites to perform this module","text":"<p>The following skills and background knowledge are required for this module.</p> <ul> <li>Basics of statistics</li> <li>Basics of geoinformation systems and handling raster/vector data</li> <li>Principles of remote sensing</li> <li>Basic programming skills (R and Google Earth Engine JavaScript will be used here)</li> </ul> <p>Follow this link for an overview of the listed prerequisites and recommendations on external material for preparation.</p>"},{"location":"module2/module2.html#software","title":"Software","text":"<p>For this module, you will need the software listed below. If you did not install the software before starting the course, follow the links to the individual software or tools, for help in setting them up.</p> <ul> <li>QGIS for visualization of time series satellite imagery and results of classification and change detection</li> <li>R language for time series satellite imagery processing and analysis</li> <li>Google Earth Engine access (create an account here)</li> </ul>"},{"location":"module2/module2.html#use-cases-and-data","title":"Use Cases and Data","text":""},{"location":"module2/module2.html#use-cases","title":"Use Cases","text":"<p>Research-oriented case studies in this module are introduced in Monitoring mountain vegetation in Karkonosze/Krkono\u0161e Mountains (Poland/Czechia), Forest disturbances in Ore Mountains (Czechia) and Vegetation disturbance detection in Polish-Slovak Tatra Mountains use case documents. Familiarize yourself with them to have a better understanding of the analyses performed in Case Studies.</p>"},{"location":"module2/module2.html#data","title":"Data","text":"<p>Data for the exercises is provided through Zenodo. Some input imagery is produced throughout the course. Below you can see the folder tree of data from Module 2.</p> <pre><code>module2/\n\u251c\u2500\u2500\u2500case_study_1\n\u2502   \u2502   README.txt\n\u2502   \u251c\u2500\u2500\u2500data_exercise/\n\u2502   \u2514\u2500\u2500\u2500results/\n\u251c\u2500\u2500\u2500case_study_3\n\u2502   \u2502   README.txt\n\u2502   \u251c\u2500\u2500\u2500data_exercise/\n\u2502   \u2514\u2500\u2500\u2500results/\n\u251c\u2500\u2500\u2500theme_1_exercise\n\u2502   \u2502   README.txt\n\u2502   \u251c\u2500\u2500\u2500data_exercise/\n\u2502   \u2514\u2500\u2500\u2500results/\n\u251c\u2500\u2500\u2500theme_2_exercise\n\u2502   \u2502   README.txt\n\u2502   \u251c\u2500\u2500\u2500data_exercise/\n\u2502   \u2514\u2500\u2500\u2500results/\n\u251c\u2500\u2500\u2500theme_4_exercise\n\u2502   \u2502   README.txt\n\u2502   \u251c\u2500\u2500\u2500data_exercise/\n\u2502   \u2514\u2500\u2500\u2500results/\n\u2514\u2500\u2500\u2500theme_5_exercise\n    \u2502   README.txt\n    \u251c\u2500\u2500\u2500data_exercise/\n    \u2514\u2500\u2500\u2500results/\n</code></pre> <p>Each folder in the main catalog contains short description of the data inside in <code>README.txt</code> file. Input data is provided in <code>data_exercise</code> folders. Empty (except Theme 1) <code>results</code> folders are provided to store the outputs. After downloading the packages you should follow the R language tutorial to create an environment and start R project in the main <code>module2</code> catalog.</p>"},{"location":"module2/module2.html#start-the-module","title":"Start the module","text":"<p>\u2026 by proceeding to the first theme on Principles of multispectral imaging</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html","title":"Principles of multispectral imaging","text":"<p>Before we move on to time series analysis we will generally introduce the topic of multispectral satellite data properties. We acknowledge that multispectral imagery can also be acquired with aerial and UAV platforms. However, since this module covers satellite data level of acquisition we will focus only on this type of data. We will present the potential of these satellite images in environmental applications.</p> <p>In this theme, you will learn about:</p> <ul> <li>visual interpretation of satellite data</li> <li>the concept of multispectral data</li> <li>use of different ranges</li> <li>spectral indices and transformations</li> <li>selected sensor characteristics</li> <li>advantages and limitations of satellite multispectral imaging</li> <li>multispectral satellite data archives</li> </ul> <p>This theme will conclude with:</p> <ul> <li>Self-evaluation quiz</li> <li>References</li> </ul> <p>To consolidate your knowledge and test it in a more practical environment complete the Exercise.</p> <p>After finishing this theme you will:</p> <ul> <li>gain knowledge of the spatial and spectral resolution in the context of satellite data</li> <li>understand how to visually interpret satellite images based on more or less complex features (e.g.\u00a0tone/brightness, size, texture, shadows, phenology)</li> <li>learn about the use of specific spectral ranges and RGB compositions to observe and analyze different phenomena</li> <li>understand how to calculate, differentiate, and use selected spectral indices and data transformation techniques like Tasseled Cap (TC) and Principal Component Analysis (PCA) for dealing with multispectral data</li> <li>gain familiarity with various satellite multispectral sensors and their specific applications in remote sensing research</li> <li>discover the advantages and limitations of satellite multispectral imaging for diverse fields of study</li> <li>know the increasing availability and variety of satellite data archives and their relevance for various applications</li> </ul>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#visual-interpretation-of-satellite-data-concept","title":"Visual interpretation of satellite data concept","text":"<p>Image data can be analyzed based on their resolutions. In the context of satellite data, we consider four types of resolution: spatial, spectral, radiometric, and fixed temporal. We will discuss the last in more detail in Theme 2, while the others will be covered in this Theme.</p> <p>Starting with the visual interpretation of satellite images, they can be grouped hierarchically, with respect to the degree of complexity (European Commission, 1994, Chuvieco, 2020). At the most basic level, tone/brightness and color are the most intuitive factors for visual interpretation. Following these, spatial characteristics such as the size, form (shape), and texture (spatial heterogeneity) of identified objects are considered. More complex factors include spatial patterns or shadows, which are connected with the height of objects. Finally, phenology is considered the most challenging to capture visually, since it is influenced by seasonal and interannual variations in climate. This requires a deep understanding of the biological and ecological processes at play, as well as the ability to interpret subtle changes in color and texture that may indicate changes in vegetation health.</p> <p>We will discuss spectral factors in the following sections, explaining how multispectral data is constructed.</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#spectral-ranges-in-the-optical-domain","title":"Spectral ranges in the optical domain","text":"<p>Optical remote sensing is based on specific parts of the electromagnetic spectrum, particularly the wavelengths from the visible, near, and shortwave infrared ranges (approximately 0.4-2.5 micrometers Landgrebe, 2003). Specific wavelength ranges can be captured into separate images of the same scene, known as bands. Such data is referred to as multispectral.</p> <p> <p></p> <p>The example of selected Landsat OLI2 spectral bands (acquired 5.06.2022, figure by course authors, Landsat 9 courtesy of the U.S. Geological Survey/ Terms of use). </p> <p>The satellite multispectral data gathering process is based on three interrelated fundamental parameters: spatial and spectral resolution and signal-to-noise (S/N) ratio. The power level of energy in each pixel is divided into the spectral bands. The finer the spatial and spectral resolution, the less power is left to overcome the system sensor noise. This is where the S/N ratio comes into play. The primary objective of remote sensing is to maximize the S/N ratio, ensuring that the recorded signal significantly surpasses the noise, thereby enhancing the quality of the data collected.</p> <p>The prerequisite for further quantitative analysis of satellite data in the optical domain is the calibration to spectral radiance, followed by the calculation of reflectance (we describe this in Theme 3). But qualitatively, we can visually analyze images as described above based on tone/brightness or color.</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#use-of-different-ranges","title":"Use of different ranges","text":"<p>Various materials have unique reflection and absorption properties of electromagnetic radiation at different wavelengths. Those different characteristics make it possible to use particular spectral ranges for their analysis and target specific phenomena. For instance band 1 (0.43-0.45 micrometers) from Landsat 8 OLI is particularly useful for coastal zone observations due to the way water absorbs and reflects light in this range.</p> <p>This dependence on the use of specific bands for specific analyses results from the number of gray levels associated with radiometric resolution of the data and brightness of particular pixels. The greater the variation in the brightness of pixels representing different objects - the more useful the band for analysis. However, the human eye is more sensitive to color variations than brightness variations, which is why color (multiband) compositions are often used in the visual interpretation of satellite images.</p> <p>Plotting three selected spectral bands together allows for the creation of an RGB image composition. While our eyes are most accustomed to compositions based on red, green, and blue bands (part (a) of Figure below), the availability of near and shortwave infrared bands allows us to create false color images (part (b)). Such compositions allow for better discrimination of e.g.\u00a0deciduous/coniferous forests, healthy/damaged vegetation, and so on.</p> <p> <p></p> <p>Multispectral data concept: left: Landsat OLI2 natural and false colours RGB compositions (a) 432 and b) 543 bands used), right: spectral curves for selected land cover classes (NOTE: the reflectance values for specific objects are from a particular time, not general. Figure by course authors, Landsat 9 courtesy of the U.S. Geological Survey/ Terms of use). </p> <p>In multispectral data, the number of bands and their width are crucial factors. Generally, this type of data includes 3 to 10 discrete and rather broad bands (distinguishing them from hyperspectral data, which can include hundreds of very narrow bands, see Theme 1 of Module 4). The figure below presents the comparison of bandwidths of several Landsat sensors as well as a comparison of multi- and hyperspectral characteristics of healthy plant.</p> <p> <p></p> <p>Differences between multispectral vs hyperspectral data, plant spectral characteristics example (each color of the line at the top of the figure corresponds to a specific Landsat sensor band, and the width is approximately proportional to the range of wavelengths that it covers. Figure by course authors). </p> <p>Many spectral bands offer numerous possibilities for analysis (e.g.\u00a0assessment of leaf pigments or cellular structures of plant as you can see above), however, they can also be correlated, leading to redundancy in gathered information. Therefore, remote sensing offers the possibility to choose the bands that interest us the most. These selected bands can both be used to calculate spectral indices and subjected to transformations.</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#spectral-indices","title":"Spectral indices","text":"<p>Different spectral bands allow for creating their combinations called spectral indices. Let\u2019s recall what spectral indices are: mathematical measures that compare the spectral reflection in more than one spectral band.</p> <p>For multispectral data analysis, both broadband and narrowband indices can be calculated, depending on the sensor used (e.g.\u00a0Henrich et al., 2009 listed 249 indices possible to calculate on only Sentinel-2 data! And many others).</p> <p>Spectral indices can be dedicated to various objects and their characteristics. Due to the great dynamics of the changes taking place, most indices are related to vegetation. There are also indices developed for studying topics such as snow cover, built-up areas, rocks and minerals, and fire/burnt areas. You can find the examples of those in Index Database or Awesome Spectral Indices.</p> <p> <p></p> <p>Example spectral indices applications (figure by course authors). </p> <p>Vegetation is one of the most visually dynamic land cover classes during the year and over many years, hence it is one of the most frequently studied objects using spectral indices. Its physiognomy and morphology are expressed in specific values of the reflectance. That, for instance, allows the user to assess whether the plant is healthy, stressed, or dead.</p> <p>Normalized Difference Vegetation Index (NDVI, Rouse et al., 1973) is one of the most common vegetation indices, which provides a measure of the general condition of vegetation and biomass. It is also often used in multitemporal analysis, as you will see throughout our course. However, there are also other indices related to:</p> <ul> <li>water content in plants, e.g.\u00a0Normalized Difference Moisture Index (NDMI, Hardisky et al., 1983),</li> <li>chlorophyll content, e.g.\u00a0Normalized Pigment Chlorophyll Index (NPCI, Pe\u00f1uelas et al., 1993),</li> <li>light use efficiency, e.g.\u00a0Structural Independent Pigment Index (SIPI, Pe\u00f1uelas et al., 1994). etc.</li> </ul> <p>Normalized Burn Ratio (NBR, Roy et al., 2006) is also frequently used for mapping burned areas.</p> <p>As you will see in the next chapter discussing the characteristics of various satellite sensors, they differ in, among others, spectral ranges. The formulas for calculating spectral indices on them are the same, but the correct band must be inserted into the formula. The Table below presents the formulas for the above-mentioned indices defined using bands from several selected sensors.</p> Table 1. Selected vegetation indices formulas. index general formula Landsat 5-7 Landsat 8-9 Sentinel-2 PlanetScope PSB.SD PlanetScope PS2.SD NDVI NIR-RED/NIR+RED B4-B3/B4+B3 B5-B4/B5+B4 B8-B4/B8+B4 B8-B6/B8+B6 B4-B3/B4+B3 NDMI NIR-SWIR/NIR+SWIR B4-B5/B4+B5 B5-B6/B5+B6 B8-B11/B8+B11 - - NPCI RED-BLUE/RED+BLUE B3-B1/B3+B1 B4-B2/B4+B2 B4-B2/B4+B2 B6-B2/B6+B2 B3-B1/B3+B1 SIPI NIR-BLUE/NIR-RED B4-B1/B4-B3 B5-B2/B5-B4 B8-B2/B8-B4 B8-B2/B8-B6 B4-B1/B4-B3 NBR NIR-SWIR/NIR+SWIR B4-B7/B4+B7 B5-B7/B5+B7 B8-B12/B8+B12 - - <p>NOTE: some of the indices cannot be calculated on particular satellite data due to the lack of bands from specific ranges (e.g.\u00a0lack of SWIR bands in PlanetScope data).</p> <p>Interestingly, both NDMI and NBR make use of NIR and SWIR bands and the general formula is the same, however, the SWIR band in NBR has a longer wavelength than in the case of NDMI, as you can see below.</p> <p> <p></p> <p>Difference between the formulas in NDMI and NBR indices (figure by course authors). </p> <p>Spectral indices can be presented on a map to analyze their values on various forms of land cover, but they can also be used as variables:</p> <ul> <li>increasing the accuracy of classification</li> </ul> <p>or</p> <ul> <li>indicating a change in time.</li> </ul> <p>We will show you examples of their use in the exercises of Themes 4 and 5, respectively.</p> <p>More about vegetation indices calculated on e.g.\u00a0multispectral Sentinel-2 data and their applications you can find in this EO4GEO course.</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#transformations","title":"Transformations","text":"<p>Apart from spectral indices, multispectral data bands can be used to transform into new informative, uncorrelated bands. This can be particularly useful when dealing with time series data or when trying to reduce redundancy in the data.</p> <p>One of the most well-known transformations is Tasselled Cap (TC) producing thematic bands, from which the three most commonly used are Greenness (TCG), Brightness (TCB), and Wetness (TCW). This transformation was introduced for Landsat MSS data crops analysis and its name is because of the shape of phenological trajectories in red-NIR feature space (Kauth and Thomas, 1976). TC transformation has since been adapted for newer Landsat sensors and other platforms such as Sentinel-2. The applications have broadened as well. For instance, in forest disturbance detection researchers often use TCW band, which uses the contrast between shortwave and near-infrared, which is a good characteristic of disturbed and regenerating vegetation (Cohen et al., 2010, we refer to this in Theme 5).</p> <p>Another transformation is Principal Component Analysis (PCA, Hotelling, 1933) which is used for dimensionality reduction or noise removal by maximising the amount of information from original spectral bands by transforming them into a new set of bands, called principal components. They are uncorrelated with each other and ordered by the amount of variance they explain in the original data. The resulting band does not contain spectral information but instead provides a different, statistically most informative quality data.</p> <p> <p></p> <p>First PCA bands RGB composition (right) and principal components from Landsat OLI2 (acquired 5.06.2022). You can note that PC7 contains more noise than information (figure by course authors, Landsat 9 courtesy of the U.S. Geological Survey/ Terms of use). </p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#selected-sensor-characteristics","title":"Selected sensor characteristics","text":"<p>Over the past decades, the development of remote sensing instruments has been carried out. An extensive database of satellites is provided by Union of Concerned Scientists (UCS), where they listed a total number of 6718 operational satellites in orbit around Earth (updated Jan 1, 2023). From over 600 are oriented on Earth Observation and more specifically on multispectral, optical, and infrared imaging as well as Earth science. They are also divided into the user\u2019s classes like government, military, commercial, and civil.</p> <p>Table 2 presents the characteristics of selected satellite multispectral sensors most often used in scientific remote sensing research. Temporal information concerning most of them is described in Theme 2 of this Module. See also Module 1 Theme 2 for different satellites characteristics.</p> Table 2. Selected satellite sensor characteristics. Satellite Sensor Abbreviation Spectral range (\u00b5m) Pixel size (m) Swath width (km) Sources Aqua Moderate Resolution Imaging Spectroradiometer MODIS VNIR+SWIR (0.40-2.155), TIR (3.66-14.28) 250-1000 2330 LAADS DAAC Landsat 1/2/3 Return Beam Vidicon RBV VNIR (0.47-0.83) 80/80/80 185 USGS LT1 USGS LT2 USGS LT3 Landsat 1/2/3/4/5 Multispectral Scanner MSS VNIR (0.50-1.10) 80/80/40/80 185 USGS LT1 USGS LT2 USGS LT3 USGS LT4 USGS LT5 Landsat 4/5 Thematic Mapper TM VNIR+SWIR (0.45-2.35) 30/30 185 USGS LT4 USGS LT5 Landsat 7 Enhanced Thematic Mapper Plus ETM+ PAN (0.52-0.90), VNIR+SWIR (0.45-2.35), TIR (10.40-12.50) 15 (PAN), 30 (MS), 60 (TIR) 185 USGS LT7 Landsat 8 Operational Land Imager/ OLI/TIRS PAN (0.50-0.68), VNIR+SWIR (0.43-2.29), TIR (10.60-12.51) 15 (PAN), 30 (MS), 100 (TIR) 185 USGS LT8 Landsat 9 Operational Land Imager 2 OLI2 PAN (0.50-0.68), VNIR+SWIR (0.43-2.29), TIR (10.30-12.50) 15 (PAN), 30 (MS), 100 (TIR) 185 USGS LT9 Pleiades-1 High-Resolution Imager HiRI PAN (0.47-0.83), VNIR (0.43-0.94) 0.5 (PAN), 2 (MS) 20 eoPortal PlanetScope PS2, PS2.SD, PSB.SD - VNIR (0.45-0.86) 3 24 Planet RapidEye RapidEye Earth-imaging System REIS VNIR (0.40-0.85) 5 77 eoPortal Sentinel-2 Multispectral Instrument MSI VNIR+SWIR (0.44-2.20) 10-60 (MS) 290 ESA SPOT 1/2/3 High Resolution Visible HRV PAN (0.51-0.73), VNIR (0.50-0.89) 10 (PAN), 20 (MS) 60 Airbus SPOT 4 High Resolution Visible Infrared HRVIR PAN (0.61-0.68), VNIR+SWIR (0.50-1.75) 10 (PAN), 20 (MS) 60 Airbus SPOT 5 High Resolution Stereoscopic HRS PAN (0.48-0.70) 5-10 120 Airbus SPOT 5 High Resolution Geometric HRG PAN (0.48-0.71), VNIR+SWIR (0.50-1.75) 2.5-5 (PAN), 10-20 (MS) 60 Airbus SPOT 6/7 New AstroSat Optical Modular Instrument Naomi PAN (0.45-0.75), VNIR (0.45-0.89) 1.5 (PAN), 6 (MS) 60 Airbus Terra Advanced Spaceborne Thermal Emission and Reflection Radiometer ASTER VNIR+SWIR (0.52-2.43), TIR (8.12-11.65) 15-30 (MS), 90 (TIR) 60 LAADS DAAC Terra Moderate Resolution Imaging Spectroradiometer MODIS VNIR+SWIR (0.40-2.155), TIR (3.66-14.28) 250-1000 2330 LAADS DAAC QuickBird Ball Global Imagery System 2000 BGIS2000 PAN (0.45-0.90), VNIR (0.45-0.90) 0.65-0.73 (PAN), 2.62-2.92 (MS) 16.8/18 Satellite Imaging Corporation WorldView-2 WorldView-110 camera WV110 PAN (0.45-0.80), VNIR (0.40-1.04) 0.46 (PAN), 1.8 (MS) 16.4 Satellite Imaging Corporation WorldView-3 WorldView-110 camera WV110 PAN (0.45-0.80), VNIR+SWIR (0.40-2.36), CAVIS (0.40-2.24) 0.31 (PAN), 1.24-3.70 (MS), 30 (CAVIS) 13.1 Satellite Imaging Corporation WorldView-4 SpaceView-110 camera WV110 PAN (0.45-0.80), VNIR (0.45-0.92) 0.31 (PAN), 1.24 (MS) 13.1 Satellite Imaging Corporation"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#advantages-and-limitations-of-satellite-multispectral-imaging","title":"Advantages and limitations of satellite multispectral imaging","text":"<p>Multiple spectral bands foster a wider range of interpretation and analysis possibilities of objects than in the case of \u2018traditional\u2019 RGB photographs. Availability of red-edge, near-infrared, and mid-infrared bands significantly strengthens vegetation analyses. The width of the spectral bands is of great importance here, which varies depending on the sensor as mentioned above.</p> <p>The temporal regularity of the satellite level of data acquisition greatly enhances the potential of multitemporal analyses. Even that advantage, however, doesn\u2019t prevent the occurrence of a clouded series of images. This phenomenon concerns data collected on request as well.</p> <p>Individual mission differences in technical parameters, which are presented in Table 1 above, make it complicated to generalise advantages and disadvantages across the whole satellite data industry. Multispectral data can also be obtained from the airborne or UAV level. Similarly, in recent years, new hyperspectral satellites have appeared (e.g.\u00a0PRISMA in 2019 or EnMAP in 2022), but since this Module deals with multispectral satellite data, the information below is limited to such sensor characteristics.</p> <p> <p></p> <p>Advantages and limitations of satellite multispectral imaging (figure by course authors). </p> <p>More points at the positive side should hint on the promising opportunities for using this data for a wide variety of applications.</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#multispectral-satellite-data-archives","title":"Multispectral satellite data archives","text":"<p>The wide range of Earth observation missions by various government agencies and private companies means that the number of available sources with access to satellite data is increasing every year. The vast amount of information requires that the data be properly catalogued and offered to the end user in as simple a form as possible so that they can easily find the data relevant to the applications and effects they wish to obtain. It also offers access to the computing power to find, process, and analyse multi-temporal (and sometimes multi-sensor) collections in some cases within a single platform. The large data archive services of major national and international government agencies such as NASA or ESA and the major private companies that provide cloud space and computing power are filled with data from many different missions. Smaller archives often offer more specialised data from smaller satellite missions or are processed to a level that allows analysis in selected areas.</p> <p>Data available in archives can be divided by several characteristics. Archives offering free access are most often projects of agencies (that own the satellites), universities, and enthusiast communities. Commercially available data are owned by companies that operate private satellites and repositories of data from them. Some of the preprocessed Analysis Ready Data may also sometimes be available for a fee. Commercial parties often offer a free sample of data or a trial plan to get you acquainted with the service. All services may offer tools for searching, browsing, downloading, processing, and analysing data. Some offer multiple functionalities simultaneously. Depending on the user\u2019s needs and level of sophistication, they can be operated using a graphical interface or, in some cases, also using a programming console and API. Large fragmentation and multiplicity of possibilities, apart from many advantages of such a solution, also make it necessary to spend time on the selection of appropriate sources, which may require creating multiple accounts in different services and the necessity of learning how to use them.</p> <p>The Table 3 below consists of multiple different purposes and content platforms available in English containing optical satellite imagery. We chose to present archives not related to just one area or mission, albeit those services can be useful for a specific purpose. A short list of such platforms can be found below.</p> Table 3. Selected satellite data platforms. Company/agency Platform (site) Description Quick links Open Commercial (free samples) Visualize Download Analysis Imagery sources/satellites Notes Amazon Amazon Web Services Earth Cloud computing platform offering various ranges of data and services. REGISTRATION LOGIN GETTING STARTED no yes no yes yes Sentinel, Landsat, NOAA, SpaceNet, regional datasets. One of the largest data providers/cloud computing services available. ESA Copernicus Data Space Ecosystem Platform provides complete, free and open access to Sentinel mission data. REGISTRATION USER GUIDE yes no yes yes no Sentinel Enables API. Maxar Discover Browser of products collected by Maxar\u2019s satellites. yes no yes no no WorldView, GeoEye, Quickbird USGS EarthExplorer USGS/NASA platform providing tools to search, browse display, export metadata, and download data from its vast archive consisting of data reaching 75 years back. REGISTRATION LOGIN HELP yes no yes yes no Landsat, plus MODIS, ASTER. Also open source datasets provided under collaboration with ISRO (Resourcesat-1 and 2), ESA (Sentinel-2), and some commercial high-resolution satellite data (IKONOS-2, OrbView-3, historical SPOT data) Enables Bulk Download with standalone manager. Enables API. GeoCento Earthimages Platform for ordering high and very high resolution imagery. CONTACT no yes yes yes no Landsat, Sentinel, Worldview, SPOT Platform for ordering high and very high resolution imagery. Sentinel Hub EO Browser Visualisation, download and simple analysis online platform. EXPLORE USER GUIDE yes no yes yes yes Landsat, Sentinel, MODIS, ENVISAT, Proba-V Allows for ordering of private satellite imagery. ESA EO-Cat Metadata and imagery browser of Earth Observation data acquired by various satellites. HELP yes no yes yes no Envisat, AVHRR, Geosat, JERS-1, Landsat, Proba, Rapideye, Worldview-2 Airbus GeoStore Platform allowing access for specific satellite missions. REGISTRATION LOGIN HELP no yes no yes no Pleiades, SPOT, DMC Google Google Cloud Storage Cloud Storage provides a variety of public datasets that can be accessed by the community and integrated into their applications. REGISTRATION HELP yes no no yes no Sentinel-2, Landsat Enables API. Google Google Earth Visualisation of the whole globe on many levels of detail. RESOURCES yes no yes no no Available in the Web browser, and as a stand alone PC and mobile app. Google Google Earth Engine A planetary-scale platform for Earth science data &amp; analysis. FAQ PLATFORM INFORMATION yes no yes yes yes Landsat, Sentinel, MODIS Enables API. EOS Landviewer Satellite observation imagery tool that allows for on-the-fly searching, processing and getting valuable insights from satellite data. USER GUIDE no yes yes yes yes Landsat, Sentinel, SPOT, Pleiades Microsoft Microsoft Planetary Computer Data archive platform with expanding functionalities. ABOUT no yes no yes no Landsat, Sentinel, ASTER Enables API. Airbus OneAtlas Airbus satellite images and add maps enabled by ordering features. TUTORIALS no yes no yes no Enables API. Sentinel Hub Sentinel Playground A free-to-use web application that allows for rapid online viewing and access to image archives. INFORMATION yes no yes no no Landsat, Sentinel, MODIS"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"<p>After going through the theory in this theme you should now be ready to take on self-evaluation quiz. You should be able to find any answer you have trouble with in the contents above or the additional references below. Good luck!</p> Question 1. Select the visual interpretation factors, ranked from most to least complex:  seasonal conditions, brightness, shadows spatial pattern, size, tone shadow, form, colour tone, texture, seasonal conditions   spatial pattern, size, tone   Question 2. What is the main purpose of performing data transformation?  dimensionality reduction via selection of spectral bands creation of new, uncorrelated bands without spectral information noise removal from shortwave and near infrared bands creation of new spectral bands   creation of new, uncorrelated bands without spectral information   Question 3. Which satellite data would be most suitable for identifying the roofing of buildings?  Landsat 9 World-View-4 Terra MODIS SPOT 4   World-View-4   Question 4. Which satellite data would be most suitable for land surface temperature analysis throughout Poland?  Sentinel-2 Terra MODIS PlanetScope SPOT 1   Terra MODIS   Question 5. Spectral indices will NOT help in assessing:  water content in plants water quality chlorophyll content in water height of the water table   height of the water table   Question 6. Which multispectral data feature best identifies a damaged plant:  narrow spectral bands in the SWIR range use of VIS and NIR bands high spatial resolution of the panchromatic image texture presenting the surroundings of the plant   use of VIS and NIR bands   Question 7. What is the main difference between multi- and hyperspectral data:  pixel size pixel brightness band width RGB composition creation   band width"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#exercise","title":"Exercise","text":"<p>Proceed with the exercise by going to the next page below or clicking this link</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#references","title":"References","text":""},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#key-references-recommended-reading-looking-up-background-details","title":"Key references (recommended reading, looking up background details)","text":"<p>Chuvieco, E. (2020). Fundamentals of satellite remote sensing: An environmental approach. CRC Press. https://doi.org/10.1201/9780429506482</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#additional-references-cited-in-this-theme","title":"Additional references cited in this theme","text":"<p>Cohen W. B., Yang Z., Kennedy R. (2010). Detecting trends in forest disturbance and recovery using yearly Landsat time series: 2. TimeSync\u2014Tools for calibration and validation, Remote Sensing of Environment, 114(12), 2911-2924. https://doi.org/10.1016/j.rse.2010.07.008</p> <p>European Commission, Directorate-General for Environment, Directorate-General for the Information Society and Media, Corine land cover \u2013 Technical guide, Publications Office, 1994. source</p> <p>Hardisky, M.A., Klemas, V. (1983). The Influence of Soil Salinity, Growth Form, and LeafMoisture on the Spectral Reflectances of Spartina alterniflora Canopies. Photogramm. Eng, Remote Sensing. source</p> <p>Henrich, V., G\u00f6tze, E., Jung, A., Sandow, C., Th\u00fcrkow, D., &amp; Gl\u00e4\u00dfer, C. (2009, March). Development of an online indices database: Motivation, concept and implementation. In Proceedings of the 6th EARSeL imaging spectroscopy sig workshop innovative tool for scientific and commercial environment applications, Tel Aviv, Israel (pp.\u00a016-18). source</p> <p>Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24(6), 417. https://doi.org/10.1037/h0071325</p> <p>Kauth, R. J., &amp; Thomas, G. S. (1976). The Tasseled Cap\u2014A Graphic Description of the Spectral-Temporal Development of Agricultural Crops as Seen by Landsat. LARS: Proceedings of the Symposium on Machine Processing of Remotely Sensed Data, West Lafayette, IN: Purdue University, 4B-41-4B-51. source</p> <p>Landgrebe, D. A. (2003). Signal theory methods in multispectral remote sensing (Vol. 24). John Wiley &amp; Sons. https://doi.org/10.1002/0471723800</p> <p>Pe\u00f1uelas, J., Baret, F., &amp; Filella, I. (1995). Semi-empirical indices to assess carotenoids/chlorophyll a ratio from leaf spectral reflectance. Photosynthetica, 31(2), 221-230. source</p> <p>Pe\u00f1uelas, J., Filella, I., Biel, C., Serrano, L., &amp; Save, R. (1993). The reflectance at the 950\u2013970 nm region as an indicator of plant water status. International Journal of Remote Sensing, 14(10), 1887-1905. https://doi.org/10.1080/01431169308954010</p> <p>Rouse Jr, J. W., Haas, R. H., Schell, J. A., &amp; Deering, D. W. (1973). Monitoring the vernal advancement and retrogradation (green wave effect) of natural vegetation (No.\u00a0NASA-CR-132982). source</p> <p>Roy, D. P., Boschetti, L., &amp; Trigg, S. N. (2006). Remote sensing of fire severity: assessing the performance of the normalized burn ratio. IEEE Geoscience and Remote Sensing Letters, 3(1), 112-116. https://doi.org/10.1109/LGRS.2005.858485</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles.html#next-unit","title":"Next unit","text":"<p>Proceed with Temporal information in satellite data</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html","title":"Principles of multispectral imaging - Exercise","text":""},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html#exercise-visual-interpretation-on-different-resolution-data","title":"Exercise - Visual interpretation on different resolution data","text":"<p>In this exercise you will be able to complete several tasks: search, select and download imagery acquired by different sensors, visualize them in QGIS software in natural and false color composites and visually interpret to recognize variety of land cover classes and effects of disturbance agents. Data used in this exercise was acquired for Tatra Mountains region (southern Poland) which has varying land cover and its vegetation is affected by agents and events such as bark beetle outbreaks, strong winds or avalanches.</p> <p>The aim of this exercise is to learn how spatial resolution affects the image interpretation possibility and how usage of true color (RGB) or color-infrared (CIR) composites change our ability to distinguish land cover classes from each other. The knowledge gathered in this exercise will be useful in exercises in Theme 2, Theme 4 and Theme 5. The exercise is divided into three parts with video tutorial for each one.</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html#part-1-data-collection","title":"Part 1: Data collection","text":"<p>This part will show how to collect image data from various sources. We will later use that data in Part 2 and 3 this exercise. We will use following data and sources:</p> Data type Date of acquisition Product level Spatial resolution Aerial orthoimagery<sup>a</sup> 26.06.2022 Orthoimagery 0.25 m PlanetScope (composition of 10 scenes)<sup>b</sup> 26.08.2022 Surface Reflectance (SR) 3 m Sentinel-2B 03.07.2022 Level 1C (SR) 10 m Landsat8 OLI 22.06.2022 L1TP (Collection 2, SR) 30 m MODIS 21.06.2022 V6.1 (SR) 500 m <p><sup>a</sup> orthoimagery is treated as easily interpretable base material due to very high spatial resolution</p> <p><sup>b</sup> video tutorial does not cover the process of searching and downloading PlanetScope data from Planet Explorer (submission for Planet\u2019s Education and Research Program)</p> <p>To collect the data we will use following databases:</p> <ul> <li>Polish National Geoportal (aerial orthoimagery; registration is not needed)</li> <li>Copernicus Data Space Ecosystem (Sentinel-2 data; registration is needed)</li> <li>Earth Explorer (Landsat and MODIS data; registration is needed)</li> <li>Earth Data (registration is needed for MODIS data download)</li> <li>Planet Education and Research Program (PlanetScope data, registration is needed)</li> </ul> <p>By clicking on the image below you will be taken to the video on data collection on YouTube:</p> <p></p> <p>All data for parts 2 and 3 are provided through Zenodo (module2_theme_1_exercise.zip).</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html#part-2-project-preparation","title":"Part 2: Project preparation","text":"<p>This part is devoted to project preparation in QGIS software. In the beginning coordinate reference system will be set up. Universal Transverse Mercator zone 34 is chosen because of it is adequate for Tatra Mountains region and PlanetScope, Sentinel-2 and Landsat data are distributed in it (the aerial orhtoimagery will be reprojected on the fly properly, MODIS data needs to be reprojected). Each type of data will be visualised in two composites. We want to compare the possibility to recognize various objects using different types of visualisations.</p> <p>By clicking on the image below you will be taken to the video on QGIS project preparation on YouTube:</p> <p></p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html#part-3-image-interpretation","title":"Part 3: Image interpretation","text":"<p>In the last part we will use QGIS project prepared in part 2. A point vector layer containing example locations of different land cover classes and effect of two disturbance agents will be added. The layer will be helpful in comparison of image data. In this part think about the utility of each type of data for land cover classification and detection of vegetation disturbance effects.</p> <p>By clicking on the image below you will be taken to the video on image interpretation on YouTube:</p> <p></p> <p>TASK</p> <p>After completing the exercise you should have the knowledge of how different land cover classes and the effects of disturbances look like on different types of satellite data.</p> <p>To expand and consolidate the skills you\u2019ve acquired, create a new point vector layer and try to independently indicate two new locations for each of the previously discussed examples. In the layer, create attributes with the name of each data source used in the exercise and mark if given land cover class or disturbance effect is identifiable on given image. Remember the relationship between the scale of phenomena/object and spatial resolution of data. Try using various RGB composites that may be more suitable for distinguishing different classes from one another.</p> <p>Good luck !</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html#data-and-software-credits","title":"Data and software credits","text":""},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html#data","title":"Data","text":"<p>Landsat 8 imagery courtesy of the U.S. Geological Survey/ Terms of use</p> <p>Sentinel-2 European Space Agency - ESA/ Terms of use,</p> <p>PlanetScope Image \u00a9 2022 Planet Labs PBC/CC BY-NC 2.0</p> <p>MODIS Vermote, E., Wolfe, R. (2021). MODIS/Aqua Surface Reflectance Daily L2G Global 1km and 500m SIN Grid V061 (Data set). NASA EOSDIS Land Processes Distributed Active Archive Center. Accessed 2023-09-01 from https://doi.org/10.5067/MODIS/MYD09GA.061</p> <p>Orthoimagery Geoportal.gov.pl/ Terms of use</p>"},{"location":"module2/01_multispectral_principles/01_multispectral_principles_exercise.html#software","title":"Software","text":"<ul> <li>QGIS Development Team (2022). QGIS Geographic Information System. Open Source Geospatial Foundation Project. http://qgis.osgeo.org</li> </ul>"},{"location":"module2/02_temporal_information/02_temporal_information.html","title":"Temporal information in satellite data","text":"<p>As discussed in the previous theme, satellite-level data acquisition uniquely allows for analysis at regular intervals since the commencement of the instrument\u2019s operation. This leads to the accumulation of massive datasets. For instance, since the onset of its mission, Landsat has amassed over 8 million Earth scenes. The purpose of this part of the course is to showcase the potential of analyzing the multitemporal aspect of such voluminous data in Earth observation studies.</p> <p>In this theme you will learn about:</p> <ul> <li>why satellite time series data are important</li> <li>satellite sensors with high or low temporal resolution </li> <li>temporal categories of time series data</li> <li>time series components and temporal dimension in environmental analysis</li> <li>types of multitemporal analysis </li> <li>time series variables</li> <li>key aspects of satellite time series data analysis </li> <li>selected tools and algorithms </li> <li>uncertainty in satellite time series data products</li> </ul> <p>This theme will conclude with:</p> <ul> <li>Self-evaluation quiz</li> <li>References</li> </ul> <p>To consolidate your knowledge and test it in more practical environment complete the Exercise.</p> <p>After finishing this theme you will:</p> <ul> <li>understand the benefits of analyzing multitemporal satellite data in Earth observation studies</li> <li>recognize the imperfections of satellite images and the importance of image selection</li> <li>appreciate the ability of satellite data to capture temporal variations and detect changes on Earth\u2019s surface over time</li> <li>comprehend the temporal resolution of different satellite sensors and their orbits</li> <li>be able to classify multitemporal analysis into intra-annual, inter-annual, and multi-year categories</li> <li>identify the components of long-term time series and their relevance in environmental analysis</li> <li>differentiate between multitemporal classification and change detection as types of temporal analysis</li> <li>understand the importance of selecting appropriate tools and algorithms for multitemporal analysis</li> <li>recognize the presence of uncertainty in satellite time series data products and factors influencing it and know the methods for reducing it</li> </ul>"},{"location":"module2/02_temporal_information/02_temporal_information.html#the-need-of-satellite-time-series-data","title":"The need of satellite time series data","text":"<p>There are numerous reasons why satellite data, acquired on a continuous basis, is highly beneficial. The first reason pertains to the imperfections that some images may exhibit. A significant challenge in projects utilizing remote sensing data isn\u2019t always the processing of the data itself, but rather its selection. While planning such a project, we may have numerous ambitions, but the reality can sometimes surprise us. In an ideal world, we would have access to vivid, clear images, but we may instead encounter issues such as cloud cover, shadows, scanning errors, pixel noise, etc. The figure below, albeit slightly exaggerated, portrays our expectations versus the reality in this context. Nevertheless, the option to choose an image from a different date ensures that our project is not destined for failure.</p> <p> <p></p> <p>Expectations vs reality when searching satellite data (figure by course authors, sources of images from the upper left to the lower right: Sentinel-2 European Space Agency - ESA/ Terms of use, Image \u00a9 2022 Planet Labs PBC/CC BY-NC 2.0, Landsat 5 courtesy of the U.S. Geological Survey/ Terms of use), Landsat 7 courtesy of the U.S. Geological Survey/ Terms of use, Landsat 8 courtesy of the U.S. Geological Survey/ Terms of use, Landsat 4 courtesy of the U.S. Geological Survey/ Terms of use). </p> <p>Another significant advantage of satellite data is that it captures the temporal variation of objects and landscapes. Objects may appear or disappear, or look entirely different across various seasons or years. This ability can help us detect such phenomena without the need for on-site exploration. For instance, how else but through satellite imagery could we identify a newly formed tiny island in a distant ocean? Or observe the decrease in the number of cars on the streets and parking lots after the 2020 lockdown announcement due to COVID-19? Satellite data provides a powerful tool for monitoring changes on Earth\u2019s surface over time.</p> <p> <p></p> <p>Examples of satellite data usage. A: Landsat data help in new island detection in Tonga, figure by L. Dauphin (NASA Earth Observatory/ Terms of use, Map data \u00a92023 Google/ Terms of use), B: PlanetScope data in the views of Downtown Johannesburg, before (top image) and during (bottom image) lockdown (Image \u00a9 2019/2020 Planet Labs PBC/CC BY-NC 2.0). </p> <p>These and many other examples underscore the undeniable benefits of satellite data. The processing and utilization of such data, which enables us to observe Earth\u2019s changes, are covered in this module and its subsequent themes.</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#temporal-resolution-of-selected-sensors","title":"Temporal resolution of selected sensors","text":"<p>The temporal resolution of satellite data from different sensors depends on the type of orbit on which the satellite is placed. Geostationary satellite systems continuously acquire images of the same part of the globe, so multiple data acquisitions can occur even on the same day or even within a few minutes. Examples of such systems are meteorological satellites such as Meteosat and NOAA. Another class of satellites includes systems orbiting in polar / Sun-synchronous orbits, like Landsat or SPOT missions. In their case, time resolution is defined as the time the satellite revisits the same part of a given area. Typically, it takes between one day and half a month for a satellite in polar orbit to make another acquisition. This time can be reduced by placing several satellites with twin sensor parameters, like in case of Sentinel-2A and 2B occupying the same orbit separated by 180 degrees.</p> <p>Overview of temporal resolution of selected satellite sensors is presented below.</p> <p> <p></p> <p>Temporal resolution and lifetime of selected satellite missions (figure by course authors, information derived from eoPortal / terms and conditions. </p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#temporal-categories-of-time-series-data","title":"Temporal categories of time series data","text":"<p>Depending on the temporal resolution and time range of satellite data, multitemporal analysis can be classified into the following categories:</p> <ul> <li>intra-annual \u2013 analysis of data acquired within the same year, such as studying phenological changes in a single season,  </li> <li>inter-annual (year-to-year) \u2013 analysis of data acquired in two different years with any interval, for instance, analyzing the effects of a hurricane or flood,  </li> <li>inter-annual (multi-year) \u2013 analysis of data obtained over more than two different years with any interval. Examples might include studying the effects of pest infestations or climate change.</li> </ul> <p>Based on these categories, satellite data can be characterized as either long (spanning many years) and/or dense (featuring numerous collections per year) time series data. These can be used either individually or in combination, depending on the purpose of the analysis.</p> <p> <p></p> <p>Long and dense time series data examples (figure by course authors, source of Sentinel-2 image: European Space Agency - ESA/ Terms of use). </p> <p>Another way to divide this data is into online and offline categories (Zhu, 2017). The online approach assumes that time series data are arriving at a specific rate, facilitating analyses in near real-time or on an ongoing basis. In contrast, the offline approach operates under the assumption that the time series data already exist.</p> <p>For managing large collections of satellite images modeled as multidimensional structures to facilitate time series analysis, the concept of an Earth Observation (EO) Data Cube has been established (Voidrot and Percivall, 2020). The primary features of an EO Data Cube include georeferenced spatial support, temporal continuity, and the absence of gaps in the spatiotemporal extent. The processing steps related to the EO Data Cube concept are explored further in Theme 3. You can read more about this in Theme 2 of Module 1.</p> <p> <p></p> <p>EO data cubes concept and the relation to time series (figure by Simoes et al., 2021/ CC BY 4.0). </p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#time-series-components-and-temporal-dimensions-in-environmental-analysis","title":"Time series components and temporal dimensions in environmental analysis","text":"<p>As introduced in Theme 1 of Module 1, each long-term time series consists of three components:</p> <ul> <li>long-term, directional trend, showing trendline and determining linear fit,</li> <li>seasonal - systematic movements, presenting cyclical patterns,</li> <li>residual \u2013 irregular, short-term fluctuations, including noise.</li> </ul> <p> <p></p> <p>Three components of a time series: trend, seasonal and residual components, all jointly forming the original time series (figure by Kuenzer et al., 2015/ CC BY 4.0). </p> <p>Depending on the study aim, certain components will be of special interest. For instance, long-term trends may be crucial for climate analysis (e.g., snow cover duration, land surface temperature, etc.), while the seasonal component may be more relevant for phenological analysis of plants (such as determining the start and end of the growing season). For crisis management involving natural hazards, fires, etc., short-term fluctuations can be paramount.</p> <p>An example of this kind of component extraction is the Breaks For Additive Season and Trend (BFAST) algorithm (Verbesselt et al., 2010). BFAST detects changes in addition to extracting the mentioned components (more details are included in Theme 5).</p> <p>To better understand the components of temporal changes, it\u2019s useful to consider two temporal dimensions in environmental analysis (Chuvieco, 2020):</p> <ul> <li>speed of the phenomenon</li> <li>persistence of the resulting change.</li> </ul> <p>While speed refers to the duration of the phenomenon, persistence affects the length at which the impact is still observed. Events can take a few hours (e.g., hurricanes, volcanic eruptions) or several years (like urban growth or desertification). Persistence is connected to the intensity of the transformation and can vary from a few days to several years or even centuries. It\u2019s important to note that these two dimensions are not related: a quick event can have a long impact (e.g., a volcanic eruption). Understanding these factors can help to determine the frequency required for the analysis.</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#types-of-temporal-analysis","title":"Types of temporal analysis","text":"<p>Based on aforementioned time series categories, components and dimensions, temporal analysis can be divided into multitemporal classification and change detection. We provide more details and address the examples of such analyses in Theme 4 (multitemporal classification) and Theme 5 (vegetation monitoring) of this module.</p> <p> <p></p> <p>Temporal analysis on satellite time series data (figure by G\u00f3mez et al., 2016, modified/ CC BY 4.0). </p> <p>In case of the classification category we can consider two kinds of products obtained. First, a base map representing reference date conditions for a single term is created. Then, this base map can be updated or backdated with change information derived from the time series (see part A in the figure below). The use of a multitemporal dataset in one land cover model aims to improve the accuracy relative to results from a single term (see part B).</p> <p> <p></p> <p>Time series data based classification (figure by G\u00f3mez et al., 2016, modified / CC BY 4.0). </p> <p>A key step in change detection is the direct comparison of two or more images acquired at different dates. Images can be analysed using an algorithm based on selected remote sensing variables, such as single spectral bands, vegetation indices, the biophysical parameters of vegetation, etc., or based on a post-classification image. The result is a map of changes showing their direction or magnitude.</p> <p>Both multitemporal classification and change detection can utilize intra- and inter-annual, online and offline data. However, it\u2019s crucial to identify a checkpoint connected to the start of the satellite mission, which allows for retrospective analyses. In this context, the Landsat program plays a significant role as it offers the longest time series of civilian optical satellite data. Since opening its archives in 2008, all new and archived images have been made freely available over the internet to any user (Wulder et al., 2012).</p> <p>However, nowadays, the Sentinel-2 program also plays a crucial role in multitemporal analysis. It\u2019s noteworthy that in 2022, open databases like Dynamic World, which presents land cover maps based on real-time (online) collected data every five days, were launched. These resources enable a wide range of environmental monitoring and change detection applications.</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#time-series-variables","title":"Time series variables","text":"<p>Satellite data time series can comprise of spectral bands presented in raw digital numbers, percentage reflectance values or variables and derivatives calculated from original data. The selection of products used in the analysis depends on the objective, available data and the chosen algorithm. The figure below presents several groups of variables related to optical satellite data.</p> <p> <p></p> <p>Example variables used in satellite time series data analysis (figure by Kuenzer et al., 2015, modified / CC BY 4.0). </p> <p>Geophysical variables can be defined by a physical unit, e.g.\u00a0top-of-the-atmosphere reflectance (TOA), surface reflectance (SR) or Leaf Area Index (LAI). Index variables are unitless and include spectral indices or Tasseled Cap Transformation bands. Based on these variables, statistics such as minimum, maximum, mean, standard deviation, variability, anomalies, and trends can be calculated.</p> <p>Thematic and textural variables represent features in themselves, e.g., forest/non-forest classes or neighborhood variance, respectively. All these variables can be used in long-term monitoring (daily, weekly, monthly, annual, or decadal), depending on the research objectives.</p> <p>It\u2019s crucial to note that each type of variable offers unique insights into the environmental conditions and changes over time. Therefore, the selection of the variables should align with the goals of the study, and often a combination of different variables will provide the most comprehensive understanding of the area being studied.</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#aspects-of-satellite-time-series-data-analysis","title":"Aspects of satellite time series data analysis","text":"<p>We have thus far examined various categories of data and explored the diverse variables that can be extracted from different types of input imagery. Leveraging this knowledge and aligning it with the objectives of a specific analysis, several crucial aspects worth considering each time emerge:</p> <ul> <li>frequencies - directly tied to temporal resolution of satellite data. For instance, the revisit time over the same area for Landsat 5-8 is 16 days, which allows for the acquisition of 22-23 images per year. For Sentinel-2, which collects data with a 5-day frequency, we can obtain up to 73 images per year.</li> <li>preprocessing - this encompasses several tasks including radiometric correction, cloud/shadow detection, compositing/fusion, metrics calculation, etc. (for more details, refer to Theme 3).</li> <li>applications - wide array of potential applications for time series in environmental analyses. The examples of applied multitemporal classification can be found in Theme 4 exercise and Case Study 1. Change detection applications can be found in Theme 5 exercise and Case study 3.</li> <li>methods - the algorithms used for multitemporal classification or change detection (see the section below in this Theme).</li> </ul> <p>In each step, careful planning and execution are crucial to derive meaningful and accurate results from the satellite data time series. Furthermore, while we have specific steps outlined, remember that these may vary based on the specific objectives of your analysis. Therefore, always consider the project\u2019s specific requirements when planning and conducting your analysis.</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#tools-and-algorithms","title":"Tools and algorithms","text":"<p>The temporal resolution and repeatability of satellite data acquisition enable the creation of specialized tools and algorithms for specific multitemporal analyses. Prior to using a particular algorithm, it\u2019s essential to characterize the quality of data and pinpoint the moment and agent of change on the multitemporal dataset within the research area. In Theme 1 of Module 1, the GEE Time Series Explorer plugin is presented, which allows you to explore satellite data time series. Cohen et al., 2010 introduced TimeSync, a Landsat time series visualization tool useful for collecting data and deriving plot-based estimates of change. You\u2019ll learn about a similar solution in the Exercise at the end of this theme, which will be highly valuable for validating results obtained from any time series analysis.</p> <p>In the context of multitemporal classification of satellite data, the algorithms can be independent of whether one, two, or hundreds of images are used. These can be common unsupervised clustering (such as k-means (Hartigan and Wong, 1979) and semi-, or supervised algorithms, for example Artificial Neural Networks (ANNs, Krogh, 2008) or Support Vector Machines (SVMs, Vapnik, 1999).</p> <p>Recently, there has been an observable rise of application of functional data in time series statistical analysis, which leads to a shift in the approach to time series modeling. This has resulted in methodologies such as functional k-means (Martino et al., 2019) or functional Random Forest (Fu et al., 2021). There are also other specific tools for incorporating temporal information into classifiers, such as Time-Weighted Dynamic Time Warping (TWDTW) based on logistic time weight (Maus et al., 2019) or Satellite Image Time Series (SITS) based on deep learning techniques (Simoes et al., 2021). We refer to these methods in Theme 4.</p> <p>For change detection in satellite data time series, numerous specific algorithms and tools have also been proposed by various authors. They\u2019re based on different approaches, which we describe in Theme 5. Apart from the ability to detect specific types of changes, there are also algorithms dedicated to particular issues, such as forest disturbance detection, and specific data like Landsat (for example, LandTrendr).</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#uncertainty-of-satellite-time-series-data-products","title":"Uncertainty of satellite time series data products","text":"<p>With a high degree of probability one can assume that a time series data analysis product will consist of many pixels or segments, which would be changed from the starting point, whether on the original reflectance image or final product. Any detectable change can be further attributed. However, there are many factors which could influence whether the changes user will observe can actually be connected with real on the ground change and the probability of occurrence of such change. Uncertainty of change analysis describes magnitude of errors, their spatial patterns and can help the end user to better understand strengths and limitations of the data (Povey, Grainger, 2015).</p> <p> <p></p> <p>Probability of pixels being correctly classified on multitemporal images (figure by Koukoulas, 2010, modified/ CC BY 4.0). </p> <p>To start with, every analysis should be preceded by thorough input data assessment in terms of factors that could potentially influence the results. These factors may include sun and atmospheric effects on imagery, terrain topography and stage of the phenological cycle (Song, Woodcock, 2003). Furthermore, errors and omissions on the processing stage (such as different types of corrections, harmonisation, transformations etc.) can also affect the degree of accuracy in detecting changes, their causes and therefore the final appearance of the post-processing image. Finally, attributing any given pixel with Change or No Change label is usually preceded by e.g.\u00a0thresholding or trajectory classification (more on that topic can be found in Theme 5). Such methods, however, are limited by their lack of robustness over different research areas and obliviousness to rate and severity of changes.</p> <p>Post-results analysis should be preceded by thorough accuracy and error assessment of the change/classification image. Most common sample-based accuracy metrics used in remote sensing, such as overall accuracy, producer\u2019s accuracy, user\u2019s accuracy and Kappa coefficient (more on the accuracy assessment in Theme 6 of Module 1 and Theme 4 of this Module) can be used as a basis for determining the degree of uncertainty. The preceding step, together with visual image assessment, should allow the user to evaluate spatial distribution of errors and magnitude of their occurrence. Should one notice spatial patterns in error distribution, the results can either be modified by for example inclusion of samples from areas with higher error rates or, in case the former step is not possible or unsuccessful, attaching confidence interval or standard error to the result (Koukoulas, 2010). In sample-based accuracy assessment such measures are in fact representing the level of uncertainty. Research shows that error-adjusted estimates may differ significantly from raw accuracy measures (Olofsson et al., 2013). The inclusion of a confidence interval allows the uncertainty to be quantified. Taking the steps above should allow the end user of the classification to make more informed decisions about further use of the results.</p> <p>To reduce uncertainty related to errors in transition areas between classes or mixed pixels in multitemporal classification it is possible to smooth the result image, e.g.\u00a0with Bayesian smoothing that uses information from a pixel\u2019s neighbourhood (Simoes et al., 2021), which can incorporate spatial effects in the results of multitemporal classification. Another option to reduce uncertainty is the Majority Vote algorithm which can combine the results from all the classification configurations (see Tarantino et al., 2020).</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"<p>After going through the theory in this theme you should now be ready to take on self-evaluation quiz. You should be able to find any answer you have trouble with in the contents above or in the additional references below. Good luck!</p> Question 1. Match the temporal resolution with the satellite sensor:  A B C D  16 days  A B C D  26 days  A B C D  5 days  A B C D  less than 1 day   A. Advanced Very High Resolution Radiometer   B. MultiSpectral Instrument   C. Enhanced Thematic Mapper+   D. New AstroSat Optical Modular Instrument    C D B A   Question 2. What type/types of land cover product/products can be obtained by satellite time series data classification?  single-date (reference time), from offline data only multitemporal, from both online and offline data single-date (reference time) and multitemporal, from offline data only single-date (reference time) and multitemporal, from both online and offline data   single-date (reference time) and multitemporal, from both online and offline data   Question 3. Match the best data category with the application:  A B C  intra-annual (year-to-year)  A B C  inter-annual (multi-year)  A B C  inter-annual (year-to-year)   A. hurricane effect   B. phenological stage of grasslands development   C. bark beetle outbreak    B C A   Question 4. What variables from satellite multispectral data can be used to analyze changes:  homogeneity, water area class, tree height Normalized Burn Ratio, surface reflectance, urban class TC Wetness, aspect, object shape heterogeneity, Leaf Area Index, slope   Normalized Burn Ratio, surface reflectance, urban class   Question 5. Seasonal component of time series contains:  long-term trendline cyclical movements irregular, noise fluctuations short-term trendline   cyclical movements   Question 6. Which of these answers is related to uncertainty in remote sensing change detection analysis? classification error matrix spatial distribution of errors unfinished, not satisfactory product of analysis errors in imagery metadata   spatial distribution of errors"},{"location":"module2/02_temporal_information/02_temporal_information.html#exercise","title":"Exercise","text":"<p>Proceed with the exercise by going to the next page below or clicking this link</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#references","title":"References","text":""},{"location":"module2/02_temporal_information/02_temporal_information.html#key-references-recommended-reading-looking-up-background-details","title":"Key references (recommended reading, looking up background details)","text":"<p>Kuenzer, C., Dech, S., &amp; Wagner, W. (2015). Remote sensing time series. Remote Sensing and Digital Image Processing, 22, 225-245. source</p> <p>Zhu, Z. (2017). Change detection using landsat time series: A review of frequencies, preprocessing, algorithms, and applications. ISPRS Journal of Photogrammetry and Remote Sensing, 130, 370-384. https://doi.org/10.1016/j.isprsjprs.2017.06.013</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#additional-references-cited-in-this-theme","title":"Additional references cited in this theme","text":"<p>Cai, S., &amp; Liu, D. (2015). Detecting change dates from dense satellite time series using a sub-annual change detection algorithm. Remote Sensing, 7(7), 8705-8727. https://doi.org/10.3390/rs70708705</p> <p>Chuvieco, E. (2020). Fundamentals of satellite remote sensing: An environmental approach. CRC Press. https://doi.org/10.1201/9780429506482</p> <p>Cohen, W. B., Yang, Z., &amp; Kennedy, R. (2010). Detecting trends in forest disturbance and recovery using yearly Landsat time series: 2. TimeSync\u2014Tools for calibration and validation. Remote Sensing of Environment, 114(12), 2911-2924. https://doi.org/10.1016/j.rse.2010.07.010</p> <p>Fu, G., Dai, X., &amp; Liang, Y. (2021). Functional random forests for curve response. Scientific Reports, 11(1), 24159. https://doi.org/10.1038/s41598-021-02265-4</p> <p>G\u00f3mez, C., White, J. C., &amp; Wulder, M. A. (2016). Optical remotely sensed time series data for land cover classification: A review. ISPRS Journal of Photogrammetry and Remote Sensing, 116, 55-72. https://doi.org/10.1016/j.isprsjprs.2016.03.008</p> <p>Hartigan, J. A., &amp; Wong, M. A. (1979). Algorithm AS 136: A k-means clustering algorithm. Journal of the Royal Statistical Society. series c (applied statistics), 28(1), 100-108. https://doi.org/10.2307/2346830</p> <p>Koukoulas, S. (2010). Change detection under uncertainty: Modeling the spatial variation of errors. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Science, Volume XXXVIII, Part 8, Kyoto Japan SOURCE</p> <p>Krogh, A. (2008). What are artificial neural networks?. Nature Biotechnology, 26(2), 195-197. https://doi.org/10.1038/nbt1386</p> <p>Martino, A., Ghiglietti, A., Ieva, F., &amp; Paganoni, A. M. (2019). A k-means procedure based on a Mahalanobis type distance for clustering multivariate functional data. Statistical Methods &amp; Applications, 28(2), 301-322. https://doi.org/10.1007/s10260-018-00446-6</p> <p>Maus, V., C\u00e2mara, G., Appel, M., &amp; Pebesma, E. (2019). dtwsat: Time-weighted dynamic time warping for satellite image time series analysis in R. Journal of Statistical Software, 88, 1-31. https://doi.org/10.18637/jss.v088.i05</p> <p>Ochtyra, A., Marcinkowska-Ochtyra, A., &amp; Raczko, E. (2020). Threshold-and trend-based vegetation change monitoring algorithm based on the inter-annual multi-temporal normalized difference moisture index series: A case study of the Tatra Mountains. Remote Sensing of Environment, 249, 112026. https://doi.org/10.1016/j.rse.2020.112026</p> <p>Olofsson, P., Foody, G. M., Stehman, S. V., &amp; Woodcock, C. E. (2013). Making better use of accuracy data in land change studies: Estimating accuracy and area and quantifying uncertainty using stratified estimation. Remote Sensing of Environment, 129, 122-131. https://doi.org/10.1016/j.rse.2012.10.031</p> <p>Povey, A. C., &amp; Grainger, R. G. (2015). Known and unknown unknowns: uncertainty estimation in satellite remote sensing. Atmospheric Measurement Techniques, 8(11), 4699-4718. https://doi.org/10.5194/amt-8-4699-2015</p> <p>Simoes, R., Camara, G., Queiroz, G., Souza, F., Andrade, P. R., Santos, L., \u2026 &amp; Ferreira, K. (2021). Satellite image time series analysis for big earth observation data. Remote Sensing, 13(13), 2428. https://doi.org/10.3390/rs13132428</p> <p>Song, C., &amp; Woodcock, C. E. (2003). Monitoring forest succession with multitemporal Landsat images: Factors of uncertainty. IEEE Transactions on Geoscience and Remote Sensing, 41(11), 2557-2567. https://doi.org/10.1109/tgrs.2003.818367</p> <p>Tarantino, C., Forte, L., Blonda, P., Vicario, S., Tomaselli, V., Beierkuhnlein, C., &amp; Adamo, M. (2021). Intra-annual sentinel-2 time-series supporting grassland habitat discrimination. Remote Sensing, 13(2), 277. https://doi.org/10.3390/rs13020277</p> <p>Vapnik, V. N. (1999). An overview of statistical learning theory. IEEE transactions on neural networks, 10(5), 988-999. https://doi.org/10.1109/72.788640</p> <p>Verbesselt, J., Hyndman, R., Newnham, G., &amp; Culvenor, D. (2010). Detecting trend and seasonal changes in satellite image time series. Remote sensing of Environment, 114(1), 106-115. https://doi.org/10.1016/j.rse.2009.08.014</p> <p>Wulder, M. A., Masek, J. G., Cohen, W. B., Loveland, T. R., &amp; Woodcock, C. E. (2012). Opening the archive: How free data has enabled the science and monitoring promise of Landsat. Remote Sensing of Environment, 122, 2-10. https://doi.org/10.1016/j.rse.2012.01.010</p> <p>Voidrot, M. F., &amp; Percivall, G. (2020, June). OGC Geospatial Coverages Data Cube Community Practice. In IOP Conference Series: Earth and Environmental Science (Vol. 509, No.\u00a01, p.\u00a0012058). IOP Publishing. https://doi.org/10.1088/1755-1315/509/1/012058</p>"},{"location":"module2/02_temporal_information/02_temporal_information.html#next-unit","title":"Next unit","text":"<p>Proceed with Image processing workflow</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html","title":"Temporal information in satellite data - Exercise","text":""},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#exercise-change-events-and-agents-in-tatra-mountains","title":"Exercise - Change Events and Agents in Tatra Mountains","text":"<p>In this exercise, you will undertake a number of tasks associated with identifying events of changes visible in the imagery, and attributing change agents to these detected events based on the trajectories of selected spectral indices.</p> <p>The main objective of this exercise is not only to illustrate various examples of change events and change agents, but also to equip you with the skills needed to prepare your own reference datasets. Photointerpretation of imagery chips and charts that present relationships between spectral indices values and ground-level changes are often the only viable sources of data for assessing analysis results, particularly when ground truth data is unavailable.</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#basic-preparation","title":"Basic preparation","text":""},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#prerequisites","title":"Prerequisites","text":"<p>For this exercise you will need the following software, data and tools:</p> <ul> <li>Software - R and RStudio. You can access environment setup tutorial for the whole Module 2 here: R environment setup tutorial. After following the setup guide you should have all the necessary packages installed.</li> <li>Data - downloaded data provided through Zenodo</li> </ul> <p>Follow the suggested working environment setup in order for the provided relative paths to work properly.</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#data","title":"Data","text":"<p>The imagery provided for this exercise comprises a time series of Landsat program satellites 5, 7, and 8 satellite images spanning from 1984 to 2016. This time series is composed of images from summer periods, defined as the timeframe between June 15 and September 15. Images for each available year (with the exceptions of 1990, 1998, and 2004 due to low-quality data - primarily due to high cloud cover) underwent atmospheric and topographic correction using the ATCOR 2/3 software. The outcomes of these corrections were validated using in-situ surface reflectance measurements. Furthermore, these images were harmonized to match the Landsat 8 surface reflectance values using a linear regression derived from the comparison of clear pixel values.</p> <p>Please note that due to cloud cover during certain periods, selected images (14 out of 30) are composites of two different images. These composites were calculated by using the higher NDVI value between the two images.</p> <p> <p></p> <p>Left: Landsat 5 TM 31.07.1984, RGB 321; right: Landsat 8 OLI 8.09.2016, RGB 432. Landsat imagery courtesy of the U.S. Geological Survey/ Terms of use </p> <p>The preprocessing of images resulted in 30 multiband images, each containing six bands corresponding to Landsat 5 bands 1-5, and 7. These bands were subsequently used to calculate spectral indices. For this exercise, you will be utilizing 30-layer raster bricks of bands 4 (NIR), 3 (Red), and 2 (Green) for displaying image chips data, along with vector data with reference points. These points have assigned attributes denoting the year of detected change, change agent, and time series values of three spectral indices: NDVI, NDMI, and NBR.</p> <p>More information about the data and processing can be found in Ochtyra et. al, 2020.</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#environment-preparation","title":"Environment preparation","text":"<p>Let\u2019s begin by loading the necessary libraries and datasets, and setting up some initial variables to use down the line.</p> <p>Firstly, load required libraries into the environment: terra, sf and dplyr.</p> <pre><code># raster I/O and processing\nlibrary(terra)\n\n# vector and attribute handling\nlibrary(sf) \n\n# tabular data manipulation\nlibrary(dplyr)\n</code></pre>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#reading-data","title":"Reading data","text":"<p>With the libraries in place, we can now load the necessary data. We\u2019ll begin with three multiband rasters corresponding to the green, red, and NIR bands respectively.</p> <pre><code># multiband raster containing 30 green bands from 1984-2016 period\ngreen &lt;- rast(\"theme_2_exercise/data_exercise/T2_green_stack.tif\") \n\n# multiband raster containing 30 red bands from 1984-2016 period\nred &lt;- rast(\"theme_2_exercise/data_exercise/T2_red_stack.tif\") \n\n# multiband raster containing 30 NIR bands from 1984-2016 period\nnir &lt;- rast(\"theme_2_exercise/data_exercise/T2_nir_stack.tif\") \n</code></pre> <p>The bands in the raster bricks are chronologically ordered, with the first layer being the earliest acquired image from 1984 and the last from 2016. Note the missing years.</p> <p>You can visualize the first couple of bands of one of rasters to get a sense of the overall data extent and band values.</p> <pre><code>e &lt;- c(400000, 460000, 5410000, 5415000)\nplot(nir, 1:6, \n     col = colorRampPalette(c(\"black\", \"white\"))(255), \n     range = c(0, 10000), \n     mar = NA,\n     plg = list(ext = e, loc = \"bottom\"))\n</code></pre> <p> <p></p> <p>First six NIR bands. </p> <p>We can now create a vector of dates corresponding to each image. Remember that some images are composites, and their assigned date corresponds to the main image that contributes the majority of pixels to the final composite.</p> <pre><code># vector of dates assigned to subsequent raster layers in each band brick\nyears &lt;- as.Date(c(\"1984-07-31\", \"1985-08-03\",\"1986-08-22\", \"1987-07-08\", \n                   \"1988-08-18\", \"1989-07-04\", \"1991-09-05\", \"1992-07-20\", \n                   \"1993-08-16\", \"1994-08-28\", \"1995-07-21\", \"1996-08-24\", \n                   \"1997-09-12\", \"1999-08-09\", \"2000-08-20\", \"2001-08-15\", \n                   \"2002-06-15\", \"2003-08-12\", \"2005-09-02\", \"2006-07-19\", \n                   \"2007-08-23\", \"2008-09-02\", \"2009-08-21\", \"2010-08-23\", \n                   \"2011-08-27\", \"2012-08-28\", \"2013-09-08\",\"2014-09-04\", \n                   \"2015-07-12\", \"2016-08-08\")) \n</code></pre> <p>Finally, let\u2019s load the reference point data into our workspace. Using the <code>st_read</code> function from the <code>sf</code> package, we can easily import and manipulate the data contained in the attribute table of the vector file.</p> <pre><code># set of 13 reference points with attribute table\npoints &lt;- st_read(\n  \"theme_2_exercise/data_exercise/T2_tatra_mountains_change_points.shp\") \n</code></pre> <p>The reference layer consists of 13 points. Nine of them have attributes regarding the year of change detection and change agent, while all of them have 90 spectral indices values (30 each for NDVI, NDMI, and NBR). The remaining four points, which have missing attribute values, will be used later in this exercise for self-training. For now, we\u2019ll work with only the first nine points.</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#visualising-imagery-chips-and-spectral-trajectories","title":"Visualising imagery chips and spectral trajectories","text":""},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#imagery-chips","title":"Imagery chips","text":"<p>Having loaded the required data, we can now prepare our plotting environment. The aim is to generate image chips \u2014 parts of the images surrounding reference points. As an example, we\u2019ll use point number 1 for now, but we\u2019ll later create a loop to process each point.</p> <p>First, we need to select one of the points and delineate the range of the chip based on its coordinates. In this instance, we\u2019ll use chips with an area of 39x39 pixels.</p> <pre><code># indicate the index (row) of point as read into the environment\npoint_number &lt;- 1 \n\n# retrieve coordinates from point\npoint_cords &lt;- st_coordinates(points)[point_number, ]\n\n# retrieve image column, where the point lies\nrow &lt;- colFromX(green, point_cords[1]) \n\n# rerieve image row, where the point lies\ncol &lt;- rowFromY(green, point_cords[2]) \n\n# set up size of the image chip\nwindow_size &lt;- 39 \n\n# half of the window size\nhalf_widow_size &lt;- floor(window_size / 2) \n\n# image columns, which will we used for image chip visualisation\ncol_cords &lt;- (col - half_widow_size) : (col + half_widow_size) \n\n# image rows, which will we used for image chip visualisation\nrow_cords &lt;- (row - half_widow_size) : (row + half_widow_size) \n</code></pre> <p>Next, we\u2019ll set up the output name for the chip mosaic, initialize the device and prepare plotting grid. Initializing new device is a step that prepares a new blank plotting area, which we will save to our disk after adding elements to it. Our aim is to display chips in a 3-row, 10-column grid, so we\u2019ll set up an appropriate layout.</p> <pre><code># set up output name of the file\noutput_name &lt;- \"theme_2_exercise/results/point_1_chips_example.png\"  \n\n# initialize device, plotting area 1920x1080 px, text size 16 points\npng(filename = output_name, \nwidth = 1920, \nheight = 800, \npointsize = 16) \n\n# plotting area set up as a 3x10 matrix\nlayout(matrix(seq(1,30),3, 10, byrow = TRUE), heights = c(1,1,1)) \n</code></pre> <p>Finally, we can start plotting our chips. In the loop below, we\u2019ll use the 3 raster bricks with NIR, red, and green bands to create 30 NIR/RED/GREEN (color infrared band) composites\u2014one for each year\u2014for the selected point. To visualize each chip with similar parameters, we\u2019ll also trim histogram stretches to preset values. Furthermore, we\u2019ll display the date and location of the selected point for each chip.</p> <pre><code># enable reading pixel values from multiband rasters\nreadStart(green)\nreadStart(red)\nreadStart(nir)\n\n# loop 30 times - once for each raster layer in the brick\nfor (j in seq(30)){ \n\n  # prepare image slice from the appropriate green raster\n  # using previously prepared rows/columns\n  o_b1 &lt;- rast(matrix(readValues(green[[j]], \n                                       col = row_cords[1], \n                                       nrows = window_size, \n                                       row = col_cords[1],\n                                       ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))  \n\n  # prepare image slice from the appropriate red raster\n  # using previously prepared rows/columns\n  o_b2 &lt;- rast(matrix(readValues(red[[j]], \n                                       col = row_cords[1], \n                                       nrows = window_size, \n                                       row = col_cords[1],\n                                       ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n  # prepare image slice from the appropriate green raster\n  # using previously prepared rows/columns\n  o_b3 &lt;- rast(matrix(readValues(nir[[j]], \n                                       col = row_cords[1], \n                                       nrows = window_size, \n                                       row = col_cords[1],\n                                       ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n  # trim histogram of green band for harmonized viewing of the whole set\n  b_min &lt;- 10\n  b_max &lt;- 1000\n  o_b1 &lt;- (o_b1 - b_min) / (b_max - b_min) * 255 \n\n  # trim histogram of red band for harmonized viewing of the whole set\n  b_min &lt;- 10\n  b_max &lt;- 1000\n  o_b2 &lt;- (o_b2 - b_min) / (b_max - b_min) * 255 \n\n  # trim histogram of NIR band for harmonized viewing of the whole set\n  b_min &lt;- 10\n  b_max &lt;- 4000\n  o_b3 &lt;- (o_b3 - b_min) / (b_max - b_min) * 255 \n\n  # convert any negative values to 0 for better viewing\n  o_b1[o_b1 &lt; 0] &lt;- 0 \n  o_b2[o_b2 &lt; 0] &lt;- 0 \n  o_b3[o_b3 &lt; 0] &lt;- 0\n\n  # convert any values above 255 to 0\n  o_b1[o_b1 &gt; 255] &lt;- 255\n  o_b2[o_b2 &gt; 255] &lt;- 255 \n  o_b3[o_b3 &gt; 255] &lt;- 255\n\n  # set up margins around each of 30 plotting blocks\n  par(mar = c(0,1,1,1)) \n\n  # plot RGB chip in the appropriate place in the layout \n  plotRGB(c(o_b3, o_b2, o_b1), \n          r = 1, g = 2, b = 3,\n          mar = 1) \n\n  # draw the location of reference point \n  points(20, 19.4, pch = \".\", col = c(\"white\"))\n  symbols(x = 19.6, \n          y = 19.4, \n          squares = 3, \n          inches = F, \n          add = T, \n          fg = \"yellow\", \n          lwd = 0.01)\n\n  # show date of the image acquisition above the RGB chip\n  title(years[j], line = -2) \n\n}\n\n# close files - we read necessary values\nreadStop(green)\nreadStop(red)\nreadStop(nir)\n\n# turn off the device - save .png image to working directory\ndev.off() \n</code></pre> <p>This is the result after executing the preceding code.</p> <p> </p> <p>You\u2019ll observe that we successfully visualized the entire image time series in one comprehensive layout, facilitating an easier visual comparison between the images.</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#spectral-trajectories","title":"Spectral trajectories","text":"<p>Next, we\u2019ll present values for three distinct spectral indices. To accomplish this, we\u2019ll adjust our plotting environment. Additionally, the data preparation phase involves extracting a series of 30 values for each spectral index from the attribute table.</p> <pre><code># set up output name of the file\noutput_name = \"theme_2_exercise/results/point_1_trajectory_example.png\" \n\npng(filename = output_name, width = 1920, height = 1080, pointsize = 16)\n\n# plotting area set up as a 4x1 matrix, \n# which will present as 3 long spanning over the whole plotting area\nlayout(matrix(seq(1, 4), 4, 1, byrow = TRUE), \n       heights = c(0.25, 1.25, 1.25, 1.25)) \n\n# the first element of the plot - title; \n# we begin by setting margins of part of the plot \npar(mar = c(0, 0, 0, 0)) \n# new element in the plot, in this case title\nplot.new() \n\n# title will contain point number \n# and change agent retrieved from attribute table\ntext(0.5, 0.5, \n     paste0(\"Spectral trajectories. Point \", \n            point_number, \n            \". Change agent: \", \n            points$chng_agnt[point_number], \".\"), \n     cex = 1.4, \n     font = 1) \n\n# new margins to use for the rest of the plot\npar(mar = c(4, 4, 1, 3)) \n\n# Add NDVI trajectory\n# in this fragment we retrieve spectral index\n# values from attribute table of vector file\nndvi_vals &lt;- points[point_number, ] %&gt;% \n  # we use pipe operator to perform several actions;\n  # first we pick the desired point from the vector file\n  st_drop_geometry() %&gt;% \n  # then we extract just the attribute table \n  # and select only spectral index values\n  select(NDVI_1984:NDVI_2016) %&gt;% \n  # in the end we create a vector of values to plot \n  unlist(., use.names = FALSE)\n\n\n# calculate minimum value - 5%\nmin_val &lt;- min(ndvi_vals, na.rm = TRUE) - 0.05 * abs(min(ndvi_vals, na.rm = TRUE))\n# calculate maximum value + 5%\nmax_val &lt;- max(ndvi_vals, na.rm = TRUE) + 0.05 * abs(max(ndvi_vals, na.rm = TRUE))\ndynamic_range &lt;- c(min_val, max_val)\n# calculate where to plot ablines\nabline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, \n                  by = 0.1)\n\n# we initiate a point-line plot of index values\nplot(ndvi_vals, \n     type = \"b\",\n     main = \"NDVI trajectory\",\n     lwd = 2, \n     xlab = \"\", \n     ylab = \"NDVI\",\n     ylim = dynamic_range, \n     xaxt = \"n\", \n     yaxt = \"n\") \n\n# add y-axis labels on the right side of the plot by 0.1\naxis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n\n# add y-axis labels on the left side of the plot\naxis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n\n# add tics on x-axis\naxis(1, at = 1:30, labels = FALSE) \n\n# Determine the range of y values\ny_range &lt;- diff(dynamic_range)\n# Choose a percentage of the range (e.g., 5%) as a buffer\nbuffer_percent &lt;- 0.1\n# Calculate the position for the text labels\ntext_y_position &lt;- min_val - buffer_percent * y_range\n\n# text with tilted x-axis labels\ntext(x = 1:30, \n     y = text_y_position, \n     labels = years, \n     srt = 35, \n     adj = 1, \n     xpd = TRUE, \n     cex = 1, \n     font = 1) \n\n# we can also add some lines in the plot to ease the reading\nsapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n# now we repeat the above steps to plot the remaining two trajectories\n\n# Add NDMI trajectory\nndmi_vals &lt;- points[point_number, ] %&gt;%\n  st_drop_geometry() %&gt;%\n  select(NDMI_1984:NDMI_2016) %&gt;%\n  unlist(., use.names = FALSE)\n\n\nmin_val &lt;- min(ndmi_vals, na.rm = TRUE) - 0.05 * abs(min(ndmi_vals, na.rm = TRUE))\nmax_val &lt;- max(ndmi_vals, na.rm = TRUE) + 0.05 * abs(max(ndmi_vals, na.rm = TRUE))\ndynamic_range &lt;- c(min_val, max_val)\nabline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, \n                  by = 0.1)\n\nplot(ndmi_vals, \n     type = \"b\",\n     main = \"NDMI trajectory\",\n     lwd = 2, \n     xlab = \"\", \n     ylab = \"NDMI\",\n     ylim = dynamic_range, \n     xaxt = \"n\", \n     yaxt = \"n\") \n\naxis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(1, at = 1:30, labels = FALSE)\n\ny_range &lt;- diff(dynamic_range)\nbuffer_percent &lt;- 0.1\ntext_y_position &lt;- min_val - buffer_percent * y_range\n\ntext(x = 1:30, \n     y = text_y_position, \n     labels = years, \n     srt = 35, \n     adj = 1, \n     xpd = TRUE, \n     cex = 1, \n     font = 1) \n\nsapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n# Add NBR trajectory\nnbr_vals &lt;- points[point_number, ] %&gt;%\n  st_drop_geometry() %&gt;%\n  select(NBR_1984:NBR_2016) %&gt;%\n  unlist(., use.names = FALSE)\n\nmin_val &lt;- min(nbr_vals, na.rm = TRUE) - 0.05 * abs(min(nbr_vals, na.rm = TRUE))\nmax_val &lt;- max(nbr_vals, na.rm = TRUE) + 0.05 * abs(max(nbr_vals, na.rm = TRUE))\ndynamic_range &lt;- c(min_val, max_val)\nabline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2,\n                  by = 0.1)\n\nplot(nbr_vals, \n     type = \"b\",\n     main = \"NBR trajectory\",\n     lwd = 2, \n     xlab = \"\", \n     ylab = \"NBR\",\n     ylim = dynamic_range, \n     xaxt = \"n\", \n     yaxt = \"n\") \naxis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(1, at = 1:30, labels = FALSE) \ny_range &lt;- diff(dynamic_range)\nbuffer_percent &lt;- 0.1\ntext_y_position &lt;- min_val - buffer_percent * y_range\n\ntext(x = 1:30, \n     y = text_y_position, \n     labels = years, \n     srt = 35, \n     adj = 1, \n     xpd = TRUE, \n     cex = 1, \n     font = 1) \n\n\nsapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n# turn off the device - save .png image to working directory\ndev.off() \n</code></pre> <p>The result of the above chunk of code should look like this.</p> <p> <p></p> <p></p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#merge-chips-with-trajectories","title":"Merge chips with trajectories","text":"<p>To streamline the process, we\u2019ll encapsulate the aforementioned functions within a loop. This loop will enable the repetitive execution of all the steps for every distinct point, eventually combining the two output images\u2014both chips and trajectories\u2014into a singular cohesive image. While much of the code remains consistent, there will be several modifications to ensure the loop\u2019s smooth operation. These changes in the code will be clearly annotated within the code chunk for clarity.</p> <pre><code># enable reading pixel values from multiband rasters\nreadStart(green)\nreadStart(red)\nreadStart(nir)\n\n# set up window size for all elements which will come out of the loop\nwindow_size &lt;- 39 \n\n# the loop will last till all of the first 9 points in the layer are used\nfor (i in seq(1, 9)){ \n\n\n  point_cords &lt;- st_coordinates(points)[i, ] \n  row &lt;- colFromX(green, point_cords[1])\n  col &lt;- rowFromY(green, point_cords[2])\n\n  half_widow_size &lt;- floor(window_size / 2)\n  col_cords &lt;- (col - half_widow_size) : (col + half_widow_size)\n  row_cords &lt;- (row - half_widow_size) : (row + half_widow_size)\n\n  # name of the file will contain information about point \n  # number and change agent\n  output_name &lt;- paste0(\"theme_2_exercise/results/Point \", \n                        i, \n                        \". \", \n                        points$chng_agnt[i], \".png\") \n\n  png(filename = output_name, width = 1920, height = 1500, pointsize = 16)\n\n  # plot area divided into more parts to fit all of the components\n  layout(matrix(c(rep(1, 10), seq(2, 31), rep(32, 10), rep(33, 10), rep(34, 10)),\n                7, 10, byrow = TRUE), \n         heights = c(0.25, 1, 1, 1, 2, 2, 2)) \n\n  par(mar = c(0, 0, 0, 0))\n  plot.new()\n  text(0.5, 0.5, paste0(\"CIR chips and spectral trajectories. Point \", \n                        i, \n                        \". Change agent: \", \n                        points$chng_agnt[i], \".\"), \n       cex = 1.4, font = 1)\n\n  par(mar = c(0,0,1,0))\n  for (j in seq(30)){\n\n\n    o_b1 &lt;- rast(matrix(readValues(green[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))  \n\n\n    o_b2 &lt;- rast(matrix(readValues(red[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    o_b3 &lt;- rast(matrix(readValues(nir[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b1 &lt;- (o_b1 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b2 &lt;- (o_b2 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 4000\n    o_b3 &lt;- (o_b3 - b_min) / (b_max - b_min) * 255 \n\n\n    o_b1[o_b1 &lt; 0] &lt;- 0 \n    o_b2[o_b2 &lt; 0] &lt;- 0 \n    o_b3[o_b3 &lt; 0] &lt;- 0\n\n    o_b1[o_b1 &gt; 255] &lt;- 255\n    o_b2[o_b2 &gt; 255] &lt;- 255 \n    o_b3[o_b3 &gt; 255] &lt;- 255\n\n\n    par(mar = c(0,1,1,1)) \n\n\n    plotRGB(c(o_b3, o_b2, o_b1), \n            r = 1, g = 2, b = 3,\n            mar = 1) \n\n\n    points(20, 19.4, pch = \".\", col = c(\"white\"))\n    symbols(x = 19.6, y = 19.4, \n            squares = 3.2, \n            inches = F, \n            add = T, \n            fg = \"yellow\", \n            lwd = 0.01)\n\n    # show date of the image acquisition above the RGB chip\n    title(years[j], line = 0.2) \n\n  }\n\n\n  par(mar = c(4, 4, 1, 3))\n\n  ndvi_vals &lt;- points[i, ] %&gt;% \n    st_drop_geometry() %&gt;% \n    select(NDVI_1984:NDVI_2016) %&gt;% \n    unlist(., use.names = FALSE)\n\n  # calculate minimum value - 5%\n  min_val &lt;- min(ndvi_vals, na.rm = TRUE) - 0.05 * abs(min(ndvi_vals, na.rm = TRUE))\n  # calculate maximum value + 5%\n  max_val &lt;- max(ndvi_vals, na.rm = TRUE) + 0.05 * abs(max(ndvi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  # calculate where to plot ablines\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, \n                    by = 0.1)\n\n  plot(ndvi_vals, \n       type = \"b\",\n       main = \"NDVI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDVI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n\n  # Determine the range of y values\n  y_range &lt;- diff(dynamic_range)\n  # Choose a percentage of the range (e.g., 5%) as a buffer\n  buffer_percent &lt;- 0.1\n  # Calculate the position for the text labels\n  text_y_position &lt;- min_val - buffer_percent * y_range\n\n  text(x = 1:30, \n       y = text_y_position, \n       labels = years, \n       srt = 35, \n       adj = 1, \n       xpd = TRUE, \n       cex = 1, \n       font = 1) \n\n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n  ndmi_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NDMI_1984:NDMI_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(ndmi_vals, na.rm = TRUE) - 0.05 * abs(min(ndmi_vals, na.rm = TRUE))\n  max_val &lt;- max(ndmi_vals, na.rm = TRUE) + 0.05 * abs(max(ndmi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2,\n                    by = 0.1)\n\n  plot(ndmi_vals, \n       type = \"b\",\n       main = \"NDMI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDMI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  nbr_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NBR_1984:NBR_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(nbr_vals, na.rm = TRUE) - 0.05 * abs(min(nbr_vals, na.rm = TRUE))\n  max_val &lt;- max(nbr_vals, na.rm = TRUE) + 0.05 * abs(max(nbr_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2,\n                    by = 0.1)\n\n  plot(nbr_vals, \n       type = \"b\",\n       main = \"NBR trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NBR\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  dev.off() \n\n}\n\n# close files - we read necessary values\nreadStop(green)\nreadStop(red)\nreadStop(nir)\n</code></pre> <p>The first resulting image should be looking like the example below.</p> <p> <p></p> <p></p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#assesment","title":"Assesment","text":"<p>Take a moment to thoroughly analyze the resultant nine images. Your primary focus should be on discerning the changes apparent within the chips. Attempt to identify the year of the change through visual interpretation, and cross-reference your findings with the date provided in the attribute table and the image\u2019s title. Additionally, endeavor to associate a change agent with the spectral index that most prominently highlights the change moment. This exercise will prove invaluable for the concluding segment, wherein you\u2019ll be tasked with discerning the date of the change and correlating the change agent autonomously.</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#self-training","title":"Self training","text":"<p>Having absorbed the insights from the previous segment, let\u2019s now apply this knowledge to a new data set. This data set, however, lacks attributes that specify the year of change and the associated change agent. Your mission? Accurately identify these attributes.</p> <p>TASK</p> <ul> <li>Execute the earlier steps for points 10-13 from <code>T2_tatra_mountains_change_points</code> (<code>points</code> variable)</li> <li>Set <code>window_size</code> to 39</li> <li>Within the loop, modify <code>output_name</code> to <code>paste0(\"Training point \", i, \". \", points$chng_agnt[[i]], \".png\")</code></li> <li>Also within the loop, change text displayed inside the image to <code>paste0(\"Training point \", i, \".\")</code></li> <li>Initiate the loop to produce chips images for <code>points[10:13, ]</code> layer.</li> </ul> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.   Code chunk   <pre><code># enable reading pixel values from multiband rasters\nreadStart(green)\nreadStart(red)\nreadStart(nir)\n\n# set up window size for all elements which will come out of the loop\nwindow_size &lt;- 39 \n\n# the loop will last till all of the first 9 points in the layer are used\nfor (i in seq(10, 13)){ \n\n\n  point_cords &lt;- st_coordinates(points)[i, ] \n  row &lt;- colFromX(green, point_cords[1])\n  col &lt;- rowFromY(green, point_cords[2])\n\n  half_widow_size &lt;- floor(window_size / 2)\n  col_cords &lt;- (col - half_widow_size) : (col + half_widow_size)\n  row_cords &lt;- (row - half_widow_size) : (row + half_widow_size)\n\n  # name of the file will contain information about point \n  # number and change agent\n  output_name &lt;- paste0(\"theme_2_exercise/results/Training point \", i, \".png\") \n\n  png(filename = output_name, width = 1920, height = 1500, pointsize = 16)\n\n  # plot area divided into more parts to fit all of the components\n  layout(matrix(c(rep(1, 10), seq(2, 31), rep(32, 10), rep(33, 10), rep(34, 10)),\n                7, 10, byrow = TRUE), \n         heights = c(0.25, 1, 1, 1, 2, 2, 2)) \n\n  par(mar = c(0, 0, 0, 0))\n  plot.new()\n  text(0.5, 0.5, paste0(\"Training point \", i, \".\"), cex = 1.4, font = 1)\n\n  par(mar = c(0,0,1,0))\n  for (j in seq(30)){\n\n\n    o_b1 &lt;- rast(matrix(readValues(green[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))  \n\n\n    o_b2 &lt;- rast(matrix(readValues(red[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    o_b3 &lt;- rast(matrix(readValues(nir[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b1 &lt;- (o_b1 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b2 &lt;- (o_b2 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 4000\n    o_b3 &lt;- (o_b3 - b_min) / (b_max - b_min) * 255 \n\n\n    o_b1[o_b1 &lt; 0] &lt;- 0 \n    o_b2[o_b2 &lt; 0] &lt;- 0 \n    o_b3[o_b3 &lt; 0] &lt;- 0\n\n    o_b1[o_b1 &gt; 255] &lt;- 255\n    o_b2[o_b2 &gt; 255] &lt;- 255 \n    o_b3[o_b3 &gt; 255] &lt;- 255\n\n\n    par(mar = c(0,1,1,1)) \n\n\n    plotRGB(c(o_b3, o_b2, o_b1), \n            r = 1, g = 2, b = 3,\n            mar = 1) \n\n\n    points(20, 19.4, pch = \".\", col = c(\"white\"))\n    symbols(x = 19.6, y = 19.4,  squares = 3.2, inches = F, add = T, fg = \"yellow\", lwd = 0.01)\n\n    # show date of the image acquisition above the RGB chip\n    title(years[j], line = 0.2) \n\n  }\n\n\n  par(mar = c(4, 4, 1, 3))\n\n  ndvi_vals &lt;- points[i, ] %&gt;% \n    st_drop_geometry() %&gt;% \n    select(NDVI_1984:NDVI_2016) %&gt;% \n    unlist(., use.names = FALSE)\n\n  # calculate minimum value - 5%\n  min_val &lt;- min(ndvi_vals, na.rm = TRUE) - 0.05 * abs(min(ndvi_vals, na.rm = TRUE))\n  # calculate maximum value + 5%\n  max_val &lt;- max(ndvi_vals, na.rm = TRUE) + 0.05 * abs(max(ndvi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  # calculate where to plot ablines\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(ndvi_vals, \n       type = \"b\",\n       main = \"NDVI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDVI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n\n  # Determine the range of y values\n  y_range &lt;- diff(dynamic_range)\n  # Choose a percentage of the range (e.g., 5%) as a buffer\n  buffer_percent &lt;- 0.1\n  # Calculate the position for the text labels\n  text_y_position &lt;- min_val - buffer_percent * y_range\n\n  text(x = 1:30, \n       y = text_y_position, \n       labels = years, \n       srt = 35, \n       adj = 1, \n       xpd = TRUE, \n       cex = 1, \n       font = 1) \n\n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n  ndmi_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NDMI_1984:NDMI_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(ndmi_vals, na.rm = TRUE) - 0.05 * abs(min(ndmi_vals, na.rm = TRUE))\n  max_val &lt;- max(ndmi_vals, na.rm = TRUE) + 0.05 * abs(max(ndmi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(ndmi_vals, \n       type = \"b\",\n       main = \"NDMI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDMI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  nbr_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NBR_1984:NBR_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(nbr_vals, na.rm = TRUE) - 0.05 * abs(min(nbr_vals, na.rm = TRUE))\n  max_val &lt;- max(nbr_vals, na.rm = TRUE) + 0.05 * abs(max(nbr_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(nbr_vals, \n       type = \"b\",\n       main = \"NBR trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NBR\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1) \n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  dev.off() \n\n}\n\n# close files - we read necessary values\nreadStop(green)\nreadStop(red)\nreadStop(nir)\n</code></pre> <p></p> <p>This should be the resulting image of the task above - image of point 13.</p> <p>Now try to answer these questions. When you are confident in your answer, expand text to see the correct answers.</p>  Question 1: In what year can we observe the change in Point 13?  A1: 2007   Question 2: What is the most probable cause of change in Point 13?  A2: Construction of a ski slope.  <p></p> <p>TRAINING TASK</p> <p>Try to answer similar questions regarding rest of the points in the self training layer. When you are ready check the correct answers here.</p>  Answers  Point 10: landslide Point 11: bark beetle outbreak Point 12: windthrow"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#data-and-software-credits","title":"Data and software credits","text":""},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#data_1","title":"Data","text":"<p>Landsat 5, 7 and 8 imagery courtesy of the U.S. Geological Survey/ Terms of use</p>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#software","title":"Software","text":"<ul> <li>R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.</li> <li>Hijmans R (2023). terra: Spatial Data Analysis. R package version 1.7-39, https://CRAN.R-project.org/package=terra</li> <li>Pebesma, E., &amp; Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016</li> <li>Pebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009</li> <li>Wickham H, Fran\u00e7ois R, Henry L, M\u00fcller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr</li> </ul>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#source-code","title":"Source code","text":"You can find the entire code used in this exercise here  <pre><code># raster processing\nlibrary(terra)\n\n# vector and attributes handling\nlibrary(sf)\n\n# tabular data manipulation\nlibrary(dplyr)\n\n\n# Multiband raster containing 30 green bands from 1984-2016 period\ngreen &lt;- rast(\"theme_2_exercise/data_exercise/T2_green_stack.tif\") \n\n# Multiband raster containing 30 red bands from 1984-2016 period\nred &lt;- rast(\"theme_2_exercise/data_exercise/T2_red_stack.tif\") \n\n# Multiband raster containing 30 NIR bands from 1984-2016 period\nnir &lt;- rast(\"theme_2_exercise/data_exercise/T2_nir_stack.tif\") \n\n\ne &lt;- c(400000, 460000, 5410000, 5415000)\nplot(nir, 1:6, \n     col = colorRampPalette(c(\"black\", \"white\"))(255), \n     range = c(0, 10000), \n     mar = NA,\n     plg = list(ext = e, loc = \"bottom\"))\n\n\n# vector of dates assigned to subsequent raster layers in each band brick\nyears &lt;- as.Date(c(\"1984-07-31\", \"1985-08-03\",\"1986-08-22\", \"1987-07-08\", \"1988-08-18\", \"1989-07-04\", \"1991-09-05\", \"1992-07-20\", \n                   \"1993-08-16\", \"1994-08-28\", \"1995-07-21\", \"1996-08-24\", \"1997-09-12\", \"1999-08-09\", \"2000-08-20\", \"2001-08-15\", \"2002-06-15\", \n                   \"2003-08-12\", \"2005-09-02\", \"2006-07-19\", \"2007-08-23\", \"2008-09-02\", \"2009-08-21\", \"2010-08-23\", \"2011-08-27\", \n                   \"2012-08-28\", \"2013-09-08\",\"2014-09-04\", \"2015-07-12\", \"2016-08-08\")) \n\n\n# set of 13 reference points with attribute table\npoints &lt;- st_read(\"theme_2_exercise/data_exercise/T2_tatra_mountains_change_points.shp\") \n\n\n# indicate the index (row) of point as read into the environment\npoint_number &lt;- 1 \n\n# retrieve coordinates from point\npoint_cords &lt;- st_coordinates(points)[point_number, ]\n\n# retrieve image column, where the point lies\nrow &lt;- colFromX(green, point_cords[1]) \n\n# rerieve image row, where the point lies\ncol &lt;- rowFromY(green, point_cords[2]) \n\n# set up size of the image chip\nwindow_size &lt;- 39 \n\n# half of the window size\nhalf_widow_size &lt;- floor(window_size / 2) \n\n# image columns, which will we used for image chip visualisation\ncol_cords &lt;- (col - half_widow_size) : (col + half_widow_size) \n\n# image rows, which will we used for image chip visualisation\nrow_cords &lt;- (row - half_widow_size) : (row + half_widow_size) \n\n\n\n# set up output name of the file\noutput_name &lt;- \"theme_2_exercise/results/point_1_chips_example.png\" \n\n# initialize device, plotting area 1920x1080 px, text size 16 points\npng(filename = output_name, \n    width = 1920, \n    height = 800, \n    pointsize = 16) \n\n# plotting area set up as a 3x10 matrix\nlayout(matrix(seq(1,30),3, 10, byrow = TRUE), heights = c(1,1,1)) \n\n\n\n# enable reading pixel values from multiband rasters\nreadStart(green)\nreadStart(red)\nreadStart(nir)\n\n# loop 30 times - once for each raster layer in the brick\nfor (j in seq(30)){ \n\n\n\n  # prepare image slice from the appropriate green raster using previously prepared rows/columns\n  o_b1 &lt;- rast(matrix(readValues(green[[j]], \n                                 col = row_cords[1], \n                                 nrows = window_size, \n                                 row = col_cords[1],\n                                 ncols = window_size),\n                      nrow = window_size, \n                      ncol = window_size, \n                      byrow = TRUE)) \n\n  # prepare image slice from the appropriate red raster using previously prepared rows/columns\n  o_b2 &lt;- rast(matrix(readValues(red[[j]], \n                                 col = row_cords[1], \n                                 nrows = window_size, \n                                 row = col_cords[1],\n                                 ncols = window_size),\n                      nrow = window_size, \n                      ncol = window_size, \n                      byrow = TRUE))\n\n  # prepare image slice from the appropriate green raster using previously prepared rows/columns\n  o_b3 &lt;- rast(matrix(readValues(nir[[j]], \n                                 col = row_cords[1], \n                                 nrows = window_size, \n                                 row = col_cords[1],\n                                 ncols = window_size),\n                      nrow = window_size, \n                      ncol = window_size, \n                      byrow = TRUE))\n\n\n\n  # trim histogram of green band for harmonized viewing of the whole set\n  b_min &lt;- 10\n  b_max &lt;- 1000\n  o_b1 &lt;- (o_b1 - b_min) / (b_max - b_min) * 255 \n\n  # trim histogram of red band for harmonized viewing of the whole set\n  b_min &lt;- 10\n  b_max &lt;- 1000\n  o_b2 &lt;- (o_b2 - b_min) / (b_max - b_min) * 255 \n\n  # trim histogram of NIR band for harmonized viewing of the whole set\n  b_min &lt;- 10\n  b_max &lt;- 4000\n  o_b3 &lt;- (o_b3 - b_min) / (b_max - b_min) * 255 \n\n  # convert any negative values to 0 for better viewing\n  o_b1[o_b1 &lt; 0] &lt;- 0 \n  o_b2[o_b2 &lt; 0] &lt;- 0 \n  o_b3[o_b3 &lt; 0] &lt;- 0\n\n  # convert any values above 255 to 0\n  o_b1[o_b1 &gt; 255] &lt;- 255\n  o_b2[o_b2 &gt; 255] &lt;- 255 \n  o_b3[o_b3 &gt; 255] &lt;- 255\n\n  # set up margins around each of 30 plotting blocks\n  par(mar = c(0,1,1,1)) \n\n  # plot RGB chip in the appropriate place in the layout \n  plotRGB(c(o_b3, o_b2, o_b1), \n          r = 1, g = 2, b = 3,\n          mar = 1) \n\n  # draw the location of reference point \n  points(20, 19.4, pch = \".\", col = c(\"white\"))\n  symbols(x = 19.6, y = 19.4,  squares = 3, inches = F, add = T, fg = \"yellow\", lwd = 0.01)\n\n  # show date of the image acquisition above the RGB chip\n  title(years[j], line = -2) \n\n}\n\n# close files - we read necessary values\nreadStop(green)\nreadStop(red)\nreadStop(nir)\n\n# turn off the device - save .png image to working directory\ndev.off() \n\n\n\n\n# set up output name of the file\noutput_name = \"theme_2_exercise/results/point_1_trajectory_example.png\" \n\npng(filename = output_name, width = 1920, height = 1080, pointsize = 16)\n\n# plotting area set up as a 4x1 matrix, \n# which will present as 3 long spanning over the whole plotting area\nlayout(matrix(seq(1, 4), 4, 1, byrow = TRUE), \n       heights = c(0.25, 1.25, 1.25, 1.25)) \n\n# the first element of the plot - title; we begin by setting margins of part of the plot \npar(mar = c(0, 0, 0, 0)) \n# new element in the plot, in this case title\nplot.new() \n\n# title will contain point number and change agent retrieved from attribute table\ntext(0.5, 0.5, \n     paste0(\"Spectral trajectories. Point \", point_number, \". Change agent: \", points$chng_agnt[point_number], \".\"), \n     cex = 1.4, \n     font = 1) \n\n# new margins to use for the rest of the plot\npar(mar = c(4, 4, 1, 3)) \n\n# Add NDVI trajectory\n# in this fragment we retrieve spectral index values from attribute table of vector file\nndvi_vals &lt;- points[point_number, ] %&gt;% \n  # we use pipe operator to perform several actions; first we pick the desired point from the vector file\n  st_drop_geometry() %&gt;% \n  # then we extract just the attribute table and select only spectral index values\n  select(NDVI_1984:NDVI_2016) %&gt;% \n  # in the end we create a vector of values to plot \n  unlist(., use.names = FALSE)\n\n\n# calculate minimum value - 5%\nmin_val &lt;- min(ndvi_vals, na.rm = TRUE) - 0.05 * abs(min(ndvi_vals, na.rm = TRUE))\n# calculate maximum value + 5%\nmax_val &lt;- max(ndvi_vals, na.rm = TRUE) + 0.05 * abs(max(ndvi_vals, na.rm = TRUE))\ndynamic_range &lt;- c(min_val, max_val)\n# calculate where to plot ablines\nabline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n# we initiate a point-line plot of index values\nplot(ndvi_vals, \n     type = \"b\",\n     main = \"NDVI trajectory\",\n     lwd = 2, \n     xlab = \"\", \n     ylab = \"NDVI\",\n     ylim = dynamic_range, \n     xaxt = \"n\", \n     yaxt = \"n\") \n\n# add y-axis labels on the right side of the plot by 0.1\naxis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n\n# add y-axis labels on the left side of the plot\naxis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n\n# add tics on x-axis\naxis(1, at = 1:30, labels = FALSE) \n\n# Determine the range of y values\ny_range &lt;- diff(dynamic_range)\n# Choose a percentage of the range (e.g., 5%) as a buffer\nbuffer_percent &lt;- 0.1\n# Calculate the position for the text labels\ntext_y_position &lt;- min_val - buffer_percent * y_range\n\n# text with tilted x-axis labels\ntext(x = 1:30, \n     y = text_y_position, \n     labels = years, \n     srt = 35, \n     adj = 1, \n     xpd = TRUE, \n     cex = 1, \n     font = 1) \n\n# we can also add some lines in the plot to ease the reading\nsapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n# now we repeat the above steps to plot the remaining two trajectories\n\n# Add NDMI trajectory\nndmi_vals &lt;- points[point_number, ] %&gt;%\n  st_drop_geometry() %&gt;%\n  select(NDMI_1984:NDMI_2016) %&gt;%\n  unlist(., use.names = FALSE)\n\n\nmin_val &lt;- min(ndmi_vals, na.rm = TRUE) - 0.05 * abs(min(ndmi_vals, na.rm = TRUE))\nmax_val &lt;- max(ndmi_vals, na.rm = TRUE) + 0.05 * abs(max(ndmi_vals, na.rm = TRUE))\ndynamic_range &lt;- c(min_val, max_val)\nabline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\nplot(ndmi_vals, \n     type = \"b\",\n     main = \"NDMI trajectory\",\n     lwd = 2, \n     xlab = \"\", \n     ylab = \"NDMI\",\n     ylim = dynamic_range, \n     xaxt = \"n\", \n     yaxt = \"n\") \n\naxis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(1, at = 1:30, labels = FALSE)\n\ny_range &lt;- diff(dynamic_range)\nbuffer_percent &lt;- 0.1\ntext_y_position &lt;- min_val - buffer_percent * y_range\n\ntext(x = 1:30, \n     y = text_y_position, \n     labels = years, \n     srt = 35, \n     adj = 1, \n     xpd = TRUE, \n     cex = 1, \n     font = 1) \n\nsapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n# Add NBR trajectory\nnbr_vals &lt;- points[point_number, ] %&gt;%\n  st_drop_geometry() %&gt;%\n  select(NBR_1984:NBR_2016) %&gt;%\n  unlist(., use.names = FALSE)\n\nmin_val &lt;- min(nbr_vals, na.rm = TRUE) - 0.05 * abs(min(nbr_vals, na.rm = TRUE))\nmax_val &lt;- max(nbr_vals, na.rm = TRUE) + 0.05 * abs(max(nbr_vals, na.rm = TRUE))\ndynamic_range &lt;- c(min_val, max_val)\nabline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\nplot(nbr_vals, \n     type = \"b\",\n     main = \"NBR trajectory\",\n     lwd = 2, \n     xlab = \"\", \n     ylab = \"NBR\",\n     ylim = dynamic_range, \n     xaxt = \"n\", \n     yaxt = \"n\") \naxis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\naxis(1, at = 1:30, labels = FALSE) \ny_range &lt;- diff(dynamic_range)\nbuffer_percent &lt;- 0.1\ntext_y_position &lt;- min_val - buffer_percent * y_range\n\ntext(x = 1:30, \n     y = text_y_position, \n     labels = years, \n     srt = 35, \n     adj = 1, \n     xpd = TRUE, \n     cex = 1, \n     font = 1) \n\n\nsapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n# turn off the device - save .png image to working directory\ndev.off() \n\n\n\n# enable reading pixel values from multiband rasters\nreadStart(green)\nreadStart(red)\nreadStart(nir)\n\n# set up window size for all elements which will come out of the loop\nwindow_size &lt;- 39 \n\n# the loop will last till all of the first 9 points in the layer are used\nfor (i in seq(1, 9)){ \n\n\n  point_cords &lt;- st_coordinates(points)[i, ] \n  row &lt;- colFromX(green, point_cords[1])\n  col &lt;- rowFromY(green, point_cords[2])\n\n  half_widow_size &lt;- floor(window_size / 2)\n  col_cords &lt;- (col - half_widow_size) : (col + half_widow_size)\n  row_cords &lt;- (row - half_widow_size) : (row + half_widow_size)\n\n  # name of the file will contain information about point \n  # number and change agent\n  output_name &lt;- paste0(\"theme_2_exercise/results/Point \", i, \". \", points$chng_agnt[i], \".png\") \n\n  png(filename = output_name, width = 1920, height = 1500, pointsize = 16)\n\n  layout(matrix(c(rep(1, 10), seq(2, 31), rep(32, 10), rep(33, 10), rep(34, 10)),\n                7, 10, byrow = TRUE), \n         heights = c(0.25, 1, 1, 1, 2, 2, 2)) # plot area divided into more parts to fit all of the components\n\n  par(mar = c(0, 0, 0, 0))\n  plot.new()\n  text(0.5, 0.5, paste0(\"CIR chips and spectral trajectories. Point \", i, \". Change agent: \", points$chng_agnt[i], \".\"), cex = 1.4, font = 1)\n\n  par(mar = c(0,0,1,0))\n  for (j in seq(30)){\n\n\n    o_b1 &lt;- rast(matrix(readValues(green[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))  \n\n\n    o_b2 &lt;- rast(matrix(readValues(red[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    o_b3 &lt;- rast(matrix(readValues(nir[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b1 &lt;- (o_b1 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b2 &lt;- (o_b2 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 4000\n    o_b3 &lt;- (o_b3 - b_min) / (b_max - b_min) * 255 \n\n\n    o_b1[o_b1 &lt; 0] &lt;- 0 \n    o_b2[o_b2 &lt; 0] &lt;- 0 \n    o_b3[o_b3 &lt; 0] &lt;- 0\n\n    o_b1[o_b1 &gt; 255] &lt;- 255\n    o_b2[o_b2 &gt; 255] &lt;- 255 \n    o_b3[o_b3 &gt; 255] &lt;- 255\n\n\n    par(mar = c(0,1,1,1)) \n\n\n    plotRGB(c(o_b3, o_b2, o_b1), \n            r = 1, g = 2, b = 3,\n            mar = 1) \n\n\n    points(20, 19.4, pch = \".\", col = c(\"white\"))\n    symbols(x = 19.6, y = 19.4,  squares = 3.2, inches = F, add = T, fg = \"yellow\", lwd = 0.01)\n\n    # show date of the image acquisition above the RGB chip\n    title(years[j], line = 0.2) \n\n  }\n\n\n  par(mar = c(4, 4, 1, 3))\n\n  ndvi_vals &lt;- points[i, ] %&gt;% \n    st_drop_geometry() %&gt;% \n    select(NDVI_1984:NDVI_2016) %&gt;% \n    unlist(., use.names = FALSE)\n\n  # calculate minimum value - 5%\n  min_val &lt;- min(ndvi_vals, na.rm = TRUE) - 0.05 * abs(min(ndvi_vals, na.rm = TRUE))\n  # calculate maximum value + 5%\n  max_val &lt;- max(ndvi_vals, na.rm = TRUE) + 0.05 * abs(max(ndvi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  # calculate where to plot ablines\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(ndvi_vals, \n       type = \"b\",\n       main = \"NDVI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDVI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n\n  # Determine the range of y values\n  y_range &lt;- diff(dynamic_range)\n  # Choose a percentage of the range (e.g., 5%) as a buffer\n  buffer_percent &lt;- 0.1\n  # Calculate the position for the text labels\n  text_y_position &lt;- min_val - buffer_percent * y_range\n\n  text(x = 1:30, \n       y = text_y_position, \n       labels = years, \n       srt = 35, \n       adj = 1, \n       xpd = TRUE, \n       cex = 1, \n       font = 1) \n\n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n  ndmi_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NDMI_1984:NDMI_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(ndmi_vals, na.rm = TRUE) - 0.05 * abs(min(ndmi_vals, na.rm = TRUE))\n  max_val &lt;- max(ndmi_vals, na.rm = TRUE) + 0.05 * abs(max(ndmi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(ndmi_vals, \n       type = \"b\",\n       main = \"NDMI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDMI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  nbr_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NBR_1984:NBR_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(nbr_vals, na.rm = TRUE) - 0.05 * abs(min(nbr_vals, na.rm = TRUE))\n  max_val &lt;- max(nbr_vals, na.rm = TRUE) + 0.05 * abs(max(nbr_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(nbr_vals, \n       type = \"b\",\n       main = \"NBR trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NBR\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  dev.off() \n\n}\n\n# close files - we read necessary values\nreadStop(green)\nreadStop(red)\nreadStop(nir)\n\n\n\n\n\n# enable reading pixel values from multiband rasters\nreadStart(green)\nreadStart(red)\nreadStart(nir)\n\n# set up window size for all elements which will come out of the loop\nwindow_size &lt;- 39 \n\n# the loop will last till all of the first 9 points in the layer are used\nfor (i in seq(10, 13)){ \n\n\n  point_cords &lt;- st_coordinates(points)[i, ] \n  row &lt;- colFromX(green, point_cords[1])\n  col &lt;- rowFromY(green, point_cords[2])\n\n  half_widow_size &lt;- floor(window_size / 2)\n  col_cords &lt;- (col - half_widow_size) : (col + half_widow_size)\n  row_cords &lt;- (row - half_widow_size) : (row + half_widow_size)\n\n  # name of the file will contain information about point \n  # number and change agent\n  output_name &lt;- paste0(\"theme_2_exercise/results/Training point \", i, \".png\") \n\n  png(filename = output_name, width = 1920, height = 1500, pointsize = 16)\n\n  layout(matrix(c(rep(1, 10), seq(2, 31), rep(32, 10), rep(33, 10), rep(34, 10)),\n                7, 10, byrow = TRUE), \n         heights = c(0.25, 1, 1, 1, 2, 2, 2)) # plot area divided into more parts to fit all of the components\n\n  par(mar = c(0, 0, 0, 0))\n  plot.new()\n  text(0.5, 0.5, paste0(\"Training point \", i, \".\"), cex = 1.4, font = 1)\n\n  par(mar = c(0,0,1,0))\n  for (j in seq(30)){\n\n\n    o_b1 &lt;- rast(matrix(readValues(green[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))  \n\n\n    o_b2 &lt;- rast(matrix(readValues(red[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    o_b3 &lt;- rast(matrix(readValues(nir[[j]], \n                                   col = row_cords[1], \n                                   nrows = window_size, \n                                   row = col_cords[1],\n                                   ncols = window_size),\n                        nrow = window_size, \n                        ncol = window_size, \n                        byrow = TRUE))\n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b1 &lt;- (o_b1 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 1000\n    o_b2 &lt;- (o_b2 - b_min) / (b_max - b_min) * 255 \n\n\n    b_min &lt;- 10\n    b_max &lt;- 4000\n    o_b3 &lt;- (o_b3 - b_min) / (b_max - b_min) * 255 \n\n\n    o_b1[o_b1 &lt; 0] &lt;- 0 \n    o_b2[o_b2 &lt; 0] &lt;- 0 \n    o_b3[o_b3 &lt; 0] &lt;- 0\n\n    o_b1[o_b1 &gt; 255] &lt;- 255\n    o_b2[o_b2 &gt; 255] &lt;- 255 \n    o_b3[o_b3 &gt; 255] &lt;- 255\n\n\n    par(mar = c(0,1,1,1)) \n\n\n    plotRGB(c(o_b3, o_b2, o_b1), \n            r = 1, g = 2, b = 3,\n            mar = 1) \n\n\n    points(20, 19.4, pch = \".\", col = c(\"white\"))\n    symbols(x = 19.6, y = 19.4,  squares = 3.2, inches = F, add = T, fg = \"yellow\", lwd = 0.01)\n\n    # show date of the image acquisition above the RGB chip\n    title(years[j], line = 0.2) \n\n  }\n\n\n  par(mar = c(4, 4, 1, 3))\n\n  ndvi_vals &lt;- points[i, ] %&gt;% \n    st_drop_geometry() %&gt;% \n    select(NDVI_1984:NDVI_2016) %&gt;% \n    unlist(., use.names = FALSE)\n\n  # calculate minimum value - 5%\n  min_val &lt;- min(ndvi_vals, na.rm = TRUE) - 0.05 * abs(min(ndvi_vals, na.rm = TRUE))\n  # calculate maximum value + 5%\n  max_val &lt;- max(ndvi_vals, na.rm = TRUE) + 0.05 * abs(max(ndvi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  # calculate where to plot ablines\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(ndvi_vals, \n       type = \"b\",\n       main = \"NDVI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDVI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n\n  # Determine the range of y values\n  y_range &lt;- diff(dynamic_range)\n  # Choose a percentage of the range (e.g., 5%) as a buffer\n  buffer_percent &lt;- 0.1\n  # Calculate the position for the text labels\n  text_y_position &lt;- min_val - buffer_percent * y_range\n\n  text(x = 1:30, \n       y = text_y_position, \n       labels = years, \n       srt = 35, \n       adj = 1, \n       xpd = TRUE, \n       cex = 1, \n       font = 1) \n\n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n\n  ndmi_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NDMI_1984:NDMI_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(ndmi_vals, na.rm = TRUE) - 0.05 * abs(min(ndmi_vals, na.rm = TRUE))\n  max_val &lt;- max(ndmi_vals, na.rm = TRUE) + 0.05 * abs(max(ndmi_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(ndmi_vals, \n       type = \"b\",\n       main = \"NDMI trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NDMI\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  nbr_vals &lt;- points[i, ] %&gt;%\n    st_drop_geometry() %&gt;%\n    select(NBR_1984:NBR_2016) %&gt;%\n    unlist(., use.names = FALSE)\n\n  min_val &lt;- min(nbr_vals, na.rm = TRUE) - 0.05 * abs(min(nbr_vals, na.rm = TRUE))\n  max_val &lt;- max(nbr_vals, na.rm = TRUE) + 0.05 * abs(max(nbr_vals, na.rm = TRUE))\n  dynamic_range &lt;- c(min_val, max_val)\n  abline_seq &lt;- seq(floor(min_val / 0.2) * 0.2, ceiling(max_val / 0.2) * 0.2, by = 0.1)\n\n  plot(nbr_vals, \n       type = \"b\",\n       main = \"NBR trajectory\",\n       lwd = 2, \n       xlab = \"\", \n       ylab = \"NBR\",\n       ylim = dynamic_range, \n       xaxt = \"n\", \n       yaxt = \"n\") \n\n  axis(2, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1)\n  axis(4, at = seq(floor(min_val), ceiling(max_val), by = 0.1), las = 1) \n  axis(1, at = 1:30, labels = FALSE) \n  y_range &lt;- diff(dynamic_range)\n  buffer_percent &lt;- 0.1\n  text_y_position &lt;- min_val - buffer_percent * y_range\n  text(x = 1:30, y = text_y_position, \n       labels = years, srt = 35, adj = 1, xpd = TRUE, cex = 1, font = 1) \n  sapply(abline_seq, function(h) abline(h = h, lty = 3))\n\n  dev.off() \n\n}\n\n# close files - we read necessary values\nreadStop(green)\nreadStop(red)\nreadStop(nir)\n\n</code></pre>"},{"location":"module2/02_temporal_information/02_temporal_information_exercise.html#this-is-the-end-of-this-exercise-proceed-with-other-themes-and-exercises-good-luck","title":"This is the end of this exercise. Proceed with other Themes and Exercises. Good luck!","text":""},{"location":"module2/03_image_processing/03_image_processing.html","title":"Image processing","text":"<p>Preprocessing of the images is a crucial step in remote sensing data analysis. Any defects of data will have an impact on the accuracy of results. Before using any dataset we recommend you to read the data manual which helps you select the proper processing pipeline for your certain purpose. Preparing images for analysis is often the most time consuming and computationally intensive part of dealing with remote sensing data, so it is essential to apply necessary tools and algorithms thoughtfully.</p> <p>In this theme you will learn about:</p> <ul> <li>goals of automating processes</li> <li>radiometric and geometric correction</li> <li>unwanted areas detection and masking</li> <li>missing information reconstruction</li> <li>data fusion</li> <li>data harmonization / normalization</li> <li>tools and algorithms used with satellite multispectral image/time series processing</li> </ul> <p>This theme will conclude with:</p> <ul> <li>Self-evaluation quiz</li> <li>References</li> </ul> <p>To consolidate your knowledge and test it in more practical environment complete the Exercise.</p> <p>After finishing this theme you will:</p> <ul> <li>understand the importance of preprocessing images in remote sensing data analysis</li> <li>recognize the impact of data defects on the accuracy of analysis results</li> <li>know the significance of reading the data manual to select the appropriate processing pipeline for your specific purpose</li> <li>be aware of the time-consuming nature of preparing images for analysis and the need for thoughtful application of tools and algorithms</li> <li>learn the steps involved in automating image processing to save time and ensure standardized data production</li> <li>understand the need for appropriate correction for quantitative and multitemporal analysis of satellite data</li> <li>understand the significance of cloud masking in optical data processing and the various methods used for detecting and masking clouds</li> <li>gain insights into missing information reconstruction techniques, such as gap filling and data fusion, to ensure complete and reliable time series analysis</li> <li>be introduced to data fusion</li> <li>familiarize yourself with various widely used tools, algorithms and packages</li> </ul>"},{"location":"module2/03_image_processing/03_image_processing.html#goals-of-automating-processes","title":"Goals of automating processes","text":"<p>Before time series analysis we need to select proper data which covers the area of interest and time period. Then images should be calibrated radiometrically to convert Digital Numbers (DN; amount of registered energy) to physical quantity as radiance and in second step reflectance. Images in time series should be fitted geometrically so sometimes must be co-registered to improve their geometric consistency. Optical data processing is also related to clouds masking. If the images are acquired by two or more different sensors they need to be harmonized due to differences between their spatial, spectral and radiometric resolutions. We can use all available images for the analyzed period or time composites created based on images from sub-periods. All these steps are important but also laborious and time consuming, especially when we deal with dense time series and areas of large spatial extent (countries or continents). Hence, the main goal of using automation of image processing is saving time and producing the data in a standardized manner.</p> <p>The exercise in this theme will show you how to prepare satellite image time series using different pipelines in mostly automatic way using code editor, so you can focus more on results interpretation and spend less time clicking the same buttons in graphical software whenever you process a new image.</p> <p> <p></p> <p>Processing pipeline overview (figure by course authors). </p>"},{"location":"module2/03_image_processing/03_image_processing.html#radiometric-and-geometric-correction","title":"Radiometric and geometric correction","text":"<p>For quantitative, multitemporal analyzes on satellite data or on data from various sensors, geometric and radiometric correction is necessary. Pixel values on raw images are expressed in DN which depend on strength of the signal (amount of energy) arriving at the certain physical sensor pixel. Such images can be analysed and compared visually but when it will be used as part of a time series it should be calibrated and corrected. Calibration process converts DN to top of atmosphere (TOA) radiance (units: W m<sup>\u22122</sup> sr<sup>\u22121</sup> \u03bcm<sup>\u22121</sup>), then it can be converted to top of atmosphere reflectance. Such data are still affected by atmospheric effects like scattering, absorption or emission of radiation. To avoid atmosphere influence the images are atmospherically corrected (via absolute or relative atmospheric correction methods, utilizing time-dependent parameters and image-based information, respectively) and in result pixels present values of surface reflectance (SR).</p> <p>If we are processing images for rugged terrain we also need to perform topographic correction due to differences in illumination of areas located on various aspects and slopes. For example, the same cover type on slopes oriented away from and towards the sun will appear darker and brighter, respectively, if compared to a horizontal geometry (Richter et al., 2009). Those differences can cause errors in analysis results of even one image. Illumination conditions strictly depend on solar elevation and azimuth angles which obviously change between different dates of acquisition so in case of time series analysis is important to reduce its impact.</p> <p> <p></p> <p>Sentinel-2 data L1C product, before and after topographic correction (Karkonosze Mountains area, RGB composite from 10.10.2021. Figure by course authors, source of image: European Space Agency - ESA/ Terms of use). </p> <p>Another key factor which should be considered during satellite image processing is impact of direction of incident irradiance and sensor viewing angle. Both of these are expressed by azimuth and zenith angles and described by Bidirectional Reflectance Distribution Function (BRDF).</p> <p> <p></p> <p>Left: Bidirectional Reflectance Distribution Function concept (figure by Commons Wikimedia/ CC BY-SA 3.0), right: the example of BRDF as pattern seen on the grass of a golf course after mowing (figure by Pixabay). </p> <p>Geometric matching of images is a key element of monitoring changes, its absence will result in the detection of false changes that do not result from changes in object features, but from image shifts. Imagery geometry errors can be systematic and random (Lillesand et al., 2015). The first results from the rotation of the Earth or distortion of the optical system and are relatively easy to correct using an appropriate mathematical model. Random errors result from changes in the height of the orbit and tilts of the optical axis from the vertical and rotation of the platform. They can be corrected on the basis of recorded data concerning the path of the platform\u2019s movement (parametric methods) or with the use of ground control points (GCPs) and Digital Terrain Model (DTM, non-parametric methods).</p>"},{"location":"module2/03_image_processing/03_image_processing.html#unwanted-areas-detection-and-masking","title":"Unwanted areas detection and masking","text":"<p>Automatic process of unwanted areas such as clouds, shadows and seasonal snow masking on satellite images is the first step of time series data processing after corrections. In the data selection stage the information about the percentage coverage of the scene with clouds is crucial. If the whole image is 90% under clouds you have still a 10% chance that your area of interest is clear (see illustrative example in Figure below). This information becomes even more important when we consider multitemporal analysis, where in each of the multiple images this coverage will be different.</p> <p> <p></p> <p>Low stratus clouds framing iceberg A-56 drifted in the South Atlantic Ocean (VIIRS NASA figure by J. Schmaltz, LANCE/EOSDIS Rapid Response/ Terms of use, source: NASA Earth Observatory/ Terms of use). </p> <p>Using the information about the percentage coverage with clouds we can filter metadata or mask the images (we will show you both approaches in the Exercise). The first one means that we can automatically choose the date that has the least cloud coverage within our specific area of interest and time range. The second one is oriented on excluding the areas covered by clouds by equalizing pixel values into binary flags (clouds occurrence and no-occurence). In this process the assumption is that clouds differ from the underlying Earth surface in terms of spectral characteristics and they are brighter and colder than these other areas. Commonly used approaches is the selection of cloud probability threshold for defining cloud/non-cloud masks. One of the most popular algorithms is Fmask (Zhu, Woodcock, 2012) which uses all spectral bands, NDVI and Normalized Difference Snow Index (NDSI), initially developed for Landsat TM and ETM+; with further improvements for next Landsat sensors or Sentinel-2 with additional Cirrus bands and the application to cloud shadow and snow detection (Qiu et al., 2019). Machine and deep learning are also the approaches to the unwanted areas detection, e.g.\u00a0s2cloudless algorithm uses gradient boosting classification method trained on global coverage training dataset. It can be used both for single-date and multitemporal data processing, however, there are also special algorithms dedicated to the latter. In general, in multitemporal data use, clouds can be considered \u2018anomalies\u2019 contrary to clear pixels in time series. Here we can find e.g.\u00a0multiTemporal mask (Tmask) that firstly fits a time series model of each single pixel using remaining clear pixels from Fmask product, and then compares model estimates with time series observations to detect pixels of clouds and shadows omitted in Fmask (Zhu, Woodcock, 2014). Of course, there are still many other algorithms, dedicated to specific sensors as well as more universal, and the choice of the best one for our needs must fit the specificity of our area and data (you can see the comparative works, in e.g.\u00a0Foga et al., 2017, Tarrio et al., 2020, and Skakun et al., 2022).</p> <p> <p></p> <p>Examples of cloud masking by various algorithms over the Sentinel-2 scene (figure by Skakun et al., 2022/ CC BY 4.0). </p> <p>In this work the performance of existing clouds masking methods was evaluated using independent datasets concerning clouds occurrence by assessing detection accuracy. Generally, today data providers develop layers regarding data quality including masks of clouds, shadows and snow/ice (e.g.\u00a0for Sentinel-2 \u2013 s2cloudless and Landsat \u2013 CFmask algorithms were selected as one of the most reliable and suitable for application over the entire archives). You will use them in the Exercise in this theme.</p>"},{"location":"module2/03_image_processing/03_image_processing.html#missing-information-reconstruction","title":"Missing information reconstruction","text":"<p>The big advantage of satellite data is the global range of their acquisition. Depending on the location on the globe, in some areas more images will be acquired, in others less, and to some extent the imaging paths may overlap, resulting in an increase in the number of downloadable scenes (Figure below shows the example of Sentinel-2 coverage and the overlap between adjacent orbits).</p> <p> <p></p> <p>Example of Sentinel-2 satellite revisit frequency due to the overlap between adjacent orbits (figure by P. Lacroix, European Space Agency - ESA/ Terms of use). </p> <p>Coverage of some areas with satellite data can be reduced by sensor\u2019s errors, seasonal snow or clouds and shadows mentioned above, etc., making interpretation difficult. Diverse algorithms for missing data reconstruction divided into spatial-, spectral-, temporal-based and hybrid methods groups are proposed in remote sensing literature (Shen et al., 2015). As we mainly focus on temporal aspects in this course, we will present more deeply those.</p> <p> <p></p> <p>Examples of missing information in satellite data: (a) partial cover, (b) random TM registration error, (c) permanent ETM+ SLC-off error, (d) clouds with shadows, (e) topographic shadows (figure by course authors, sources of images: (a), (b): Landsat 5 courtesy of the U.S. Geological Survey/ Terms of use), (c): Landsat 7 courtesy of the U.S. Geological Survey/ Terms of use, (d), (e): Landsat 8 courtesy of the U.S. Geological Survey/ Terms of use). </p> <p>One of the solution of this uncompleted satellite data problem in multitemporal analysis is integration of data from different terms of data acquisition into gap-free compositions. Purely statistical approaches use mean, median or medoid (value closest to median from the actual observations) values of unmasked pixels to fill the missing pixel with numerical value. There are many other established approaches to perform this and majority of them follow \u2018the best available pixel\u2019 strategy (Zhu, 2017). One is the highest NDVI value presented below.</p> <p> <p></p> <p>Concept of creating time composites from three different dates images based on the highest NDVI value (figure by course authors). </p> <p>As you can see, the selection of pixels for composition consists of the highest possible value of NDVI, which can be helpful especially in vegetation analysis. It can also be a median value of another index or single band (Potapov et al., 2011 found the median NIR band value better than NDVI on Landsat time series) or combination of index value with Brightness Temperature (as pixel\u2019s temperature is a good indicator for cloud; Roy et al., 2010). There are also other possible conditions to apply for the composition, e.g.\u00a0temporal distance to target Day of the Year (DOY) acquisition or distance to the clouds (Griffiths et al., 2013). The comparison of 10 different compositing algorithms can be found in Qiu et al., 2023. The results of this work demonstrated that no single algorithm outperformed all others, but that performance depended on compositing intervals and cloud cover.</p> <p>Another strategy to provide gap-free composites is generating synthetic images by model predictions (e.g.\u00a0regression on all available 2001-2004 Landsat data provided by Zhu et al., 2015) which can mitigate the effect of seasonality visible in the best available pixel composites. When comparing these both strategies for further change detection, synthetic images can produce more omission errors like not noticing subtle, gradual changes in a time series, while composites can lead to more commission errors, detecting change when there are still some clouds, cloud shadow or missing values.</p> <p> <p></p> <p>Composite and synthetic time series data example (Landsat courtesy of the U.S. Geological Survey/ Terms of use). </p> <p>Since we have dense time series data or we want to analyze seasonal variations an important step could be noise reduction by the use of smoothing / filtering methods (see Theme 1 in Module 1). They are used for the single points of a time series, where the assumption is that these are chronological and display regular fluctuations (Shen et al., 2015). There are three approaches to perform such filtering, depending on the way they handle time series data:</p> <ul> <li> sliding window filter methods based on predefined criteria (e.g.\u00a0adaptive Savitzky-Golay filter), </li> <li> function-based methods (e.g.\u00a0Asymmetric Gaussian and Double Logistic function), </li> <li> frequency domain methods (e.g.\u00a0Fourier transform). </li> </ul> <p>In most cases they are dealing with NDVI (you can read the examples in: Roerink et al., 2000, Song et al., 2012, Julien, Sobrino 2019), which is a well-known metric for measuring vegetation condition/productivity and in time series presents strong seasonality.</p>"},{"location":"module2/03_image_processing/03_image_processing.html#data-fusion","title":"Data fusion","text":"<p>The above examples showed how to handle data completion based on data from the same sensor, taking advantage of their own resolutions (especially temporal). However, if the information from them is of a poor quality, the reconstruction effect will not be satisfied. In such a case, multisource data can bring additional valuable information from another sensor.</p> <p>For example, freely available MODIS data provides daily global observations allowing to rapidly track changes and maximizing the chance to obtain cloud-free data. However, its spatial resolution (250-1000 m) is a limit in fine-scale environmental applications, for which Landsat would be better in terms of spatial resolution (30 m), but it\u2019s revisit time is longer (16 days). Hence, MODIS and Landsat data are often combined to increase spatiotemporal potential. Image fusion methods to predict Landsat-like synthetic images from dense MODIS time series data were developed, e.g.\u00a0Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM, Gao et al., 2006) or its enhanced version more useful in heterogeneous landscapes due to the use of observed reflectance trend between two points in time and spectral unmixing (ESTARFM, Zhu et al., 2010). For more detailed multitemporal analysis, Sentinel-2 and PlanetScope data fusion can be performed, combining the spatial, temporal and spectral advantages of both sensors. You can see the example of developing super-resolution (2.5 m) Sentinel-2 data based on PlanetScope pixel size in the work of Latte, Lejeune (2020). Here PlanetScope data were also normalized radiometrically (we refer to this below).</p> <p> <p></p> <p>Sentinel-2 and PlanetScope spectral resolution and wavelength range (figure by Latte, Lejeune, 2020/ CC BY 4.0). </p> <p>In this module we focus only on optical data for whom the main \u2018Achilles\u2019 heel\u2019 is cloud coverage. If you want to work in an area regularly covered with clouds or where you study rapid changes and there is no room for gaps or creating compositions, you may consider fusion of optical with Synthetic Aperture Radar (SAR) data where this inconvenience might be mitigated (several examples can be found here).</p>"},{"location":"module2/03_image_processing/03_image_processing.html#data-harmonization-normalization","title":"Data harmonization / normalization","text":"<p>As mentioned above it is possible to use data from several different sensors to build a time series of images. The advantage of such an approach is getting denser time series with more clear observations/pixels. But due to the differences in spectral, spatial and radiometric domains images from different sensors should be harmonized to avoid errors in analysis. Great example of potential harmonization data is a collection named Harmonized Landsat Sentinel-2 (HLS) providing consistent, 30-meters surface reflectance data from Landsat 8 and 9 OLI and Sentinel-2 MSI from Sentinel-2 A and B satellites collected every 2-3 days (see more details in Theme 2 of Module 1. For the same continued constellation of satellites with different sensors in the multitemporal analysis, it is necessary to assess whether the differences are significant and for what purpose of the analysis the data are to be used. You may find that your data collection already provides consistency (e.g.\u00a0Collection 2 of Landsat images).</p> <p>In multitemporal analysis the absolute radiometric correction is more challenging than image-based methods (scene-to-scene normalization) which are more common. Normalization methods can be divided into direct application of linear regression or those using pseudo-invariant features (PIFs) which means targets whose spectral reflectance have not changed over time (Schott et al., 1988). Examples of such targets are dark and bright places such as bare rocks or car park surfaces made from concrete or asphalt. They can be selected manually or automatically.</p> <p>Radiometric normalization is necessary for e.g.\u00a0PlanetScope data due to the differences in Dove microsatellites spectral response, which are sensor-specific. Other satellite constellation images such as MODIS, Landsat and Sentinel-2 data can be used for their normalization (Latte, Lejeune, 2020).</p> <p> <p></p> <p>PlanetScope data normalization effect (on the right) based on Sentinel-2 data (figure by Latte, Lejeune, 2020/ CC BY 4.0). </p>"},{"location":"module2/03_image_processing/03_image_processing.html#overview-of-tools-and-algorithms-used-with-satellite-multispectral-imagetime-series-processing","title":"Overview of tools and algorithms used with satellite multispectral image/time series processing","text":"<p>Most of the processing steps applied to the multitemporal imagery data are identical with those used in single image preparation workflow. Some - like composites or harmonization - are specific for datasets with temporal dimension. Different types of tools are present in many resources like closed software, open platforms or extensions, libraries and packages expanding the base capabilities of programming languages, software or platforms. Here is a sample of different tools and algorithms commonly used in processing of multispectral imagery. This selection contains only applications used during data preparation. Data analysis is a whole other collection, parts of which can be found in contents of Theme 4 and Theme 5. Strictly code-based tools are extensions of Google Earth Engine JavaScript and R. More examples of Python based algorithms can be found in Theme 2 of Module 1.</p>"},{"location":"module2/03_image_processing/03_image_processing.html#satellitesensor-processors","title":"Satellite/sensor processors","text":"<ul> <li> Landsat  Landsat Product Generation System (LPGS)   Used to produce Landsat Collections Level-1. It applies standard processing parameters. Level-1 scene-based products are created using the best processing level available for each scene.  SOURCES: Landsat Level-1 Processing Details  Landsat Ecosystem Disturbance Adaptive Processing System (LEDAPS)   Used to produce top-of-atmosphere reflectance from Landsat Thematic Mapper and Enhanced Thematic Mapper Plus Level 1 digital numbers and to apply atmospheric corrections to generate a surface-reflectance (SR Level 2) products.  SOURCES: Technical description, Collection 2 product guide products with LEDAPS v3.4.0, Detailed algorithm description  Land Surface Reflectance Code (LaSRC)   Used to produce top-of-atmosphere reflectance from Operational Land Imagers Level 1 digital numbers and to apply atmospheric corrections to generate a surface-reflectance (SR Level 2) products.  SOURCES: Technical description, Collection 2 product guide products with LaSRC v1.5.0 , Detailed algorithm description </li> <li> Sentinel-2  Instrument Data Processing (IDP)   The processing from Level-0 up to Level-1C is performed by IDP functionality of the Copernicus Ground Segment. Level-0 and Level-1A products are Copernicus Ground Segment internal products not made available to users.  SOURCES: Copernicus Ground Segment description, Sentinel-2 User Handbook, Level-0 Algorithms Overview, Level-1A Algorithms Overview, Level-1B Algorithms Overview, Level-1C Algorithms Overview  Sen2Cor / Sentinel-2 Toolbox functionalities   Each observation from the Level-1C, is processed by the European Space Agency (ESA) through the Sen2Cor algorithm. The Level-2A operational processor generates, from algorithms of scene classification and atmospheric correction, BOA reflectance products.  SOURCES: Level-2A Algorithms Overview, The Sentinel-2 Toolbox, Paper with detailed processing description (Gascon et al., 2017) </li> <li> MODIS  MODIS Adaptive Processing System (MODAPS)   Algorithm is used to produce Level 1A, Level 1B, geolocation and cloud mask products and the Higher-level MODIS land and atmosphere products.  SOURCES: About MODAPS, MODAPS page (not updated) </li> </ul>"},{"location":"module2/03_image_processing/03_image_processing.html#platformsframeworksengines","title":"Platforms/frameworks/engines","text":"<ul> <li> Google Earth Engine  API with native data types, objects and methods   Built-in tools for cloud-based imagery processing, including filtering, reducing, loops/function mapping over image collections.  SOURCES: GEE Get started article, API Reference, Gorelick et al., 2017 paper <li> Framework for Operational Radiometric Correction for Environmental monitoring (FORCE)  C-based satellite imagery processing engine   Data cubes processing to prepare analysis ready data.  SOURCES: Tutorials, Modules overview, Frantz, 2017 paper <li> Decomposition and Analysis of Time Series Software (DATimeS)  MATLAB-based stand-alone image processing GUI toolbox.   Focuses on generating spatially continuous (gap-filled) maps.  SOURCES: Tutorial video, Promotional video, Belda et al., 2020 paper <li> Sen2Like  Python-based Sentinel-2 data processor.   Aims to produce harmonised/fused surface reflectance imagery with higher periodicity by integrating additional compatible optical mission sensors.  SOURCES: User manual, Reference, Saunier et al., 2019 paper, Saunier et al., 2022 paper"},{"location":"module2/03_image_processing/03_image_processing.html#r-packageslibraries","title":"R packages/libraries","text":"gdalcubes: Earth Observation Data Cubes from Satellite Image Collections  Filtering, reducing, aggregating and applying arithmetic expressions on data cubes.  geoTS: Methods for Handling and Analyzing Time Series of Satellite Images  Includes maximum gap (amount of consecutive missing values) calculation and fitting harmonic regression models to periodic time series.  landsat: Radiometric and Topographic Correction of Satellite Imagery  Processing of Landsat or other multispectral satellite imagery. Includes relative normalization, image-based radiometric correction, and topographic correction options.  MODIS: Acquisition and Processing of MODIS Products  Basic MODIS download and processing functionalities.  MODIStsp: Find, Download and Process MODIS Land Products Data  Creation of time series of rasters derived from MODIS Land Products data.  rgee: Google Earth Engine for R  Binding package for calling Google Earth Engine API from within R  rsat: Dealing with Multiplatform Satellite Images  Various functions, including gap filling, cloud masking and smoothing.  RStoolbox: A Collection of Remote Sensing Tools  Corrections, coregistering, derivatives calculation.  satellite: Handling and Manipulating Remote Sensing Data  Contains functions which are useful for handling, manipulating, and visualizing satellite-based remote sensing data.  sen2r: Find, Download and Process Sentinel-2 Data  Contains Sen2cor processing algorithm, as well as functions to mask cloudy pixels and compute derivaties.  sen2rts: Build and Analyse Sentinel-2 Time Series  Library which helps to extract and manage time series from Sentinel-2 archives created with the package sen2r."},{"location":"module2/03_image_processing/03_image_processing.html#google-earth-engine-javascript-extensions","title":"Google Earth Engine JavaScript extensions","text":"A set of tools to use in Google Earth Engine Code Editor by Rodrigo E. Principe  Modules include cloud masks, spectral indices, compositing, batch processing and various tools.  Open Earth Engine Library (OEEL)  \u2018Collection of code goodies for Google Earth Engine(GEE)\u2019 (cloud masking, filtering, compositing etc.)  Sensor Invariant Atmospheric Correction (SIAC)  GUI application for atmospheric correction os Sentinel-2 data. Example paper Harmonization of Landsat and Sentinel 2  Documentation of technical part of Minh D. Nguyen\u2019s masters thesis, including filtering, corrections, maskings, compositing etc. Paper  Various sets of tools and scripts  kr-stn/EarthEngine_scripts renelikestacos/Google-Earth-Engine-JavaScript-Examples aazuspan/geeTools"},{"location":"module2/03_image_processing/03_image_processing.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"<p>After going through the theory in this theme you should now be ready to take on self-evaluation quiz. You should be able to find any answer you have trouble with in the contents above or in the additional references below. Good luck!</p> Question 1. What causes user-independent gaps in time series data planned for use in land cover mapping?  cloudiness, sensor registration errors, seasonal accumulation of snow cover sensor registration errors, image spatial cropping, cloud shadows cloud cover, incomplete set of spectral bands, seasonal accumulation of snow cover cloud shadows, incomplete set of spectral bands, sensor registration errors   cloudiness, sensor registration errors, seasonal accumulation of snow cover   Question 2. Which fusion of data would be most useful in monitoring crops on individual plots?  MODIS+Sentinel-2 MODIS+Landsat Sentinel-2+Landsat Sentinel-2+PlanetScope   Sentinel-2+PlanetScope   Question 3. Which parameters are not used for compositing gap-free images?  brightness temperature, distance to the clouds, median NDVI value mean NDVI value, acquisition Day of the Year, brightness temperature added subsequent NIR band values, distance to the clouds, acquisition Day of the Year distance to the clouds, median NIR band value, acquisition Day of the Year   added subsequent NIR band values, distance to the clouds, acquisition Day of the Year   Question 4. Having in mind correctness and computational optimization, choose correct order of processing:  radiometric correction, derivatives calculation, gap filling radiometric correction, gap filling, metadata filtering composite building, cloud masking, derivatives calculation metadata filtering, cloud masking, composite building   metadata filtering, cloud masking, composite building   Question 5. Which of the following processing steps would be redundant if you chose Sentinel-2 Level-2A (surface reflectance) products for phenological analysis in the Alps?  topographic correction smoothing clouds masking spatial filtering   topographic correction   Question 6. Which raw data (from the same sensor) processing steps are always necessary for mountain terrain change analysis?  atmospheric correction, topographic correction, spectral smoothing geometric correction, atmospheric correction, topographic correction geometric correction, normalization, harmonization atmospheric correction, geometric correction, temporal composites creation   geometric correction, atmospheric correction, topographic correction   Question 7. The value of percentage cover of clouds in images metadata allow to:  filter clouds with the least value filter images with the least value mask images based on this value classify clouds based on the highest value   filter images with the least value"},{"location":"module2/03_image_processing/03_image_processing.html#exercise","title":"Exercise","text":"<p>Proceed with the exercise by going to the next page below or clicking this link</p>"},{"location":"module2/03_image_processing/03_image_processing.html#references","title":"References","text":""},{"location":"module2/03_image_processing/03_image_processing.html#key-references-recommended-reading-looking-up-background-details","title":"Key references (recommended reading, looking up background details)","text":"<p>Lillesand, T., Kiefer, R. W., &amp; Chipman, J. (2015). Remote sensing and image interpretation. John Wiley &amp; Sons. SOURCE</p> <p>Qiu, S., Zhu, Z., Olofsson, P., Woodcock, C. E., &amp; Jin, S. (2023). Evaluation of Landsat image compositing algorithms. Remote Sensing of Environment, 285, 113375. https://doi.org/10.1016/j.rse.2022.113375</p> <p>Shen, H., Li, X., Cheng, Q., Zeng, C., Yang, G., Li, H., &amp; Zhang, L. (2015). Missing information reconstruction of remote sensing data: A technical review. IEEE Geoscience and Remote Sensing Magazine, 3(3), 61-85. https://doi.org/10.1109/mgrs.2015.2441912</p> <p>Zhu, Z. (2017). Change detection using landsat time series: A review of frequencies, preprocessing, algorithms, and applications. ISPRS Journal of Photogrammetry and Remote Sensing, 130, 370-384 https://doi.org/10.1016/j.isprsjprs.2017.06.013</p>"},{"location":"module2/03_image_processing/03_image_processing.html#additional-references-cited-in-this-theme","title":"Additional references cited in this theme","text":"<p>Belda, S., Pipia, L., Morcillo-Pallar\u00e9s, P., Rivera-Caicedo, J. P., Amin, E., De Grave, C., &amp; Verrelst, J. (2020). DATimeS: A machine learning time series GUI toolbox for gap-filling and vegetation phenology trends detection. Environmental Modelling &amp; Software, 127, 104666. https://doi.org/10.1016/j.envsoft.2020.104666</p> <p>Foga, S., Scaramuzza, P. L., Guo, S., Zhu, Z., Dilley Jr, R. D., Beckmann, T., Dwyer, J. L., Hughes, J. H., Laue, B. (2017). Cloud detection algorithm comparison and validation for operational Landsat data products. Remote sensing of Environment, 194, 379-390. https://doi.org/10.1016/j.rse.2017.03.026</p> <p>Frantz, D. (2019). FORCE\u2014Landsat + Sentinel-2 Analysis Ready Data and Beyond. Remote Sensing, 11(9), 1124. https://doi.org/10.3390/rs11091124</p> <p>Gao, F., Masek, J., Schwaller, M., &amp; Hall, F. (2006). On the blending of the Landsat and MODIS surface reflectance: Predicting daily Landsat surface reflectance. IEEE Transactions on Geoscience and Remote sensing, 44(8), 2207-2218. https://doi.org/10.1109/tgrs.2006.872081</p> <p>Gascon, F., Bouzinac, C., Th\u00e9paut, O., Jung, M., Francesconi, B., Louis, J., Lonjou, V., Lafrance, B., Massera, S., Gaudel-Vacaresse, A., Languille, F., Alhammoud, B., Viallefont, F., Pflug, B., Bieniarz, J., Clerc, S., Pessiot, L., Tr\u00e9mas, T., Cadau, E., \u2026 Fernandez, V. (2017). Copernicus Sentinel-2A Calibration and Products Validation Status. Remote Sensing, 9(6), 584. https://doi.org/10.3390/rs9060584</p> <p>Griffiths, P., van der Linden, S., Kuemmerle, T., &amp; Hostert, P. (2013). A pixel-based Landsat compositing algorithm for large area land cover mapping. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 6(5), 2088-2101. https://doi.org/10.1109/jstars.2012.2228167</p> <p>Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., &amp; Moore, R. (2017). Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment, 202, 18\u201327. https://doi.org/10.1016/j.rse.2017.06.031</p> <p>Jensen, J. R. (2000). Remote sensing of the environment: An earth resource perspective Prentice Hall. Upper Saddle River (NJ), USA. SOURCE</p> <p>Julien, Y., &amp; Sobrino, J. A. (2019). Optimizing and comparing gap-filling techniques using simulated NDVI time series from remotely sensed global data. International Journal of Applied Earth Observation and Geoinformation, 76, 93-111. https://doi.org/10.1016/j.jag.2018.11.008</p> <p>Latte, N., &amp; Lejeune, P. (2020).* PlanetScope Radiometric Normalization and Sentinel-2 Super-Resolution (2.5 m): A Straightforward Spectral-Spatial Fusion of Multi-Satellite Multi-Sensor Images Using Residual Convolutional Neural Networks*. Remote Sensing, 12(15), 2366. https://doi.org/10.3390/rs12152366</p> <p>Potapov, P., Turubanova, S., &amp; Hansen, M. C. (2011). Regional-scale boreal forest cover and change mapping using Landsat data composites for European Russia. Remote Sensing of Environment, 115(2), 548-561. https://doi.org/10.1016/j.rse.2010.10.001</p> <p>Qiu, S., Zhu, Z., &amp; He, B. (2019). Fmask 4.0: Improved cloud and cloud shadow detection in Landsats 4\u20138 and Sentinel-2 imagery. Remote Sensing of Environment, 231, 111205. https://doi.org/10.1016/j.rse.2019.05.024</p> <p>Richter, R., Kellenberger, T., &amp; Kaufmann, H. (2009). Comparison of topographic correction methods. Remote Sensing, 1(3), 184-196. https://doi.org/10.3390/rs1030184</p> <p>Roerink, G. J., Menenti, M., &amp; Verhoef, W. (2000). Reconstructing cloudfree NDVI composites using Fourier analysis of time series. International Journal of Remote Sensing, 21(9), 1911-1917. https://doi.org/10.1080/014311600209814</p> <p>Roy, D. P., Ju, J., Kline, K., Scaramuzza, P. L., Kovalskyy, V., Hansen, M., Loveland, T. R., Vermote, E., &amp; Zhang, C. (2010). Web-enabled Landsat Data (WELD): Landsat ETM+ composited mosaics of the conterminous United States. Remote Sensing of Environment, 114(1), 35-49. https://doi.org/10.1016/j.rse.2009.08.011</p> <p>Saunier, S., Louis, J., Debaecker, V., Beaton, T., Cadau, E. G., Boccia, V., &amp; Gascon, F. (2019). Sen2like, A Tool To Generate Sentinel-2 Harmonised Surface Reflectance Products - First Results with Landsat-8. IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium. https://doi.org/10.1109/igarss.2019.8899213</p> <p>Saunier, S., Pflug, B., Lobos, I. M., Franch, B., Louis, J., De Los Reyes, R., Debaecker, V., Cadau, E. G., Boccia, V., Gascon, F., &amp; Kocaman, S. (2022). Sen2Like: Paving the Way towards Harmonization and Fusion of Optical Data. Remote Sensing, 14(16), 3855. https://doi.org/10.3390/rs14163855</p> <p>Schill, S. R., Jensen, J. R., Raber, G. T., &amp; Porter, D. E. (2004). Temporal modeling of bidirectional reflection distribution function (BRDF) in coastal vegetation. GIScience &amp; Remote Sensing, 41(2), 116-135. https://doi.org/10.2747/1548-1603.41.2.116</p> <p>Schmidt, G., Jenkerson, C. B., Masek, J., Vermote, E., &amp; Gao, F. (2013). Landsat ecosystem disturbance adaptive processing system (LEDAPS) algorithm description. Open-File Report. https://doi.org/10.3133/ofr20131057</p> <p>Schott, J. R., Salvaggio, C., &amp; Volchok, W. J. (1988). Radiometric scene normalization using pseudoinvariant features. Remote Sensing of Environment, 26(1), 1-16. https://doi.org/10.1016/0034-4257(88)90116-2</p> <p>Skakun, S., Wevers, J., Brockmann, C., Doxani, G., Aleksandrov, M., Bati\u010d, M., Frantz, D., Gascon, F., G\u00f3mez-Chova, L., Hagolle, O., L\u00f3pez-Puigdollers, D., Louis, J., Lubej, M., Mateo-Garc\u00eda, G., Osman, J., Peressutti, D., Pflug, B., Puc, J., Richter, R., \u2026 \u017dust, L. (2022). Cloud Mask Intercomparison eXercise (CMIX): An evaluation of cloud masking algorithms for Landsat 8 and Sentinel-2. Remote Sensing of Environment, 274, 112990. https://doi.org/10.1016/j.rse.2022.112990</p> <p>Song, C., Huang, B., &amp; You, S. (2012). Comparison of three time-series NDVI reconstruction methods based on TIMESAT. 2012 IEEE International Geoscience and Remote Sensing Symposium. https://doi.org/10.1109/igarss.2012.6351057</p> <p>Tarrio, K., Tang, X., Masek, J. G., Claverie, M., Ju, J., Qiu, S., Zhu, Z., &amp; Woodcock, C. E. (2020). Comparison of cloud detection algorithms for Sentinel-2 imagery. Science of Remote Sensing, 2, 100010. https://doi.org/10.1016/j.srs.2020.100010</p> <p>Vermote, E., Roger, J. C., Franch, B., &amp; Skakun, S. (2018, July). LaSRC (Land Surface Reflectance Code): Overview, application and validation using MODIS, VIIRS, LANDSAT and Sentinel 2 data\u2019s. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium (pp.\u00a08173-8176). IEEE. https://doi.org/10.1109/igarss.2018.8517622</p> <p>Zhu, X., Chen, J., Gao, F., Chen, X., &amp; Masek, J. G. (2010). An enhanced spatial and temporal adaptive reflectance fusion model for complex heterogeneous regions. Remote Sensing of Environment, 114(11), 2610\u20132623. https://doi.org/10.1016/j.rse.2010.05.032</p> <p>Zhu, Z., Woodcock, C. E., Holden, C., &amp; Yang, Z. (2015). Generating synthetic Landsat images based on all available Landsat data: Predicting Landsat surface reflectance at any given time. Remote Sensing of Environment, 162, 67-83. https://doi.org/10.1016/j.rse.2015.02.009</p> <p>Zhu, Z., &amp; Woodcock, C. E. (2012). Object-based cloud and cloud shadow detection in Landsat imagery. Remote Sensing of Environment, 118 (15), 83-94. https://doi.org/10.1016/j.rse.2011.10.028</p> <p>Zhu, Z., &amp; Woodcock, C. E. (2014). Automated cloud, cloud shadow, and snow detection in multitemporal Landsat data: An algorithm designed specifically for monitoring land cover change. Remote Sensing of Environment, 152, 217-234. https://doi.org/10.1016/j.rse.2014.06.012</p> <p>Data Fusion - combine satellite datasets to unlock new possibilities! by Maxim Lamare. Sentinel Hub Blog. Access: 31.08.2023. https://medium.com/sentinel-hub/data-fusion-combine-satellite-datasets-to-unlock-new-possibilities-26356c481169</p>"},{"location":"module2/03_image_processing/03_image_processing.html#next-unit","title":"Next unit","text":"<p>Proceed with Multitemporal classification</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html","title":"Image processing - Exercise","text":""},{"location":"module2/03_image_processing/03_image_processing_exercise.html#exercise-image-processing-pipelines","title":"Exercise - Image Processing Pipelines","text":"<p>In this exercise we will explore various examples of processing pipelines. The term pipeline in this case means a series of processing steps, which lead to preparation of analysis ready data. Each pipeline is used to prepare data for diverse purposes and applications that will be presented and demonstrated in practice in further themes and case studies.</p> <p>The main objective of this exercise is to show several different processing steps, which are necessary in multitemporal data preparation. Bear in mind, the methods demonstrated here merely represent a fraction of all the available techniques. Based on the provided examples, you should be able to comprehend the principles behind each step and apply different methods to your own specific requirements and datasets.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#getting-started","title":"Getting started","text":"<p>All of the practical (coding) parts of this exercise will be conducted in the Google Earth Engine platform using JavaScript based Code Editor. If you are not familiar with this tool, we strongly recommend you review introductory tutorials, which are available and linked in GEE software introduction in this Course. Throughout this exercise, we\u2019ll utilize a blend of built-in functions, custom functions and functions from external resources to complete various tasks. We will supply necessary links and explanations in the relevant sections. For general reference documentation see GEE API reference website.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#processing-pipeline-1","title":"Processing pipeline 1","text":"<p>The purpose of this pipeline is to obtain a multitemporal Sentinel-2 classification dataset for the Karkonosze area from 2022. The results of this processing pipeline will be utilized in the exercise in Theme 4.</p> <p> <p></p> <p>Processing pipeline. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#area-of-interest","title":"Area of Interest","text":"<p>We\u2019ll commence by defining our Area of Interest (<code>AOI</code>). This area encompasses the Polish and Czech sectors of the Karkonosze National Parks, as well as the urban and rural regions situated south of the city of Jelenia G\u00f3ra on the Polish side. This will form our classification area.</p> <pre><code>// Define AOI\nvar AOI = ee.FeatureCollection(\"projects/etrainee-module2/assets/aoi_karkonosze_sentinel\");\n\n// Pan to and display area of interest\nMap.centerObject(AOI, 11);\nMap.addLayer(AOI, {'color': 'darksalmon'}, 'Karkonosze');\n</code></pre> <p> <p></p> <p>Karkonosze Area of Interest (AOI). </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#imagery","title":"Imagery","text":"<p>For the purpose of this exercise we will use Sentinel-2 Surface Reflectance (L2A) data. Our objective is to utilize images from 2022 (June to October) that are either cloud-free or have cloud cover of less than 5% over our <code>AOI</code>. In order to do that we want to mask clouds and cloud shadows to calculate the fraction of masked pixels inside the Area of Interest.</p> <p>To start with let\u2019s subset the whole <code>S2_SR_HARMONIZED</code> collection to the images that contain <code>AOI</code>, were acquired between desired dates, and have <code>CLOUDY_PIXEL_PERCENTAGE</code> metadata value less than 90%. We want to discard unnecessary and low quality images to reduce computing time in further steps.</p> <pre><code>// Define date ranges for filtering image collection\nvar startDate = '2022-06-01';\nvar endDate = '2022-10-31';\n\n// Define pre-filter percentage\nvar cloudFilter = 90;\n\n// Import and filter Sentinel-2 Level 2A images\nvar s2SrCol = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n    .filterBounds(AOI)\n    .filterDate(startDate, endDate)\n    .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', cloudFilter));\n</code></pre> <p>To observe the results of this filtering, insert the following line into your script and rerun the code:</p> <p>print(s2SrCol);</p> <p>Upon executing the script, you should see an <code>ImageCollection</code> with 42 elements in the console tab. Each image is itemized in the <code>features</code> list with its respective <code>bands</code> and <code>properties</code>. Inspect the <code>s2SrCol</code> object to familiarize yourself with how such objects are structured within the GEE framework.</p> <p> <p></p> <p>Filtering result. </p> <p>Note: you can print any variable to see the result of each step in the console. We recommend to do that after each step to see what each step does to the data we use.</p> <p>To further process the images we will utilize another image collection available in Google Earth Engine - s2cloudless. We will use the images from that collection to identify the presence of clouds and cloud shadows appear on the Surface Reflectance (SR) images. Armed with this information, we will proceed with masking these elements from each image.</p> <p>Note: the part using <code>s2cloudless</code> collection is adapted from Python based Sentinel-2 Cloud Masking with s2cloudless tutorial</p> <p>To proceed we want to filter the imagery from <code>s2cloudless</code> collection with the same criteria as before, with the exception of the <code>cloudFilter</code>.</p> <pre><code>// Import and filter s2cloudless\nvar s2CloudlessCol = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n    .filterBounds(AOI)\n    .filterDate(startDate, endDate);\n</code></pre> <p>Now we want to join those two collections of images. Matching images can be identified by the <code>system:index</code> (e.g., system:index: 20220602T100559_20220602T101148_T33UWS) attribute contained in the image <code>properties</code>.</p> <pre><code>// Join the filtered s2cloudless collection to the SR collection\n// by the 'system:index' property\nvar s2SrCloudlessCol =\n// ee.Join.saveFirst means that new element will be added \n// to the primary image as a property\nee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply({\n    'primary': s2SrCol,\n    'secondary': s2CloudlessCol,\n    'condition': ee.Filter.equals({\n        'leftField': 'system:index',\n        'rightField': 'system:index'\n    })\n}));\n\nprint(s2SrCloudlessCol);\n</code></pre> <p>The resulting joined <code>ImageCollection</code> should consist of 42 images. You can find the attached s2cloudless <code>probability</code> band in the <code>properties</code>.</p> <p> <p></p> <p>Joined collection. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#cloud-and-shadow-masking","title":"Cloud and shadow masking","text":"<p>In the following section, we\u2019ll conduct cloud masking and cloud shadow masking, based on the <code>s2cloudless</code> <code>probability</code> images. The resulting masks should offer improved robustness and accuracy compared to standard cloud masking using <code>S2_SR_HARMONIZED</code> cloud bands and <code>SCL</code> classes. The specified values of variables below have been fine-tuned through empirical tests, delivering satisfactory results. However, they can be modified as per user preference.</p> <pre><code>// Cloud/shadow masking variables\n\n//Cloud probability (%); based on `probability` image;\n// values greater than are considered cloud\nvar CLD_PRB_THRESH = 20;\n\n//Near-infrared reflectance (B8 band); values below that threshold\n// are considered potential cloud shadow\nvar NIR_DRK_THRESH = 0.2;\n\n// Maximum distance (km) to search for cloud shadows from cloud edges\nvar CLD_PRJ_DIST = 1;\n\n// Distance (m) to dilate the edge of cloud-identified objects\nvar BUFFER = 50;\n</code></pre> <p>Now we can define functions for the actual masking. These functions are derived from this tutorial.</p> <p>The first function <code>addCloudBands</code> produces a cloud mask based on the provided <code>probability</code> layer and <code>CLD_PRB_TRESH</code> variable. The second function <code>addShadowBands</code> produces cloud shadows using <code>NIR (B8)</code> band values and the direction of cloud shadows bases on the image <code>MEAN_SOLAR_AZIMUTH_ANGLE</code> property. Finally, <code>addCldShdwMask</code> adds calculated masks to the image bands.</p> <p>To apply each function to each image in <code>s2SrCloudlessCol</code> collection we will use the <code>.map()</code> function, which is a more efficient implementation of a regular <code>for</code> loop.</p> <pre><code>// CLOUD COMPONENTS\n// Function to add the s2cloudless probability layer\n// and derived cloud mask as bands to an S2_SR_HARMONIZED image input.\nfunction addCloudBands(img){\n\n  // Get s2cloudless image, subset the probability band.\n    var cldPrb = ee.Image(img.get('s2cloudless')).select('probability');\n\n    // Condition s2cloudless by the probability threshold value.\n    // Creates new raster with cloud pixels\n    var isCloud = cldPrb.gt(CLD_PRB_THRESH).rename('clouds');\n\n    // Add the cloud probability layer and cloud mask as image bands.\n    return img.addBands(ee.Image([cldPrb, isCloud]));\n}\n\n// SHADOW COMPONENTS\nfunction addShadowBands(img){\n\n  // Identify water pixels from the SCL band.\n    var notWater = img.select('SCL').neq(6);\n\n    // Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n    var SR_BAND_SCALE = 1e4;\n    var darkPixels = img.select('B8')\n    .lt(NIR_DRK_THRESH*SR_BAND_SCALE)\n    .multiply(notWater)\n    .rename('dark_pixels');\n\n    // Determine the direction to project cloud shadow from clouds \n    // (assumes UTM projection).\n    var shadowAzimuth = ee.Number(90)\n                  .subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n\n    // Create a cloud projection based on calculated parameters\n    var cldProj = (img.select('clouds')\n        .directionalDistanceTransform(shadowAzimuth, CLD_PRJ_DIST*10)\n        .reproject({'crs': img.select(0).projection(), 'scale': 100})\n        .select('distance')\n        .mask()\n        .rename('cloud_transform'));\n\n    // Identify the intersection of dark pixels with cloud shadow projection.    \n    var shadows = cldProj.multiply(darkPixels).rename('shadows');\n\n    // Add dark pixels, cloud projection, and identified shadows as image bands.\n    return img.addBands(ee.Image([darkPixels, cldProj, shadows]));\n}\n\n// Add combined cloud and shadow mask to image bands.\nfunction addCldShdwMask(img){\n\n  // Add cloud component bands.\n    var imgCloud = addCloudBands(img);\n\n     // Add cloud shadow component bands.\n    var imgCloudShadow = addShadowBands(imgCloud);\n\n    // Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n    var isCldShdw = imgCloudShadow\n              .select('clouds')\n              .add(imgCloudShadow.select('shadows'))\n              .gt(0);\n\n    // Remove small cloud-shadow patches\n    // and dilate remaining pixels by BUFFER input.\n    // 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n    var isCldShdw2 = (isCldShdw.focalMin(2).focalMax(BUFFER*2/20)\n        .reproject({'crs': img.select([0]).projection(), 'scale': 20})\n        .rename('cloudmask'));\n\n    // Add the final cloud-shadow mask to the image.\n    return imgCloudShadow.addBands(isCldShdw2);\n}\n</code></pre> <p>Now we can apply the functions to previously pre-filtered and merged collection <code>s2SrCloudlessCol</code>.</p> <p>TASK</p> <p>Apply the <code>addCldShdwMask</code> function to <code>s2SrCloudlessCol</code> collection using <code>.map()</code> function. Assign the result to <code>s2ColAddMask</code> variable and print the result to console.</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Add calculated cloud and shadow mask bands to each image in the collection.\nvar s2ColAddMask = s2SrCloudlessCol.map(addCldShdwMask);\nprint(s2ColAddMask);\n</code></pre> <p></p> <p>Each image in the collection should now have additional six bands (<code>probability</code>, <code>clouds</code>, <code>dark_pixels</code>, <code>cloud_transform</code>, <code>shadows</code>, <code>cloudmask</code>).</p> <p> <p></p> <p>Mask bands. </p> <p>With the cloud and shadow masks computed, our next step is to apply them to each image within the collection. When masking is complete we will also subset each resulted image to include only 10 m and 20 m Sentinel-2 bands. We can exclude bands like <code>SCL</code> and others, because we will no longer use them in our processing steps. Bands with spatial resolution of 60 m are also not valuable for the classification task, which this dataset is being prepared for.</p> <pre><code>// Function to apply masks to spectral bands.\nfunction applyCldShdwMask(img) {\n  // Make a copy of input original image to preserve properties\n  var orig = img;\n\n  // Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n  var notCldShdw = img.select('cloudmask').not();\n\n  // Subset reflectance bands and update their masks,\n  // return the result as 0-1 SR bands.\n  img =  img\n        .select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'])\n        .updateMask(notCldShdw)\n        .divide(10000);\n\n  // Copy the original image properties\n  return img.copyProperties(orig, orig.propertyNames());\n}\n\n// Apply masking function to the collection.\nvar s2ColMasked = s2ColAddMask.map(applyCldShdwMask);\nprint(s2ColMasked);\n</code></pre> <p>Each image in the resulting collection should now have 10 selected spectral bands.</p> <p> <p></p> <p>Masked S-2 10 m and 20 m bands. </p> <p>We can inspect the results of the masking process on the example image. In order to do that we will select the first image from both pre- and post-masking collections and display them along with selected mask bands.</p> <pre><code>// Extract first image from pre-masking and post-masking collections\n// to show the results on map\nvar exampleImage = s2ColAddMask.first();\nvar maskedExampleImage = s2ColMasked.first();\n\n// Subset layers and prepare them for display\nvar clouds = exampleImage.select('clouds').selfMask();\nvar shadows = exampleImage.select('shadows').selfMask();\nvar cloudMask = exampleImage.select('cloudmask').selfMask();\n\n// ADD LAYERS TO THE MAP.\n\n// Add original image layer\nMap.addLayer(exampleImage, {bands: ['B4', 'B3', 'B2'], min: 200, max: 1300}, 'Image');\n//Add clouds layer\nMap.addLayer(clouds, {palette: '74fffc'}, 'CLOUDS');\n//Add cloud shadows layer\nMap.addLayer(shadows, {palette: '0a0cff'}, 'SHADOWS');\n//Add cloud mask layer\nMap.addLayer(cloudMask, {palette: 'ff550c'}, 'CLOUD/SHADOWS MASK');\n//Add masked image layer\nMap.addLayer(maskedExampleImage, {bands: ['B4', 'B3', 'B2'], min: 0.02, max: 0.13}, 'Masked Image');\n</code></pre> <p>Toggle different layers to see the extent of different masks and the final product.</p> <p> <p></p> <p>Masking results. </p> <p>Additional task: open this link, which leads to the results of cloud masking using default parameters acquired from Sentinel-2 Cloud Masking with s2cloudless tutorial. Compare different mask layers and try to assess which parameters affect the final image the most.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#cloud-coverage-over-area-of-interest","title":"Cloud coverage over area of interest","text":"<p>After obtaining an <code>ImageCollection</code> with 42 masked images, we aim to further filter those images that are clear over our area of interest. To accomplish this, we\u2019ll employ the applied mask to determine the percentage of cloud cover over the research area.</p> <p>Below, we present a function that calculates the cloud cover percentage. We will then apply this function to the masked collection.</p> <pre><code>// Define the function to return the percentage of masked pixels\n// in an image region as a property. \nfunction calculateMaskedPercent(image) {\n  // Select one of the bands and count unmasked pixels within research area\n   var unmaskedPixelCount = image.select('B2').reduceRegion({\n    reducer: ee.Reducer.count(),\n    geometry: AOI,\n    scale: 10,\n    maxPixels: 1e10\n  }).get('B2');\n\n  // Select one of the bands and count all pixels within research area\n  var totalPixelCount  = image.select('B2').unmask().reduceRegion({\n    reducer: ee.Reducer.count(),\n    geometry: AOI,\n    scale: 10,\n    maxPixels: 1e10\n  }).get('B2');\n\n // Calculate cloud cover percentage over AOI\n  var cloudCoverAoi = ee.Number(1)\n      .subtract(ee.Number(unmaskedPixelCount).divide(totalPixelCount))\n      .multiply(100)\n      .round();\n\n  // Return the original image with new cloud_cover_aoi property\n  return image.set('cloud_cover_aoi', cloudCoverAoi);\n}\n\n// Apply the function to the collection\nvar s2ColCloudCoverAoi =  s2ColMasked.map(calculateMaskedPercent);\nprint(s2ColCloudCoverAoi);\n</code></pre> <p> <p></p> <p>Cloud/shadow cover AOI property. </p> <p>We are now able to filter image collection based on the recently added property to extract the least clouded images suitable for multitemporal classification. We will set a strict 5% cloud/shadow cover threshold.</p> <p>TASK</p> <p>Filter <code>s2ColCloudCoverAoi</code> collection by <code>cloud_cover_aoi</code> property. Exclude all images with value greater than 5%.</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Filter collection to only images with `cloud_cover_aoi` less than or equal 5%\nvar s2FilteredCol = s2ColCloudCoverAoi.filter(ee.Filter.lte('cloud_cover_aoi', 5));\nprint(s2FilteredCol);\n</code></pre> <p></p> <p>The resulting collection consists of six images from June (x3), July (x2) and October (x1).</p> <p> <p></p> <p>Filtered collection. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#ndvi-calculation","title":"NDVI calculation","text":"<p>The next step will be NDVI calculation. For the classification purposes in Theme 4, we want to test the performance of the this most widely used index in differentiating land cover classes.</p> <p>In the function below we will utilize our knowledge on how to calculate this index and <code>normalizedDifference</code> method from Google Earth Engine.</p> <pre><code>// B4 - RED band, B8 - NIR band\nfunction addNDVI(img) {\n  var ndvi = img.normalizedDifference(['B8', 'B4']).rename('NDVI');\n  return img.addBands(ndvi);\n}\n\nvar s2FilteredColNdvi = s2FilteredCol.map(addNDVI);\nprint(s2FilteredColNdvi);\n</code></pre> <p>Each image should now have additional <code>NDVI</code> band.</p> <p> <p></p> <p>Filtered collection with NDVI band. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#band-renaming-and-export","title":"Band renaming and export","text":"<p>The next part of this exercise entails transforming band names and exporting the entire collection as a single image, with spectral bands and NDVI tagged with the date of the source image. To execute this, we will require some additional functions.</p> <p>First, we\u2019ll add year, month and day to each band name. In example, <code>B2</code> from the first image will be renamed to <code>2022-06-19_B2</code>.</p> <pre><code>// Add dates to bands names\nfunction renameBands(image) {\n\n  // Get the date of the image\n  var date = ee.Date(image.get('system:time_start'));\n  var dateString = date.format('YYYY-MM-dd');\n\n  // Get the list of band names in the image\n  var bandNames = image.bandNames();\n\n  // Map over the band names and rename them\n  var renamedBands = bandNames.map(function(bandName){\n    return dateString.cat('_').cat(bandName);\n  });\n\n  // Use .select() to rename the original band names\n  var renamedImage = image.select(bandNames, renamedBands);\n\n  return renamedImage;\n}\n\n// Add dates to bands names\nvar s2FilteredColNdviRenamed = s2FilteredColNdvi.map(renameBands);\nprint(s2FilteredColNdviRenamed);\n</code></pre> <p> <p></p> <p>Band names with dates. </p> <p>Now we\u2019ll save all the new names to a variable. We\u2019ll need that variable in the following step to recover changed band names after using <code>toBands()</code> function.</p> <pre><code>// Function to get all band names from the entire collection\nfunction getAllBandNames(collection) {\n  // Convert the ImageCollection into a list\n  var bandNamesList = collection\n    .toList(collection.size())\n\n  // Map over each image in the list to get its band names\n    .map(function(image) {\n      // Convert each element in the list (image) into an ee.Image object\n      var imageObject = ee.Image(image);\n\n      // Get the band names of the current image using ee.Image.bandNames()\n      return imageObject.bandNames();\n    })\n\n  // Flatten the list to obtain a single list containing all band names\n    .flatten();\n\n  // Return the list of all band names\n  return bandNamesList;\n}\n</code></pre> <p>Apply the function to <code>s2FilteredColNdviRenamed</code>.</p> <pre><code>var bandNames = getAllBandNames(s2FilteredColNdviRenamed);\nprint(bandNames);\n</code></pre> <p> <p></p> <p>Names list. </p> <p>The last step is to convert the <code>s2FilteredColNdviRenamed</code> <code>ImageCollection</code> to a single multi-band image. <code>toBands()</code> function add image ID prefix to each band, so we\u2019ll replace those band names with the ones stored in <code>bandNames</code> variable.</p> <pre><code>// Convert the collection to a single multi-band image\nvar s2ImageMultiband = s2FilteredColNdviRenamed.toBands();\n\n// Replace temporary names with previously prepared date-marked names.\ns2ImageMultiband = s2ImageMultiband.select(\n  s2ImageMultiband.bandNames(), bandNames);\n\nprint(s2ImageMultiband);\n</code></pre> <p> <p></p> <p>Single multi-band image. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#gap-filling","title":"Gap filling","text":"<p>Next, we\u2019ll prepare and apply a function to fill any remaining gaps and prepare the data for export. Some parts of the images may contain masked areas (less than 5% of AOI). We\u2019ll employ a filling algorithm to address these gaps. This algorithm operates as follows:</p> <ul> <li> replace missing values with the nearest preceding value (i.e., substitute July\u2019s missing values with June\u2019s values) </li> <li> if the preceding values are all NA: substitute missing values with the nearest subsequent value </li> <li> for the first image, if data is missing, it uses the nearest subsequent value </li> <li> if the entire series lacks a valid value, leave it as \u2018NA\u2019 </li> </ul> <pre><code>function fillGaps (img) {\n  // Get the image projection\n  var proj = img.projection();\n\n  // Get the band names of the image\n  var bandNames = img.bandNames();\n\n  // Make an Image object out of every element from a list of individual bands\n  var colList = bandNames.map(function(n) { return img.select([n]) });\n\n  // Define the mosaic function to fill gaps for each band\n  var mosaicFunction = function(band) {\n    // Get the index of the current band\n    var bandIndex = colList.indexOf(band);\n\n    // Get the index of the next band\n    var bandIndexPlus1 = bandIndex.add(1);\n\n    // Create two lists of bands before and after the current band\n    var colLtIndex = colList.slice(0, bandIndex, 1);\n    var colGtIndex = colList.slice(bandIndexPlus1).reverse();\n\n    // Create a list containing the current band\n    var colEqIndex = ee.List([]).add(colList.get(bandIndex));\n\n    // Concatenate the lists to create a new list of bands arranged as follows:\n    // [bands after current band, bands before current band, current band]\n    var all = colGtIndex.cat(colLtIndex).cat(colEqIndex).flatten();\n\n    // Convert the list of bands to an ImageCollection\n    var cc = ee.ImageCollection.fromImages(all);\n\n    // Map over the ImageCollection to rename bands to 'band'\n    var ccBandnames = cc.map(function(img) {return img.select(['.*'], ['band'])});\n\n    // Mosaic the ImageCollection to fill gaps for the current band\n    // The pixel in the final mosaic is the latest valid (non-masked pixel)\n    // That's why we put them in this order in `all` variable\n    var mosaicked = ccBandnames.mosaic();\n\n    return mosaicked;\n  };\n\n  // Map the mosaic function over the list of bands to fill gaps for each band\n  var mosaicked = colList.map(mosaicFunction);\n\n  // Create an ImageCollection from the mosaic result\n  var tempCol = ee.ImageCollection.fromImages(mosaicked);\n\n  //  Convert the ImageCollection to a single image using toBands()\n  tempCol = tempCol.toBands();\n\n  // Select the bands with original band names from the newly created image\n  var fixedBandNamesImg = tempCol.select(tempCol.bandNames(), bandNames);\n\n  // Reproject the image to its original projection\n  var filledGapsImg = fixedBandNamesImg.reproject(proj);\n\n  // Return the image with gaps filled for all bands\n  return filledGapsImg;\n}\n\n\nvar b2Filled = fillGaps(s2ImageMultiband.select('.*B2'));\nvar b3Filled = fillGaps(s2ImageMultiband.select('.*B3'));\nvar b4Filled = fillGaps(s2ImageMultiband.select('.*B4'));\nvar b5Filled = fillGaps(s2ImageMultiband.select('.*B5'));\nvar b6Filled = fillGaps(s2ImageMultiband.select('.*B6'));\nvar b7Filled = fillGaps(s2ImageMultiband.select('.*B7'));\nvar b8Filled = fillGaps(s2ImageMultiband.select('.*B8'));\nvar b8aFilled = fillGaps(s2ImageMultiband.select('.*B8A'));\nvar b11Filled = fillGaps(s2ImageMultiband.select('.*B11'));\nvar b12Filled = fillGaps(s2ImageMultiband.select('.*B12'));\nvar ndviFilled = fillGaps(s2ImageMultiband.select('.*NDVI'));\n</code></pre> <p>The final step before exporting is arranging all the filled bands in the proper order once more. This operation consists of two steps. First we\u2019ll create a single multi-band image from the filled images consisting of specific bands (<code>b2Filled</code>, <code>ndviFilled</code> etc.).</p> <pre><code>var s2ImageMultibandFilled = ee.Image.cat(b2Filled, b3Filled, b4Filled, b5Filled, \n b6Filled, b7Filled, b8Filled, b8aFilled, b11Filled, b12Filled, ndviFilled);\n\nprint(s2ImageMultibandFilled);\n</code></pre> <p> <p></p> <p>Filled multi-band image. </p> <p>And finally, we\u2019ll rearrange the bands back to the same order as in <code>s2ImageMultiband</code>.</p> <pre><code>var properNames = s2ImageMultiband.bandNames();\n\nfunction sortBands(image, properNames) {\n  // Get the band names of the proper image.\n  var properBandNames = properNames;\n\n  // Get the band names of the wrong image.\n  var wrongBandNames = image.bandNames();\n\n  // Create a list of sorted band indices.\n  var sortedBandIndices = properBandNames.map(\n    function(bandName) { return wrongBandNames.indexOf(bandName) });\n\n  // Create a new image with the bands sorted in the specified order.\n  var sortedBands = image.select(sortedBandIndices);\n  return sortedBands;\n}\n\n\nvar s2ExportReady = sortBands(s2ImageMultibandFilled, bandNames);\n\nprint(s2ExportReady);\n</code></pre> <p> <p></p> <p>Image ready to be exported. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#exporting-data","title":"Exporting data","text":"<p>All that\u2019s left now is to export previously prepared data. We\u2019ll rename it to <code>T4_image_data</code>, so we recognize it later, when we\u2019ll need to use it in Theme 4 exercise. Change the folder to the appropriate one in your Google Drive (by default it\u2019ll create a new one if it doesn\u2019t find existing folder with a matching name).</p> <pre><code>\nExport.image.toDrive({\n  image: s2ExportReady,\n  description: 'T4_image_data',\n  // Change the folder to one in your Google Drive\n  folder: 'ETR_GEE',\n  region: AOI,\n  maxPixels: 1e13,\n  scale: 10\n});\n</code></pre> <p> <p> </p> <p>Export image to your Google Drive. </p> <p>Here you can find the link to the whole code presented in Pipeline 1</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#processing-pipeline-2","title":"Processing pipeline 2","text":"<p>The goal of this processing pipeline is to generate yearly Landsat cloud-free composites for the period of 1984-2022, specifically for the Tatras area. Outputs of this processing pipeline will be used in the exercise in Theme 5 and in Case Study 3.</p> <p> <p></p> <p>Processing pipeline. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#packages","title":"Packages","text":"<p>We will begin by loading the necessary external packages. In this instance, we will be utilizing the spectral package.</p> <p>TASK</p> <p>Attach <code>spectral</code> package as seen in the GitHub README.</p> <pre><code>// Load external spectral package\nvar spectral = require(\"users/dmlmont/spectral:spectral\");\n</code></pre> <p>This package will be used later to calculate spectral indices for each composite. Now, let\u2019s define the area of interest.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#area-of-interest_1","title":"Area of interest","text":"<pre><code>// Define Area of Interest (AOI)\nvar AOI = ee.FeatureCollection(\"projects/etrainee-module2/assets/aoi_tatras\");\n</code></pre> <p>Our area of interest includes both the Polish and Slovakian sides of the Tatra National Parks. In our forthcoming exercises that focus on change detection, this region will be our primary area of interest.</p> <pre><code>// Pan to and display area of interest\nMap.centerObject(AOI, 10);\nMap.addLayer(AOI, {'color': 'darksalmon'}, 'Tatras');\n</code></pre> <p> <p></p> <p>Area of interest (Tatras). </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#imagery_1","title":"Imagery","text":"<p>Moving on to imagery, this pipeline will utilize Landsat Collection 2 Level 2 Tier 1 data. Compared to Collection 1 data, the images from this collection have undergone preprocessing with improved algorithms. The Tier 1 data is deemed suitable for multi-temporal analysis. More information on Collection 2 and data tiers can be found here.</p> <pre><code>// Define collections for Landsat 4, 5, 7, 8, and 9\nvar collections = {\n  lt4: ee.ImageCollection(\"LANDSAT/LT04/C02/T1_L2\"),\n  lt5: ee.ImageCollection(\"LANDSAT/LT05/C02/T1_L2\"),\n  lt7: ee.ImageCollection(\"LANDSAT/LE07/C02/T1_L2\"),\n  lt8: ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\"),\n  lt9: ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")\n};\n</code></pre> <p>Next steps in the processing pipeline will filter and process each image in all collections to prepare homogeneous dataset.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#filtering-and-processing-data-collections","title":"Filtering and processing data collections","text":"<p>Naturally, we do not require the entire collection of Landsat images from each satellite/sensor for the analysis specific for our AOI. To filter each collection, we will apply a spatial filter, a temporal filter (to retain only imagery captured between May 1st and September 30th), and a metadata filter (images from WRS-2 row 26 that contain the area of interest in whole or in part).</p> <p>TASK</p> <p>Declare a variable <code>collectionFilter</code> of type Filter that includes the following rules:</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Define a collection filter.\nvar collectionFilter = ee.Filter.and(\n  ee.Filter.bounds(AOI),\n  ee.Filter.calendarRange(1984, 2022, 'year'),\n  ee.Filter.calendarRange(5, 9, 'month'),\n  ee.Filter.eq('WRS_ROW', 26)\n);\n</code></pre> <p></p> <p>Next, we will need to define a set of functions to apply to each image. We will start with five different Landsat satellite collections. Landsats 4, 5, 7 and Landsats 8 and 9 have two distinct sets of band sets. For further processing, we want to extract only those bands that are common between the two groups and rename them to simplify later processing.</p> <p>TASK</p> <p>Prepare 2 functions: <code>renameBandsLandsat457</code> and <code>renameBandsLandsat89</code> renaming the appropriate bands to the following names: \u2018Blue\u2019, \u2018Green\u2019, \u2018Red\u2019, \u2018NIR\u2019, \u2018SWIR1\u2019, \u2018SWIR2\u2019.</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Define function to get and rename bands of interest from Landsat 4, 5 and 7.\n// Define function to rename bands of interest from Landsat 4, 5 and 7.\nvar renameBandsLandsat457 = function(img) {\n  return img.select(\n    ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7'],\n    ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']\n  );\n};\n\n// Define function to rename bands of interest from Landsat 8 and Landsat 9.\nvar renameBandsLandsat89 = function(img) {\n  return img.select(\n    ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7'],\n    ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']\n  );\n};\n</code></pre> <p></p> <p>To check how the function works add the two following lines to the code and Run it again. It compares the image before and after using the <code>renameBandsLandsat457</code> function.</p> <pre><code>print(collections.lt5.first());\nprint(renameBandsLandsat457(collections.lt5.first()));\n</code></pre> <p>The result in the console should look like this:</p> <p> <p></p> <p></p> <p>To develop cloud-free composites, a method must be chosen that selects a clear pixel, captured within a specified timeframe, for inclusion in the final composite. We\u2019ll use proximity to a target day-of-year (doy, DOY) as the deciding criterion in this scenario.</p> <p>We\u2019ll introduce a new property that reflects the absolute difference between the DOY when the image was captured and the target DOY, which is the 213th day of the year (or 212 in JavaScript, as it starts counting from 0), corresponding to August 1st. This property is important for enabling us to apply mosaicking to collections sorted based on this property. Clear pixels captured closer to the target DOY will supersede those taken further away from August 1st.</p> <pre><code>// Define function to add absolute difference between target\n// day of year (213, August 1st) and image acquisition doy.\nvar addDoyAbsProperty = function(img) {\n  var doy = ee.Date(img.get('system:time_start')).getRelative('day', 'year');\n  var doyAbs = ee.Number(212).subtract(doy).abs();\n  return img.set('doy_abs', doyAbs);\n};\n</code></pre> <p>The next step is to define a function to mask clouds and cloud shadows. The <code>QA_PIXEL</code> band, present in all Level 2 Landsat images, will be used for this purpose. Based on the values in this raster, clouded and shadowed pixels can be identified and masked. For improved masking performance, we\u2019ll run a morphological filter through the masked image to extend the masked parts around the edges. This enhancement will result in more potentially viable pixels being masked, but will also help eliminate more cloud edges, which are often not classified as clouds in the \u2018QA_PIXEL\u2019 band.</p> <pre><code>// Define function to mask clouds and clouds shadows with additional pixels\n// masked around these areas.\nvar cloudMaskKernel = function(img) {\n  var dilatedCloudBitMask = 1 &lt;&lt; 1;\n  var cloudShadowBitMask = 1 &lt;&lt; 3;\n  var cloudsBitMask = 1 &lt;&lt; 4;\n  var snowBitMask = 1 &lt;&lt; 5;\n  var qa = img.select('QA_PIXEL');\n  var mask = qa.bitwiseAnd(cloudShadowBitMask)\n    .or(qa.bitwiseAnd(cloudsBitMask))\n    .or(qa.bitwiseAnd(dilatedCloudBitMask))\n    .or(qa.bitwiseAnd(snowBitMask));\n  mask = mask.focalMax(7, \"square\");\n  return qa.updateMask(mask.not());\n};\n</code></pre> <p>By default, the values in Landsat Collection 2 Level 2 surface reflectance (SR) products are stored as integers to conserve storage. To calculate SR values, we need to apply a scaling factor. After scaling, it\u2019s crucial to ensure the data falls within the range [0, 1], so any artifacts with values below 0 and above 1 should be masked, as we cannot discern the true surface reflectance value of these areas.</p> <p>TASK</p> <p>Declare function <code>applyScalingFactor</code> to multiply it by coefficients and mask outliers.</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Define function to calculate surface reflection values from DN.\nvar applyScalingFactor = function(img) {\n  var scaledImage = img.multiply(0.0000275).add(-0.2);\n  return scaledImage.updateMask(scaledImage.gte(0).and(scaledImage.lte(1)));\n};\n</code></pre> <p></p> <p>With all the necessary functions defined, we can wrap them within a single function for applying to each collection. The two functions below are differentiated by their renaming function, which varies between two Landsat satellite groups. Each function performs the following steps:</p> <ul> <li> copies the original image to a new variable to retain necessary properties </li> <li> calculates a mask for clouds and cloud shadows </li> <li> extracts six bands and renames them as \u2018Blue\u2019, \u2018Green\u2019, etc. </li> <li> scales pixel values in each band to SR values. </li> <li> copies the \u2018system:time_start\u2019 property to the new image. </li> <li> adds a property containing absolute DOY </li> <li> applies the clouds/cloud shadows mask to the image </li> </ul> <p>TASK</p> <p>Wrap the steps above with <code>prepLandsat457</code> and <code>prepLandsat89</code> functions above using the functions we wrote previously.</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Define functions to apply all other functions needed to create time series composites.\nvar prepLandsat457 = function(img) {\n  var originalImage = img;\n  var cloudMask = cloudMaskKernel(img);\n  var outputImage = renameBandsLandsat457(img);\n  outputImage = applyScalingFactor(outputImage);\n  outputImage = ee.Image(outputImage.copyProperties(originalImage, ['system:time_start']));\n  outputImage = addDoyAbsProperty(outputImage);\n  return outputImage.updateMask(cloudMask);\n};\n\nvar prepLandsat89 = function(img) {\n  var originalImage = img;\n  var cloudMask = cloudMaskKernel(img);\n  var outputImage = renameBandsLandsat89(img);\n  outputImage = applyScalingFactor(outputImage);\n  outputImage = ee.Image(outputImage.copyProperties(originalImage, ['system:time_start']));\n  outputImage = addDoyAbsProperty(outputImage);\n  return outputImage.updateMask(cloudMask);\n};\n</code></pre> <p></p> <p>With all the filtering and processing functions defined, we can now apply them to each Landsat collection. After the filtering process, each image in the collection will be processed using the functions described. The remaining processed images can then be merged into one collection.</p> <p>TASK</p> <p>Filter each Landsat collection and apply appropriate wrapper functions. Then merge all the collections into <code>tatrasCol</code> variable.</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Filter and process collections according to the parameters.\nvar lt4Col = collections.lt4.filter(collectionFilter).map(prepLandsat457);\nvar lt5Col = collections.lt5.filter(collectionFilter).map(prepLandsat457);\nvar lt7Col = collections.lt7.filter(collectionFilter).map(prepLandsat457);\nvar lt8Col = collections.lt8.filter(collectionFilter).map(prepLandsat89);\nvar lt9Col = collections.lt9.filter(collectionFilter).map(prepLandsat89);\n\n// Merge all collections into one.\nvar tatrasCol = lt4Col\n  .merge(lt5Col)\n  .merge(lt7Col)\n  .merge(lt8Col)\n  .merge(lt9Col);\n\nprint(tatrasCol);\n</code></pre> <p></p> <p> <p></p> <p>Merged collection. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#yearly-cloud-free-composite-creation","title":"Yearly cloud-free composite creation","text":"<p>After we\u2019ve filter and processed initial <code>ImageCollections</code> we can now proceed with a function to create composite images. It is based on the <code>mosaic</code> function, which in a nutshell stacks bands on one another, so as the pixel values from the layer \u2018on top\u2019 are visible in the mosaicked image (except the NA pixels, which are filled by the layers below the top one).</p> <p>First, we want the images closest to the target DOY to be \u2018on top\u2019, so we need to sort them before mosaicking.</p> <pre><code>// Function to create mosaic based on day-of-year. \n// Non-clouded pixels with lowest temporal distance from \n// 213 day-of-year (August 1) will be present in the final composite\nvar compositeDoy = function(col) {\n  var colSort = col.sort('doy_abs', false);\n  return colSort.mosaic();\n};\n</code></pre> <p>The actual function needs to involve filtering the entire collection to extract images captured within a single year. These images will then be sorted by their <code>doy_abs</code> value (in descending order) and subsequently mosaicked (<code>compositeDoy</code> function).</p> <p>To iterate through each year between 1984-2022 collection we shall prepare a list containing a sequence.</p> <pre><code>var startYear = 1984;\nvar endYear = 2022;\nvar yearList = ee.List.sequence(startYear, endYear);\n</code></pre> <p>For each year, we\u2019ll extract the appropriate images, create a composite, reproject the image, and add two properties. The resulting list will contain 39 images, each with six bands.</p> <pre><code>var yearCompList = yearList.map(function(year) {\n\n  // Get only the images that are collected during one year\n  var yearCol = tatrasCol.filter(ee.Filter.calendarRange(year, year, 'year'));\n\n  // Create a composite using previously defined function\n  var yearComp = compositeDoy(yearCol);\n\n  // Return a reprojected image with two additional properties\n  yearComp = yearComp.reproject({crs: 'EPSG:32634', scale: 30}); \n  return yearComp.set({\n    'year': ee.Number(year).int(),\n    'save': ee.String(year)\n  });\n});\n\nprint(yearCompList);\n</code></pre> <p> <p></p> <p>Yearly composite images list. </p> <p>Finally, transform a <code>List</code> of <code>Images</code> into <code>ImageCollection</code>.</p> <p>TASK</p> Click here to see the solution. Try to solve the task yourself and use the solution if you get stuck or to check if you are on the right way to the solution.  <pre><code>// Convert the annual composite image List to an ImageCollection\nvar yearCompCol = ee.ImageCollection.fromImages(yearCompList);\nprint(yearCompCol);\n</code></pre> <p></p> <p> <p></p> <p>Yearly composite image collection. </p> <p>Display the first (1984) image from the collection to see the results.</p> <pre><code>var ex = yearCompCol.first();\n\nvar rgbVis = {\n  bands: ['Red', 'Green', 'Blue'],\n  min: 0,\n  max: 0.2\n};\n\nMap.addLayer(ex, rgbVis, \"Example Cloud Free Composite\");\n</code></pre> <p> <p></p> <p>Example of cloud free image. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#tassed-cap-bands-and-spectral-indices","title":"Tassed Cap bands and spectral indices","text":"<p>With our yearly composites prepared, we can now calculate and add Tasseled Cap bands and spectral indices. We\u2019ll calculate Tasseled Cap components using code and coefficients adapted from the LandTrendr GEE implementation. The external package spectral will be utilized for spectral indices. This package simplifies the process of defining necessary bands and calculating any index in the database using the <code>computeIndex</code> function. For the purpose of the exercise in Theme 5 and Case Study 3 we will calculate the following 8 bands:</p> <ul> <li> Tasseled Cap Brightness (TCB) </li> <li> Tasseled Cap Greenness (TCG) </li> <li> Tasseled Cap Wetness (TCW) </li> <li> Normalized Difference Vegetation Index (NDVI) </li> <li> Normalized Burn Ratio (NBR) </li> <li> Normalized Burn Ratio 2 (NBR2) </li> <li> Normalized Difference Moisture Index (NDMI) </li> <li> Normalized Difference Water Index (NDWI) </li> </ul> <pre><code>// Function to add Tasseled Cap transformation bands\nvar tasseledCap = function(img){ \n  // brightness coefficients\n  var brtCoeffs = ee.Image\n                  .constant([0.2043, 0.4158, 0.5524, 0.5741, 0.3124, 0.2303]);\n  // greenness coefficients\n  var grnCoeffs = ee.Image\n                  .constant([-0.1603, -0.2819, -0.4934, 0.7940, -0.0002, -0.1446]); \n  // wetness coefficients\n  var wetCoeffs = ee.Image\n                  .constant([0.0315, 0.2021, 0.3102, 0.1594, -0.6806, -0.6109]); \n\n  // Sum reducer - sum band values after multiplying them by coefficients\n  var sumReducer = ee.Reducer.sum(); \n  var brightness = img.multiply(brtCoeffs).reduce(sumReducer);\n  var greenness = img.multiply(grnCoeffs).reduce(sumReducer); \n  var wetness = img.multiply(wetCoeffs).reduce(sumReducer); \n  var tc = brightness.addBands(greenness)\n                     .addBands(wetness)\n                     // Rename the bands\n                     .select([0,1,2], ['TCB','TCG','TCW']); \n  return img.addBands(tc);\n};\n\n\nvar yearCompColTC = yearCompCol.map(tasseledCap);\n\n\n// Function to calculate selected indices\nfunction addIndices(img) {\n  // REQUIRED PARAMETERS ACCORDING TO THE REQUIRED BANDS\n  var parameters = {\n    \"B\": img.select(\"Blue\"),\n    \"G\": img.select(\"Green\"),\n    \"R\": img.select(\"Red\"),\n    \"N\": img.select(\"NIR\"),\n    \"S1\": img.select(\"SWIR1\"),\n    \"S2\": img.select(\"SWIR2\")\n  };\n  //NDVI, NBR, NBR2, NDMI, NDWI\n  return spectral\n          .computeIndex(img,[\"NDVI\", \"NBR\", \"NBR2\", \"NDMI\", \"NDWI\"], parameters);\n}\n\n// Calculate indices\nvar colIndices = yearCompColTC.map(addIndices);\n\nprint(colIndices);\n</code></pre> <p> <p></p> <p>Calculated TC bands and spectral indices added to the yearly image. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#band-renaming","title":"Band renaming","text":"<p>We now have all the data we need for export. Before proceeding, we need to reorganize this data into yearly stacks of each individual band/spectral index. Each band should also include the year of acquisition in its name, so it can be easily identified and utilized for plotting, among other tasks.</p> <pre><code>// Function to add years to bands names  \nfunction renameBandsYear(img) {\n    // Get the year property\n    var year = ee.String(img.get('year'));\n\n    // Get the band names and map over them to add the year\n    var bands = img.bandNames();\n    var newBands = bands.map(function(name) {\n        return ee.String(year).cat('_').cat(ee.String(name));\n    });\n\n    // Rename the bands and return the image\n    return img.rename(newBands);\n}\n\n// Rename bands\ncolIndices = colIndices.map(renameBandsYear);\n\nprint(colIndices);\n</code></pre> <p> <p></p> <p>Renamed bands.  The next chunk of code will execute the following tasks:</p> <ul> <li> flatten the entire <code>ImageCollection</code> into one image containing every band from that <code>colIndices</code> </li> <li> retrieve the band names following the flattening </li> <li> generate a list of band names with the prefix removed (ID of image from <code>colIndices</code>) from each band. </li> <li> replace modified band names with the correct band names, which include the year of acquisition and spectral band/index </li> </ul> <pre><code>\n// Transform image collection to one image\nvar oneImage = colIndices.toBands();\n\n// Get band names\nvar bandNames = oneImage.bandNames();\n\n// Map over band names to remove prefix\nvar newBandNames = bandNames.map(function(name) {\n    var y = ee.String(name).split('_').slice(1).join('_');\n    return ee.String('b').cat(y);\n});\n\n// Rename the bands\noneImage = oneImage.rename(newBandNames);\nprint(oneImage);\n</code></pre> <p> <p></p> <p>Renamed bands in one image. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#gap-fill-and-export-preparation","title":"Gap fill and export preparation","text":"<p>Next, we\u2019ll prepare and apply a function to fill any remaining gaps and prepare the data for export. Gaps may still appear following composite creation due to insufficient data within images from each year. We\u2019ll employ a filling algorithm to address these gaps. This algorithm operates as follows:</p> <ul> <li> replace missing values with the nearest preceding value (i.e., substitute 1985\u2019s missing values with 1984\u2019s values) </li> <li> if the preceeding values are all NA: substitute missing values with the nearest subsequent value </li> <li> for the first image, if data is missing, it uses the nearest subsequent value </li> <li> if the entire series lacks a valid value, leave it as \u2018NA\u2019 </li> </ul> <pre><code>// Function to fill gaps\n// It substitutes missing values with the nearest preceding value. \n// For the first image, if data is missing, it uses the nearest subsequent value.\nfunction fillGaps (img) {\n  // Get the image projection\n  var proj = img.projection();\n\n  // Get the band names of the image\n  var bandNames = img.bandNames();\n\n  // Make an Image object out of every element from a list of individual bands\n  var colList = bandNames.map(function(n) { return img.select([n]) });\n\n  // Define the mosaic function to fill gaps for each band\n  var mosaicFunction = function(band) {\n    // Get the index of the current band\n    var bandIndex = colList.indexOf(band);\n\n    // Get the index of the next band\n    var bandIndexPlus1 = bandIndex.add(1);\n\n    // Create two lists of bands before and after the current band\n    var colLtIndex = colList.slice(0, bandIndex, 1);\n    var colGtIndex = colList.slice(bandIndexPlus1).reverse();\n\n    // Create a list containing the current band\n    var colEqIndex = ee.List([]).add(colList.get(bandIndex));\n\n    // Concatenate the lists to create a new list of bands arranged as follows:\n    // [bands after current band, bands before current band, current band]\n    var all = colGtIndex.cat(colLtIndex).cat(colEqIndex).flatten();\n\n    // Convert the list of bands to an ImageCollection\n    var cc = ee.ImageCollection.fromImages(all);\n\n    // Map over the ImageCollection to rename bands to 'band'\n    var ccBandnames = cc.map(function(img) {return img.select(['.*'], ['band'])});\n\n    // Mosaic the ImageCollection to fill gaps for the current band\n    // The pixel in the final mosaic is the latest valid (non-masked pixel)\n    // That's why we put them in this order in `all` variable\n    var mosaicked = ccBandnames.mosaic();\n\n    return mosaicked;\n  };\n\n  // Map the mosaic function over the list of bands to fill gaps for each band\n  var mosaicked = colList.map(mosaicFunction);\n\n  // Create an ImageCollection from the mosaic result\n  var tempCol = ee.ImageCollection.fromImages(mosaicked);\n\n  //  Convert the ImageCollection to a single image using toBands()\n  tempCol = tempCol.toBands();\n\n  // Select the bands with original band names from the newly created image\n  var fixedBandNamesImg = tempCol.select(tempCol.bandNames(), bandNames);\n\n  // Reproject the image to its original projection\n  var filledGapsImg = fixedBandNamesImg.reproject(proj);\n\n  // Return the image with gaps filled for all bands\n  return filledGapsImg;\n}\n</code></pre> <p>The save name should be set as the year, and the data type should be changed to float. To perform all three tasks for each time series, we\u2019ll write the following function.</p> <pre><code>// Create bands/indices stacks images.\nvar fillAndConvert = function(bandName, oneImg) {\n  var img = oneImg.select('.*' + bandName);\n  img = img.set({'save': ee.String(bandName)});\n  img = img.float();\n  return fillGaps(img);\n};\n</code></pre> <p>From <code>oneImage</code> we can filter each band subsequently and apply the filling algorithm with preparing the data for export.</p> <pre><code>var blue = fillAndConvert('Blue', oneImage);\nvar green = fillAndConvert('Green', oneImage);\nvar red = fillAndConvert('Red', oneImage);\nvar nir = fillAndConvert('NIR', oneImage);\nvar swir1 = fillAndConvert('SWIR1', oneImage);\nvar swir2 = fillAndConvert('SWIR2', oneImage);\nvar tcb = fillAndConvert('TCB', oneImage);\nvar tcg = fillAndConvert('TCG', oneImage);\nvar tcw = fillAndConvert('TCW', oneImage);\nvar ndvi = fillAndConvert('NDVI', oneImage);\nvar nbr = fillAndConvert('NBR', oneImage);\nvar nbr2 = fillAndConvert('NBR2', oneImage);\nvar ndmi = fillAndConvert('NDMI', oneImage);\nvar ndwi = fillAndConvert('NDWI', oneImage);\n</code></pre> <p>Each image should contain 39 bands.</p> <pre><code>print(blue);\n</code></pre> <p> <p></p> <p>Blue bands. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#export-image-stacks","title":"Export image stacks","text":"<p>We\u2019ll show two ways to export the data, so that it can be then used in different exercises further in this Module.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#google-assets-theme-5-exercise","title":"Google Assets (Theme 5 exercise)","text":"<p>The function below can be used to export any of the stacks to your Google Assets folder. Remember to adjust the parameters for each stack you wish to export. Change the <code>assetID</code> for your path i.e.\u00a0<code>projects/ee-yourname/assets/</code> and add the name of the image at the end.</p> <pre><code>Export.image.toAsset({\n  // Name of the stack\n  image: ndvi,\n  // Location and name of the asset\n  assetId:'projects/etrainee-module2/assets/ndvi',\n  // Save/task name\n  description: 'NDVI',\n  // Clip export to AOI region, use Landsat 30 m scale;\n  // no need to change these parameters\n  region: AOI,\n  scale: 30,\n  maxPixels: 1e13\n});\n</code></pre> <p> <p> </p> <p>Export image to your Google Assets. </p> <p>TASK Export the other bands and indices to Assets.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#google-drive-case-study-3","title":"Google Drive (Case Study 3)","text":"<p>The function below can be used to export any of the stacks to your Google Drive folder. Remember to adjust the parameters for each stack you wish to export. You can then download the images to your hard drive.</p> <pre><code>Export.image.toDrive({\n  // Name of the stack\n  image: ndvi,\n  // Save name\n  description: 'NDVI',\n  // Change the folder to one in your Google Drive\n  folder: 'etrainee_gee',\n  // Clip export to AOI region, use Landsat 30 m scale;\n  // no need to change these parameters\n  region: AOI,\n  scale: 30,\n  maxPixels: 1e13\n});\n</code></pre> <p> <p> </p> <p>Export image to your Google Drive. </p> <p>TASK Export the other bands and indices to Drive.</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#appendix","title":"Appendix","text":"<p>There are other options for creating a yearly composite. If you want to test different methods you can modify the code provided above. Here we present 3 alternative methods.</p> <p>Median composite</p> Click here to see the solution. <pre><code>var yearCompList = yearList.map(function(year) {\n  var yearCol = tatryCol.filter(ee.Filter.calendarRange(year, year, 'year'));\n\n  // Switch method here\n  var yearComp = median(yearCol);\n\n  yearComp = yearComp.reproject({crs: 'EPSG:32634', scale: 30}); \n  return yearComp.set({\n    'year': ee.Number(year).int(),\n    'save': ee.String(year)\n  });\n});\n\nprint(yearCompList);\n</code></pre> <p></p> <p>Medoid composite</p> Click here to see the solution. <pre><code>// Function to create mosaic based on medoid value\n// Copied from Landtrendr / modified\nvar medoidMosaic = function(inCollection) {\n\n var finalCollection = inCollection;\n  // calculate median across images in collection per band\n var median = finalCollection.median();                                                                       \n  // calculate the difference between the median and the observation per image per band\n var difFromMedian = finalCollection.map(function(img) {\n    var diff = ee.Image(img).subtract(median).pow(ee.Image.constant(2));                                       // get the difference between each image/band and the corresponding band median and take to power of 2 to make negatives positive and make greater differences weight more\n    return diff.reduce('sum').addBands(img);                                                                   // per image in collection, sum the powered difference across the bands - set this as the first band add the SR bands to it - now a 6 band image collection\n  });\n\n  // get the medoid by selecting the image pixel with the smallest difference between median and observation per band \n  return ee.ImageCollection(difFromMedian).reduce(ee.Reducer.min(7)).select([1,2,3,4,5,6], ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']); // find the powered difference that is the least - what image object is the closest to the median of teh collection - and then subset the SR bands and name them - leave behind the powered difference band\n};\n\n\n\nvar yearCompList = yearList.map(function(year) {\n  var yearCol = tatryCol.filter(ee.Filter.calendarRange(year, year, 'year'));\n\n  // Switch method here\n  var yearComp = medoidMosaic(year_col);\n\n  yearComp = yearComp.reproject({crs: 'EPSG:32634', scale: 30}); \n  return yearComp.set({\n    'year': ee.Number(year).int(),\n    'save': ee.String(year)\n  });\n});\n\nprint(yearCompList);\n</code></pre> <p></p> <p>Quality/highest NDVI composite</p> Click here to see the solution. <pre><code>// Before applying the functions to the whole collection\nvar addNDVI = function(image) {\n  var ndvi = image.normalizedDifference(['NIR', 'Red']).rename('NDVI');\n  return image.addBands(ndvi);\n};\n\nvar prepLandsat457 = function(img) {\n  var originalImage = img;\n  var cloudMask = cloudMaskKernel(img);\n  var outputImage = renameBandsLandsat457(img);\n  outputImage = applyScalingFactor(outputImage);\n  outputImage = ee.Image(outputImage.copyProperties(originalImage, ['system:time_start']));\n  outputImage = addNDVI(outputImage);\n  return outputImage.updateMask(cloudMask);\n};\n\n\nvar prepLandsat89 = function(img) {\n  var originalImage = img;\n  var cloudMask = cloudMaskKernel(img);\n  var outputImage = renameBandsLandsat89(img);\n  outputImage = applyScalingFactor(outputImage);\n  outputImage = ee.Image(outputImage.copyProperties(originalImage, ['system:time_start']));\n  outputImage = addNDVI(outputImage);\n  return outputImage.updateMask(cloudMask);\n};\n\n// ... \n\n// continue with the script\n\n\n\nvar yearCompList = yearList.map(function(year) {\n  var yearCol = tatryCol.filter(ee.Filter.calendarRange(year, year, 'year'));\n\n  // Switch method here\n  var yearComp= yearCol.qualityMosaic('NDVI');\n\n  yearComp = yearComp.reproject({crs: 'EPSG:32634', scale: 30}); \n  return yearComp.set({\n    'year': ee.Number(year).int(),\n    'save': ee.String(year)\n  });\n});\n\nprint(yearCompList);\n\n// ... \n\n// continue with the script\n\n// Function to add Tasseled Cap transformation bands\nvar tasseledCap = function(img){ \n\n  var img2 = img.select([0, 1, 2, 3, 4, 5]);\n  // brightness coefficients\n  var brtCoeffs = ee.Image.constant([0.2043, 0.4158, 0.5524, 0.5741, 0.3124, 0.2303]);\n  // greenness coefficients\n  var grnCoeffs = ee.Image.constant([-0.1603, -0.2819, -0.4934, 0.7940, -0.0002, -0.1446]); \n  // wetness coefficients\n  var wetCoeffs = ee.Image.constant([0.0315, 0.2021, 0.3102, 0.1594, -0.6806, -0.6109]); \n\n  // Sum reducer - sum band values after multiplying them by coefficients\n  var sumReducer = ee.Reducer.sum(); \n  var brightness = img2.multiply(brtCoeffs).reduce(sumReducer);\n  var greenness = img2.multiply(grnCoeffs).reduce(sumReducer); \n  var wetness = img2.multiply(wetCoeffs).reduce(sumReducer); \n  var tc = brightness.addBands(greenness)\n                     .addBands(wetness)\n                     // Rename the bands\n                     .select([0,1,2], ['TCB','TCG','TCW']); \n  return img.addBands(tc);\n};\n\nvar yearCompColTC = yearCompCol.map(tasseledCap);\n\n// Calculate selected indices\nfunction add_indices(img) {\n  // REQUIRED PARAMETERS ACCORDING TO THE REQUIRED BANDS\n  var parameters = {\n    \"B\": img.select(\"Blue\"),\n    \"G\": img.select(\"Green\"),\n    \"R\": img.select(\"Red\"),\n    \"N\": img.select(\"NIR\"),\n    \"S1\": img.select(\"SWIR1\"),\n    \"S2\": img.select(\"SWIR2\")\n  };\n  //NDVI, NBR, NBR2, NDMI, NDWI\n  return spectral.computeIndex(img,[\"NBR\", \"NBR2\", \"NDMI\", \"NDWI\"], parameters);\n}\n\nvar colIndices = yearCompColTC.map(addIndices);\n\nprint(colIndices);\n\n// ... \n\n// continue with the script\n\n</code></pre> <p></p> <p>Here you can find the link to the whole code presented in Pipeline 2</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#processing-pipeline-3","title":"Processing pipeline 3","text":"<p>The purpose of this pipeline is to obtain a multitemporal Sentinel-2 classification dataset for the selected region of Karkonosze Mountains (above 1200 m) from vegetative period in 2018 and 2019. The results of this processing pipeline will be utilized in the Case Study 1.</p> <p> <p></p> <p>Processing pipeline. </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#area-of-interest_2","title":"Area of Interest","text":"<p>Area of Interest (<code>AOI</code>) encompasses the part of Karkonosze Mountains above 1200 m. More on that area in the data usecase description here.</p> <pre><code>// Define AOI\nvar AOI = ee.FeatureCollection(\"projects/ee-kgryguc/assets/cs1_1200m\");\n\n// Pan to and display area of interest\nMap.centerObject(AOI, 11);\nMap.addLayer(AOI, {'color': 'darksalmon'}, 'Karkonosze (above 1200m)');\n</code></pre> <p> <p></p> <p>Area of Interest (AOI). </p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#imagery_2","title":"Imagery","text":"<p>Similarly to the Pipeline 1 we will use Sentinel-2 Surface Reflectance (L2A) data. In this pipeline we want to use images from 2018 and 2019 (from 10th May to 20th September) that are either cloud-free or have cloud cover of less than 5% over our <code>AOI</code>. In order to do that we want to mask clouds and cloud shadows to calculate the fraction of masked pixels inside the Area of Interest.</p> <p>The first part will be similar to that in Pipeline 1. This time to filter the imagery to contain only images between desired dates from both years we\u2019ll use slightly different filters.</p> <pre><code>// Define pre-filter percentage\nvar cloudFilter = 90;\n\n// Filter for images between 15th May and 15th September 2018\nvar filter2018 = ee.Filter.date('2018-05-10', '2018-09-20');\n\n// Filter for images between 15th May and 15th September 2019\nvar filter2019 = ee.Filter.date('2019-05-10', '2019-09-20');\n\n// Combine the two filters\nvar combinedFilter = ee.Filter.or(filter2018, filter2019);\n\n\n// Import and filter Sentinel-2 Level 2A images\nvar s2SrCol = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n    .filterBounds(AOI)\n    .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', cloudFilter))\n    .filter(combinedFilter);\n</code></pre> <p>To observe the results of this filtering, insert the following line into your script and rerun the code:</p> <p>print(s2SrCol);</p> <p>You should see an <code>ImageCollection</code> with 90 elements in the console tab.</p> <p>Then we\u2019ll utilize <code>s2cloudless</code> data again.</p> <pre><code>// Import and filter s2cloudless\nvar s2CloudlessCol = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n    .filterBounds(AOI)\n    .filter(combinedFilter);\n\n// Join the filtered s2cloudless collection to the SR collection\n// by the 'system:index' property\nvar s2SrCloudlessCol =\n// ee.Join.saveFirst means that new element will be added \n// to the primary image as a property\nee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply({\n    'primary': s2SrCol,\n    'secondary': s2CloudlessCol,\n    'condition': ee.Filter.equals({\n        'leftField': 'system:index',\n        'rightField': 'system:index'\n    })\n}));\n</code></pre>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#cloud-and-shadow-masking-cloud-coverage-over-area-of-interest","title":"Cloud and shadow masking / cloud coverage over area of interest","text":"<p>This part is the same as in Pipeline 1 and was discussed there in more detail. Refer to that part if you need help.</p> <pre><code>// Cloud/shadow masking variables\n\n//Cloud probability (%); based on `probability` image;\n// values greater than are considered cloud\nvar CLD_PRB_THRESH = 20;\n\n//Near-infrared reflectance (B8 band);\n// values below that threshold are considered potential cloud shadow\nvar NIR_DRK_THRESH = 0.2;\n\n// Maximum distance (km) to search for cloud shadows from cloud edges\nvar CLD_PRJ_DIST = 1;\n\n// Distance (m) to dilate the edge of cloud-identified objects\nvar BUFFER = 50;\n\n// CLOUD COMPONENTS\n// Function to add the s2cloudless probability layer and derived cloud mask\n// as bands to an S2_SR_HARMONIZED image input.\nfunction addCloudBands(img){\n\n  // Get s2cloudless image, subset the probability band.\n    var cldPrb = ee.Image(img.get('s2cloudless')).select('probability');\n\n    // Condition s2cloudless by the probability threshold value.\n    // Creates new raster with cloud pixels\n    var isCloud = cldPrb.gt(CLD_PRB_THRESH).rename('clouds');\n\n    // Add the cloud probability layer and cloud mask as image bands.\n    return img.addBands(ee.Image([cldPrb, isCloud]));\n}\n\n// SHADOW COMPONENTS\nfunction addShadowBands(img){\n\n  // Identify water pixels from the SCL band.\n    var notWater = img.select('SCL').neq(6);\n\n    // Identify dark NIR pixels that are not water \n    // (potential cloud shadow pixels).\n    var SR_BAND_SCALE = 1e4;\n    var darkPixels = img.select('B8')\n    .lt(NIR_DRK_THRESH*SR_BAND_SCALE)\n    .multiply(notWater)\n    .rename('dark_pixels');\n\n    // Determine the direction to project cloud shadow from clouds\n    // (assumes UTM projection).\n    var shadowAzimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n\n    // Create a cloud projection based on calculated parameters\n    var cldProj = (img\n        .select('clouds')\n        .directionalDistanceTransform(shadowAzimuth, CLD_PRJ_DIST*10)\n        .reproject({'crs': img.select(0).projection(), 'scale': 100})\n        .select('distance')\n        .mask()\n        .rename('cloud_transform'));\n\n    // Identify the intersection of dark pixels with cloud shadow projection.    \n    var shadows = cldProj.multiply(darkPixels).rename('shadows');\n\n    // Add dark pixels, cloud projection, and identified shadows as image bands.\n    return img.addBands(ee.Image([darkPixels, cldProj, shadows]));\n}\n\n// Add combined cloud and shadow mask to image bands.\nfunction addCldShdwMask(img){\n\n  // Add cloud component bands.\n    var imgCloud = addCloudBands(img);\n\n     // Add cloud shadow component bands.\n    var imgCloudShadow = addShadowBands(imgCloud);\n\n    // Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n    var isCldShdw = imgCloudShadow\n                .select('clouds')\n                .add(imgCloudShadow.select('shadows')).gt(0);\n\n    // Remove small cloud-shadow patches; dilate remaining pixels by BUFFER input.\n    // 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n    var isCldShdw2 = (isCldShdw.focalMin(2).focalMax(BUFFER*2/20)\n        .reproject({'crs': img.select([0]).projection(), 'scale': 20})\n        .rename('cloudmask'));\n\n    // Add the final cloud-shadow mask to the image.\n    return imgCloudShadow.addBands(isCldShdw2);\n}\n\n// Add calculated cloud and shadow mask bands to each image in the collection.\nvar s2ColAddMask = s2SrCloudlessCol.map(addCldShdwMask);\n\n\n// Function to apply masks to spectral bands.\nfunction applyCldShdwMask(img) {\n  // Make a copy of input original image to preserve properties\n  var orig = img;\n\n  // Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n  var notCldShdw = img.select('cloudmask').not();\n\n  // Subset reflectance bands and update their masks,\n  // return the result as 0-1 SR bands.\n  img =  img.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'])\n  .updateMask(notCldShdw)\n  .divide(10000);\n\n  // Copy the original image properties\n  return img.copyProperties(orig, orig.propertyNames());\n}\n\n// Apply masking function to the collection.\nvar s2ColMasked = s2ColAddMask.map(applyCldShdwMask);\n\n\n// Define the function to return the percentage of masked pixels in an image region as a property. \nfunction calculateMaskedPercent(image) {\n  // Select one of the bands and count unmasked pixels within research area\n  var unmaskedPixelCount = image.select('B2').reduceRegion({\n    reducer: ee.Reducer.count(),\n    geometry: AOI,\n    scale: 10,\n    maxPixels: 1e13\n  }).get('B2');\n\n  // Select one of the bands and count all pixels within research area\n  var totalPixelCount  = image.select('B2').unmask().reduceRegion({\n    reducer: ee.Reducer.count(),\n    geometry: AOI,\n    scale: 10,\n    maxPixels: 1e13\n  }).get('B2');\n\n// Calculate cloud cover percentage over AOI\n  var cloudCoverAoi = ee.Number(1)\n      .subtract(ee.Number(unmaskedPixelCount)\n      .divide(totalPixelCount))\n      .multiply(100)\n      .round();\n\n  // Return the original image with new cloud_cover_aoi property\n  return image.set('cloud_cover_aoi', cloudCoverAoi);\n}\n\n// Apply the function to the collection\nvar s2ColCloudCoverAoi =  s2ColMasked.map(calculateMaskedPercent);\n\n// Filter collection to only images with `cloud_cover_aoi` less than or equal 5%\nvar s2FilteredCol = s2ColCloudCoverAoi.filter(ee.Filter.lte('cloud_cover_aoi', 5));\n</code></pre>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#image-clip-and-maskingtopographic-shadow-masking","title":"Image clip and masking/topographic shadow masking","text":"<p>In the end we want to end up with AOI clipped to the exact extent of the shapefile. We\u2019ll clip the image now to speed up the remaining processes and shrink the size of the exported image.</p> <pre><code>// Clip the image to contain only pixels from AOI\ns2FilteredCol = s2FilteredCol.map(function(image){return image.clip(AOI)});\n</code></pre> <p>New part in this Pipeline is masking water and topographic shadows. We\u2019ll use simple NIR band thresholding, which may not be a perfect solution, but for this purpose should work fine. The thresholds for each term were set by empirical tests.</p> <pre><code>// Mask water and topographic shadows\nvar maskBelowNIRThreshold = function(image) {\n  // Get image date in yyyy-mm-dd format\n  var date = image.date().format('yyyy-MM-dd');\n\n  // Select NIR band\n  var nir = image.select('B8');\n\n  // Empirically derived threshold for each term of acquisition\n  var thresholds = {\n    '2018-05-14': 0.11,\n    '2018-05-31': 0.16,\n    '2018-08-07': 0.11,\n    '2018-08-27': 0.11,\n    '2018-09-18': 0.10,\n    '2019-06-25': 0.16,\n    '2019-06-30': 0.16\n    };\n\n  // From the thresholds make a ee.Dictionary\n  var thresholdDict = ee.Dictionary(thresholds);\n\n  // Choose the matching threshold\n  var threshold = thresholdDict.get(date, null);\n\n  // If no threshold is found for that date, return the original image\n  // If there's a threshold prepare a mask,\n  // with NIR values below threshold being masked\n  var mask = ee.Algorithms.If(ee.Algorithms.IsEqual(threshold, null), \n                              1, \n                              nir.gte(ee.Number(threshold)));\n  // Return masked image\n  return image.updateMask(mask);\n};\n\n\n// Map the function over the collection\nvar maskedCollection = s2FilteredCol.map(maskBelowNIRThreshold);\n</code></pre>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#band-renaming-and-export_1","title":"Band renaming and export","text":"<p>The final part is also the same as in Pipeline 1.</p> <pre><code>\nfunction renameBands(image) {\n\n  // Get the date of the image\n  var date = ee.Date(image.get('system:time_start'));\n  var dateString = date.format('YYYY-MM-dd');\n\n  // Get the list of band names in the image\n  var bandNames = image.bandNames();\n\n  // Map over the band names and rename them\n  var renamedBands = bandNames.map(function(bandName){\n    return dateString.cat('_').cat(bandName);\n  });\n\n  // Use .select() to rename the original band names\n  var renamedImage = image.select(bandNames, renamedBands);\n\n  return renamedImage;\n}\n\nvar s2FilteredColRenamed = maskedCollection.map(renameBands);\n\n\n// Function to get all band names from the entire collection\nfunction getAllBandNames(collection) {\n  // Convert the ImageCollection into a list\n  var bandNamesList = collection\n    .toList(collection.size())\n\n  // Map over each image in the list to get its band names\n    .map(function(image) {\n      // Convert each element in the list (image) into an ee.Image object\n      var imageObject = ee.Image(image);\n\n      // Get the band names of the current image using ee.Image.bandNames()\n      return imageObject.bandNames();\n    })\n\n  // Flatten the list to obtain a single list containing all band names\n    .flatten();\n\n  // Return the list of all band names\n  return bandNamesList;\n}\n\n// Get all unique band names from the entire renamed ImageCollection\nvar bandNames = getAllBandNames(s2FilteredColRenamed);\n\n\n// Convert the collection to a single multi-band image\nvar s2ImageMultiband = s2FilteredColRenamed.toBands();\n\n// Replace temporary names with previously prepared date-marked names.\ns2ImageMultiband = s2ImageMultiband.select(\n  s2ImageMultiband.bandNames(), bandNames);\n</code></pre> <p>Export the image to finalize this pipeline.</p> <pre><code>Export.image.toDrive({\n  image: s2ImageMultiband,\n  description: 'CS1_image_data',\n  // Change the folder to one in your Google Drive\n  folder: 'ETR_GEE',\n  region: AOI,\n  maxPixels: 1e13,\n  scale: 10\n});\n</code></pre> <p>Here you can find the link to the whole code presented in Pipeline 3</p>"},{"location":"module2/03_image_processing/03_image_processing_exercise.html#this-is-the-end-of-this-exercise-proceed-with-other-themes-and-exercises-good-luck","title":"This is the end of this exercise. Proceed with other Themes and Exercises. Good luck!","text":""},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html","title":"Multitemporal classification","text":"<p>Semi-automatic supervised classification based on multitemporal datasets has the potential to improve mapping performance in comparison to single-date collected data, especially for naturally changing vegetation classes. Phenological characteristics and time-related spectral behaviour are key to an improved potential of input data consisting of images acquired over a period of time.</p> <p>In this theme, you will learn about:</p> <ul> <li>Multitemporal reference dataset</li> <li>Machine learning algorithms</li> <li>Selection of the best terms of data acquisition</li> <li>Results validation and accuracy assesment</li> </ul> <p>This theme will conclude with:</p> <ul> <li>Self-evaluation quiz</li> <li>References</li> </ul> <p>To consolidate your knowledge and test it in more practical environment complete the Exercise.</p> <p>After completing this theme, you will:</p> <ul> <li>understand the potential of satellite multitemporal datasets for improving mapping performance</li> <li>be familiar with the challenges of collecting reference data for multitemporal classification</li> <li>know the availability of publicly available multitemporal land cover datasets and alternative data sources for reference data collection, such as citizen science projects</li> <li>be familiar with machine learning algorithms commonly used for multitemporal classification, and be introduced to modifications of algorithms that consider spatio-temporal dependencies for improved classification</li> <li>understand the role of feature selection in optimizing multitemporal classification by reducing dimensionality</li> <li>be aware of R packages that include machine/statistical learning algorithms for classification</li> <li>understand the strategies for validation and accuracy assessment of multitemporal classification results, including methods like k-fold cross-validation and bootstrapping</li> </ul>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#multitemporal-reference-dataset","title":"Multitemporal reference dataset","text":"<p>Reference data for the classification of satellite images can be collected in the field or sourced from developed (reliable!) products. Below, we will discuss a few issues related to the data itself and the temporal aspect.</p> <p>Reference data sampling for time series data classification in general is a challenging task. Ideally, the reference data should come from field campaigns performed near the satellite data acquisition dates.</p> <p> <p></p> <p>The concept of synchronized GPS field measurements with the dates of acquiring satellite data (alpine grasslands in Karkonosze Mts., figure by course authors). </p> <p>However, this scenario is rather impossible to fulfill when dealing with time series composed of regularly and frequently collected images (such as Sentinel-2 every 5 days), especially in terrains like mountains.</p> <p>In such cases, when the classes are relatively easy to recognize, it might be helpful to use additional daily acquired high-resolution data (for instance PlanetScope) to enrich the image interpretation needed to make the reference dataset denser. Another data source can be Google Earth, where high-resolution imagery from aerial platforms is available. However, we should deeply analyze the areas where the known objects of interest are located (spatial surroundings and/or spectral characteristics) to check the consistency between available data in different terms.</p> <p>If we are unable to collect reference data in the field, we can use various publicly available datasets developed at the global, continental, regional, or local scale. Some of these datasets are only available for one date or year, but many consist of time series (see Table 1, and note that only classes excluding presented changes are listed there). These are mainly related to land use/land cover, but there are also more detailed databases for specific classes, such as agricultural areas, forests, grasslands, etc. (see more in Garc\u00eda-\u00c1lvarez et al., 2022).</p> Table 1. Selected multitemporal land cover datasets with global/continental extent developed on satellite data. Extent Product name Dates Pixel size Source data Maximum no of. classes Source global World Cover 2020, 2021 10 m Sentinel-2 11 2020: https://doi.org/10.5281/zenodo.5571936 2021: https://doi.org/10.5281/zenodo.7254221 global Esri Land Cover 2017-2022 10 m Sentinel-2 9 https://livingatlas.arcgis.com/landcoverexplorer global Dynamic World 2015-present 10 m Sentinel-2 9 https://dynamicworld.app/ global TimeSpec4LULC 2002-2021 500 km MODIS 29 https://zenodo.org/record/5020024#.Y9mruz3MKUk global CGLS-LC100\u2014Copernicus Global Land Service Dynamic Land Cover Map 2015-2019 100 m PROBA-V, Sentinel-2, MODIS 24 https://land.copernicus.eu/global/products/lc european CLC\u2014CORINE Land Cover 1990, 2000, 2006, 2012, 2018 100 m Landsat, SPOT; ITS P6, RapidEye, LISS III, Sentinel 44 https://land.copernicus.eu/pan-european/corine-land-cover european Annual Land Cover Product 2000-2019 30 m Landsat, VIIRS/SUOMI NPP 33 https://medium.com/swlh/europe-from-above-space-time-machine-learning-reveals-our-changing-environment-1b05cb7be520 Africa West Africa Land Use Land Cover 1975, 2000, 2013 2 km Landsat 30 https://eros.usgs.gov/westafrica/ South America LBA-ECO LC-08\u2014Land Cover Map of South America 1987, 1991 1 km AVHRR 42 https://daac.ornl.gov/LBA/guides/LC08_EOS_Maps.html North America NALCMS\u2014North American Land Change Monitoring System 2005, 2010, 2015 30, 250 m Landsat, MODIS 19 http://www.cec.org/north-american-land-change-monitoring-system/ <p>At the top of the table there are multitemporal databases based of 10 m Sentinel-2 data updated annually: World Cover and Esri Land Cover, as well as in almost real time: Dynamic World (see interesting comparison of these three in Venter et al., 2022). Unlike the CORINE database developed as a result of images visual interpretation, these are the results of automatic classifications using machine/deep learning, and possible errors and lower accuracy in unverified areas should be taken into account. It\u2019s also important to note that based on maps, we know there are transition zones between classes.</p> <p>In general, you should not take the boundaries of all classes from such products literally. They are supposed to be reference materials that will help you determine the classes of the legend and make it easier for you to interpret the objects. Which parts of such a set you consider reliable is up to you.</p> <p>A unique way to have data from the field for local scale analysis, without being able to collect it on our own, is to use samples collected within citizen science projects. In these projects, geolocated and labeled field observations are acquired by volunteers, e.g.\u00a0with their smartphones. There are also some tutorials how to interpret the objects of interest making the dataset as much reliable as possible. We refer you to read about some projects and platforms in Theme 6 of Module 1. Example publication where citizen science collected data were used for classification was elaborated by Houskeeper et al., 2022.</p> <p> <p></p> <p>The example of utilization of citizen science data. Left: The Zooniverse platform in which citizen scientists indicate kelp canopy locations for the Floating Forests project, right: giant kelp classification result based on Landsat data (figure by Houskeeper et al., 2022/ CC BY 4.0). </p> <p>Regardless of whether we use field data or existing maps/databases, it is useful to apply simple change detection to exclude possible changes between analysed dates, especially when using data from several years. This can be achieved by calculating difference between spectral index values of the actual and previous years (for instance pixels with absolute differences of NDVI \u22640.05 flagged as \u2018change\u2019; Immitzer et al., 2019) or to apply some disturbance detection related algorithm like e.g.\u00a0LandTrendr, see Theme 5. Such information can be important in developing a reference set for classification that is to point to the same constant objects. For the construction of multitemporal reference dataset, especially when using a wider time range for classification, a time series visualisation tool such as TimeSync or the one you learned in an exercise in Theme 2 may be also useful (you will use it to develop your own reference data set in the exercise at the end).</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#machine-learning-algorithms","title":"Machine learning algorithms","text":"<p>When classifying multitemporal data, a large number of predictor variables can be involved. Therefore, it\u2019s important to select the optimal algorithm in terms of computational cost, accuracy, and so on. Literature on multitemporal classification often confirms the suitability of supervised learning. The most frequently used machine learning algorithms are Random Forest (RF, Breiman, 2001), Support Vector Machines (SVMs, Vapnik, 1999) and Artificial Neural Networks (ANNs) including deep learning algorithms, such as Convolutional Neural Networks (CNNs, LeCun, 1998). Below are the examples of their use with satellite data in mapping of e.g.:</p> <ul> <li>grasslands (Rapinel et al., 2019, Tarantino et al., 2021, Marcinkowska-Ochtyra et al., 2023),</li> <li>tree species (Immitzer et al., 2019, Ho\u015bci\u0142o and Lewandowska, 2019),</li> <li>mountain vegetation, including forests (Kollert et al., 2021), non-forests (Wakuli\u0144ska and Marcinkowska-Ochtyra et al., 2020) and both (Kluczek et al., 2022),</li> <li>crops (Gadiraju et al., 2020, Ashourloo et al., 2022),</li> <li>coastal areas (Marzialetti et al., 2019, Munizaga et al., 2022), etc.</li> </ul> <p>Recently, increasing amounts of multitemporal satellite data has led to modifications of some known algorithms. These modifications take into account spatio-temporal dependencies into classification, particularly for deep learning, e.g.\u00a0Temporal Convolutional Network (TCN, Bai et al., 2018), Temporal Convolutional Neural Networks (TempCNNs, Pelletier et al., 2019), A DUal view Point deep Learning architecture for time series classificatiOn (DuPLO, Interdonato et al., 2019).</p> <p>Other modifications are the algorithms that that break time series into segments and then classify them, e.g.\u00a0Continuous Change Detection and Classification (CCDC, Zhu and Woodcock, 2014), which links two types of analyses: detecting land cover changes and providing land cover maps using Random Forest for any given time. We refer to this in Theme 5.</p> <p>Another approach is presented in Time-Weighted Dynamic Time Warping algorithm (TWDTW, Maus et al., 2019), modified version of dynamic time warping (DTW) method to match time series data segments into predefined temporal patterns. In general, the standard DTW compares a temporal signature of a known event with an unknown time series by finding all possible alignments between two time series and providing a dissimilarity measure (values close to zero show the highest similarity between the time series). TWDTW is additionally sensitive to various vegetation (natural and cultivated) seasonal dynamics.</p> <p> <p></p> <p>The matches of the \u2018soybean\u2019 class pattern in time series using TWDTW on MODIS data (solid black line - long-term time series, the colored lines - temporal patterns; gray dashed lines - the respective matching points (figure by Maus et al., 2019/ CC BY 3.0). </p> <p>Note: Since this module mainly focuses on data with medium spatial resolution, we present the described algorithms in a pixel-based, not object-based approach. The latter would be more justified using high-resolution data, however you can find also examples on its use with e.g.\u00a0Sentinel-2 (article about the use of different segmentation methods for delineation of field boundaries by Watkins and Van Niekerk, 2019).</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#r-packages-including-machinestatistical-learning-algorithms","title":"R packages including Machine/Statistical Learning algorithms","text":"<p>Since different modules and themes of this course use different classification tools, we will focus on the R packages available for this purpose. Most of the packages mentioned below are used to train, tune, develop and apply the statistical classification models. Application to the numerical and categorical data results in accuracy metrics and confusion matrices. Models can also be applied to produce classification images with e.g.\u00a0<code>predict</code> function from the <code>raster</code> package or grid CSV files.</p> <p>The following table focuses on the tools, which enable obtaining and analyzing both accuracy metrics and classification image.</p> Table 2. Selected R packages used in classification of satellite imagery. Package name Title Description Classification algorithms Key functions Authors C50 C5.0 Decision Trees and Rule-Based Models C5.0 decision trees and rule-based models for pattern recognition that extend the work of Quinlan (1993, ). C5.0 Decision Trees and Rule-Based Models <code>C5.0</code>, <code>plot.C5.0</code> Reference manual Max Kuhn, Steve Weston, Mark Culp, Nathan Coulter, Ross Quinlan (Author of imported C code), RuleQuest Research (Copyright holder of imported C code), Rulequest Research Pty Ltd.\u00a0(Copyright holder of imported C code) caTools Tools: Moving Window Statistics, GIF, Base64, ROC AUC, etc Contains several basic utility functions including: moving (rolling, running) window statistic functions, read/write for GIF and ENVI binary files, fast calculation of AUC, LogitBoost classifier, base64 encoder/decoder, round-off-error-free sum and cumsum, etc. LogitBoost <code>LogitBoost</code> Reference manual Jarek Tuszynski e1071 Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien Functions for latent class analysis, short time Fourier transform, fuzzy clustering, support vector machines, shortest path computation, bagged clustering, naive Bayes classifier, generalized k-nearest neighbour. Support Vector Machines, Fuzzy C-Means Clustering, Generalized k-Nearest Neighbors Classification, Naive Bayes Classifier <code>svm</code>, <code>cmeans</code>, <code>gknn</code>, <code>naiveBayes</code>, <code>tune</code> Reference manual David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, Friedrich Leisch, Chih-Chung Chang, Chih-Chen Lin ipred Improved Predictors Improved predictive models by indirect classification and bagging for classification, regression and survival problems as well as resampling based estimators of prediction error. Bagging Classification Trees, k-Nearest Neighbour Classification <code>bagging</code>, <code>ipredknn</code> Reference manual Andrea Peters, Torsten Hothorn, Brian D. Ripley, Terry Therneau, Beth Atkinson kernlab Kernel-Based Machine Learning Lab Kernel-based machine learning methods for classification, regression, clustering, novelty detection, quantile regression and dimensionality reduction. Among other methods \u2018kernlab\u2019 includes Support Vector Machines, Spectral Clustering, Kernel PCA, Gaussian Processes and a QP solver. Support Vector Machines, Least Squares Support Vector Machine, Gaussian processes for classification <code>ksvm</code>, <code>lssvm</code>, <code>gausspr</code>, <code>sigest</code> Reference manual Alexandros Karatzoglou, Alex Smola, Kurt Hornik, National ICT Australia (NICTA)] Michael A. Maniscalco, Choon Hui Teo nnet Feed-Forward Neural Networks and Multinomial Log-Linear Models Software for feed-forward neural networks with a single hidden layer, and for multinomial log-linear models. Single-Hidden-Layer Neural Network <code>nnet</code> Reference manual Brian Ripley, William Venables randomForest Breiman and Cutler\u2019s Random Forests for Classification and Regression Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) . Random Forest <code>randomForest</code>, <code>tuneRF</code>, <code>rfcv</code>, <code>importance</code>, <code>varImpPlot</code> Reference manual Fortran original by Leo Breiman and Adele Cutler, R port by Andy Liaw and Matthew Wiener. randomForestSRC Fast Unified Random Forests for Survival, Regression, and Classification (RF-SRC) Fast OpenMP parallel computing of Breiman\u2019s random forests for univariate, multivariate, unsupervised, survival, competing risks, class imbalanced classification and quantile regression. New Mahalanobis splitting for correlated outcomes. Extreme random forests and randomized splitting. Suite of imputation methods for missing data. Fast random forests using subsampling. Confidence regions and standard errors for variable importance. New improved holdout importance. Case-specific importance. Minimal depth variable importance. Visualize trees on your Safari or Google Chrome browser. Anonymous random forests for data privacy. Random Forest <code>rfsrc</code>, <code>tune</code>, <code>vimp</code> Reference manual Hemant Ishwaran, Udaya B. Kogalur rpart Recursive Partitioning and Regression Trees Recursive partitioning for classification, regression and survival trees. An implementation of most of the functionality of the 1984 book by Breiman, Friedman, Olshen and Stone. Recursive Partitioning and Regression Trees <code>rpart</code>, <code>plot.rpart</code> Reference manual Terry Therneau, Beth Atkinson, Brian Ripley (producer of the initial R port, maintainer 1999-2017) xgboost Extreme Gradient Boosting Extreme Gradient Boosting, which is an efficient implementation of the gradient boosting framework from Chen &amp; Guestrin (2016). This package is its R interface. The package includes efficient linear model solver and tree learning algorithms. The package can automatically do parallel computation on a single machine which could be more than 10 times faster than existing gradient boosting packages. It supports various objective functions, including regression, classification and ranking. The package is made to be extensible, so that users are also allowed to define their own objectives easily. Extreme Gradient Boosting <code>xgboost</code> Reference manual Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen , Rory Mitchell, Ignacio Cano, Tianyi Zhou, Mu Li, Junyuan Xie, Min Lin, Yifeng Geng, Yutian Li, Jiaming Yuan, XGBoost contributors (base XGBoost implementation) <p>Packages that combine different utilities like pre-processing, tuning, multi-method fitting, importance estimations, visualizations etc.:</p> <ul> <li>caret</li> <li>caretEnsemble</li> <li>CORElearn</li> <li>DALEX</li> <li>h2o</li> <li>mltools</li> <li>mlr3</li> <li>rminer</li> <li>sits</li> <li>SuperLearner</li> <li>tidymodels</li> </ul>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#selection-of-the-best-terms-of-data-acquisition","title":"Selection of the best terms of data acquisition","text":"<p>Depending on the classified object, the date of data acquisition will be key in the most accurate identification. For vegetation classes, phases of its development are particularly significant. For example, for grasslands in the Karkonosze Mountains that begin to discolor in autumn, September would be a more appropriate time to distinguish them from their surroundings than other months like August, when they are spectrally similar to subalpine tall forbs.</p> <p>Creating the optimal spectro-temporal dataset for specific purpose can be done in three ways:</p> <ol> <li> <p>classifying each image for each term separately and assessing the accuracy of each result,</p> </li> <li> <p>classification of multitemporal data, where particular or all terms are combined into one dataset to increase the potential of the data by allowing more temporal variability to be used,</p> </li> <li> <p>classification of multitemporal data and selection of the most important variables in the classification and using the model with only those variables.</p> </li> </ol> <p> <p></p> <p>Time-series datasets used for selection of the best term of data acquisition for classification (A - three single-date datasets, B - multitemporal dataset in each possible terms combination, C - the best variables selected from multitemporal dataset, figure by course authors). </p> <p>The first two do not require the use of additional tools, while the last one uses feature selection (FS) methods (see the next section).</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#feature-selection-as-potential-to-optimise-multitemporal-classification","title":"Feature selection as potential to optimise multitemporal classification","text":"<p>Multitemporal classification involves many variables. For instance, five Sentinel-2 scenes with selected 10 spectral bands each will result in 50 variables.</p> <p>This can lead to longer data processing times and may also lead to the inclusion of correlated bands, which carry similar information. Including additional bands, such as spectral or textural indices may be useful, but it also increases the dataset. Hence, selecting the most important ones and including them in the development of the final product can significantly improve the accuracy of the obtained result. Apart from the obtained accuracies, the data storage, processing time, and simplicity of classification models are also important, especially in large scale analysis. Therefore, the reduction of the number of variables to the most influential ones makes this multitemporal classification process more robust.</p> <p>Generally, to reduce dimensionality in data we can apply transformation (such as PCA transformation described in Theme 1) or feature selection methods.</p> <p> <p></p> <p>Dimensionality reduction methods with examples (figure by course authors). </p> <p>Feature selection approaches can be distinguished into three groups (Li et al., 2017):</p> <ul> <li>Filter - where bands selection is independent of the classifier. An example is a statistic measuring Pearson\u2019s correlation between two variables.</li> <li>Wrapper - where a specific classification model is applied for training and testing. An example is the backward elimination method, which starts with all features included in the model and then removes the least relevant features.</li> <li>Embedded - where band selection is performed within classifier construction. The advantage of some machine learning algorithms is the ability to identify the most influential bands thanks to the variable importance metric calculation mechanism. For example, in Random Forest, it is based on an average decrease in accuracy or an average decrease in the Gini coefficient (the higher the value of the measure, the greater the usefulness of a given variable).</li> </ul> <p> <p></p> <p>Aggregated feature importance derived from tree species Sentinel-2 classification result from 2015-2017 by the use of variable importance in Random Forest (figure by Immitzer et al., 2019, modified/ CC BY 4.0). </p> <p>Feature selection methods can be divided into unsupervised and supervised. The first one select representative bands based on the original image characteristics, while the second one use label information to assess the quality of particular bands.</p> <p>The choice of the final variables is user-dependent. It will affect both the accuracy of the classification and its duration so the compromise between high accuracy and time-effectiveness of data acquisition and processing is needed. Georganos et al., 2017 proposed a metric called classification optimization score (COS) that rewards model simplicity and penalises increasing computational time using a given number of features in the classification model.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#results-validation-and-accuracy-assesment","title":"Results validation and accuracy assesment","text":"<p>Validation of multitemporal classification results is often not very straightforward, and different strategies can be implemented to validate it qualitatively and quantitatively (see more on this in Theme 6 of Module 1). It can be performed as follows:</p> <ul> <li>visual images interpretation (where pixel size plays a key role in distinguishing objects)</li> <li>the use of existing maps or databases from inventories (where the key is information whether the data are up-to-date)</li> <li>splitting the whole reference dataset - classification - and then, assessing the accuracy of obtained result.</li> </ul> <p>While the first two points can be independent of the data used for classification, the third defines the entire classification and validation procedure simultaneously. Different strategies can be adopted for the split of the whole data set: equal division (50%/50%), the greater part intended for training and the smaller part for validation (e.g.\u00a063.2%/36.8%), or the greater part intended for validation (e.g.\u00a030%/70%).</p> <p>When we have a reference sample set, we can split it in various proportions iteratively which will lead to obtain multiple results, allowing them to be considered more objective than those obtained once, due to less randomness. This can be performed via methods like k-fold cross-validation or bootstrapping (Kohavi 1995). In the first technique, the k parameter refers to the different number of dataset subsets divided randomly. This number is used for the model training while the rest is used for validation. In each training and evaluation step, the prediction error is calculated and after repeating it k-times, the overall measure is provided. In bootstrapping (Efron and Tibshirani, 1997), the training dataset is randomly selected with replacement and remaining samples are used for validation n-times. Contrary to k-fold cross-validation, they can change from fold to fold. The prediction error is also averaged from each iteration. These both approaches differ in terms of results variance.</p> <p> <p></p> <p>10-fold cross-validation vs bootstrapping illustration (figure by S. Rashchka bootstrap_point632_score, modified/ CC BY 4.0). </p> <p>If you are interested in details on differences within such iterative methods, you can read the article of Lyons et al., 2018 and test their code in R where three methods were compared (each of them proved that a single split into training/validation data often gives inaccurate or misleading results).</p> <p>The example of comparison of bootstrapping and cross-validation on PlanetScope data classification performance was described by Phinzi et al., 2021.</p> <p> <p></p> <p>Overall accuracies obtained for two classifiers (SVM and RF) using cross-validation and bootstrapping in mapping complex gully systems in wet and dry seasons (figure by Phinzi et al., 2021/ CC BY 4.0). </p> <p>When using such methods we have the information about the distribution of the obtained classification accuracies so that we can calculate mean, median, standard deviation, minimum, maximum value etc. What\u2019s more, you can compare which of the distinguished classes are more or less stable in terms of the accuracy obtained, which proves that they are easier or more difficult to identify. This shows a lot about the variability of classified objects, in multitemporal classification - over time (see the example below where F1 accuracies for grassland Natura 2000 habitats were calculated 100 times).</p> <p> <p></p> <p>Boxplots presenting F1 accuracies obtained 100 times with CNNs for Natura 2000 habitats classification on Sentinel-2 time series data (6210 code - semi-natural dry grasslands and scrubland facies on calcareous substrates, 6410 code - Molinia meadows on calcareous, peaty, or clay silt-laden soils, 6510 code - lowland hay meadows, figure by Marcinkowska-Ochtyra et al., 2023/ CC BY 4.0). </p> <p>As you can see above, commonly used measures in accuracy assessment are (Congalton, 1991, Van Rijsbergen, 1977):</p> <ul> <li>Overall accuracy (OA) calculated for whole classified image result,</li> <li>Producer (PA) and User (UA) accuracies and F1 measure being harmonic mean of these both, calculated for individual classes.</li> </ul> <p>They are calculated based on error matrix. You can find more on this in Theme 6 of Module 1.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"<p>You have now completed this theme content. Now you can evaluate yourself by answering the questions in quiz. Good luck !</p> Question 1. Multitemporal classification requires:  applying variable importance tool ensuring radiometric consistency subtraction of classified images the use of field collected data   ensuring radiometric consistency   Question 2. The most optimal spectro-temporal dataset for classification can not be indicated by:  classification of each term separately and the assessment of accuracy of each result classification of each term separately and calculating the accuracy measure for only the best one assessed visually classification of multitemporal dataset and then selection of the most important features via variable importance tool classification of multitemporal datasets with different dates combinations and then assessing their accuracies   classification of each term separately and calculating the accuracy measure for only the best one assessed visually   Question 3. Why the use of iterative methods of accuracy assesment are important in multitemporal classification result analysis?    We have the information about the distribution of the obtained classification accuracies and we can calculate statistics (mean, median, standard deviation, minimum, maximum value etc.), we can compare which of the classes are more or less stable in terms of the accuracy obtained, it presents variability of classified objects over time, single split is more prone to randomness.   Question 4. Which characteristic can be related the most to multitemporal classification algorithm?  detecting changes and classification of first date image data matching time series data into predefined temporal patterns including spatio-spectral dependencies into classification assessing quality and calculation of errors on pre-processed image data   matching time series data into predefined temporal patterns   Question 5. Which feature selection method will allow to assess final classification model performance itself?  Principal Component Analysis Pearson\u2019s correlation Backward elimination Random Forest   Random Forest   Question 6. How to assess which class is easy to recognize on multitemporal dataset via classification?  I can identify pixels classified such as this class by error matrix analysis Overall accuracy for classified image is very high F1 measure calculated iteratively gave me similarly high minimum and maximum values obtained Variable importance tool provided me the list of the most influential bands for classification of this class   F1 measure calculated iteratively gave me similarly high minimum and maximum values obtained   Question 7. What does the reduction of the spectral dimension allow in the classification of multitemporal data? Name at least three effects.    To select only the most informative bands, to increase the accuracy of classification, to select the most important terms for identifying a given class, to reduce the volume of data, to optimize/shorten the classification process\u2026"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#exercise","title":"Exercise","text":"<p>Proceed with the exercise by going to the next page below or clicking this link</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#references","title":"References","text":""},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#key-references-recommended-reading-looking-up-background-details","title":"Key references (recommended reading, looking up background details)","text":"<p>G\u00f3mez, C., White, J. C., &amp; Wulder, M. A. (2016). Optical remotely sensed time series data for land cover classification: A review. ISPRS Journal of Photogrammetry and Remote Sensing, 116, 55-72. https://doi.org/10.1016/j.isprsjprs.2016.03.008</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#additional-references-cited-in-this-theme","title":"Additional references cited in this theme","text":"<p>Ashourloo, D., Nematollahi, H., Huete, A., Aghighi, H., Azadbakht, M., Shahrabi, H. S., &amp; Goodarzdashti, S. (2022). A new phenology-based method for mapping wheat and barley using time-series of Sentinel-2 images. Remote Sensing of Environment, 280, 113206. https://doi.org/10.1016/j.rse.2022.113206</p> <p>Bai, S., Kolter, J. Z., &amp; Koltun, V. (2018). An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271. https://doi.org/10.48550/arXiv.1803.01271</p> <p>Breiman, L. (2001). Random forests. Machine learning, 45, 5-32. https://doi.org/10.1023/A:1010933404324</p> <p>Congalton, R. G. (1991). A review of assessing the accuracy of classifications of remotely sensed data. Remote Sensing of Environment, 37(1), 35-46. https://doi.org/10.1016/0034-4257(91)90048-B</p> <p>Efron, B., &amp; Tibshirani, R. (1997). Improvements on cross-validation: the 632+ bootstrap method. Journal of the American Statistical Association, 92(438), 548-560. https://doi.org/10.1080/01621459.1997.10474007</p> <p>Gadiraju, K. K., Ramachandra, B., Chen, Z., &amp; Vatsavai, R. R. (2020, August). Multimodal deep learning based crop classification using multispectral and multitemporal satellite imagery. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (pp.\u00a03234-3242). https://doi.org/10.1145/3394486.3403375</p> <p>Garc\u00eda-\u00c1lvarez, D., Camacho Olmedo, M. T., Paegelow, M., &amp; Mas, J. F. (2022). Land Use Cover Datasets and Validation Tools: Validation Practices with QGIS, Springer International Publishing. https://doi.org/10.1007/978-3-030-90998-7</p> <p>Georganos, S., Grippa, T., Vanhuysse, S., Lennert, M., Shimoni, M., Kalogirou, S., &amp; Wolff, E. (2018). Less is more: Optimizing classification performance through feature selection in a very-high-resolution remote sensing object-based urban application. GIScience &amp; remote sensing, 55(2), 221-242. https://doi.org/10.1080/15481603.2017.1408892</p> <p>Ho\u015bci\u0142o, A., &amp; Lewandowska, A. (2019). Mapping forest type and tree species on a regional scale using multi-temporal Sentinel-2 data. Remote Sensing, 11(8), 929. https://doi.org/10.3390/rs11080929</p> <p>Houskeeper, H. F., Rosenthal, I. S., Cavanaugh, K. C., Pawlak, C., Trouille, L., Byrnes, J. E., \u2026 &amp; Cavanaugh, K. C. (2022). Automated satellite remote sensing of giant kelp at the Falkland Islands (Islas Malvinas). Plos One, 17(1), e0257933. https://doi.org/10.1371/journal.pone.0257933</p> <p>Immitzer, M., Neuwirth, M., B\u00f6ck, S., Brenner, H., Vuolo, F., &amp; Atzberger, C. (2019). Optimal input features for tree species classification in Central Europe based on multi-temporal Sentinel-2 data. Remote Sensing, 11(22), 2599. https://doi.org/10.3390/rs11222599</p> <p>Interdonato, R., Ienco, D., Gaetano, R., &amp; Ose, K. (2019). DuPLO: A DUal view Point deep Learning architecture for time series classificatiOn. ISPRS Journal of Photogrammetry and Remote Sensing, 149, 91-104. https://doi.org/10.1016/j.isprsjprs.2019.01.011</p> <p>Kluczek, M., Zagajewski, B., &amp; Kycko, M. (2022). Airborne HySpex hyperspectral versus multitemporal Sentinel-2 images for mountain plant communities mapping. Remote Sensing, 14(5), 1209. https://doi.org/10.3390/rs14051209</p> <p>Kohavi, R. (1995, August). A study of cross-validation and bootstrap for accuracy estimation and model selection. In Ijcai (Vol. 14, No.\u00a02, pp.\u00a01137-1145). source</p> <p>Kollert, A., Bremer, M., L\u00f6w, M., &amp; Rutzinger, M. (2021). Exploring the potential of land surface phenology and seasonal cloud free composites of one year of Sentinel-2 imagery for tree species mapping in a mountainous region. International Journal of Applied Earth Observation and Geoinformation, 94, 102208. https://doi.org/10.1016/j.jag.2020.102208</p> <p>Li, J., Tang, J., &amp; Liu, H. (2017, August). Reconstruction-based Unsupervised Feature Selection: An Embedded Approach. In IJCAI (pp.\u00a02159-2165). https://doi.org/10.24963/ijcai.2017/300</p> <p>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324. https://doi.org/10.1109/5.726791</p> <p>Lyons, M. B., Keith, D. A., Phinn, S. R., Mason, T. J., &amp; Elith, J. (2018). A comparison of resampling methods for remote sensing classification and accuracy assessment. Remote Sensing of Environment, 208, 145-153. https://doi.org/10.1016/j.rse.2018.02.026</p> <p>Maus, V., C\u00e2mara, G., Appel, M., &amp; Pebesma, E. (2019). dtwsat: Time-weighted dynamic time warping for satellite image time series analysis in R. Journal of Statistical Software, 88, 1-31. https://doi.org/10.18637/jss.v088.i05</p> <p>Marcinkowska-Ochtyra, A., Ochtyra, A., Raczko, E., &amp; Kope\u0107, D. (2023). Natura 2000 Grassland Habitats Mapping Based on Spectro-Temporal Dimension of Sentinel-2 Images with Machine Learning. Remote Sensing, 15(5), 1388. https://doi.org/10.3390/rs15051388</p> <p>Marzialetti, F., Giulio, S., Malavasi, M., Sperandii, M. G., Acosta, A. T. R., &amp; Carranza, M. L. (2019). Capturing coastal dune natural vegetation types using a phenology-based mapping approach: The potential of Sentinel-2. Remote Sensing, 11(12), 1506. https://doi.org/10.3390/rs11121506</p> <p>Munizaga, J., Garc\u00eda, M., Ureta, F., Novoa, V., Rojas, O., &amp; Rojas, C. (2022). Mapping Coastal Wetlands Using Satellite Imagery and Machine Learning in a Highly Urbanized Landscape. Sustainability, 14(9), 5700. https://doi.org/10.3390/su14095700</p> <p>Phinzi, K., Abriha, D., &amp; Szab\u00f3, S. (2021). Classification efficacy using k-fold cross-validation and bootstrapping resampling techniques on the example of mapping complex gully systems. Remote Sensing, 13(15), 2980. https://doi.org/10.3390/rs13152980</p> <p>Pelletier, C., Webb, G. I., &amp; Petitjean, F. (2019). Temporal convolutional neural network for the classification of satellite image time series. Remote Sensing, 11(5), 523. https://doi.org/10.3390/rs11050523</p> <p>Rapinel, S., Mony, C., Lecoq, L., Clement, B., Thomas, A., &amp; Hubert-Moy, L. (2019). Evaluation of Sentinel-2 time-series for mapping floodplain grassland plant communities. Remote Sensing of Environment, 223, 115-129. https://doi.org/10.1016/j.rse.2019.01.018</p> <p>Simoes, R., Camara, G., Queiroz, G., Souza, F., Andrade, P. R., Santos, L., \u2026 &amp; Ferreira, K. (2021). Satellite image time series analysis for big earth observation data. Remote Sensing, 13(13), 2428. https://doi.org/10.3390/rs13132428</p> <p>Tarantino, C., Forte, L., Blonda, P., Vicario, S., Tomaselli, V., Beierkuhnlein, C., &amp; Adamo, M. (2021). Intra-annual sentinel-2 time-series supporting grassland habitat discrimination. Remote Sensing, 13(2), 277. https://doi.org/10.3390/rs13020277</p> <p>Van Rijsbergen, C. J. (1977). A theoretical basis for the use of co\u2010occurrence data in information retrieval. Journal of documentation. https://doi.org/10.1108/eb026637</p> <p>Vapnik, V. N. (1999). An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10(5), 988-999. https://doi.org/10.1109/72.788640</p> <p>Venter, Z. S., Barton, D. N., Chakraborty, T., Simensen, T., &amp; Singh, G. (2022). Global 10 m Land Use Land Cover Datasets: A Comparison of Dynamic World, World Cover and Esri Land Cover. Remote Sensing, 14(16), 4101. https://doi.org/10.3390/rs14164101</p> <p>Wakuli\u0144ska, M., &amp; Marcinkowska-Ochtyra, A. (2020). Multi-temporal sentinel-2 data in classification of mountain vegetation. Remote Sensing, 12(17), 2696. https://doi.org/10.3390/rs12172696</p> <p>Watkins, B., &amp; Van Niekerk, A. (2019). A comparison of object-based image analysis approaches for field boundary delineation using multi-temporal Sentinel-2 imagery. Computers and Electronics in Agriculture, 158, 294-302. https://doi.org/10.1016/j.compag.2019.02.009</p> <p>Zhu, Z., &amp; Woodcock, C. E. (2014). Continuous change detection and classification of land cover using all available Landsat data. Remote sensing of Environment, 144, 152-171. https://doi.org/10.1016/j.rse.2014.01.011</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification.html#next-unit","title":"Next unit","text":"<p>Proceed with Vegetation monitoring and disturbance detection</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html","title":"Multitemporal classification - Exercise","text":""},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#exercise-multitemporal-classification-of-land-cover-in-karkonosze-mountains-region","title":"Exercise - Multitemporal classification of land cover in Karkonosze Mountains region","text":"<p>In this exercise, you will gain practical experience with the topic of multitemporal classification, which is presented in Theme 4 theoretical part. You will employ a multitemporal Sentinel-2 dataset to carry out a Random Forest classification. Upon completing the necessary steps, you\u2019ll have the ability to compare results obtained from different sets of input data, as well as evaluate the accuracy attained for various classes.</p> <p>The primary aim of this exercise is to demonstrate one of the many approaches you can employ in performing multitemporal satellite image classification. Based on the techniques learned here, you\u2019ll be equipped to apply these methods to a range of input data types, algorithms, and accuracy assessment procedures.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#basic-preparation","title":"Basic preparation","text":""},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#prerequisites","title":"Prerequisites","text":"<p>For this exercise you will need the following software, data and tools:</p> <ul> <li>Software - R and RStudio. You can access environment setup tutorial for the whole Module 2 here: R environment setup tutorial. After following the setup guide you should have all the necessary packages installed.</li> <li>Data - downloaded data provided through Zenodo. If you went through Module 2 Theme 3 exercise Pipeline 1 you can download image the data from your Google Drive.</li> </ul> <p>Follow the suggested working environment setup in order for the provided relative paths to work properly.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#data","title":"Data","text":""},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#imagery-data","title":"Imagery data","text":"<p>The imagery provided for this exercise consists of Sentinel-2 satellite imagery. The process of data preparation is described in the Module 2 Theme 3 exercise Pipeline 1.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#reference-data","title":"Reference data","text":"<p>The process of reference data preparation is describe in this document. Click here to open in in a new tab.</p> <p>Reference data preparation tutorial</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#loading-libraries-and-reading-data","title":"Loading libraries and reading data","text":"<p>Initiate a new R script in RStudio within your working directory (new file icon is in top left corner) and name it, for instance, <code>theme_4_exercise_script.R</code>.</p> <p>In this newly created script, we aim to load the necessary libraries and data, as well as set up some initial variables, which we will utilize later.</p> <p>Begin by loading libraries that contain the functions essential for completing this exercise (<code>terra</code>, <code>dplyr</code>, <code>caret</code>, and <code>randomForest</code>) into your environment.</p> <pre><code># raster and vector I/O and processing\nlibrary(terra)\n\n# tabular data manipulation\nlibrary(dplyr) \n\n# training/test layers preparation\nlibrary(caret) \n\n# RF model preparation\nlibrary(randomForest) \n</code></pre> <p>At this point, we can import the necessary data into the RStudio environment. We require a reference vector layer and multiband image data. These can be found in the dedicated folder for Theme 4.</p> <pre><code># object representing reference vector data\nreference_data &lt;- vect(\"theme_4_exercise/data_exercise/T4_reference_data.shp\")\n\n# object representing multiband raster with all the available bands\nimage_data &lt;- rast(\"theme_4_exercise/data_exercise/T4_image_data.tif\")\n</code></pre> <p>The bands in the multiband raster are arranged by date: the first 11 bands (10 spectral bands + NDVI) correspond to the first acquisition period (2022-06-19), with subsequent periods following in order.</p> <p>The reference data comprises 450 polygons, with 50 each for nine classes. You can get an overview of both the image and reference data by executing the following commands in the R console.</p> <pre><code>image_data\nreference_data\n</code></pre> <pre><code>class       : SpatRaster \ndimensions  : 1687, 2459, 66  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 534440, 559030, 5619440, 5636310  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 33N (EPSG:32633) \nsource      : T4_image_data.tif \nnames       : 2022-~19_B2, 2022-~19_B3, 2022-~19_B4, 2022-~19_B5, 2022-~19_B6, ...\n\nclass       : SpatVector \ngeometry    : polygons \ndimensions  : 450, 2  (geometries, attributes)\nextent      : 534690.7, 558829.3, 5619701, 5635629  (xmin, xmax, ymin, ymax)\nsource      : T4_reference_data.shp\ncoord. ref. : WGS 84 / UTM zone 33N (EPSG:32633) \nnames       :   ID             class\ntype        : &lt;int&gt;             &lt;chr&gt;\nvalues      :   1         coniferous forest\n                2         coniferous forest\n                3         coniferous forest\n</code></pre>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#pixel-values-extraction","title":"Pixel values extraction","text":"<p>With the data now loaded, our next step is to extract all the image values corresponding to each of the reference polygons. Each reference polygon encompasses nine 10x10 m pixels. Consequently, after extraction, we\u2019ll have a total of 4050 samples (450 x 9) to utilize in our training and validation data.</p> <p>The extract function from the <code>terra</code> package, also denoted as <code>terra::extract</code>, is employed to read pixel values based on the vector layer\u2019s location. This vector layer could either be a point layer (yielding one pixel value) or a polygon layer (yielding multiple pixel values).</p> <p>We\u2019ll feed three arguments into the <code>terra::extract</code> function:</p> <ul> <li><code>x</code> - a variable containing the image data</li> <li><code>y</code> - a variable containing the vector reference polygons</li> <li><code>exact</code> - if set to <code>TRUE</code>, an additional coverage fraction for each cell is added as a column</li> </ul> <p>The extraction results will be stored in the <code>pixel_reference</code> variable as a data frame. This function may take a few minutes to execute.</p> <pre><code>pixel_reference &lt;- extract(image_data, reference_data, exact = TRUE) \n</code></pre> <p>Inspect the produced extraction results.</p> <pre><code>nrow(pixel_reference)\ncolnames(pixel_reference)\n</code></pre> <pre><code>[1] 4066\n\n [1] \"ID\"              \"2022-06-19_B2\"   \"2022-06-19_B3\"   \"2022-06-19_B4\"  \n [5] \"2022-06-19_B5\"   \"2022-06-19_B6\"   \"2022-06-19_B7\"   \"2022-06-19_B8\"  \n [9] \"2022-06-19_B8A\"  \"2022-06-19_B11\"  \"2022-06-19_B12\"  \"2022-06-19_NDVI\"\n[13] \"2022-06-24_B2\"   \"2022-06-24_B3\"   \"2022-06-24_B4\"   \"2022-06-24_B5\"  \n[17] \"2022-06-24_B6\"   \"2022-06-24_B7\"   \"2022-06-24_B8\"   \"2022-06-24_B8A\" \n[21] \"2022-06-24_B11\"  \"2022-06-24_B12\"  \"2022-06-24_NDVI\" \"2022-06-27_B2\"  \n[25] \"2022-06-27_B3\"   \"2022-06-27_B4\"   \"2022-06-27_B5\"   \"2022-06-27_B6\"  \n[29] \"2022-06-27_B7\"   \"2022-06-27_B8\"   \"2022-06-27_B8A\"  \"2022-06-27_B11\" \n[33] \"2022-06-27_B12\"  \"2022-06-27_NDVI\" \"2022-07-19_B2\"   \"2022-07-19_B3\"  \n[37] \"2022-07-19_B4\"   \"2022-07-19_B5\"   \"2022-07-19_B6\"   \"2022-07-19_B7\"  \n[41] \"2022-07-19_B8\"   \"2022-07-19_B8A\"  \"2022-07-19_B11\"  \"2022-07-19_B12\" \n[45] \"2022-07-19_NDVI\" \"2022-07-24_B2\"   \"2022-07-24_B3\"   \"2022-07-24_B4\"  \n[49] \"2022-07-24_B5\"   \"2022-07-24_B6\"   \"2022-07-24_B7\"   \"2022-07-24_B8\"  \n[53] \"2022-07-24_B8A\"  \"2022-07-24_B11\"  \"2022-07-24_B12\"  \"2022-07-24_NDVI\"\n[57] \"2022-10-20_B2\"   \"2022-10-20_B3\"   \"2022-10-20_B4\"   \"2022-10-20_B5\"  \n[61] \"2022-10-20_B6\"   \"2022-10-20_B7\"   \"2022-10-20_B8\"   \"2022-10-20_B8A\" \n[65] \"2022-10-20_B11\"  \"2022-10-20_B12\"  \"2022-10-20_NDVI\" \"fraction\"\n</code></pre> <p>Note: The ID column is not drawn from the original data source but is generated by the <code>extract</code> function as the index of the record from which values are extracted. In the case of the data prepared for this exercise, the ID column of the reference data matches the ID in the data frame with extracted values. If you wish to replicate this part of the exercise with your own input reference data, we suggest adding a column/attribute with row index numbers. For example:</p> <pre><code>ref &lt;- vect(\"source.shp\")\nref_df &lt;- as.data.frame(ref)\nref_id &lt;- cbind(ref_df, ID = seq(1, nrow(ref_df)))\n</code></pre> <p>If everything proceeded as expected, there should be 4066 rows in the resulting data frame. Earlier, we noted that, given each reference polygon is 30x30 meters in size, we would anticipate a total of 4050 pixel values. A higher number suggests some polygons are misaligned, intersecting parts of additional polygons. To rectify this, we aim to filter out those pixels with minimal coverage by applying a data frame filter to rows with a coverage fraction of less than 0.5.</p> <p> <p></p> <p>Polygon id == 2 visible misalignment in relation to image pixel mesh. </p> <pre><code>pixel_reference &lt;- filter(pixel_reference, fraction &gt; 0.5)\n</code></pre> <p>Inspect the filtered result.</p> <pre><code>nrow(pixel_reference)\ncolnames(pixel_reference)\n</code></pre> <pre><code>[1] 4050\n\n [1] \"ID\"              \"2022-06-19_B2\"   \"2022-06-19_B3\"   \"2022-06-19_B4\"  \n [5] \"2022-06-19_B5\"   \"2022-06-19_B6\"   \"2022-06-19_B7\"   \"2022-06-19_B8\"  \n [9] \"2022-06-19_B8A\"  \"2022-06-19_B11\"  \"2022-06-19_B12\"  \"2022-06-19_NDVI\"\n[13] \"2022-06-24_B2\"   \"2022-06-24_B3\"   \"2022-06-24_B4\"   \"2022-06-24_B5\"  \n[17] \"2022-06-24_B6\"   \"2022-06-24_B7\"   \"2022-06-24_B8\"   \"2022-06-24_B8A\" \n[21] \"2022-06-24_B11\"  \"2022-06-24_B12\"  \"2022-06-24_NDVI\" \"2022-06-27_B2\"  \n[25] \"2022-06-27_B3\"   \"2022-06-27_B4\"   \"2022-06-27_B5\"   \"2022-06-27_B6\"  \n[29] \"2022-06-27_B7\"   \"2022-06-27_B8\"   \"2022-06-27_B8A\"  \"2022-06-27_B11\" \n[33] \"2022-06-27_B12\"  \"2022-06-27_NDVI\" \"2022-07-19_B2\"   \"2022-07-19_B3\"  \n[37] \"2022-07-19_B4\"   \"2022-07-19_B5\"   \"2022-07-19_B6\"   \"2022-07-19_B7\"  \n[41] \"2022-07-19_B8\"   \"2022-07-19_B8A\"  \"2022-07-19_B11\"  \"2022-07-19_B12\" \n[45] \"2022-07-19_NDVI\" \"2022-07-24_B2\"   \"2022-07-24_B3\"   \"2022-07-24_B4\"  \n[49] \"2022-07-24_B5\"   \"2022-07-24_B6\"   \"2022-07-24_B7\"   \"2022-07-24_B8\"  \n[53] \"2022-07-24_B8A\"  \"2022-07-24_B11\"  \"2022-07-24_B12\"  \"2022-07-24_NDVI\"\n[57] \"2022-10-20_B2\"   \"2022-10-20_B3\"   \"2022-10-20_B4\"   \"2022-10-20_B5\"  \n[61] \"2022-10-20_B6\"   \"2022-10-20_B7\"   \"2022-10-20_B8\"   \"2022-10-20_B8A\" \n[65] \"2022-10-20_B11\"  \"2022-10-20_B12\"  \"2022-10-20_NDVI\"\n</code></pre> <p>An additional step is to reintegrate the <code>class</code> information from the original <code>reference_data</code> into the extracted values. To accomplish this, we\u2019ll create another data frame based on the original attributes of the <code>reference_data</code> vector file. We can then merge information from the two data frames using the identifying values from the <code>ID</code> columns.</p> <pre><code>reference_class &lt;- as.data.frame(reference_data)\n\npixel_reference &lt;- merge(pixel_reference, reference_class, \n                         by = \"ID\", \n                         all = TRUE)\n</code></pre> <p>After this step, there should be an additional column in the data frame. Lastly, we aim to rearrange the columns so that they start with <code>ID</code> and <code>class</code>, followed by the names of the bands from which data was extracted. Since we\u2019ve already filtered the data, we can discard the <code>fraction</code> column. To achieve this, we\u2019ll use <code>dplyr</code> functions <code>select</code> and <code>relocate</code>: <code>select</code> to remove the <code>fraction</code> column and <code>relocate</code> to move the <code>ID</code> and <code>class</code> columns to the beginning of the data frame. The <code>%&gt;%</code> operator, also known as a pipe-line operator from the <code>magrittr</code> package, is used to send the value from the previous function to the next.</p> <pre><code>pixel_reference &lt;- select(pixel_reference, -fraction) %&gt;%\n  relocate(ID, class)\n\ncolnames(pixel_reference)\n</code></pre> <pre><code> [1] \"ID\"              \"class\"           \"2022-06-19_B2\"   \"2022-06-19_B3\"  \n [5] \"2022-06-19_B4\"   \"2022-06-19_B5\"   \"2022-06-19_B6\"   \"2022-06-19_B7\"  \n [9] \"2022-06-19_B8\"   \"2022-06-19_B8A\"  \"2022-06-19_B11\"  \"2022-06-19_B12\" \n[13] \"2022-06-19_NDVI\" \"2022-06-24_B2\"   \"2022-06-24_B3\"   \"2022-06-24_B4\"  \n[17] \"2022-06-24_B5\"   \"2022-06-24_B6\"   \"2022-06-24_B7\"   \"2022-06-24_B8\"  \n[21] \"2022-06-24_B8A\"  \"2022-06-24_B11\"  \"2022-06-24_B12\"  \"2022-06-24_NDVI\"\n[25] \"2022-06-27_B2\"   \"2022-06-27_B3\"   \"2022-06-27_B4\"   \"2022-06-27_B5\"  \n[29] \"2022-06-27_B6\"   \"2022-06-27_B7\"   \"2022-06-27_B8\"   \"2022-06-27_B8A\" \n[33] \"2022-06-27_B11\"  \"2022-06-27_B12\"  \"2022-06-27_NDVI\" \"2022-07-19_B2\"  \n[37] \"2022-07-19_B3\"   \"2022-07-19_B4\"   \"2022-07-19_B5\"   \"2022-07-19_B6\"  \n[41] \"2022-07-19_B7\"   \"2022-07-19_B8\"   \"2022-07-19_B8A\"  \"2022-07-19_B11\" \n[45] \"2022-07-19_B12\"  \"2022-07-19_NDVI\" \"2022-07-24_B2\"   \"2022-07-24_B3\"  \n[49] \"2022-07-24_B4\"   \"2022-07-24_B5\"   \"2022-07-24_B6\"   \"2022-07-24_B7\"  \n[53] \"2022-07-24_B8\"   \"2022-07-24_B8A\"  \"2022-07-24_B11\"  \"2022-07-24_B12\" \n[57] \"2022-07-24_NDVI\" \"2022-10-20_B2\"   \"2022-10-20_B3\"   \"2022-10-20_B4\"  \n[61] \"2022-10-20_B5\"   \"2022-10-20_B6\"   \"2022-10-20_B7\"   \"2022-10-20_B8\"  \n[65] \"2022-10-20_B8A\"  \"2022-10-20_B11\"  \"2022-10-20_B12\"  \"2022-10-20_NDVI\"\n</code></pre> <p>Save the extracted data frame to the external file in case you need to reload it, so you don\u2019t have to wait for the extraction process to complete. By saving to <code>.RDS</code> file you can then read that file into custom variable name.</p> <pre><code>saveRDS(pixel_reference, \n        file = \"theme_4_exercise/data_exercise/pixel_reference.RDS\")\n\n# in case you need to load it use the command below\n# pixel_reference &lt;- readRDS(\"theme_4_exercise/data_exercise/pixel_reference.RDS\")\n</code></pre>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#classification-scenario-1-the-entire-dataset","title":"Classification scenario 1: the entire dataset","text":""},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#trainingvalidation-data-preparation","title":"Training/validation data preparation","text":"<p>Now that we have the reference dataset prepared, we can commence the classification scenario. Initially, we\u2019ll divide the complete dataset into a training set and a validation set. By executing the commands below, we can check the number of samples for each class we have, as well as the number of reference polygons per class.</p> <pre><code>table(pixel_reference$class)\ntable(reference_data$class)\n</code></pre> <pre><code>&gt; table(pixel_reference$class)\n\nbroad-leaved forest       built-up area   coniferous forest \n                450                 450                 450 \n             fields             meadows  natural grasslands \n                450                 450                 450 \n              rocks               scrub               water \n                450                 450                 450\n\n\n&gt; table(reference_data$class)\n\nbroad-leaved forest       built-up area   coniferous forest \n                 50                  50                  50 \n             fields             meadows  natural grasslands \n                 50                  50                  50 \n              rocks               scrub               water \n                 50                  50                  50\n</code></pre> <p>To ensure reproducibility of partitioning we will set seed.</p> <pre><code>set.seed(14)\n</code></pre> <p>The partitioning process consists of two steps. The first step involves randomly selecting 50% of polygons from each class and storing their position number in the <code>train_index</code> variable. This approach ensures that pixels from a single polygon are used exclusively in either the training or validation set, preventing overlap. Given that each polygon contains the same number of pixels, we will have an equal number of training and validation samples. This balanced division of the reference dataset into training and validation sets aims to enhance model performance and reduce any bias towards specific classes.</p> <p>In the second step, we\u2019ll filter the entire <code>pixel_reference</code> set into <code>train_data</code> and <code>val_data</code>. This is done using the <code>train_index</code> variable to extract corresponding values by the <code>ID</code> column from the <code>pixel_reference</code> table.</p> <pre><code>train_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE)\n\ntrain_data &lt;- pixel_reference[ pixel_reference$ID %in% train_index, ]\nval_data &lt;- pixel_reference[ !(pixel_reference$ID %in% train_index), ]\n</code></pre> <p>Now we should have two sets of equal number of reference pixels for each class.</p> <pre><code>table(train_data$class)\ntable(val_data$class)\n</code></pre> <pre><code>&gt; table(train_data$class)\n\nbroad-leaved forest       built-up area   coniferous forest \n                225                 225                 225 \n             fields             meadows  natural grasslands \n                225                 225                 225 \n              rocks               scrub               water \n                225                 225                 225 \n&gt; table(val_data$class)\n\nbroad-leaved forest       built-up area   coniferous forest \n                225                 225                 225 \n             fields             meadows  natural grasslands \n                225                 225                 225 \n              rocks               scrub               water \n                225                 225                 225\n</code></pre>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#parameters-tuning","title":"Parameters tuning","text":"<p>In order to achieve a satisfying result we want to tune the model parameters. One of the Random Forest algorithm parameter is mtry, which is the number of random variables used in each tree. To find the value, which should yield the highest accuracy numbers we will use <code>tuneRF</code> function from the <code>randomForest</code> package.</p> <p>The function takes several arguments:</p> <ul> <li><code>x</code> (first argument) - matrix or data frame of predictor variables;</li> <li><code>y</code> (second argument) - response vector (factor for classification);</li> <li><code>ntreeTry</code> - number of trees used during the tuning step;</li> <li><code>improve</code> - the (relative) improvement in out-of-bag (OOB) error must be at least this much for the search to continue;</li> <li><code>stepFactor</code> - at each iteration, <code>mtry</code> value is inflated (or deflated) by this value.</li> </ul> <p>We will again set seed and assign the function\u2019s results to <code>tune</code> variable.</p> <pre><code>set.seed(141)\n\ntune &lt;- tuneRF(train_data[, 3:length(train_data)], \n               as.factor(train_data$class),\n               ntreeTry = 500,\n               improve = 0.001,\n               stepFactor = 1.2)\n\ntune\n</code></pre> <pre><code>&gt; tune\n       mtry    OOBError\n7.OOB     7 0.004444444\n8.OOB     8 0.003456790\n9.OOB     9 0.002962963\n10.OOB   10 0.003456790\n</code></pre> <p> <p></p> <p>mtry parameter tuning. </p> <p>The lowest Out-Of-Bag (OOB) error values were achieved with an <code>mtry</code> value of 9. Theoretically, a higher <code>mtry</code> value results in a more robust model, but it can also increase the correlation among the variables chosen for the split. In this case, we will not test different values and will set the <code>mtry</code> parameter in the model to 9 due to its lowest error.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#model","title":"Model","text":"<p>The next step after tuning the parameters is training the classification model. The <code>randomForest</code> function from the package of the same name is an implementation of the original Random Forest algorithm. The following code snippet uses this function with the following arguments:</p> <ul> <li><code>x</code> (first argument) - a data frame or a matrix of predictors (from <code>train_data</code> variable);</li> <li><code>y</code> (second argument) - a response vector (a factor for classification);</li> <li><code>ntree</code> - the number of trees used to produce the final model; 500 is a number that has been empirically tested to usually yield satisfactory results;</li> <li><code>mtry</code> - number of variables randomly sampled as candidates at each split (tuned in the previous step);</li> <li><code>importance</code> - a boolean (TRUE/FALSE) indicating whether the model should store variable importance (required for producing the plots);</li> <li><code>do.trace</code> - useful for keeping track of the modelling progress.</li> </ul> <pre><code>model_rf &lt;- randomForest(train_data[ , 3:length(train_data)],\n                         as.factor(train_data$class), \n                         ntree = 500,\n                         mtry = 9, \n                         importance = TRUE,\n                         do.trace = 50)\n</code></pre> <p>To be able to access the model later it is recommended to save it locally. Create new <code>results</code> folder to store files produced in this script.</p> <pre><code>saveRDS(model_rf, file = \"theme_4_exercise/results/model_rf.RDS\")\n</code></pre>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#accuracy-assessment","title":"Accuracy assessment","text":"<p>If we access the variable <code>model_rf</code> (by running this variable in console) we will see the basic model information and confusion matrix calculated for the training data.</p> <pre><code>&gt; model_rf\n\nCall:\n randomForest(x = train_data[, 3:length(train_data)], \n y = as.factor(train_data$class), \n ntree = 500, \n mtry = 9, \n importance = TRUE, \n do.trace = 50)\n\n\n               Type of random forest: classification\n               Number of trees: 500\n               No. of variables tried at each split: 9\n\n        OOB estimate of  error rate: 0.4%\n</code></pre> <p>Confusion matrix</p> <pre><code>Confusion matrix:\n                    broad-leaved forest built-up area coniferous forest\nbroad-leaved forest                 225             0                 0\nbuilt-up area                         0           223                 0\nconiferous forest                     0             0               225\nfields                                0             0                 0\nmeadows                               0             0                 0\nnatural grasslands                    0             0                 0\nrocks                                 0             5                 0\nscrub                                 0             0                 0\nwater                                 0             0                 0\n\n                    fields meadows natural grasslands rocks scrub water\nbroad-leaved forest      0       0                  0     0     0     0\nbuilt-up area            0       0                  0     2     0     0\nconiferous forest        0       0                  0     0     0     0\nfields                 224       1                  0     0     0     0\nmeadows                  0     225                  0     0     0     0\nnatural grasslands       0       0                225     0     0     0\nrocks                    0       0                  0   220     0     0\nscrub                    0       0                  0     0   225     0\nwater                    0       0                  0     0     0   225\n\n                    class.error\nbroad-leaved forest 0.000000000\nbuilt-up area       0.008888889\nconiferous forest   0.000000000\nfields              0.004444444\nmeadows             0.000000000\nnatural grasslands  0.000000000\nrocks               0.022222222\nscrub               0.000000000\nwater               0.000000000\n</code></pre> <p>Our primary objective is to evaluate the performance of the model using the reference data. To obtain an accurate measure of the model\u2019s effectiveness, it\u2019s crucial to test it against data it hasn\u2019t seen during the training process. This subset of the data is known as the validation set. To achieve this, we\u2019ll make use of the <code>val_data</code> we prepared earlier. After prediction, we\u2019ll contrast the model\u2019s predicted classes against the true classes in the validation dataset.</p> <pre><code>predicted_rf &lt;- predict(model_rf, val_data[ , 3:length(val_data)])\n\nconfusion_matrix_predicted_rf &lt;- confusionMatrix(predicted_rf, \n                                                 as.factor(val_data$class), \n                                                 mode = \"everything\")\nconfusion_matrix_predicted_rf\n</code></pre> <pre><code>Confusion Matrix and Statistics\n\n                     Reference\nPrediction            broad-leaved forest built-up area coniferous forest\n  broad-leaved forest                 217             0                 9\n  built-up area                         0           200                 0\n  coniferous forest                     3             0               209\n  fields                                0             5                 0\n  meadows                               0             3                 0\n  natural grasslands                    0             0                 0\n  rocks                                 0            10                 0\n  scrub                                 5             1                 7\n  water                                 0             6                 0\n\n                     Reference\nPrediction            fields meadows natural grasslands rocks scrub water\n  broad-leaved forest      0       0                  0     0     0     0\n  built-up area            2       0                  0    25     4     0\n  coniferous forest        0       0                  0     0     9     0\n  fields                 194      50                  0     0     0     0\n  meadows                 29     166                 19     0     2     0\n  natural grasslands       0       9                206     0     0     0\n  rocks                    0       0                  0   200     5     0\n  scrub                    0       0                  0     0   205     0\n  water                    0       0                  0     0     0   225\n\nOverall Statistics\n\n               Accuracy : 0.8998          \n                 95% CI : (0.8858, 0.9125)\n    No Information Rate : 0.1111          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16\n\n                  Kappa : 0.8872\n\n Mcnemar's Test P-Value : NA\n\nStatistics by Class:\n\n                     Class: broad-leaved forest Class: built-up area\nSensitivity                              0.9644              0.88889\nSpecificity                              0.9950              0.98278\nPos Pred Value                           0.9602              0.86580\nNeg Pred Value                           0.9956              0.98606\nPrecision                                0.9602              0.86580\nRecall                                   0.9644              0.88889\nF1                                       0.9623              0.87719\nPrevalence                               0.1111              0.11111\nDetection Rate                           0.1072              0.09877\nDetection Prevalence                     0.1116              0.11407\nBalanced Accuracy                        0.9797              0.93583\n\n                     Class: coniferous forest Class: fields Class: meadows\nSensitivity                            0.9289        0.8622        0.73778\nSpecificity                            0.9933        0.9694        0.97056\nPos Pred Value                         0.9457        0.7791        0.75799\nNeg Pred Value                         0.9911        0.9825        0.96733\nPrecision                              0.9457        0.7791        0.75799\nRecall                                 0.9289        0.8622        0.73778\nF1                                     0.9372        0.8186        0.74775\nPrevalence                             0.1111        0.1111        0.11111\nDetection Rate                         0.1032        0.0958        0.08198\nDetection Prevalence                   0.1091        0.1230        0.10815\nBalanced Accuracy                      0.9611        0.9158        0.85417\n\n                     Class: natural grasslands Class: rocks Class: scrub\nSensitivity                             0.9156      0.88889       0.9111\nSpecificity                             0.9950      0.99167       0.9928\nPos Pred Value                          0.9581      0.93023       0.9404\nNeg Pred Value                          0.9895      0.98619       0.9889\nPrecision                               0.9581      0.93023       0.9404\nRecall                                  0.9156      0.88889       0.9111\nF1                                      0.9364      0.90909       0.9255\nPrevalence                              0.1111      0.11111       0.1111\nDetection Rate                          0.1017      0.09877       0.1012\nDetection Prevalence                    0.1062      0.10617       0.1077\nBalanced Accuracy                       0.9553      0.94028       0.9519\n\n                     Class: water\nSensitivity                1.0000\nSpecificity                0.9967\nPos Pred Value             0.9740\nNeg Pred Value             1.0000\nPrecision                  0.9740\nRecall                     1.0000\nF1                         0.9868\nPrevalence                 0.1111\nDetection Rate             0.1111\nDetection Prevalence       0.1141\nBalanced Accuracy          0.9983\n</code></pre> <p>TASK</p> <p>Identify classes with highest and lowest accuracy metrics. Start with <code>F1</code>.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#variable-importance","title":"Variable importance","text":"<p>One of the major advantages of models generated by the <code>randomForest</code> package is the wealth of information they provide, which can be further utilized to assess the model\u2019s performance. The <code>varImpPlot</code> function from the <code>randomForest</code> package leverages the stored OOB errors to identify the best-performing predictors (bands). The <code>importance</code> values used to generate the plots are stored inside the model, which should be accessible via the <code>model_rf</code> variable. Setting <code>type = 1</code> results in the production of a plot showing the mean decrease in accuracy measure. To learn more about this type of plot and <code>type = 2</code> plots, type and run <code>?randomForest::importance()</code> into the R console.</p> <pre><code>varImpPlot(model_rf, type = 1, n.var = 30)\n</code></pre> <p> <p></p> <p>Variable importance plot. </p> <p>TASK</p> <p>Identify terms and bands with the highest Mean Decrease Accuracy values. Modify <code>n.var</code> parameter to see more or less variables.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#image-classification-using-trained-rf-model","title":"Image classification using trained RF model","text":"<p>With our trained model, we can now classify the image data we have to produce a classification image. To do this, we will use the <code>predict</code> function from the <code>terra</code> package and provide the following arguments:</p> <ul> <li><code>object</code> - the image data</li> <li><code>model</code> - the fitted statistical model</li> <li><code>filename</code> - path to the result and file name with extension</li> <li><code>datatype</code> - for the classification result <code>INT1U</code> (0-255) will suffice</li> <li><code>na.rm</code> - ignore cells with NA values</li> <li><code>overwrite</code> - save over existing file of the same name</li> </ul> <p>The classification process should take approximately 2-4 minutes.</p> <pre><code>terra::predict(image_data, model_rf, \n               filename = \"theme_4_exercise/results/predicted_image_all_bands.tif\", \n               datatype = \"INT1U\", \n               na.rm = TRUE, \n               overwrite = TRUE)\n</code></pre> <p>TASK</p> <p>Display the resulting image in QGIS. Use the attached symbology.clr file to assign class names and colors.</p> <p> <p></p> <p>Predicted image (all variables). </p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#classification-scenarios-2-and-3","title":"Classification scenarios 2 and 3","text":"<p>Now that you have experienced the classification process, the next step is to apply this knowledge to scenarios 2 and 3.</p> <ul> <li>Scenario 2: Only spectral bands</li> <li>Scenario 3: Only NDVI bands</li> </ul> <p>The workflow remains the same, but you need to modify the reference data to include only values from specific bands.</p>  Reference for classification scenario 2  <pre><code>reference_scenario2 &lt;- select(pixel_reference, -contains(\"NDVI\"))\n</code></pre>  Reference for classification scenario 3  <pre><code>reference_scenario3 &lt;- select(pixel_reference, ID, class, ends_with(\"NDVI\"))\n</code></pre> <p>TASK</p> <ul> <li>Divide the reference datasets into training and validation subsets.</li> <li>Tune <code>mtry</code> parameter.</li> <li>Create and save Random Forest classification model.</li> <li>Apply the model to test (validation) data and image.</li> <li>Compare the accuracy results and visual images.</li> </ul>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#optional-additional-tasks","title":"Optional additional tasks","text":"<p>After completing all the scenarios, consider comparing the results. You could create a table with the F1 accuracies for the class of interest in each scenario. Additionally, consider performing the following tests:</p> <ul> <li>Extract a single term and perform classification.</li> <li>Extract only the best-performing bands based on variable importance from a selected scenario and repeat the classification</li> <li>Test different <code>mtry</code> values and compare the results.</li> </ul> <p>If you want to further explore the vast topic of classification consider reading the <code>randomForest</code> and <code>caret</code> packages documentation and experiment with different model tuning and classification parameters.</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#data-and-software-credits","title":"Data and software credits","text":""},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#data_1","title":"Data","text":"<p>Sentinel-2 imagery European Space Agency - ESA/ Terms of use processed in and downloaded from Google Earth Engine by Gorelick et al., 2017</p>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#software","title":"Software","text":"<ul> <li>R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.</li> <li>A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18\u201322.https://CRAN.R-project.org/package=randomForest</li> <li>Hijmans R (2023). terra: Spatial Data Analysis. R package version 1.7-39, https://CRAN.R-project.org/package=terra</li> <li>Kuhn, M. (2008). Building Predictive Models in R Using the caret Package. Journal of Statistical Software, 28(5), 1\u201326. https://doi.org/10.18637/jss.v028.i05 https://CRAN.R-project.org/package=caret</li> <li>Wickham H, Fran\u00e7ois R, Henry L, M\u00fcller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr</li> </ul>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#source-code","title":"Source code","text":"You can find the entire code used in this exercise here  <pre><code># raster and vector I/O and processing\nlibrary(terra)\n\n# tabular data manipulation\nlibrary(dplyr) \n\n# training/test layers preparation\nlibrary(caret) \n\n# RF model preparation\nlibrary(randomForest) \n\n# object representing reference vector data\nreference_data &lt;- vect(\"theme_4_exercise/data_exercise/T4_reference_data.shp\")\n\n# object representing multiband raster with all the available bands\nimage_data &lt;- rast(\"theme_4_exercise/data_exercise/T4_image_data.tif\")\n\n\nimage_data\nreference_data\n\npixel_reference &lt;- extract(image_data, reference_data, exact = TRUE) \n\n\nnrow(pixel_reference)\ncolnames(pixel_reference)\n\npixel_reference &lt;- filter(pixel_reference, fraction &gt; 0.5)\n\nnrow(pixel_reference)\ncolnames(pixel_reference)\n\nreference_class &lt;- as.data.frame(reference_data)\n\npixel_reference &lt;- merge(pixel_reference, reference_class, \n                         by = \"ID\", \n                         all = TRUE)\n\npixel_reference &lt;- select(pixel_reference, -fraction) %&gt;%\n  relocate(ID, class)\n\nsaveRDS(pixel_reference,\n        file = \"theme_4_exercise/data_exercise/pixel_reference.RDS\") \n\ncolnames(pixel_reference)\n\n\nset.seed(14)\n\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE)\n\ntrain_data &lt;- pixel_reference[ pixel_reference$ID %in% train_index, ]\nval_data &lt;- pixel_reference[ !(pixel_reference$ID %in% train_index), ]\n\n\ntable(train_data$class)\ntable(val_data$class)\n\nset.seed(141)\n\ntune &lt;- tuneRF(train_data[, 3:length(train_data)], \n               as.factor(train_data$class),\n               ntreeTry = 500,\n               improve = 0.001,\n               stepFactor = 1.2)\n\ntune\n\n\nmodel_rf &lt;- randomForest(train_data[ , 3:length(train_data)], as.factor(train_data$class), \n                         ntree = 500,\n                         mtry = 9, \n                         importance = TRUE,\n                         do.trace = 50)\n\nsaveRDS(model_rf,\n        file = \"theme_4_exercise/results/model_rf.RDS\") \n\nmodel_rf\n\n\n\npredicted_rf &lt;- predict(model_rf, val_data[ , 3:length(val_data)])\n\nconfusion_matrix_predicted_rf &lt;- confusionMatrix(predicted_rf, as.factor(val_data$class), mode = \"everything\")\nconfusion_matrix_predicted_rf\n\nvarImpPlot(model_rf, type = 1, n.var = 30)\n\n\nterra::predict(image_data, model_rf, \n               filename = \"theme_4_exercise/results/predicted_image_all_bands.tif\", \n               datatype = \"INT1U\", \n               na.rm = TRUE, \n               overwrite = TRUE)\n</code></pre>"},{"location":"module2/04_multitemporal_classification/04_multitemporal_classification_exercise.html#this-is-the-end-of-this-exercise-proceed-with-other-themes-and-exercises-good-luck","title":"This is the end of this exercise. Proceed with other Themes and Exercises. Good luck!","text":""},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html","title":"Vegetation change and disturbance detection","text":"<p>In Theme 2, we introduced various types of multitemporal data analysis. In this Module we put most emphasis on classification and change detection. Content of this Theme we\u2019ll delve deeper into the latter. We will focus on different definitions of change types, explore various approaches and algorithms for detecting changes (such as disturbances or recovery) in vegetation, as well as on assessing the accuracy of the results.</p> <p>In this theme, you will learn about:</p> <ul> <li>approaches to change detection on optical satellite data</li> <li>different types of changes</li> <li>vegetation disturbance and recovery</li> <li>time series variables as predictors of vegetation disturbance</li> <li>monitoring of changes: spectral trajectory in different time series data</li> <li>algorithms</li> <li>results validation and accuracy assessment</li> <li>global vegetation change detection products</li> </ul> <p>This theme will conclude with:</p> <ul> <li>Self-evaluation quiz</li> <li>References</li> </ul> <p>To consolidate your knowledge and test it in more practical environment complete the Exercise.</p> <p>After finishing this theme you will:</p> <ul> <li>gain knowledge of multitemporal data analysis techniques, particularly in classification and change detection.</li> <li>understand various methods for change detection in optical satellite data.</li> <li>distinguish different types of changes (abrupt, gradual, seasonal) and their corresponding detection algorithms.</li> <li>grasp mathematical aspects of change detection,</li> <li>appreciate the role of time intervals and density in change detection studies.</li> <li>discover the use of time series variables in predicting vegetation disturbance and recovery patterns.</li> <li>learn how to use different satellite data sources for varied cases and fuse data from different sensors to enhance resolutions.</li> <li>understand the process of validating results and assessing accuracy in change detection studies.</li> <li>overview global vegetation change detection products and understand their strengths and limitations.</li> </ul>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#approaches-to-change-detection-on-optical-satellite-data","title":"Approaches to change detection on optical satellite data","text":"<p>The basic categorization of automatic change detection methods includes two approaches: comparative analysis of two images (\\(t_1\\) - \\(t_2\\)) and simultaneous analysis of multitemporal images (\\(t_1\\) \\(t_2\\) \u2026 \\(t_n\\), Singh, 1989).</p> <p>The simplest methods, initially used in the analysis of changes belonging to the first group, are:</p> <ul> <li>signal difference \u2013 subtracting or dividing the reflectance or index values from two images,</li> <li>regression analysis - pixels from time \\(t_1\\) are assumed to be a linear function of the time \\(t_2\\) pixels,</li> <li>using a second PCA component from transformation performed on the two images bands,</li> <li>comparison of classification results and simultaneous classification of two images,</li> <li>Change Vector Analysis, which involves finding the size and direction of the change vector for each pixel between two images,</li> <li>subtracting the background, where a reference image of the background is determined as having low values of difference, and a binary mask representing pixels of changes is created based on this,</li> <li>hybrid methods, combining different approaches into a single change detection procedure, or combining results obtained from different methods leading to a new result.</li> </ul> <p>From mathematical point of view, the change can be extracted from the second group via (Zhu, 2017):</p> <ul> <li>differencing - indicating large differences in comparing images from different terms,</li> <li>temporal segmentation - segmenting the time series into a series of straight line segments based on the residual-error and angle criterions,  </li> <li>trajectory classification - from hypothesised trajectories representing signatures specific to different kinds of changes or from multitemporal supervised classification,  </li> <li>statistical boundary - indicating significant departure from the statistical boundary followed by time series,  </li> <li>regression - estimating the long-term movements or trends in time series.</li> </ul> <p>In both approaches thresholding indicating significant deviation from the predefined threshold is also listed. It is a special method which can be used after performing e.g.\u00a0differencing or regression analysis. As you can see some other methods are also the same for both \\(t_1\\) - \\(t_2\\) and \\(t_1\\) \\(t_2\\) \u2026 \\(t_n\\) analysis.</p> <p> <p></p> <p>Temporal segmentation of TC Wetness calculated on Landsat data (A - on all clear observations, B - annual compositions, figure by Pasquarella et al., 2021, modified/ CC BY 4.0). </p> <p>Historically, change studies were based mainly on less frequent time intervals, e.g.\u00a0one image every 3 years (e.g.\u00a0Miller and Yool, 2002). In the case of vegetation, regeneration after disturbance may take several years, so significant changes may be omitted in the analysis of images obtained at too distant dates. Nowadays, the second group of analyzes based on denser time series is commonly used, providing more detailed information for environmental monitoring. Having such a data set spanning a broad time period, there is no need to sequence pairs of images and search for single events that occurred between two dates, which would lead to the omission of valuable information (Huang et al., 2010). It is more effective to search for characteristic spectral features in the entire set of spectral values of multitemporal data.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#types-of-changes","title":"Types of changes","text":"<p>The possibility to detect changes based on satellite data depends on the system capacity to account for variability at different temporal scales mentioned in Theme 2. Consequently,, three types of changes can be differentiated from time series data: abrupt, gradual and seasonal. Each type is detailed below.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#abrupt-and-gradual-changes","title":"Abrupt and gradual changes","text":"<p>Abrupt refer to short-term, large magnitude changes (Zhu, 2017) and can be instigated by agents such as fires, floods, hurricanes, deforestation etc. In contrast, gradual refer to long-term, small magnitude date-to-date changes. These ones can be caused by e.g.\u00a0climate changes, land management or pests outbreaks.</p> <p> <p></p> <p>Example pixels representing gradual (red squares) and abrupt (black dots) changes occurrence (abrupt change detected in 2003, figure by course authors). </p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#seasonal-changes","title":"Seasonal changes","text":"<p>There are also seasonal changes, driven by annual temperature and rainfall interactions that impact plant phenology or proportional cover of land cover types with different phenology of vegetation (Verbesselt et al., 2010). Analysis concerning these types of changes use differences between images as consideration for detecting seasonal metric/parameter connected to the plants phase of growth in a given time, e.g.\u00a0start, peak or end of the growing season.</p> <p> <p></p> <p>Seasonality parameters: (a) beginning of season, (b) end of season, (c) length of season, (d) base value, (e) time of middle of season, (f) maximum value, (g) amplitude, (h) small integrated value, (h+i) large integrated value (the red and blue lines are filtered and original data, respectively, figure by Eklundh, 2023/ CC BY-NC-ND 2.5 SE). </p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#vegetation-disturbance-and-recovery","title":"Vegetation disturbance and recovery","text":"<p>The word change can encompass both positive and negative shifts from an object\u2019s stable state. Disturbance in the context of vegetation refers to a negative phenomenon that disrupts the steady state.</p> <p>When discussing disturbances it is also important to indicate the scale of analysis. At global or regional scales disturbances are generally defined as processes that lead to the significant removal of canopy leaf area and live biomass (McDowell et al., 2015). According to this definition, it isn\u2019t necessary for entire individuals to die offf, which is a crucial point of consideration in a local scale analysis, such as a minor fire impacting individual trees. Scale is then directly related to spatial resolution of remote sensing data.</p> <p> <p></p> <p>Dependence of spatial resolution on the extent of disturbance (figure by course authors). </p> <p>The frequency of data collection also plays a critical role in quantifying terrestrial disturbances. This is particularly relevant when accounting for the speed of a disturbance event and potential recovery - a positive change in the context of such studies. For example, a wildfire signifies a fast, abrupt disturbance, whereas drought or insect infestations might represent slower disturbances.</p> <p> <p></p> <p>Dependence of temporal resolution on the frequency of observed disturbances/recovery (figure by course authors). </p> <p>The third vital resolution in disturbance detection is the spectral resolution to which we refer in the next section of this Theme.</p> <p>These three metrics are typically used for ecosystems stability assessment in relation to disturbance and recovery:</p> <ul> <li>resistance - the ecosystem\u2019s ability to resist disturbances,</li> <li>resilience - the ecosystem\u2019s ability to recover to the original state after a disturbance,</li> <li>variability - the total variation of a system in response to environmental analysis.</li> </ul> <p>These metrics can be measured by the use of spectral index anomaly time series. You can see the example in the article of Huang et al., 2021, where authors use Landsat data.</p> <p> <p></p> <p>Detection of the disturbance-recovery process and measures of ecosystem stability based on spectral index anomaly time series (DM - disturbance magnitude, RT - recovery time, RR - recovery rate, RM - recovery magnitude, SD - overall temporal variability, TD and TR - time point of disturbance and recovery, respectively, figure by Huang et al., 2021/ CC BY-NC-ND 4.0.). </p> <p>Note: we will further use the more general term change to represent general methodological issues related to detecting disturbance or changes. However, with an understanding of the distinctions between these terms, you will be able to apply them correctly in specific cases.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#time-series-variables-as-predictors-of-vegetation-disturbance","title":"Time series variables as predictors of vegetation disturbance","text":"<p>Each spectral band of satellite data carries unique information. In the case of vegetation, specific bands can help determine whether the plant is healthy or damaged. Combination of these bands, such as vegetation indices, are commonly employed to monitor vegetation disturbances and recovery. Here we can find NDVI (examples of use in: Vogelmann et al., 2012 or Brooks et al., 2014), burned areas detection with NBR (Huang et al., 2010, Kennedy et al., 2010), and vegetation water content analyses with NDMI (DeVries et al., 2015, Ochtyra et al., 2020) or TC Wetness (Griffiths et al., 2014, Oeser et al., 2017). The selection of the most appropriate variable is essential for the accurate detection of the disturbance signal.</p> <p>There are various options to select the most relevant one. The majority of studies used an a priori selected index/band based on information found in the literature. Cohen et al., 2018 presented comprehensive comparison of different variables and they developed Disturbance Signal to Noise Ratio to assess their performance. Such selection can be also preceded by statistical evaluation of differences between used variables (example of such evaluation in Ochtyra et al., 2020).</p> <p>Landsat-based Tasseled Cap transformation bands were used to develop Disturbance Index (DI, Healey et al., 2005). This specific derivative was designed to highlight the non-vegetated areas spectral signatures associated with stand-replacing disturbance and separate them from other forest signatures.</p> <p>DI is calculated by rescaled values of Brightness, Greenness, and Wetness values in a following equation:</p> <p>DI = B\\(r\\)-(G\\(r\\)+W\\(r\\)),</p> <p>where B\\(r\\), G\\(r\\), W\\(r\\) represent rescaled Brightness, Greenness, and Wetness values, which are calculated as the difference between image transformation bands and their standard deviations divided by mean values for forested pixels (extracted e.g.\u00a0from land cover map), as below:</p> <p>B\\(r\\) = (B\\(-\\)B\\(\\mu\\))/B\\(\\sigma\\)</p> <p>G\\(r\\) = (G\\(-\\)G\\(\\mu\\))/G\\(\\sigma\\)</p> <p>W\\(r\\) = (W\\(-\\)W\\(\\mu\\))/W\\(\\sigma\\)</p> <p>where:</p> <p>B\\(\\mu\\), G\\(\\mu\\), W\\(\\mu\\) denote mean forest Brightness, Greenness, and Wetness,</p> <p>B\\(\\sigma\\), G\\(\\sigma\\), W\\(\\sigma\\) represent standard deviation of forest Brightness, Greenness, and Wetness.</p> <p>Disturbed areas generally high positive B\\(r\\) values and low negative G\\(r\\) and W\\(r\\).</p> <p>Another index related to disturbance detection is Moderate Resolution Imaging Spectrometer (MODIS) Global Disturbance Index (MGDI). Due to the low spatial resolution of MODIS it is used at more generalized (even global) scale. This index incorporates vegetation greenness and surface temperature data.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#monitoring-of-changes-spectral-trajectory-in-different-time-series-data","title":"Monitoring of changes: spectral trajectory in different time series data","text":"<p>Extracting change from time series satellite data can be complex, as it involves all sorts of changes including seasonal, gradual, and abrupt shifts. The process is further complicated by factors such as clouds, haze, aerosols, lighting differences, and geometric inconsistency. In order to detect changes as accurately as possible, it is crucial to select data with resolutions that best investigate the phenomenon under study. Then these additional factors need to be eliminated (as shown in Theme 3). Based on pixel values in the time series we can analyse the trend over time and the relationship between the variables. This type of analysis is referred to as spectral trajectory analysis (you can read about this more in Theme 4 of Module 1.</p> <p>To capture the stages of plant development in phenological changes detection, the highest possible temporal resolution of the inter-annual data, at most every few days, is most desirable. The spatial resolution is important here, as meteorological image data are collected for the same area even several times a day, but their pixel size (greater than 1 km) limits the extraction of vegetation alone. The spectral ranges most frequently used are visible light, near and shortwave infrared. Apart from using the original reflectance values, information from more than one band is often combined to enhance the plant properties visible in the image (commonly used index is NDVI, being a good indicator for vegetation phenology).</p> <p>MODIS data can outperform Landsat in phenology analysis, highlighting the importance of higher temporal resolution needed to phenological change detection, even if the spatial resolution is lower. The Sentinel-2 constellation is noteworthy here as well. It provides a relatively small pixel size, useful spectral bands as well as a quite fast revisit time thanks to double data acquisition from Sentinel-2A and Sentinel-2B twin satellites. The only limitation may be cloudiness. To improve the temporal and spatial resolution of single sensors alone they are harmonised or fused together (see Theme 3 of this Module and Theme 5 of Module 1).</p> <p>Very high temporal resolution is not necessary in abrupt changes detection in the long-term perspective (intra-annual). These changes can be caused by e.g.\u00a0deforestation, so their sudden nature will appear as a rapid drop/increase in the reflectance or index value. High spectral resolution is then also not a crucial requirement for abrupt events detection. More important factor is that the sensor is operating on spectral bands that best show the phenomenon under study. Landsat data 16-day revisit time can be enough to indicate such drastic change.</p> <p>Abrupt changes often concern larger areas than gradual changes (e.g.\u00a0pest gradations). Therefore very high spatial resolution is also not required. For gradual changes the measured differences between data from subsequent dates will be less pronounced so the key factors affecting the accuracy of change detection are the details in the spectral and spatial domain. A dense time series of data also allows a more detailed analysis of the trend (with a lower frequency it could be a problematic to pinpoint the actual time of change).</p> <p> <p></p> <p>NDVI trends derived from different sensors: Sentinel-2, Landsat 8 and MODIS (note that the study area is located in the southern hemisphere, figure by Kavats et al., 2020/ CC BY 4.0). </p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#algorithms","title":"Algorithms","text":"<p>Freely available long time series of satellite data, such as Landsat, have opened the way to the development of various change/ disturbance detection algorithms. The literature presents different approaches, highlighting their capability to detect particular change (abrupt, gradual, seasonal).</p> <p>Table 1 lists selected algorithms developed for change/disturbance detection with the information about change type possible to capture.</p> Table 1. Selected algorithms for change/ disturbance detection on satellite data. Algorithm Change type Reference Breaks For Additive Seasonal and Trend (BFAST) abrupt, gradual, seasonal Verbesselt et al., 2010 Continuous Change Detection and Classification (CCDC) abrupt, gradual, seasonal Zhu, Woodcock, 2014 COntinuous monitoring of Land Disturbance (COLD) abrupt, gradual Zhu et al., 2020 Detecting Breakpoints and Estimating Segments in Trend (DBEST) abrupt, gradual, seasonal Jamali et al., 2015 Exponentially Weighted Moving Average Change Detection (EWMACD) abrupt Brooks et al., 2014 Harmonic Analyses of NDVI Time-Series (HANTS) seasonal Zhou et al., 2015 Image Trends from Regression Analysis (ITRA) gradual Vogelmann et al., 2012 Jumps Upon Spectrum and Trend (JUST) abrupt, gradual, seasonal Ghaderpour, Vujadinovic, 2020 Landsat-based detection of Trends in Disturbance and Recovery (LandTrendr) abrupt, gradual Kennedy et al., 2010 MIICA (Multi-index Integrated Change Analysis) abrupt Jin et al., 2013 Phenological parameters estimation tool (PPET) seasonal McKellip et al., 2010 Seasonal trend decomposition Loess (STL) seasonal Cleveland et al., 1990 Sub-annual change detection algorithm (SCD) abrupt Cai, Liu, 2015 Threshold- and trend-based vegetation change monitoring algorithm (TVCMA) abrupt, gradual Ochtyra et al., 2020 TimeStats seasonal Udelhoven, 2011 Time-Series Classification approach based on Change Detection (TSCCD) abrupt, gradual Yan et al.\u00a02019 Vegetation Change Tracker (VCT) abrupt Huang et al., 2010 Vegetation Regeneration and Disturbance Estimates Through Time (VeRDET) abrupt, gradual Hughes et al., 2017 <p>The algorithms can be used as independent tools implemented in environments such as R, IDL, MATLAB, Python or Google Earth Engine. There are also programs dedicated to multitemporal analyses that use the algorithms, such as STL in TIMESAT software.</p> <p>As you can see, most of the algorithms are able to detect abrupt changes, and some detect gradual changes, although the authors point out that this is a more challenging task. There are also robust phenological change detection algorithms, like the aforementioned BFAST dealing with all types of changes.</p> <p>Note: bearing in mind that each of them works in a different way and works better for different applications, it is valuable to compare their performance, such as in collective work of Cohen et al.\u00a0(2017)) or to combine several algorithms into one polyalgorithm (as proposed by Saxena et al., (2018).</p> <p>The algorithms dealing with abrupt/gradual changes have been developed to use mainly data from the Landsat mission, though most are also being adapted to Sentinel-2. In some cases a longer time series may be required for the algorithm to work properly. Phenological changes related algorithms are typically intended for use with MODIS data, although data from the above sensors can also be used (e.g.\u00a0BFAST). With the recent increase in availability of Planet data, algorithms like Thresholding Rewards and Penances algorithm (TRP, Francini et al., 2020) have been developed to assess forest loss in near real-time.</p> <p>After applying the algorithm to a time series of data, the changes detected by it can be assigned to specific phenomena (e.g.\u00a0disturbance agents). It can be done by e.g.\u00a0supervised classification.</p> <p> <p></p> <p>Forests disturbance detection with agents attribution workflow (figure by Oeser et al., 2017/ CC BY 4.0). </p> <p>In order to familiarize you with how these algorithms work (some of which you will use in the Exercise and Case study 3, below we provide more detailed descriptions on selected ones.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#the-landsat-based-detection-of-trends-in-disturbance-and-recovery-landtrendr","title":"The Landsat-based detection of Trends in Disturbance and Recovery (LandTrendr)","text":"<p>LandTrendr (Kennedy et al., 2010)) algorithm uses the segmentation method to detect abrupt changes such as forest disturbance, and, between them, gradual ones are captured thanks to the slope fitting for each segment.</p> <p> <p></p> <p>LandTrendr concept (figure by Mugiraneza et al., 2020/ CC BY 4.0). </p> <p>As the results, the year of detection with number of changes, magnitude of spectral change calculated as spectral distance between vertices, RMSE of fitted temporal segments (FTV) are provided.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#vegetation-change-tracker-vct","title":"Vegetation Change Tracker (VCT)","text":"<p>VCT (Huang et al., 2010) normalizes each image into a forest likelihood measures described below, and uses the thresholding approach to detect disturbances.</p> <p>In the first part, forest z-score is calculated as follows:</p> <p>\\(FZ_i\\) = \\(\\frac{bp_i-\\bar{b_i}}{SD_i}\\)</p> <p>where \\(b_i\\) and \\(SD_i\\) are mean and standard deviation of the band \\(i\\) spectral values of known forest pixels, and \\(bp_i\\) is any pixel value in the image.</p> <p>Then, for multispectral satellite data, an integrated forest z-score (IFZ) value calculation is based on integration \\(IFZ\\) over the spectral bands as:</p> <p>\\(IFZ\\) = \\(\\sqrt{\\frac{1}{N}\\displaystyle\\sum_{i=1}^N(FZ_i)^2}\\)</p> <p>where N is the number of bands. Bands from green and shortwave infrared ranges are used (in case of Landsat use the authors of VCT proved that NIR range is not used, because it is less sensitivity to non-fire disturbances than the other bands). To improve detection of fire disturbance events VCT also calculates NBR index described in more detail in Theme 1.</p> <p>As the results, the year of disturbance, VCT land cover mask and magnitude of disturbance in spectral bands and calculated indices are provided.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#threshold-and-trend-based-vegetation-change-monitoring-algorithm-tvcma","title":"Threshold- and trend-based vegetation change monitoring algorithm (TVCMA)","text":"<p>TVCMA (Ochtyra et al., 2020) uses two separate approaches: thresholding, indicating where and when the disturbances occurred, and a regression analysis, presenting the general trend in the time series for each pixel. This allows for monitoring both the abrupt and gradual changes.</p> <p>Contrary to simple thresholding sensitive to outlier detections (which may be the result of weather anomalies or data processing inaccuracies), three logical conditions have been included to reduce the number of outliers.</p> <p> <p></p> <p>Example of algorithm performance using predefined conditions: a) disturbance detection and b) short-term change omission (figure by course authors). </p> <p>As the results, the number of detected disturbances, the Spearman\u2019s correlation coefficient between the modeled trend line and satellite observations, and p-values are provided.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#the-breaks-for-additive-season-and-trend-bfast","title":"The Breaks for Additive Season and Trend (BFAST)","text":"<p>BFAST (Verbesselt et al., 2010) algorithm uses breakpoints to detect breaks on each individual time series component: trend, seasonal and remainders.</p> <p> <p></p> <p>Original data (Yt) and fitted seasonal (St), trend (Tt) and remainder (et) components for MODIS time series of a Longre Co lake in the Tibetan Plateau (figure by Che et al., 2017, modified/ CC BY 4.0). </p> <p>BFAST belongs to the statistical boundary methods, as it is based on the ordinary least squares (OLS) residuals-based MOving SUM (MOSUM) procedures for monitoring structural changes derived from both econometry and statistics (Zelleis, 2005). On this basis it is possible to test whether one or more breakpoints in the trend or seasonal component are occurring.</p> <p>BFAST family has nowadays extended. BFAST Monitor (DeVries et al., 2015) provides additional functionality for monitoring disturbances at the end of time series, in near real-time) and BFAST Lite (Masili\u016bnas et al., 2021) in comparison to regular BFAST is faster, more flexible and handles missing values in time series.</p> <p>The time and magnitude of change are provided as results by these algorithms.</p> <p>To see how to practically use BFAST in R in deforestation analysis follow this video by Geografif.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#results-validation-and-accuracy-assessment","title":"Results validation and accuracy assessment","text":"<p>Creation of reliable validation dataset is a time-consuming procedure but it plays a key role in assessment of time series analysis results. In Theme 2 exercise we practically demonstrated how to create a validation dataset for 33-year Landsat data series using only image compositions and indices values.</p> <p>Comprehensive review on accuracy evaluation you can find in Theme 6 of Module 1. In this section we provide some metrics that will be used in the exercise and Case study 3.</p> <p>These include measures for binary results (change/no change): True Positive (TP) and True Negative (TN) rates, which refers to the number of correctly detected as changed and unachanged pixels, respectively (Olson and Delen, 2008). On the other hand, False Positives (FP) and False Negatives (FN) indicates the number of pixels wrongly detected as changed or unchanged, respectively. Based on them, Producer (PA) and User (UA) accuracies for both changes and no-changes are calculated, as follows:</p> <p>\\(PA_C = \\frac{TP}{TP+FN}\\) and \\(PA_N = \\frac{TN}{TN+FP}\\)</p> <p>\\(UA_C = \\frac{TP}{TN+FP}\\) and \\(UA_N = \\frac{TN}{TN+FN}\\)</p> <p>where \\(PA_C\\) and \\(UA_C\\) means producer and user accuracy for change, respectively, and \\(PA_N\\) and \\(UA_N\\) means the same for no-change.</p> <p>Accuracies of detected change and no-change classes are also related to the error rates, such as ommission and commission, calculated by following: \\(1-PA\\) and \\(1-UA\\), respectively.</p> <p>For the entire analyzed result (both changes and no-changes) the Overall Accuracy (OA) is calculated to indicate the prediction\u2019s accuracy:</p> <p>\\(OA = \\frac{TP+TN}{TP+TN+FP+FN}\\)</p> <p>However, OA is less robust in such analyses, because, despite the detected changes, it also depends on the number of true negatives. Even with low efficiency of actual change detection, the OA value will be high, making this measure less useful than e.g.\u00a0comission rate.</p> <p>To assess the performance of different algorithms, you can utilize these measures calculated on the same reference datasets. For a detailed analysis of how the results of different forest disturbance detection algorithms, refer to the article of Cohen et al., 2017.</p> <p> <p></p> <p>Omission and commission rates for seven map product sets (figure by Cohen et al., 2017/ CC BY 4.0). </p> <p>Once we deem the result satisfactory, we can calculate the area of changes (e.g.\u00a0disturbances and recovery) that occurred in specific years, as well as change rate in units like e.g.\u00a0km<sup>2</sup> per year (more information you in Liu et al., 2017).</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#global-vegetation-change-detection-products","title":"Global vegetation change detection products","text":"<p>Several global products derived from satellite-based change detection are available.</p> <p>Global Forest Watch is an online platform that provides data and tools for monitoring forests. The info about forest loss and gain is based on Global Forest Change (GFC) 2000-2020 produced by the team of Hansen et al., 2013, as well as on land cover based on The Global 2000-2020 Land Cover and Land Use Change, developed by Potapov et al., 2022. Except forests, the second one contains also built-up lands, croplands, water, perennial snow and ice layers.</p> <p> <p></p> <p>The example of GLCLUC2020 cropland extent and dynamics for each 1 \u00d7 1\u00b0 grid cell. (A) Cropland area 2019, % cell area. (B) Net cropland area change, 2000\u20132019, % cell area. (C) Cropland gain 2000\u20132019 within forest loss 2000\u20132020 area, % cropland gain area within a cell. (D). Cropland loss 2000\u20132019 within the year 2020 built-up lands, % cropland loss area within a cell, figure by Potapov et al., 2022/ CC BY 4.0). </p> <p>There are also products that present observed phenomenas or classes in different dates, from which you can indirectly obtain the information about the changes (e.g.\u00a0vegetation condition or chlorophyll content from MODIS data).</p> <p>Remember that we have listed such multitemporal land cover products in Table 1 in Theme 4, e.g.\u00a0Dynamic World. Therefore, you can capture changes in environmental components based on the maps from specific dates.</p> <p>However, when utilizing such data, it is important to note that it is compiled on a global scale. You can find many examples in the literature outlining the limitations and errors for specific areas analysis. For regional or local application a very thorough and accurate approach is recommended to achieve satisfactory results.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"<p>You are now at the end of the lesson and should have a knowledge about the use of satellite data for vegetation change / disturbance detection. Use the questions below to assess your learning success:</p> Question 1. For detection of which change types will be the most appropriate time-series consisting of one image per year?    gradual and seasonal abrupt and gradual seasonal and abrupt all of the above    abrupt and gradual   Question 2. Which type of image data analysis method is not used in multitemporal change detection algorithms?  thresholding image enhancement segmentation differencing   image enhancement   Question 3. Commonly used vegetation disturbance detection variables are related to:  water content nitrogen content burn severity soil salinity   water content   Question 4. What data is best to detect seasonal changes in the Amazon forests between 2005 and 2023?  Planet Landsat Sentinel-2 MODIS   MODIS   Question 5. Gradual changes in time series are:  short-term, small magnitude year-to-year short-term, large magnitude year-to-year long-term, small magnitude year-to-year long-term, large magnitude year-to-year   long-term, small magnitude year-to-year   Question 6. Which of the following algorithms will detect a pest infestation and why?  VCT thanks to the IFt index calculation BFAST thanks to the trend component TVCMA thanks to simple thresholding LandTrendr thanks to remainder component   BFAST thanks to the trend component   Question 7. Change / disturbance detection algorithm result cannot be:  the number and magnitude of changes the year of detection and error of temporal composition creation error of fitted temporal segment and calculated index correlation coefficient between the trend line and satellite data and the magnitude of spectral changes   the year of detection and error of temporal composition creation"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#exercise","title":"Exercise","text":"<p>Proceed with the exercise by going to the next page below or clicking this link</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#references","title":"References","text":""},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#key-references-recommended-reading-looking-up-background-details","title":"Key references (recommended reading, looking up background details)","text":"<p>Cohen, W. B., Healey, S. P., Yang, Z., Stehman, S. V., Brewer, C. K., Brooks, E. B., Gorelick, N., Huang, C., Hughes, M.J., Kennedy, R.E., Loveland, T.R., Moisen, G.G., Schroeder, T.A., Vogelmann, J.E., Woodcock, C.E., Yang, L., &amp; Zhu, Z. (2017). How similar are forest disturbance maps derived from different Landsat time series algorithms?. Forests, 8(4), 98. https://doi.org/10.3390/f8040098</p> <p>Huang, C., Goward, S. N., Masek, J. G., Thomas, N., Zhu, Z., &amp; Vogelmann, J. E. (2010). An automated approach for reconstructing recent forest disturbance history using dense Landsat time series stacks. Remote Sensing of Environment, 114(1), 183-198. https://doi.org/10.1016/j.rse.2009.08.017</p> <p>Kennedy, R. E., Yang, Z., &amp; Cohen, W. B. (2010). Detecting trends in forest disturbance and recovery using yearly Landsat time series: 1. LandTrendr\u2014Temporal segmentation algorithms. Remote Sensing of Environment, 114(12), 2897-2910. https://doi.org/10.1016/j.rse.2010.07.008</p> <p>Liu, S., Wei, X., Li, D., &amp; Lu, D. (2017). Examining forest disturbance and recovery in the subtropical forest region of zhejiang province using landsat time-series data. Remote Sensing, 9(5), 479. https://doi.org/10.3390/rs9050479</p> <p>McDowell, N. G., Coops, N. C., Beck, P. S., Chambers, J. Q., Gangodagamage, C., Hicke, J. A., \u2026 &amp; Allen, C. D. (2015). Global satellite monitoring of climate-induced vegetation disturbances. Trends in plant science, 20(2), 114-123. https://doi.org/10.1016/j.tplants.2014.10.008</p> <p>Ochtyra, A., Marcinkowska-Ochtyra, A., &amp; Raczko, E. (2020). Threshold-and trend-based vegetation change monitoring algorithm based on the inter-annual multi-temporal normalized difference moisture index series: A case study of the Tatra Mountains. Remote Sensing of Environment, 249, 112026. https://doi.org/10.1016/j.rse.2020.112026</p> <p>Zhu, Z. (2017). Change detection using landsat time series: A review of frequencies, preprocessing, algorithms, and applications. ISPRS Journal of Photogrammetry and Remote Sensing, 130, 370-384. https://doi.org/10.1016/j.isprsjprs.2017.06.013</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#additional-references-cited-in-this-theme","title":"Additional references cited in this theme","text":"<p>Brooks, E., Wynne, R. H., Thomas, V. A., Blinn, C. E., &amp; Coulston, J. (2014). Exponentially Weighted Moving Average Change Detection Around the Country (and the World). In AGU Fall Meeting Abstracts (Vol. 2014, pp.\u00a0B51L-01). SOURCE</p> <p>Cleveland, R. B., Cleveland, W. S., McRae, J. E., &amp; Terpenning, I. (1990). STL: A seasonal-trend decomposition. Journal of Official Statistics, 6(1), 3-73. SOURCE</p> <p>Cohen, W. B., Yang, Z., Healey, S. P., Kennedy, R. E., &amp; Gorelick, N. (2018). A LandTrendr multispectral ensemble for forest disturbance detection. Remote Sensing of Environment, 205, 131-140.https://doi.org/10.1016/j.rse.2017.11.015</p> <p>Che, X., Yang, Y., Feng, M., Xiao, T., Huang, S., Xiang, Y., &amp; Chen, Z. (2017). Mapping extent dynamics of small lakes using downscaling MODIS surface reflectance. Remote Sensing, 9(1), 82. https://doi.org/10.3390/rs9010082</p> <p>DeVries, B., Verbesselt, J., Kooistra, L., &amp; Herold, M. (2015). Robust monitoring of small-scale forest disturbances in a tropical montane forest using Landsat time series. Remote Sensing of Environment, 161, 107-121. https://doi.org/10.1016/j.rse.2015.02.012</p> <p>Francini, S., McRoberts, R. E., Giannetti, F., Mencucci, M., Marchetti, M., Scarascia Mugnozza, G., &amp; Chirici, G. (2020).* Near-real time forest change detection using PlanetScope imagery*. European Journal of Remote Sensing, 53(1), 233-244. https://doi.org/10.1080/22797254.2020.1806734</p> <p>Ghaderpour, E., &amp; Vujadinovic, T. (2020). Change detection within remotely sensed satellite image time series via spectral analysis. Remote Sensing, 12(23), 4001. https://doi.org/10.3390/rs12234001</p> <p>Griffiths, P., Kuemmerle, T., Baumann, M., Radeloff, V. C., Abrudan, I. V., Lieskovsky, J., \u2026 &amp; Hostert, P. (2014). Forest disturbances, forest recovery, and changes in forest types across the Carpathian ecoregion from 1985 to 2010 based on Landsat image composites. Remote Sensing of Environment, 151, 72-88. https://doi.org/10.1016/j.rse.2013.04.022</p> <p>Hansen, M. C., Potapov, P. V., Moore, R., Hancher, M., Turubanova, S. A., Tyukavina, A., \u2026 &amp; Townshend, J. (2013). High-resolution global maps of 21st-century forest cover change. Science, 342(6160), 850-853. https://doi.org/10.1126/science.1244693</p> <p>Healey, S. P., Cohen, W. B., Zhiqiang, Y., &amp; Krankina, O. N. (2005). Comparison of Tasseled Cap-based Landsat data structures for use in forest disturbance detection. Remote Sensing of Environment, 97(3), 301-310. https://doi.org/10.1016/j.rse.2005.05.009</p> <p>Huang, Z., Liu, X., Yang, Q., Meng, Y., Zhu, L., &amp; Zou, X. (2021). Quantifying the spatiotemporal characteristics of multi-dimensional karst ecosystem stability with Landsat time series in southwest China. International Journal of Applied Earth Observation and Geoinformation, 104, 102575. https://doi.org/10.1016/j.jag.2021.102575</p> <p>Hughes, M. J., Kaylor, S. D., &amp; Hayes, D. J. (2017). Patch-based forest change detection from Landsat time series. Forests, 8(5), 166. https://doi.org/10.3390/f8050166</p> <p>Jamali, S., J\u00f6nsson, P., Eklundh, L., Ard\u00f6, J., &amp; Seaquist, J. (2015). Detecting changes in vegetation trends using time series segmentation. Remote Sensing of Environment, 156, 182-195. https://doi.org/10.1016/j.rse.2014.09.010</p> <p>Jin, S., Yang, L., Danielson, P., Homer, C., Fry, J., &amp; Xian, G. (2013). A comprehensive change detection method for updating the National Land Cover Database to circa 2011. Remote Sensing of Environment, 132, 159-175. https://doi.org/10.1016/j.rse.2013.01.012</p> <p>J\u00f6nsson, P., &amp; Eklundh, L. (2004). TIMESAT\u2014a program for analyzing time-series of satellite sensor data. Computers &amp; geosciences, 30(8), 833-845. https://doi.org/10.1016/j.cageo.2004.05.006</p> <p>Kavats, O., Khramov, D., Sergieieva, K., &amp; Vasyliev, V. (2020). Monitoring of sugarcane harvest in Brazil based on Optical and SAR data. Remote Sensing, 12(24), 4080. https://doi.org/10.3390/rs12244080</p> <p>Kuenzer, C., Dech, S., &amp; Wagner, W. (2015). Remote sensing time series. Remote Sensing and Digital Image Processing, 22, 225-245. source</p> <p>Masili\u016bnas, D., Tsendbazar, N. E., Herold, M., &amp; Verbesselt, J. (2021). BFAST Lite: A lightweight break detection method for time series analysis. Remote Sensing, 13(16), 3308. https://doi.org/10.3390/rs13163308</p> <p>McKellip, R.D., Ross, K.W., Spruce, J.P., Smoot, J.C., Ryan, R.E., Gasser, G.E., Prados, D.L., Vaughan, R.D., (2010). Phenological Parameters Estimation Tool. NASA Tech. Briefs, New York. SOURCE</p> <p>Miller, J. D., &amp; Yool, S. R. (2002). Mapping forest post-fire canopy consumption in several overstory types using multi-temporal Landsat TM and ETM data. Remote Sensing of Environment, 82(2-3), 481-496. https://doi.org/10.1016/S0034-4257(02)00071-8</p> <p>Mugiraneza, T., Nascetti, A., &amp; Ban, Y. (2020). Continuous monitoring of urban land cover change trajectories with landsat time series and landtrendr-google earth engine cloud computing. Remote Sensing, 12(18), 2883. https://doi.org/10.3390/rs12182883</p> <p>Oeser, J., Pflugmacher, D., Senf, C., Heurich, M., &amp; Hostert, P. (2017). Using intra-annual Landsat time series for attributing forest disturbance agents in Central Europe. Forests, 8(7), 251. https://doi.org/10.3390/f8070251</p> <p>Olson, D. L., &amp; Delen, D. (2008). Advanced data mining techniques. Springer Science &amp; Business Media.source</p> <p>Pasquarella, V. J., Ar\u00e9valo, P., Bratley, K. H., Bullock, E. L., Gorelick, N., Yang, Z., &amp; Kennedy, R. E. (2022). Demystifying LandTrendr and CCDC temporal segmentation. International Journal of Applied Earth Observation and Geoinformation, 110, 102806. https://doi.org/10.1016/j.jag.2022.102806</p> <p>Saxena, R., Watson, L. T., Wynne, R. H., Brooks, E. B., Thomas, V. A., Zhiqiang, Y., &amp; Kennedy, R. E. (2018). Towards a polyalgorithm for land use change detection. ISPRS Journal of Photogrammetry and Remote Sensing, 144, 217-234. https://doi.org/10.1016/j.isprsjprs.2018.07.002</p> <p>Singh, A. (1989). Review article digital change detection techniques using remotely-sensed data. International Journal of Remote Sensing, 10(6), 989-1003. https://doi.org/10.1080/01431168908903939</p> <p>Udelhoven, T. (2010). TimeStats: A software tool for the retrieval of temporal patterns from global satellite archives. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 4(2), 310-317. https://doi.org/10.1109/jstars.2010.2051942</p> <p>Verbesselt, J., Hyndman, R., Newnham, G., &amp; Culvenor, D. (2010). Detecting trend and seasonal changes in satellite image time series. Remote sensing of Environment, 114(1), 106-115. https://doi.org/10.1016/j.rse.2009.08.014</p> <p>Vogelmann, J. E., Xian, G., Homer, C., &amp; Tolk, B. (2012). Monitoring gradual ecosystem change using Landsat time series analyses: Case studies in selected forest and rangeland ecosystems. Remote Sensing of Environment, 122, 92-105. https://doi.org/10.1016/j.rse.2011.06.027</p> <p>Yan, J., Wang, L., Song, W., Chen, Y., Chen, X., &amp; Deng, Z. (2019). A time-series classification approach based on change detection for rapid land cover mapping. ISPRS Journal of Photogrammetry and Remote Sensing, 158, 249-262. https://doi.org/10.1016/j.isprsjprs.2019.10.003</p> <p>Zhou, J., Jia, L., &amp; Menenti, M. (2015). Reconstruction of global MODIS NDVI time series: Performance of Harmonic ANalysis of Time Series (HANTS). Remote Sensing of Environment, 163, 217-228. https://doi.org/10.1016/j.rse.2015.03.018</p> <p>Zhu, Z., &amp; Woodcock, C. E. (2014). Continuous change detection and classification of land cover using all available Landsat data. Remote sensing of Environment, 144, 152-171. https://doi.org/10.1016/j.rse.2014.01.011</p> <p>Zhu, Z., Zhang, J., Yang, Z., Aljaddani, A. H., Cohen, W. B., Qiu, S., &amp; Zhou, C. (2020). Continuous monitoring of land disturbance based on Landsat time series. Remote Sensing of Environment, 238, 111116. https://doi.org/10.1016/j.rse.2019.03.009</p> <p>Zeileis, A. (2005). A unified approach to structural change tests based on ML scores, F statistics, and OLS residuals. Econometric Reviews, 24(4), 445-466. https://doi.org/10.1080/07474930500406053</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring.html#next-unit","title":"Next unit","text":"<p>You have finished all of themes in this module. You can now proceed with case studies:</p> <ul> <li>Monitoring tundra grasslands (Karkonosze)</li> <li>Effects of pollution in Ore Mountains</li> <li>Forest disturbance detection (Tatras)</li> </ul>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html","title":"Vegetation change and disturbance detection - Exercise","text":""},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#exercise-forest-disturbance-mapping-in-tatras-using-landtrendr","title":"Exercise - Forest disturbance mapping in Tatras using LandTrendr","text":"<p>In this exercise, you\u2019ll delve into vegetation disturbance detection using LandTrendr algorithm and its products. Employing the Google Earth Engine implementation of this tool, along with the R programming language, you\u2019ll extract valuable data about the vegetation status in the Tatra Mountains from Landsat imagery.</p> <p>It\u2019s important to understand that vegetation monitoring boasts a wide range of tools and algorithms for its implementation. The method showcased here is just one of many possibilities. Building upon the techniques and insights you gain here, we hope you\u2019ll be equipped to extend your research, adapting to various input data, algorithms, and tools.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#basic-preparation","title":"Basic preparation","text":""},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#prerequisites","title":"Prerequisites","text":"<p>This exercise will consist of two parts. The first part will be conducted in the Google Earth Engine platform using JavaScript based Code Editor. If you are unfamiliar with this tool, we strongly recommend acquainting yourself with introductory tutorials, which are available and linked in GEE software introduction in this Course. Throughout this exercise, we\u2019ll utilize a blend of built-in functions, custom functions and functions from external resources to complete various tasks. We will supply necessary links and explanations in the relevant sections. For general reference documentation see GEE API reference website.</p> <p>The second part will use R and RStudio. You can access environment setup tutorial for the whole Module 2 here: R environment setup tutorial. After following the setup guide you should have all the necessary packages installed.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#data","title":"Data","text":"<p>Download data provided through Zenodo.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#imagery-data","title":"Imagery data","text":"<p>The imagery supplied for this exercise consists of Landsat satellite imagery time series. The data preparation process is described in the Module 2 Theme 3 exercise Pipeline 2. We\u2019ll use indices time series that you can access through Google Assets.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#reference-data","title":"Reference data","text":"<p>For this exercise, 100 reference points have been earmarked to validate the outcomes derived using LandTrendr. These points were randomly spread out across the research area. Each point was assessed based on chips (method of producing chips is presented in Theme 2 exercise). The attribute table from the point layer was extracted and will be used in the second part of the exercise as a validation dataset. In table rows there are consecutive reference points recorded and in columns there are subsequent years from 1985 (the first possible disturbance observation) to 2022. In cases where no disturbance was identified, the corresponding cell was marked with a \u201c0\u201d. Conversely, if a disturbance was evident, the cell was marked with a \u201c1\u201d.</p> <p>To validate our results, we\u2019ll extract values from LandTrendr\u2019s outputs for each point. Subsequently, we\u2019ll juxtapose this extracted data against the validation table to ensure accuracy and reliability.</p> <p> <p></p> <p>Validation table. </p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#part-1-landtrendr-in-google-earth-engine","title":"Part 1 - LandTrendr in Google Earth Engine","text":"<p>In this part of the exercise we\u2019ll use the Google Earth Engine implementation of the LandTrendr algorithm. You can read about this tool on this website https://emapr.github.io/LT-GEE/index.html and in this article:</p> <ul> <li>Kennedy RE, Yang Z, Gorelick N, Braaten J, Cavalcante L, Cohen WB, Healey S. Implementation of the LandTrendr Algorithm on Google Earth Engine. Remote Sensing. 2018; 10(5):691.https://doi.org/10.3390/rs10050691</li> </ul> <p>To start with, let\u2019s load the data. For the purpose of this exercise we\u2019ll use two spectral indices time series images we prepared in Module 2 Theme 3 exercise Pipeline 2: NBR and NDMI. If you exported them to your own Google Earth Engine assets you can change the path in the script below to match your output folder. Otherwise, you can access data stored in <code>etrainee-module2</code> project.</p> <pre><code>// Import image data\nvar nbr = ee.Image(\"projects/etrainee-module2/assets/nbr\");\nvar ndmi = ee.Image(\"projects/etrainee-module2/assets/ndmi\");\n\nprint(nbr);\n</code></pre> <p>You should be able to see a multiband NBR image in the console.</p> <p> <p></p> <p></p> <p>We\u2019ll also add two vector files: Tatras Area of Interest and forest mask based on 1984 and 2022 spatial extent of forests (combination of both extents).</p> <pre><code>// Define Area of Interest (AOI)\nvar AOI = ee.FeatureCollection(\"projects/etrainee-module2/assets/aoi_tatras\");\n// Define forest mask\nvar forestMask = ee.FeatureCollection(\"projects/etrainee-module2/assets/tatras_forest_mask\");\n</code></pre> <p>For the first part we\u2019ll produce LandTrendr algorithm results using NBR index. To proceed we need to prepare the multiband image in accordance with LandTrendr requirements. The input data to the LandTrendr algorithm should be an <code>ImageCollection</code>. Each subsequent image should contain one band with that year\u2019s spectral index and be named the same (i.e.\u00a0<code>NBR</code>). Each image should also have time property assigned.</p> <p>Most spectral indices must also be multiplied by -1 before submitting it to the LandTrendr algorithm. Explanation can be found here:</p> <p>Two really important steps in image collection building include 1) masking cloud and cloud shadow pixels during annual image compositing and to 2) ensure that the spectral band or index that is to be segmented is oriented so that vegetation loss is represented by a positive delta. For instance, NBR in its native orientation results in a negative delta when vegetation is lost from one observation to the next. In this case, NBR must be multiplied by -1 before being segmented. Conversely, if Landsat TM band 5 (SWIR) is selected for segmentation, inversion of the spectral values is not required, since natively, vegetation loss is represented by a positive delta.</p> <pre><code>var image = nbr;\nvar imageName = 'nbr';\n\n\nimage = image.multiply(-1);\n\n// Extract band names from image\nvar bands = image.bandNames();\nprint(bands);\n\n// Make a list of bands from Image\nvar list = bands.map(function(n) { return image.select([n]) });\n\n// Create ImageCollection from list\nvar collection = ee.ImageCollection.fromImages(list);\n\n// Define a function to rename each band to the same name and add time property\nvar renameAndSetTime = function(img) {\n  // Get the band names from the image\n  var bandNames = img.bandNames();\n\n  // Extract the year from the band name 'bYYYY_NBR'.\n  // This will give \"YYYY\" as a string.\n  var yearString = ee.String(bandNames.get(0)).slice(1, 5); \n\n  // Convert the year string to a number\n  var year = ee.Number.parse(yearString);\n\n  // Rename the band to 'NBR'\n  // 0 represents the first (and only) band of the image\n  var renamedImg = img.select([0]).rename('NBR');  \n\n  // Set the system:time_start property\n  var date = ee.Date.fromYMD(year, 7, 1);\n  return renamedImg.set('system:time_start', date.millis());\n};\n\n// Map the function over your ImageCollection\nvar processedCollection = collection.map(renameAndSetTime);\n\nprint(processedCollection);\n</code></pre> <p>The resulting collection should look like this in the console.</p> <p> <p></p> <p></p> <p>We are now ready for another step, which is running LandTrendr algorithm. In the following code sections we\u2019ll be orchestrating a sequence of operations that will allow us to yield results as exportable imagery. We\u2019ll then move to RStudio, where we\u2019ll assess the results.</p> <p>Initiating our process, it\u2019s imperative to lay out a list of parameters we aim to evaluate. LandTrendr has several parameters, which can influence the output of the algorithm. For the purpose of this exercise we\u2019ll set arbitrary values for seven of them and we\u2019ll test the results with a range of values for spikeThreshold parameter, which is defined as threshold for dampening the spikes (1.0 means no dampening). This, in essence, acts as a filter to control the sensitivity of detecting sudden, short-lived changes in the data series.</p> <pre><code>// List of  thresholds to test\nvar spikeThreshold = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1];\n\n// List of  thresholds as strings (to be used in exporting part)\nvar spikeThresholdStr = [\"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"1\"];\n</code></pre> <p>Let\u2019s unpack the upcoming code block step by step to ensure comprehension:</p> <ul> <li>We\u2019ll iterate in a loop for each element of <code>spikeThreshold</code> list</li> <li>In each loop we\u2019ll set LandTrendr parameters, updating <code>spikeThreshold</code> each time</li> <li>We\u2019ll run the LandTrendr algorithm and extract the outputs needed for further steps</li> <li>Levereging the structure of LandTrendr outputs we\u2019ll extract vertex images and fitted values images. We\u2019ll end up with two multiband images per iteration.</li> <li>We\u2019ll mask the resulting images with forest mask</li> <li>Finally, each iteration will yield two export ready multiband images</li> </ul> <p>The number of iterations, and consequently, the exported images depends on the number of values present in the <code>spikeThreshold</code> list. This framework allows for a rigorous evaluation of how varying the <code>spikeThreshold</code> value impacts the final results.</p> <pre><code>for (var i=0 ; i&lt;spikeThreshold.length; i++){\n\n  // Set LandTrendr parameters\n  var runParams = { \n    timeSeries: processedCollection,\n    maxSegments:            5,\n    spikeThreshold:         spikeThreshold[i],\n    vertexCountOvershoot:   3,\n    preventOneYearRecovery: true,\n    recoveryThreshold:      0.25,\n    pvalThreshold:          0.05,\n    bestModelProportion:    0.75,\n    minObservationsNeeded:  6\n  };\n\n  // Add processedCollection to parameters\n  runParams.timeSeries = processedCollection;\n\n  // Run LandTrendr algorithm\n  var LTresult = ee.Algorithms.TemporalSegmentation.LandTrendr(runParams);\n\n  // Select the results\n  var lt = LTresult.select(\"LandTrendr\");\n\n  // Set start and end year\n  var startYear = 1984;\n  var endYear = 2022;\n\n  // Function to extract vertex images\n  var isVertex = ee.Image(\n    ee.List.sequence(startYear, endYear)\n      .iterate(\n        function (year, acc) {\n          year = ee.Number(year);\n          var years = lt.arraySlice(0, 0, 1);\n          var isVertex = lt\n            .arrayMask(years.eq(year));\n          isVertex = isVertex\n            .updateMask(isVertex.arrayLength(1)) \n            .arraySlice(0, 3, 4)\n            .arrayGet([0, 0])\n            .unmask(0)\n            .byte()\n            .rename(year.format('%d'));\n          return ee.Image(acc).addBands(isVertex);\n        },\n        ee.Image([])\n      )\n  );\n\n\n  isVertex = isVertex.reproject({crs: 'EPSG:32634', scale: 30});\n  isVertex = isVertex.clip(forestMask).unmask(0).short();\n\n\n  Export.image.toDrive({\n    image: isVertex, \n    description: imageName+\"_ver_\"+spikeThresholdStr[i], \n    region: AOI, \n    folder: 'theme5_landtrendr',\n    scale: 30, \n    crs: 'EPSG:32634', \n    maxPixels: 1e13\n  });\n\n\n  // Function to extract fitted values images\n  var fittedValues = ee.Image(\n    ee.List.sequence(startYear, endYear)\n      .iterate(\n        function (year, acc) {\n          year = ee.Number(year);\n          var years = lt.arraySlice(0, 0, 1);\n          var isVertex = lt\n            .arrayMask(years.eq(year));\n          isVertex = isVertex\n            .updateMask(isVertex.arrayLength(1)) // Mask if no matching year\n            .arraySlice(0, 2, 3)\n            .arrayGet([0, 0])\n            .unmask(0) // Assum no vertex when no data for year\n            .rename(year.format('%d'));\n          return ee.Image(acc).addBands(isVertex);\n        },\n        ee.Image([])\n      )\n  );\n\n  fittedValues = fittedValues.reproject({crs: 'EPSG:32634', scale: 30});\n  fittedValues = fittedValues.clip(forestMask).unmask(0).float();\n\n\n  Export.image.toDrive({\n    image: fittedValues, \n    description: imageName+\"_fv_\"+spikeThresholdStr[i], \n    region: AOI, \n    folder: 'theme5_landtrendr',\n    scale: 30, \n    crs: 'EPSG:32634', \n    maxPixels: 1e13\n  });\n\n\n}\n</code></pre> <p>After successfully running this part of the script you should be able to see 16 images waiting to be exported.</p> <p> <p></p> <p></p> <p>Export each image to your Google Drive. You can change the folder to which the results will be exported. From your Google Drive folder download the images to your local hard drive.</p> <p>After you queued images for download you can repeat the steps above to prepare LandTrendr outputs usng NDMI as input. For the next part of the exercise we\u2019ll want to have additional 16 NDMI products exported to local hard drive.</p> <p>Change these lines and rerun the whole script to proceed with NDMI part:</p> <pre><code>var image = ndmi;\nvar imageName = 'ndmi';\n\n ...\n\n  // Rename the band to 'NDMI'\n  var renamedImg = img.select([0]).rename('NDMI'); \n</code></pre>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#part-2-processing-landtrendr-results-in-rstudio","title":"Part 2 - Processing LandTrendr results in RStudio","text":"<p>After successfully downloading all the necessary results move them to <code>theme_5_exercise/data_exercise</code> folder. We supplied them in our Zenodo data package just in case something went wrong during part 1. You can replace them if you managed to successfully complete part 1.</p> <p>Create a new script inside <code>theme_5_exercise</code> folder. You can name it <code>theme_5_exercise.R</code>. Start with loading required packages.</p> <pre><code># raster I/o\nlibrary(terra) \n\n# vector I/0\nlibrary(sf) \n\n# reading .xlsx files\nlibrary(readxl) \n\n# writing.xlsx files\nlibrary(writexl) \n\n# tabular data manipulation\nlibrary(dplyr) \n</code></pre> <p>Next let\u2019s read input data for this exercise. We\u2019ll start with NBR rasters.</p> <pre><code># List vertices raster paths\nverts_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                            pattern='nbr_ver', \n                            full.names = TRUE)\n# List fitted values raster paths\nfv_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                         pattern='nbr_fv', \n                         full.names = TRUE)\n\n# Get names of rasters \nverts_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                          pattern='nbr_ver')\nfv_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                       pattern='nbr_ver')\n\n# Read vertices and fitted values rasters to lists\nverts_rasters_list &lt;- lapply(verts_rasters, rast)\nfv_rasters_list &lt;- lapply(fv_rasters, rast)\n\n# Rename lists elements\nnames(verts_rasters_list) &lt;- verts_names\nnames(fv_rasters_list) &lt;- fv_names\n</code></pre> <p>Now read validation points dataset.</p> <pre><code># Read validation points\nval_points &lt;- st_read(\"theme_5_exercise/data_exercise/T5_points.shp\")\n</code></pre> <p>The outputs you\u2019ve gotten from LandTrendr consist of raw vertices and their corresponding fitted values. Vertices in LandTrendr represent breakpoints in the time series of a given pixel, indicating significant changes in its trajectory, while fitted values are smoother curves representing predicted values based on the observed data. We are interested in only those breakpoints (vertices) which point towards a decrease or disturbance in vegetation. To isolate these, it\u2019s imperative to measure the magnitude of change between each segment\u2019s start and end points. This magnitude is essentially the difference (delta) between the end vertex\u2019s fitted value and the start vertex\u2019s fitted value for a given segment. For indices such as NBR and NDMI (multiplied by -1 before submitting them to LandTrendr), a significant positive delta is a strong indicator of vegetation disturbance or stress. Through a loop mechanism, for every pair of vertex-fitted values images we\u2019ve exported, the code calculates the magnitudes of change. The results will be structured as matrices, with each cell indicating the magnitude of vegetation change at the start vertex\u2019s position.</p> <pre><code># Empty list for storing the results\nmagnitudes_list &lt;- list()\n\n# For each par of vertices-fitted values images\nfor (r in seq(1:length(verts_rasters_list))) {\n\n  # Read appropriate rasters\n  vertices &lt;- verts_rasters_list[[r]]\n  fitted_values &lt;- fv_rasters_list[[r]]\n\n  # Extract values for 100 validation points\n  vert_pts &lt;- extract(vertices, val_points) %&gt;%\n    select(2:40)\n  vals_pts &lt;- extract(fitted_values, val_points) %&gt;%\n    select(2:40)\n\n  # Prepare empty output matrix\n  output &lt;- matrix(data = NA, nrow = 100, ncol = 39)\n\n  # For each row(point)\n  for (row in 1:nrow(vert_pts)) {\n\n    # Get only pixel values for that row(point)\n    vertices_pixel &lt;- vert_pts[row, ]\n    fitted_values_pixel &lt;- vals_pts[row, ]\n\n    # Check for the occurence of vertices\n    vertex_indices &lt;- which(vertices_pixel == 1)\n\n    # Prepare empty vector for magnitude calculation results\n    magnitude_of_change &lt;- rep(NA, length(vertices_pixel))\n\n    # Calculate magnitudes\n    for (i in 1:(length(vertex_indices) - 1)) {\n\n      start_index &lt;- vertex_indices[i]\n      end_index &lt;- vertex_indices[i + 1]\n\n      difference &lt;- fitted_values_pixel[1, end_index] - fitted_values_pixel[1, start_index]\n      magnitude_of_change[end_index] &lt;- difference\n    }\n\n    # Insert magnitude values to the matching row in empty output matrix\n    output[row,] &lt;- magnitude_of_change\n    # Insert temporary 0 value to the first column\n    output[row, 1] &lt;- 0\n\n    # Check the occurrences of magnitude \n    mag_indices &lt;- which(!is.na(output[row, ]))\n\n    # Prepare empty vector for the moved values\n    moved_row &lt;- rep(NA, 39)\n\n    # Move magnitude values one vertex position backwards\n    for (i in 2:length(mag_indices)) {\n\n      val &lt;- output[row, mag_indices[i]]\n      moved_row[mag_indices[i-1]] &lt;- val\n\n    }\n\n    # Replace the row in output with a moved row\n    output[row,] &lt;- moved_row\n\n\n  }\n\n  # Copy output values\n  output_mask_loss &lt;- output\n\n  # Retain only positive magnitudes indicating vegetation loss\n  output_mask_loss[output_mask_loss &lt; 0] &lt;- NA\n\n  # Add the magnitude matrix to list storing the results\n  magnitudes_list[[r]] &lt;- output_mask_loss\n\n}\n</code></pre> <p>Run <code>View(magnitude_list[[1]])</code> in the console so you can visually inspect an example matrix from the list of matrices derived from the LandTrendr results. Each matrix contains the magnitudes of vegetation loss/disturbance across the validation points.</p> <p> <p></p> <p></p> <p>So far we\u2019ve acquired a matrix with magnitudes of loss/disturbance events for each of the eight NBR LandTrendr results we downloaded. We now want to test the impact of different magnitude thresholds on the final accuracy of the algorithm. You\u2019ll be testing 80 thresholds, ranging from 0 (no change) to 0.8 (significant change). For each threshold value, you\u2019ll mask (or filter out) all vertices with magnitude below the given threshold. The reason for masking these vertices is to exclude potential \u201cfalse positives\u201d from the analysis. After applying the threshold, the filtered result is then compared to a validation table. This table contains manually verified data against which the LandTrendr results are compared to determine accuracy. In order to construct a table corresponding to the validation one we will need to shift vertices one year forwards. This is necessary, because the year of detection of disturbance event is the first year after end vertex of segment with positive delta.</p> <p>To streamline this process, it\u2019s recommended to prepare a vector containing all the threshold values to test. Furthermore, a function can be created to systematically apply each threshold to the matrices, mask out irrelevant vertices, and then compare the results against the validation table.</p> <pre><code># Magnitude thresholds\nthresholds &lt;- seq(0, 0.8, by = 0.01)\n\n# Function to apply magnitude threshold\napply_magnitude_threshold &lt;- function(mag_mat, threshold) {\n\n  output_mask_loss_th &lt;- mag_mat\n\n  # mask vertices below threshold\n  output_mask_loss_th[output_mask_loss_th &lt; threshold] &lt;- NA\n\n  # no change  - NAs to 0\n  output_mask_loss_th[is.na(output_mask_loss_th)] &lt;- 0\n\n  # change/vertex - above 0 to 1\n  output_mask_loss_th[output_mask_loss_th &gt; 0] &lt;- 1\n\n  # Shift vertices so that they represent year of detection\n  output_mask_loss_shift &lt;- output_mask_loss_th[, 1:38]\n  colnames(output_mask_loss_shift) &lt;- c(1985:2022)\n\n  return(output_mask_loss_shift)\n\n}\n</code></pre> <p>Prepare a list of names to be able to differentiate results of each test.</p> <pre><code># Prepare list of threshold scenario names\n# replace \".\" with \"_\"\nnames_list &lt;- paste0(\"Threshold_\", \n                     gsub(\"\\\\.\", \"_\", abs(thresholds))) \n</code></pre> <p>We want to calculate statistics for each iteration. The function below calculates several basic statistics derived from confusion matrix.</p> <pre><code># Prepare a function to calculate statistics\ncalculate_confusion_matrix_stats &lt;- function(matrix, validation_matrix, set_name) {\n\n  # Calculate TP, FP, TN, FN\n  TP &lt;- sum(matrix == 1 &amp; validation_matrix == 1)\n  FP &lt;- sum(matrix == 1 &amp; validation_matrix == 0)\n  TN &lt;- sum(matrix == 0 &amp; validation_matrix == 0)\n  FN &lt;- sum(matrix == 0 &amp; validation_matrix == 1)\n\n  # Calculate the metrics\n  Accuracy &lt;- (TP + TN) / (TP + FP + FN + TN)\n  Precision &lt;- TP / (TP + FP)\n  Sensitivity &lt;- TP / (TP + FN)\n  Specificity &lt;- TN / (TN + FP)\n  F1_Score &lt;- 2 * (Sensitivity * Precision) / (Sensitivity  + Precision)\n\n  # Create a data frame with the confusion matrix stats\n  stats &lt;- data.frame(Set = set_name, TP, FP, TN, FN, Accuracy, Precision, \n                      Sensitivity, Specificity, F1_Score)\n\n  return(stats)\n}\n</code></pre> <p>We are now ready to read validation table and prepare the results. We\u2019ll save each result to a separate <code>.xlsx</code> file.</p> <pre><code># Read validation table\nval_table &lt;- read_xlsx(\"theme_5_exercise/data_exercise/T5_validation_table.xlsx\") %&gt;%\n  select(2:39)\n\n# Prepare  a storage for results\nresults_list &lt;- list()\n\n# For each matrix in magnitudes_list\nfor (l in seq(1, length(magnitudes_list))) {\n\n  # Calculate matrices with each threshold applied\n  thresholds_results &lt;- lapply(thresholds, apply_magnitude_threshold, \n                               mag_mat = magnitudes_list[[l]])\n  thresholds_results &lt;- setNames(thresholds_results, names_list)\n\n  # Calculate accuracy statistics\n  stats_list &lt;- lapply(seq_along(thresholds_results), \n                function(i) calculate_confusion_matrix_stats(thresholds_results[[i]],\n                            val_table, names(thresholds_results)[i]))\n\n  # construct one data frame with accuracy statistics\n  stats_df &lt;- do.call(rbind, stats_list)\n\n  # add the constructed data frame to the list of results\n  results_list[[l]] &lt;- stats_df\n\n  # save the data frame with results to .xlsx file\n  write_xlsx(stats_df, paste0(\"theme_5_exercise/results/\", \n                               verts_names[l], \n                               \".xlsx\"))\n\n}\n</code></pre> <p>Inspect the results.</p> <p> <p></p> <p></p> <p>In the showcased results, we observe data specific to a scenario where the <code>spikeThreshold</code> was set to 0.7. Notably, the maximum F1 statistic achieved was 0.5 for a magnitude threshold of 0.14. Analyzing this further, the LandTrendr algorithm aligned with our validation dataset on 30 occasions, denoting True Positives. However, it also flagged 21 vertices not found in our validation data, indicating False Positives. Additionally, the algorithm missed detecting 39 disturbance events that were indeed present in our validation dataset, resulting in False Negatives. To get a comprehensive understanding, it\u2019s crucial to juxtapose these findings with outcomes from other <code>spikeThreshold</code> and magnitude change threshold combinations.</p> <p>Once you\u2019ve made your comparisons and drawn your conclusions, you can delve into the subsequent task: applying the aforementioned methodologies to NDMI.</p> <p>TASK: Compare the results of LandTrendr results thresholding for NBR and NDMI indices. Repeat the steps shown above for NDMI. Start with replacing the input data rasters. Then follow the rest of the workflow to produce a set of accuracy tables</p> <pre><code># List vertices raster paths\nverts_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                            pattern='ndmi_ver', \n                            full.names = TRUE)\n# List fitted values raster paths\nfv_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                         pattern='ndmi_fv', \n                         full.names = TRUE)\n\n# Get names of rasters \nverts_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                          pattern='ndmi_ver')\nfv_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", \n                       pattern='ndmi_ver')\n</code></pre> <p>The comparable result (<code>spikeThreshold</code> equal to 0.7) for NDMI should look like this:</p> <p> <p></p> <p></p> <p>Inspect other results and compare them to NBR achieved statistics.</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#optional-additional-tasks","title":"Optional additional tasks","text":""},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#multiple-parameter-testing","title":"Multiple parameter testing","text":"<p>As mentioned before you can test multiple LandTrendr parameter values to compare the results. This code below shows an example on how to implement more loops in the beggining of the Google Earth Engine code.</p> <pre><code>\nvar maxSegments=[6, 7, 8];\nvar spikeThreshold=[0.75, 0.8, 0.85];\nvar vertexCountOvershoot=[3, 4, 5];\nvar recoveryThreshold=[0.75, 0.8];\nvar pValueTreshold=[0.05, 0.1];\nvar bestModelProportion=[0.75, 0.8];\nvar minObservationsNeeded=[6];\n\nfor(var x = 0 ; x&lt;maxSegments.length; x++){\n    for (var y = 0 ;y&lt;spikeThreshold.length; y++){\n        for (var z = 0;z&lt;vertexCountOvershoot.length; z++){\n            for (var a = 0 ;a&lt;recoveryThreshold.length;a++){\n                for (var b = 0; b&lt;pValueTreshold.length;b++){\n                    for (var c = 0; c&lt;bestModelProportion.length;c++){\n                        for (var d = 0; d&lt;minObservationsNeeded.length;d++){\n\n                          var runParams = {\n                            timeSeries: processedCollection,\n                            maxSegments:            maxSegments[x],\n                            spikeThreshold:         spikeThreshold[y],\n                            vertexCountOvershoot:   vertexCountOvershoot[z],\n                            preventOneYearRecovery: true,\n                            recoveryThreshold:      recoveryThreshold[a],\n                            pvalThreshold:          pValueTreshold[b],\n                            bestModelProportion:    bestModelProportion[c],\n                            minObservationsNeeded:  minObservationsNeeded[d],\n                          };\n\n                          // ...\n                          // ... the rest of the code\n                          // ...\n\n</code></pre>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#landtrendr-ui-web-application","title":"LandTrendr UI web application","text":"<p>You can check how the application and algorithm works using UI LandTrendr Change Mapper (direct link to the GEEapp here: https://emaprlab.users.earthengine.app/view/lt-gee-change-mapper).</p> <p>Play around with different settings and parameters. This app uses LandTrendr based method of preparing multitemporal input datasets (medoid compositing). Indices values are also multiplied by 1000, so equivalent of magnitude filter 0.14 in our case would be 140 in the GEE app.</p> <p>This is the result you should see when running the app in Tatra Mountains using parameters we used in our exercise. The map presents the newest disturbance vertex with magnitude of over 140.</p> <p> <p></p> <p></p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#data-and-software-credits","title":"Data and software credits","text":""},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#data_1","title":"Data","text":"<p>Landsat 4, 5, 7, 8 and 9 imagery courtesy of the U.S. Geological Survey/ Terms of use processed in and downloaded from Google Earth Engine by Gorelick et al., 2017</p>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#software","title":"Software","text":"<ul> <li>QGIS Development Team (2022). QGIS Geographic Information System. Open Source Geospatial Foundation Project. http://qgis.osgeo.org</li> <li>R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.</li> <li>Hijmans R (2023). terra: Spatial Data Analysis. R package version 1.7-39, https://CRAN.R-project.org/package=terra</li> <li>Kennedy, R.E., Yang, Z., Gorelick, N., Braaten, J., Cavalcante, L., Cohen, W.B., Healey, S. (2018). Implementation of the LandTrendr Algorithm on Google Earth Engine. Remote Sensing. 10, 691.https://doi.org/10.3390/rs10050691</li> <li>Ooms J (2023). writexl: Export Data Frames to Excel \u2018xlsx\u2019 Format. R package version 1.4.2, https://CRAN.R-project.org/package=writexl.</li> <li>Pebesma, E., &amp; Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016</li> <li>Pebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009</li> <li>Wickham H, Fran\u00e7ois R, Henry L, M\u00fcller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr</li> <li>Wickham H, Bryan J (2023). readxl: Read Excel Files. https://readxl.tidyverse.org, https://github.com/tidyverse/readxl.</li> </ul>"},{"location":"module2/05_vegetation_monitoring/05_vegetation_monitoring_exercise.html#source-code","title":"Source code","text":"You can find the entire code used in this exercise here   **[Google Earth Engine code](https://code.earthengine.google.com/c26bdc73c893b317e24f2158e66602a2?noload=true)**  **R code**   <pre><code>\nlibrary(terra) # raster I/o\nlibrary(sf) # vector I/0\nlibrary(readxl) # reading .xlsx files\nlibrary(writexl) # writing.xlsx files\nlibrary(dplyr) # tabular data manipulation\n\n\n# List vertices raster paths\nverts_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='nbr_ver', full.names = TRUE)\n# List fitted values raster paths\nfv_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='nbr_fv', full.names = TRUE)\n\n# Get names of rasters \nverts_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='nbr_ver')\nfv_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='nbr_ver')\n\n# Read vertices and fitted values rasters to lists\nverts_rasters_list &lt;- lapply(verts_rasters, rast)\nfv_rasters_list &lt;- lapply(fv_rasters, rast)\n\n# Rename lists elements\nnames(verts_rasters_list) &lt;- verts_names\nnames(fv_rasters_list) &lt;- fv_names\n\n\n# Read validation points\nval_points &lt;- st_read(\"theme_5_exercise/data_exercise/T5_points.shp\")\n\n# Empty list for storing the results\nmagnitudes_list &lt;- list()\n\n# Empty list for storing the results\nmagnitudes_list &lt;- list()\n\n# For each par of vertices-fitted values images\nfor (r in seq(1:length(verts_rasters_list))) {\n\n  # Read appropriate rasters\n  vertices &lt;- verts_rasters_list[[r]]\n  fitted_values &lt;- fv_rasters_list[[r]]\n\n\n  # Extract values for 100 validation points\n  vert_pts &lt;- extract(vertices, val_points) %&gt;%\n    select(2:40)\n  vals_pts &lt;- extract(fitted_values, val_points) %&gt;%\n    select(2:40)\n\n\n  # Prepare empty output matrix\n  output &lt;- matrix(data = NA, nrow = 100, ncol = 39)\n\n  # For each row(point)\n  for (row in 1:nrow(vert_pts)) {\n\n    # Get only pixel values for that row(point)\n    vertices_pixel &lt;- vert_pts[row, ]\n    fitted_values_pixel &lt;- vals_pts[row, ]\n\n    # Check for the occurence of vertices\n    vertex_indices &lt;- which(vertices_pixel == 1)\n\n    # Prepare empty vector for magnitude calculation results\n    magnitude_of_change &lt;- rep(NA, length(vertices_pixel))\n\n    # Calculate magnitudes\n    for (i in 1:(length(vertex_indices) - 1)) {\n\n      start_index &lt;- vertex_indices[i]\n      end_index &lt;- vertex_indices[i + 1]\n\n      difference &lt;- fitted_values_pixel[1, end_index] - fitted_values_pixel[1, start_index]\n      magnitude_of_change[end_index] &lt;- difference\n    }\n\n    # Insert magnitude values to the matching row in empty output matrix\n    output[row,] &lt;- magnitude_of_change\n    # Insert temporary 0 value to the first column\n    output[row, 1] &lt;- 0\n\n\n    # Check the occurrences of magnitude \n    mag_indices &lt;- which(!is.na(output[row, ]))\n\n    # Prepare empty vector for the moved values\n    moved_row &lt;- rep(NA, 39)\n\n    # Move magnitude values one vertex position backwards\n    for (i in 2:length(mag_indices)) {\n\n      val &lt;- output[row, mag_indices[i]]\n      moved_row[mag_indices[i-1]] &lt;- val\n\n    }\n\n    # Replace the row in output with a moved row\n    output[row,] &lt;- moved_row\n\n\n  }\n\n  # Copy output values\n  output_mask_loss &lt;- output\n\n  # Retain only positive magnitudes indicating vegetation loss\n  output_mask_loss[output_mask_loss &lt; 0] &lt;- NA\n\n  # Add the magnitude matrix to list storing the results\n  magnitudes_list[[r]] &lt;- output_mask_loss\n\n}\n\n\n# Magnitude thresholds\nthresholds &lt;- seq(0, 0.8, by = 0.01)\n\n\n# Function to apply magnitude threshold\napply_magnitude_threshold &lt;- function(mag_mat, threshold) {\n\n  output_mask_loss_th &lt;- mag_mat\n\n  # mask vertices below threshold\n  output_mask_loss_th[output_mask_loss_th &lt; threshold] &lt;- NA\n\n  # no change  - NAs to 0\n  output_mask_loss_th[is.na(output_mask_loss_th)] &lt;- 0\n\n  # change/vertex - above 0 to 1\n  output_mask_loss_th[output_mask_loss_th &gt; 0] &lt;- 1\n\n  # Shift vertices so that they represent year of detection\n  output_mask_loss_shift &lt;- output_mask_loss_th[, 1:38]\n  colnames(output_mask_loss_shift) &lt;- c(1985:2022)\n\n  return(output_mask_loss_shift)\n\n}\n\n# Prepare list of threshold scenario names\nnames_list &lt;- paste0(\"Threshold_\", gsub(\"\\\\.\", \"_\", abs(thresholds))) # replace \".\" with \"_\"\n\n# Prepare a function to calculate statistics\ncalculate_confusion_matrix_stats &lt;- function(matrix, validation_matrix, set_name) {\n\n  # Calculate TP, FP, TN, FN\n  TP &lt;- sum(matrix == 1 &amp; validation_matrix == 1)\n  FP &lt;- sum(matrix == 1 &amp; validation_matrix == 0)\n  TN &lt;- sum(matrix == 0 &amp; validation_matrix == 0)\n  FN &lt;- sum(matrix == 0 &amp; validation_matrix == 1)\n\n  # Calculate the metrics\n  Accuracy &lt;- (TP + TN) / (TP + FP + FN + TN)\n  Precision &lt;- TP / (TP + FP)\n  Sensitivity &lt;- TP / (TP + FN)\n  Specificity &lt;- TN / (TN + FP)\n  F1_Score &lt;- 2 * (Sensitivity * Precision) / (Sensitivity  + Precision)\n\n  # Create a data frame with the confusion matrix stats\n  stats &lt;- data.frame(Set = set_name, TP, FP, TN, FN, Accuracy, Precision, Sensitivity, Specificity, F1_Score)\n\n  return(stats)\n}\n\n# Read validation table\nval_table &lt;- read_xlsx(\"theme_5_exercise/data_exercise/T5_validation_table.xlsx\") %&gt;%\n  select(2:39)\n\n# Prepare  a storage for results\nresults_list &lt;- list()\n\n# For each matrix in magnitudes_list\nfor (l in seq(1, length(magnitudes_list))) {\n\n  # Calculate matrices with each threshold applied\n  thresholds_results &lt;- lapply(thresholds, apply_magnitude_threshold, mag_mat = magnitudes_list[[l]])\n  thresholds_results &lt;- setNames(thresholds_results, names_list)\n\n  # Calculate\n  stats_list &lt;- lapply(seq_along(thresholds_results), function(i) calculate_confusion_matrix_stats(thresholds_results[[i]], val_table, names(thresholds_results)[i]))\n\n  # one table with confusion results\n  stats_df &lt;- do.call(rbind, stats_list)\n\n  results_list[[l]] &lt;- stats_df\n\n  write_xlsx(stats_df, paste0(\"theme_5_exercise/results/\", verts_names[l], \".xlsx\"))\n\n}\n\n\n### NDMI\n\n# List vertices raster paths\nverts_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='ndmi_ver', full.names = TRUE)\n# List fitted values raster paths\nfv_rasters &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='ndmi_fv', full.names = TRUE)\n\n# Get names of rasters \nverts_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='ndmi_ver')\nfv_names &lt;- list.files(path=\"theme_5_exercise/data_exercise\", pattern='ndmi_ver')\n\n\n# Read vertices and fitted values rasters to lists\nverts_rasters_list &lt;- lapply(verts_rasters, rast)\nfv_rasters_list &lt;- lapply(fv_rasters, rast)\n\n# Rename lists elements\nnames(verts_rasters_list) &lt;- verts_names\nnames(fv_rasters_list) &lt;- fv_names\n\n\n\n# Empty list for storing the results\nmagnitudes_list &lt;- list()\n\n# For each par of vertices-fitted values images\nfor (r in seq(1:length(verts_rasters_list))) {\n\n  # Read appropriate rasters\n  vertices &lt;- verts_rasters_list[[r]]\n  fitted_values &lt;- fv_rasters_list[[r]]\n\n\n  # Extract values for 100 validation points\n  vert_pts &lt;- extract(vertices, val_points) %&gt;%\n    select(2:40)\n  vals_pts &lt;- extract(fitted_values, val_points) %&gt;%\n    select(2:40)\n\n\n  # Prepare empty output matrix\n  output &lt;- matrix(data = NA, nrow = 100, ncol = 39)\n\n  # For each row(point)\n  for (row in 1:nrow(vert_pts)) {\n\n    # Get only pixel values for that row(point)\n    vertices_pixel &lt;- vert_pts[row, ]\n    fitted_values_pixel &lt;- vals_pts[row, ]\n\n    # Check for the occurence of vertices\n    vertex_indices &lt;- which(vertices_pixel == 1)\n\n    # Prepare empty vector for magnitude calculation results\n    magnitude_of_change &lt;- rep(NA, length(vertices_pixel))\n\n    # Calculate magnitudes\n    for (i in 1:(length(vertex_indices) - 1)) {\n\n      start_index &lt;- vertex_indices[i]\n      end_index &lt;- vertex_indices[i + 1]\n\n      difference &lt;- fitted_values_pixel[1, end_index] - fitted_values_pixel[1, start_index]\n      magnitude_of_change[end_index] &lt;- difference\n    }\n\n    # Insert magnitude values to the matching row in empty output matrix\n    output[row,] &lt;- magnitude_of_change\n    # Insert temporary 0 value to the first column\n    output[row, 1] &lt;- 0\n\n\n    # Check the occurrences of magnitude \n    mag_indices &lt;- which(!is.na(output[row, ]))\n\n    # Prepare empty vector for the moved values\n    moved_row &lt;- rep(NA, 39)\n\n    # Move magnitude values one vertex position backwards\n    for (i in 2:length(mag_indices)) {\n\n      val &lt;- output[row, mag_indices[i]]\n      moved_row[mag_indices[i-1]] &lt;- val\n\n    }\n\n    # Replace the row in output with a moved row\n    output[row,] &lt;- moved_row\n\n\n  }\n\n  # Copy output values\n  output_mask_loss &lt;- output\n\n  # Retain only positive magnitudes indicating vegetation loss\n  output_mask_loss[output_mask_loss &lt; 0] &lt;- NA\n\n  # Add the magnitude matrix to list storing the results\n  magnitudes_list[[r]] &lt;- output_mask_loss\n\n}\n\n\n# Prepare  a storage for results\nresults_list &lt;- list()\n\n# For each matrix in magnitudes_list\nfor (l in seq(1, length(magnitudes_list))) {\n\n  # Calculate matrices with each threshold applied\n  thresholds_results &lt;- lapply(thresholds, apply_magnitude_threshold, mag_mat = magnitudes_list[[l]])\n  thresholds_results &lt;- setNames(thresholds_results, names_list)\n\n  # Calculate\n  stats_list &lt;- lapply(seq_along(thresholds_results), function(i) calculate_confusion_matrix_stats(thresholds_results[[i]], val_table, names(thresholds_results)[i]))\n\n  # one table with confusion results\n  stats_df &lt;- do.call(rbind, stats_list)\n\n  results_list[[l]] &lt;- stats_df\n\n  write_xlsx(stats_df, paste0(\"theme_5_exercise/results/\", verts_names[l], \".xlsx\"))\n\n}\n</code></pre>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html","title":"Case study: Monitoring tundra grasslands (Karkonosze/Krkono\u0161e)","text":"<p>In this case study we will focus on classifying non-forest tundra vegetation in higher parts of Karkonosze/Krkono\u0161e Mountains, laying on the Polish-Czech border. The process will consist of extracting reference data based on field inventory from multitemporal Sentinel-2 imagery and classifying it with the Support Vector Machine algorithm. Additional analysis will consist of feature importance assessment performed for grasslands class as the main object of this case study.</p> <p>This exercise we\u2019ll be conducted with R and RStudio. If you haven\u2019t already done this, refer to this tutorial regarding the environment setup: R environment setup tutorial.</p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#study-area-and-data","title":"Study area and data","text":"<p>Before you continue, get familiar with the use case: Land cover monitoring in Karkonosze/Krkono\u0161e Mountains (Poland/Czechia).</p> <p>\u2026 and read the paper accompanying the use case and case study:</p> <p>Wakuli\u0144ska, M., &amp; Marcinkowska-Ochtyra, A. (2020). Multi-temporal sentinel-2 data in classification of mountain vegetation. Remote Sensing, 12(17), 2696. https://doi.org/10.3390/rs12172696</p> <p>Download exercise data provided through Zenodo.</p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#imagery-data","title":"Imagery data","text":"<p>In the case study Sentinel\u20102 satellite images with no/minimal cloud coverage from 7 terms of 2018 and 2019 will be used. The dates were chosen to take into account the variability of alpine and subalpine grasslands during one growing season. The limitation was the high occurrence of clouds during the chosen period. The data with 2A processing level were selected. The processing pipeline for preparing data for this Case Study is showcased in Theme 3 exercise processing pipeline 3.</p> <p>The reference data came mainly from the field, as these are not simple land cover classes but more complex ones, botanical information was required. The designated classes included: grasslands (G; of special importance), deciduous shrubs vegetation (DSV), bogs and fens (BF), subalpine tall-forbs (STF), rocks and scree vegetation (RSV), heathlands (H), subalpine dwarf pine scrubs (SDPS) and forests (F).</p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#environment-preparation-loading-required-libraries-and-data","title":"Environment preparation: loading required libraries and data","text":"<p>To start with, we want to load necessary libraries and data and set up some initial variables, which we will use further down the line.</p> <p>Firstly, load required libraries into the environment: <code>terra</code>, <code>sf</code>, <code>dplyr</code>, <code>caret</code>, <code>kernlab</code> and <code>ggsci</code>. Functions included in these packages will be used in the latter parts of this case study.</p> <pre><code># raster I/o and processing\nlibrary(terra) \n\n# vector I/O and processing\nlibrary(sf)\n\n# tabular data manipulation\nlibrary(dplyr) \n\n# model tuning and training\nlibrary(caret) \n\n# applying model\nlibrary(kernlab) \n\n# visualization colour palettes\nlibrary(ggsci) \n</code></pre> <p>We can move to loading up the required data. We shall start with the raster data. The imagery used in this case study are Sentinel-2 images collected in 2018 and 2019 within the growing season (exact dates are 14 May, 31 May, 7 August, 27 August, 18 September (2018), 25 June and 30 June (2019)). Data preparation pipeline for this case study was presented in the pipeline 3 in Theme 3 exercise.</p> <p>Note: we have combined the set of all seven scenes into one to select data from it for each of the 3 scenarios. The bands are ordered chronologically, from May 2018 to June 2019.</p> <pre><code># load .tif image of stacked 70 bands (10 bands per 7 acquisition dates)\nimage_data &lt;- rast(\"case_study_1/data_exercise/CS1_image_data.tif\") \n</code></pre> <p>Now add the vector reference data to our workspace. It consists of field collected polygons with appropriate classes assigned (including grasslands, which are the most interesting from this case study point of view) , which will be used to classify multitemporal Sentinel-2 images from 2018 and 2019. Load <code>.shp</code> file to the <code>reference_data</code> variable.</p> <pre><code># load reference .shp dataset containing reference polygons\n# with class names and acronyms\nreference_data &lt;- st_read(\"case_study_1/data_exercise/CS1_reference_data.shp\") \n</code></pre>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#reference-values-extraction","title":"Reference values extraction","text":"<p>The two main required data sources are now loaded into the workspace, so we can proceed with preparing data for classification. First of all, we need to extract pixel values from loaded images of areas covered by the reference data. In order to do that we will use our own function, which will assign each pixel in each polygon values from spectral bands of Sentinel-2 imagery.</p> <pre><code>extract_pixel_values &lt;- function(image, vector_reference, class_field_name){\n\n  # number of polygon used to extract\n  counter &lt;- 1 \n\n  # information about progress printed in the console\n  print(\"Number of polygons done:\") \n\n  # number of polygons in the reference dataset\n  number_of_polygons &lt;- nrow(vector_reference) \n\n  # empty data frame to store the results of extraction\n  extracted_values &lt;- data.frame() \n\n  # which columns stores the class names\n  class_column_index &lt;- which(names(vector_reference) == class_field_name) \n\n  for (polygon_index in seq(number_of_polygons)) {\n\n    selected_polygon &lt;- vector_reference[polygon_index, ]\n    class_name &lt;- selected_polygon$class\n\n    # extract pixel values from selected polygon\n    polygon_pixel_values &lt;- extract(image, selected_polygon, df = TRUE) \n\n    # add class name and polygon index to extracted values\n    polygon_pixel_values &lt;- cbind(polygon_pixel_values[ ,2: ncol(polygon_pixel_values)], \n                                  class_name, \n                                  polygon_index) \n\n    # add the extracted values with additional information to the results data frame                              \n    extracted_values &lt;- rbind(extracted_values, polygon_pixel_values) \n\n    # information about progress printed in the console\n    print(paste0(counter, \"/\", number_of_polygons)) \n\n    counter &lt;- counter + 1 # go to the next polygon\n\n  }\n  return(extracted_values) # function results - data frame with extracted values\n}\n</code></pre> <p>Now we shall use the above function to extract the values with the data from the reference dataset we loaded earlier. This process should take about 3 minutes. You can observe the progress in the console.</p> <pre><code>\n# use the above function to extract pixel values from the reference dataset, \n# set class_field_name as \"class\" (the name of the field containing desired \n# information in the .shp loaded to reference_data variable)\n\npixel_reference &lt;- extract_pixel_values(image_data, reference_data, \"class\") \n</code></pre> <p>In order for the classifier to work properly we only want to retain observations with no NA values.</p> <pre><code>pixel_reference &lt;- na.omit(pixel_reference)\n</code></pre> <p>Save the extracted and filtered values to <code>.RDS</code> file.</p> <pre><code># save pixel reference\nsaveRDS(pixel_reference, file = \"case_study_1/results/pixel_reference.RDS\") \n</code></pre> <p>We now have all the data we need to proceed. We will start with classifying single scenes, then multitemporal classification and end with best features selection classification. Each classification will be performed using the Support Vector Machine algorithm with radial kernel function. Each time we\u2019ll preceed the classifcation with parameters tuning. The scheme of the procedure in the entire case study is presented in the figure below.</p> <p> <p></p> <p>Case study 1 scheme of procedure. </p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#analysis","title":"Analysis","text":""},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#classification-scenario-1-single-scene","title":"Classification scenario 1: single scene","text":"<p>We will start the single scene classification process with extracting only Sentinel-2 bands values from the single scene. In case of the earliest acquired image (14 May 2018) these will be the first 10 columns of the pixel_reference data frame, as well two last columns including class names and polygon indices.</p> <pre><code># select columns with pixel values from the first term of acquisition\n# plus class names and indices\npixel_reference_14May &lt;- pixel_reference %&gt;%\n  select(`2018-05-14_B2`:`2018-05-14_B12`, class_name, polygon_index) \n</code></pre> <p>In order to make our work reproducible we shall set a common seed number.</p> <pre><code>set.seed(1410)\n</code></pre> <p>We can now assess how many pixel samples for every class we acquired during values extraction.</p> <pre><code># show in the console the amount of pixel samples for every class\ntable(pixel_reference_14May$class_name) \n</code></pre> <p>In comparison we can see how many polygons for each class.</p> <pre><code># show in the console the amount of polygons for every class\ntable(reference_data$class) \n</code></pre> <p>In this part we will divide our reference dataset into training and validation parts. We will divide the whole set into 2 parts of equal number of polygons in each of 2 parts. Notice that the number of pixels may be different in both sets due to differences in polygon sizes.</p> <pre><code># randomly pick half of observations based on class field\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) \n\n# select training pixel observations using polygon indexes\ntraining_data &lt;- pixel_reference_14May[pixel_reference_14May$polygon_index %in% train_index, ] \n\n# select validation pixel observations using polygon indexes\nvalidation_data &lt;- pixel_reference_14May[!(pixel_reference_14May$polygon_index %in% train_index), ] \n\n# show in the console the amount of pixel samples in training data\ntable(training_data$class_name) \n\n# show in the console the amount of pixel samples in validation data\ntable(validation_data$class_name) \n</code></pre> <p>Now that we established the training and validation datasets we can perform a step called tuning classification parameters. For determining best parameters we will use tenfold cross validation of results achieved with different sets of predetermined parameters.</p> <pre><code># set 10-fold cross validation to find the best model\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \n\n# test these parameters to find the best combination\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) \n\n# train the model using SVM with radial kernel using selected parameters\nmodel &lt;- train(training_data[, 1:10], \n               training_data$class_name, \n               method = \"svmRadial\", \n               trControl = control, \n               tuneGrid = tune_parameters) \n</code></pre> <p>We can plot the variable importance. Features are sorted descending from the most useful in classification (importance summed for all the classes). For now, we will not dig deeper into this subject. We will return to feature selection and importance analysis in classification scenario 3.</p> <pre><code>varImp(model) # show predictors apparent importance\nplot(varImp(model)) # plot predictors apparent importance\n</code></pre> <p> <p></p> <p>Variable importance. </p> <p>We will now assess the accuracy of the model by applying it to the validation dataset.</p> <pre><code># use the chosen model to predict classes in the validation dataset\npredictions &lt;- predict(model, validation_data[, 1:10]) \n\n# show confusion matrix based on validation results\nconfusion_matrix &lt;- confusionMatrix(predictions, \n                                    as.factor(validation_data$class_name), \n                                    mode = \"everything\") \n\n# show accuracy metrics in the console\nprint(round(confusion_matrix$overall, 3)) \n\n# save confusion matrix\nsaveRDS(confusion_matrix, \n        file = \"case_study_1/results/14May_confusion_matrix.RDS\") \n</code></pre> <p>In this part of the exercise we will use the model to classify the image. In order to do that we no need to extract the bands from the multitemporal data brick, which were acquired on 14 May.</p> <pre><code>bands_14May &lt;- subset(image_data, c(1:10))\n</code></pre> <p>And now we can apply the model to the appropriate 10 bands. This will classify the image into the classes provided earlier with the reference dataset.</p> <p>We will first classify only the spatial subset of the whole image to use it in results comparison.</p> <pre><code>aoi &lt;- vect(\"case_study_1/data_exercise/aoi.shp\")\n\n# crop the whole image to the interesting polygon\nbands_14May_crop &lt;- crop(bands_14May, ext(aoi)) \n\ntry(terra::predict(bands_14May_crop, \n                  model, \n                  filename = \"case_study_1/results/14May_sample_classification_result.tif\", \n                  progress = \"text\", \n                  format = \"GTiff\", \n                  na.rm = TRUE), \n    silent = TRUE)\n</code></pre> <p>You have successfully classified 1 of the 7 images contained in the case study data repository. Your task is now to go over necessary steps to classify the remaining 3 images. In order to speed things up we will provide a looped function of the steps above to automatically produce the results for 6 remaining dates.</p> <p>First prepare two variables to be used inside the loop. The first one indicates bands of different sets (which are numbers of fields in the <code>pixel_reference</code> variable). The second one will be used to name the results.</p> <pre><code>scene_list &lt;- list(c(11:20), c(21:30), c(31:40), c(41:50), c(51:60), c(61:70))\nscene_scenario_name &lt;- c(\"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\")\n</code></pre> <p>The steps presented earlier are repeated with small adjustments to be able to work inside the loop.</p> <pre><code>for (i in seq(length(scene_scenario_name))) {\n\n  scene_index &lt;- scene_list[[i]]\n  scenario_name &lt;- scene_scenario_name[i]\n\n  print(scenario_name)\n\n  scenario_pixel_reference &lt;- pixel_reference[c(scene_index, 71, 72)]\n\n  set.seed(1410)\n\n  train_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE)\n\n  training_data &lt;- scenario_pixel_reference[scenario_pixel_reference$polygon_index %in% train_index, ]\n  validation_data &lt;- scenario_pixel_reference[!(scenario_pixel_reference$polygon_index %in% train_index), ]\n\n  control &lt;- trainControl(method = \"cv\", number = 10)\n\n  tune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95))\n\n  model &lt;- train(training_data[, 1:10], \n                 training_data$class_name, \n                 method = \"svmRadial\", \n                 trControl = control,\n                 tuneGrid = tune_parameters)  \n\n  predictions &lt;- predict(model, validation_data[, 1:10])\n  confusion_matrix &lt;- confusionMatrix(predictions, \n                                      as.factor(validation_data$class_name), \n                                      mode = \"everything\")\n\n  print(round(confusion_matrix$overall, 3)) \n\n  saveRDS(confusion_matrix, \n          file = paste0(\"case_study_1/results/\", \n                        scenario_name, \n                        \"_confusion_matrix.RDS\"))\n\n  scenario_bands &lt;- subset(image_data, c(scene_index))\n\n  scenario_bands_crop &lt;- crop(scenario_bands, ext(aoi))\n\n  try(terra::predict(scenario_bands_crop, \n                     model, \n                     filename = paste0(\"case_study_1/results/\", \n                                        scenario_name, \n                                        \"_sample_classification_result.tiff\"), \n                     progress = \"text\", \n                     format = \"GTiff\", \n                     na.rm = TRUE))\n\n}\n</code></pre> <p>After the loop has completed you should be able to see 4 resulting images and confusion matrices inside your working directory. Let\u2019s compare the results.</p> <p>First, let\u2019s load all the saved confusion matrices.</p> <pre><code>cm_14May &lt;- readRDS(\"case_study_1/results/14May_confusion_matrix.RDS\")\ncm_31May &lt;- readRDS(\"case_study_1/results/31May_confusion_matrix.RDS\")\ncm_7Aug &lt;- readRDS(\"case_study_1/results/7Aug_confusion_matrix.RDS\")\ncm_27Aug &lt;- readRDS(\"case_study_1/results/27Aug_confusion_matrix.RDS\")\ncm_18Sep &lt;- readRDS(\"case_study_1/results/18Sep_confusion_matrix.RDS\")\ncm_25Jun &lt;- readRDS(\"case_study_1/results/25Jun_confusion_matrix.RDS\")\ncm_30Jun &lt;- readRDS(\"case_study_1/results/30Jun_confusion_matrix.RDS\")\n</code></pre> <p>We will set the labels for easier plotting. You can relate acronyms to the full class names by looking at the reference dataset attributes table.</p> <pre><code>labels &lt;- c(\"BF\", \"DSV\", \"F\", \"G\", \"H\", \"NV\", \"RSV\", \"SDPS\", \"STF\")\n</code></pre> <p>From the confusion matrices variables we want to pull F1 metric to compare the results to one another. In order to do this we shall access one of the numbers stored inside the variable (we chose F1 because it combines producer and user accuracies, making the comparison more general, of course you can select and compare other available measures in the confusion matrix).</p> <pre><code>f1_14May &lt;- cm_14May$byClass[, 7]\nf1_31May &lt;- cm_31May$byClass[, 7]\nf1_7Aug &lt;- cm_7Aug$byClass[, 7]\nf1_27Aug &lt;- cm_27Aug$byClass[, 7]\nf1_18Sep &lt;- cm_18Sep$byClass[, 7]\nf1_25Jun &lt;- cm_25Jun$byClass[, 7]\nf1_30Jun &lt;- cm_30Jun$byClass[, 7]\n</code></pre> <p>With that we are able to plot the results. We will show them in 2 ways: by points and bars.</p> <pre><code># set colour palette for all the remaining plots\ncolour_palette &lt;- pal_d3(\"category10\", alpha = 0.7)(9) \n\n\npng(filename = \"case_study_1/results/comp_points.png\", \n    width = 1920, \n    height = 1080, \n    res = 200, \n    pointsize = 9)\n\nplot(f1_14May, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n     ylim = c(0,1), pch = 1, col = colour_palette[1], lwd = 2, las = 2,\n     main = \"Comparison of F1 scores for different terms of data acquisition.\")\n\nabline(h = c(0,0.2,0.4,0.6,0.8,1), lty = 3)\npoints(f1_31May, pch = 2, col = colour_palette[2], lwd = 2)\npoints(f1_7Aug, pch = 3, col = colour_palette[3], lwd = 2)\npoints(f1_27Aug, pch = 4, col = colour_palette[4], lwd = 2)\npoints(f1_18Sep, pch = 5, col = colour_palette[5], lwd = 2)\npoints(f1_25Jun, pch = 7, col = colour_palette[6], lwd = 2)\npoints(f1_30Jun, pch = 8, col = colour_palette[7], lwd = 2)\naxis(1, at = 1:9, labels = labels)\nlegend(\"bottom\", \n       legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\"),\n       col = colour_palette[1:7], pch = c(1, 2, 3, 4, 5, 6, 7), cex = 1.1,\n       title = \"Terms of data acquisition\")\ndev.off()\n</code></pre> <p> <p></p> <p>F1 accuracy for different terms of data acquisition. </p> <pre><code># barplot\nf1_matrix &lt;- matrix(data = c(f1_14May, f1_31May, f1_7Aug, f1_27Aug, f1_18Sep, f1_25Jun, f1_30Jun),  ncol = 9, nrow = 7, byrow = TRUE)\n\npng(filename = \"case_study_1/results/comp_barplot.png\", width = 1920, height = 1080, res = 200, pointsize = 9)\n\npar(mar=c(5, 4, 4, 8), xpd = TRUE)\nbarplot(f1_matrix, beside = TRUE, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n        ylim = c(0, 1), las = 2, col = colour_palette[1:7],\n        main = \"Comparison of F1 scores for different terms of data acquisition.\")\nlegend(\"topright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\"),\n       fill = colour_palette[1:7],  title = \"Terms of\\ndata acquisition\", inset=c(-0.1, 0))\npar(xpd=FALSE)\nabline(h = c(0, 0.2, 0.4, 0.6, 0.8, 1), lty = 3)\naxis(1, at=c(4.5, 12.5, 20.5, 28.5, 36.5, 44.5, 52.5, 60.5, 68.5), labels = labels)\n\ndev.off()\n</code></pre> <p> <p></p> <p>F1 accuracy for different terms of data acquisition. </p> <p>TASK: Assess the results using F1 values shown on the plots. Focus on the grasslands (G) class.</p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#classification-scenario-2-multitemporal-set","title":"Classification scenario 2: multitemporal set","text":"<p>After we classified each individual term we now want to classify the multitemporal set. The steps will be very similar to what we have done previously with single term classification. The main difference will be that we now want to use all 70 bands in model building and classification.</p> <p>TASK: Perform classification scenario consisting of all the available bands. Try to recreate the steps learned in the single term classification. Save the results: confusion matrix as <code>All_terms_confusion_matrix.RDS</code> and image as <code>All_terms_sample_classification_result.tif</code>. In case of any problems you can look below at the solution.</p>  Solution with code.  <pre><code>set.seed(1410)\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) # randomly pick half of observations based on class field\n\ntraining_data &lt;- pixel_reference[pixel_reference$polygon_index %in% train_index, ] # select training pixel observations using polygon indexes\nvalidation_data &lt;- pixel_reference[!(pixel_reference$polygon_index %in% train_index), ] # select validation pixel observations using polygon indexes\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10) # set 10-fold cross validation to find the best model\n\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) # test these parameters to find the best combination\n\nmodel &lt;- train(training_data[, 1:70], training_data$class_name, method = \"svmRadial\", trControl = control, tuneGrid = tune_parameters) # train the model using SVM with radial kernel using selected parameters\n\npredictions &lt;- predict(model, validation_data[, 1:70]) # use the chosen model to predict classes in the validation dataset\nconfusion_matrix &lt;- confusionMatrix(predictions, as.factor(validation_data$class_name), mode = \"everything\") # show confusion matrix based on validation results\nprint(round(confusion_matrix$overall, 3)) # show confusion metrics in the console\nsaveRDS(confusion_matrix, file = \"case_study_1/results/All_terms_confusion_matrix.RDS\") # save confusion matrix\n\n\nimage_data_crop &lt;- crop(image_data, ext(aoi)) # crop the whole image to the interesting polygon\n\ntry(terra::predict(image_data_crop, model, filename = \"case_study_1/results/All_terms_sample_classification_results.tif\", progress = \"text\", format = \"GTiff\", na.rm = TRUE), silent = TRUE)\n</code></pre> <p>Now also compare the results of the multitemporal classification to the single term classification. Add one more result from the confusion matrix to the plots we produced earlier. We will only present the barplot version (but point plots are a valid presentation option as well).</p> <pre><code>cm_all_terms &lt;- readRDS(\"case_study_1/results/All_terms_confusion_matrix.RDS\")\nf1_all_terms &lt;- cm_all_terms$byClass[, 7]\n\n\n# barplot\nf1_matrix &lt;- matrix(data = c(f1_14May, f1_31May, f1_7Aug, f1_27Aug, \n                              f1_18Sep, f1_25Jun, f1_30Jun, f1_all_terms),  \n                              ncol = 9, nrow = 8, byrow = TRUE)\n\npng(filename = \"case_study_1/results/comp2_barplot.png\", \n    width = 1920, height = 1080, res = 200, pointsize = 9)\n\npar(mar=c(5, 4, 4, 8), xpd = TRUE)\nbarplot(f1_matrix, beside = TRUE, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n        ylim = c(0, 1), las = 2, col = colour_palette[1:8],\n        main = \"Comparison of F1 scores for different classification input datasets.\")\n\nlegend(\"topright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \n                              \"18Sep\", \"25Jun\", \"30Jun\", \"All_terms\"),\n       fill = colour_palette[1:8],  \n       title = \"Classification input datasets\", \n       bty = \"n\", \n       xjust = 1, \n       inset=c(-0.1, 0))\n\npar(xpd = FALSE)\nabline(h = c(0, 0.2, 0.4, 0.6, 0.8, 1), lty = 3)\naxis(1, at = c(5, 14, 23, 32, 41, 50, 59, 68, 77), labels = labels)\n\ndev.off()\n</code></pre> <p> <p></p> <p>F1 accuracy for different classification input datasets. </p> <p>TASK: Assess the results using F1 values shown on the plots. Focus on the grasslands (G) class.</p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#classification-scenario-3-selected-features","title":"Classification scenario 3: selected features","text":"<p>In this case study we use the area under ROC (Receiver Operator Characteristic) curves to measure the importance of every input feature. ROC curve is graphical plotting of true positive and false positive rates at different thresholds for binary classification results. See more details about it on classification problems in Serrano et al., 2010. Since we are focused on grasslands we want to discover the best variables for this class, not for the whole classified image (for the second one you can use e.g.\u00a0statistical variable importance analysis in Random Forests classifier).</p> <p> <p></p> <p>ROC curve generated for grasslands class. </p> <p>Most of the classification scenarios will be similar to what we previously showed. One additional step now is best feature selection for classification. We will assess this by looking at the graphs and numbers showing apparent importance. This will allow us to choose which predictors (bands) to use in the final model. For now, let\u2019s go through the necessary steps.</p> <pre><code>\nset.seed(1410)\n\n# randomly pick half of observations based on class field\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) \n\n# select training pixel observations using polygon indexes\ntraining_data &lt;- pixel_reference[pixel_reference$polygon_index %in% train_index, ] \n\n# select validation pixel observations using polygon indexes\nvalidation_data &lt;- pixel_reference[!(pixel_reference$polygon_index %in% train_index), ] \n\n# set 10-fold cross validation to find the best model\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \n\n# test these parameters to find the best combination\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) \n\n# train the model using SVM with radial kernel\nmodel &lt;- train(training_data[, 1:70], \n               training_data$class_name, \n               method = \"svmRadial\", \n               trControl = control, \n               tuneGrid = tune_parameters) \n</code></pre> <p>Now we can see the variable importance values for all the classes. We\u2019ll plot the first 15 features</p> <pre><code># show predictor apparent importance\nfeature_selection &lt;- varImp(model) \n\n# plot predictor apparent importance\nplot(feature_selection, top = 15) \n</code></pre> <p>Plotting many variables for a few classes causes the plot to be almost unreadable. We can fortunately pull the necessary values from the model. This information will now be used to sort the variable importance in descending order using previously mentioned ROC scores for the grasslands class. Predictor names with the highest apparent importance for grasslands classification can be then identified and chosen for the final classification model. In our case we will choose 10 best performing predictors.</p> <pre><code># values of apparent importance for grasslands class\nROC_scores_grasslands &lt;- feature_selection$importance$grasslands \n\n# predictor names for best performing predictors\nnames(ROC_scores_grasslands) &lt;- rownames(feature_selection$importance) \n\n# arrange predictors from best to least performing\nROC_scores_grasslands &lt;- sort(ROC_scores_grasslands, decreasing = TRUE) \n</code></pre> <p>The values could be plotted in many readable ways. For this case study we supply the horizontal barplot, with colours of the bars corresponding to the dates of data acquisition.</p>  Code to prepare this barplot.  <pre><code>colours &lt;- c(colour_palette[6], colour_palette[6], colour_palette[3], colour_palette[2], colour_palette[3], colour_palette[7], colour_palette[2], colour_palette[4], colour_palette[7], colour_palette[4], colour_palette[2], colour_palette[3], colour_palette[6], colour_palette[7], colour_palette[4], colour_palette[5], colour_palette[5], colour_palette[1], colour_palette[1], colour_palette[1], colour_palette[6], colour_palette[5], colour_palette[7], colour_palette[2], colour_palette[6], colour_palette[6], colour_palette[3], colour_palette[2], colour_palette[2], colour_palette[7], colour_palette[1], colour_palette[6], colour_palette[7], colour_palette[2], colour_palette[6])\n\nrev_colours &lt;- rev(colours)\n\nROC_scores_grasslands_increasing &lt;- sort(ROC_scores_grasslands[1:35])\n\npng(filename = \"case_study_1/results/grasslands_importance.png\", width = 1920, height = 1080, res = 200, pointsize = 9)\n\n\nbarplot(ROC_scores_grasslands_increasing, \n        names = names(ROC_scores_grasslands_increasing), \n        horiz = TRUE, \n        main = \"Grasslands class feature importance\",\n        xlab = \"Importance\", \n        las = 1,\n        border = NA,\n        cex.names = 0.5,\n        xlim = c(60, 100),\n        xpd = FALSE,\n        col = rev_colours\n)\n\nlegend(\"bottomright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\"),\n       fill = colour_palette[1:7],  title = \"Terms of data acquisition\")\n\ndev.off()\n</code></pre> <p> <p></p> <p>The most important features for grasslands class. </p> <p>Based on the plot and the values we can assess which predictors would work best in classifying grasslands with the highest accuracy. Now from the <code>pixel_reference</code> variable we are able to pull values of the best 10 performing predictors and use it to classify the image. The overall procedure is similar to the previous steps.</p> <pre><code># pull 10 best performing predictors names\nfirst_10 &lt;- names(ROC_scores_grasslands)[1:10] \n\n# get first most informative predictors\nbest_10_predictors &lt;- pixel_reference[, c(which(names(pixel_reference) %in% first_10), 71, 72)] \n</code></pre> <p>TASK: Use <code>best_10_predictors</code> to perform classification. Divide the dataset into training and validation data, train and apply model, and save confusion matrix as <code>FS_confusion_matrix.RDS</code> and image as <code>FS_sample_classification_result.tif</code> (FS meaning Feature Selection).</p>  Solution with code.  <pre><code>set.seed(1410)\n# randomly pick half of observations based on class field\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) \n\n# select training pixel observations using polygon indexes\ntraining_data &lt;- best_10_predictors[best_10_predictors$polygon_index %in% train_index, ] \n\n# select validation pixel observations using polygon indexes\nvalidation_data &lt;- best_10_predictors[!(best_10_predictors$polygon_index %in% train_index), ] \n\n# set 10-fold cross validation to find the best model\ncontrol &lt;- trainControl(method = \"cv\", number = 10) \n\n# test these parameters to find the best combination\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) \n\n# train the model using SVM with radial kernel using selected parameters\nmodel &lt;- train(training_data[, 1:10], \n               training_data$class_name, \n               method = \"svmRadial\", \n               trControl = control, \n               tuneGrid = tune_parameters) \n\n# use the chosen model to predict classes in the validation dataset\npredictions &lt;- predict(model, validation_data[, 1:10]) \n\n# show confusion matrix based on validation results\nconfusion_matrix &lt;- confusionMatrix(predictions, \n                                    as.factor(validation_data$class_name), \n                                    mode = \"everything\") \n\n# show confusion metrics in the console\nprint(round(confusion_matrix$overall, 3)) \n\n# save confusion matrix\nsaveRDS(confusion_matrix, file = \"case_study_1/results/FS_confusion_matrix.RDS\") \n\n\nscenario_bands &lt;- subset(image_data, first_10)\n\n# crop the whole image to the interesting polygon\nimage_data_crop &lt;- crop(scenario_bands, ext(aoi)) \n\ntry(terra::predict(image_data_crop, \n                  model, \n                  filename = \"case_study_1/results/FS_terms_sample_classification_result.tif\", \n                  progress = \"text\", \n                  format = \"GTiff\", \n                  na.rm = TRUE), \n    silent = TRUE)\n</code></pre> <p>Finally we can compare all the results in one plot.</p> <pre><code>cm_fs &lt;- readRDS(\"case_study_1/results/FS_confusion_matrix.RDS\")\nf1_fs &lt;- cm_fs$byClass[, 7]\n\nf1_matrix &lt;- matrix(data = c(f1_14May, f1_31May, f1_7Aug, f1_27Aug, f1_18Sep, \n                            f1_25Jun, f1_30Jun, f1_all_terms, f1_fs),\n                            ncol = 9, nrow = 9, byrow = TRUE)\n\npng(filename = \"case_study_1/results/comp3_barplot.png\", \n    width = 1920, height = 1080, res = 200, pointsize = 9)\npar(mar=c(5, 4, 4, 9), xpd = TRUE)\nbarplot(f1_matrix, beside = TRUE, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n        ylim = c(0, 1), las = 2, col = colour_palette[1:9],\n        main = \"Comparison of F1 scores for different classification input datasets.\")\nlegend(\"topright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \n                              \"25Jun\", \"30Jun\", \"All_terms\", \"FS\"),\n       fill = colour_palette[1:9],  \n       title = \"Classification input datasets\", \n       bty = \"n\", xjust = 1, inset = c(-0.1, 0))\n\npar(xpd = FALSE)\nabline(h = c(0, 0.2, 0.4, 0.6, 0.8, 1), lty = 3)\naxis(1, at = c(5.5, 15.5, 25.5, 35.5, 45.5, 55.5, 65.5, 75.5, 85.5), \n     labels = labels)\n\ndev.off()\n</code></pre> <p> <p></p> <p>F1 accuracy for different classification input datasets. </p> <p>Display the resulting image in QGIS. Use the attached symbology.clr file to assign class names and colors.</p> <p> <p></p> <p>Predicted image. </p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#discussion","title":"Discussion","text":"<p>Points to discuss: - comparison of F1 results for grasslands class between single term classifications - comparison of F1 results for grasslands class between single term and multitemporal classifications - feature selection analysis: which variables had the most impact on multitemporal classification - time spent on classification vs.\u00a0accuracy (plot)</p> <p> <p></p> <p>Accuracy / time relationship. </p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#references","title":"References","text":"<p>Serrano, A. J., Soria, E., Martin, J. D., Magdalena, R., &amp; Gomez, J. (2010, July). Feature selection using roc curves on classification problems. In The 2010 International Joint Conference on Neural Networks (IJCNN) (pp.\u00a01-6). IEEE. https://doi.org/10.1109/ijcnn.2010.5596692</p> <p>Wakuli\u0144ska, M., &amp; Marcinkowska-Ochtyra, A. (2020). Multi-temporal sentinel-2 data in classification of mountain vegetation. Remote Sensing, 12(17), 2696. https://doi.org/10.3390/rs12172696</p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#other-case-studies","title":"Other case studies","text":"<ul> <li>Forest disturbance detection (Tatras)</li> <li>Effects of pollution (Ore Mountains)</li> </ul>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#module-themes","title":"Module themes","text":"<ul> <li>Principles of multispectral imaging</li> <li>Temporal information in satellite data</li> <li>Image processing workflow</li> <li>Multitemporal classification</li> <li>Vegetation change and disturbance detection</li> </ul>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#data-and-software-credits","title":"Data and software credits","text":""},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#data","title":"Data","text":"<p>Sentinel-2 imagery European Space Agency - ESA/ Terms of use processed in and downloaded from Google Earth Engine by Gorelick et al., 2017</p>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#software","title":"Software","text":"<ul> <li>R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.</li> <li>Hijmans R (2023). terra: Spatial Data Analysis. R package version 1.7-39, https://CRAN.R-project.org/package=terra</li> <li>Karatzoglou A, Smola A, Hornik K (2023). kernlab: Kernel-Based Machine Learning Lab. R package version 0.9-32, https://CRAN.R-project.org/package=kernlab.</li> <li>Karatzoglou A, Smola A, Hornik K, Zeileis A (2004). kernlab - An S4 Package for Kernel Methods in R. Journal of Statistical Software, 11(9), 1-20. doi:10.18637/jss.v011.i09.</li> <li>Kuhn, M. (2008). Building Predictive Models in R Using the caret Package. Journal of Statistical Software, 28(5), 1\u201326. https://doi.org/10.18637/jss.v028.i05</li> <li>Pebesma, E., &amp; Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016</li> <li>Pebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009</li> <li>Wickham H, Fran\u00e7ois R, Henry L, M\u00fcller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr</li> <li>Xiao N (2023). ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for \u2018ggplot2\u2019. R package version 3.0.0, https://CRAN.R-project.org/package=ggsci.</li> </ul>"},{"location":"module2/06_cs_tundra_grasslands/06_cs_tundra_grasslands.html#source-code","title":"Source code","text":"You can find the entire code used in this exercise here  <pre><code>library(terra) # raster I/o and processing\nlibrary(sf) # vector I/O and processing\nlibrary(dplyr) # tabular data manipulation\nlibrary(caret) # model tuning and training\nlibrary(kernlab) # applying model\nlibrary(ggsci) # visualization colour palettes\n\n\nimage_data &lt;- rast(\"case_study_1/data_exercise/CS1_image_data.tif\") # load .tif image of stacked 70 bands (10 bands per 7 acquisition dates)\n\nreference_data &lt;- st_read(\"case_study_1/data_exercise/CS1_reference_data.shp\") # load reference .shp dataset containing reference polygons with class names and acronyms\n\n\nextract_pixel_values &lt;- function(image, vector_reference, class_field_name){\n\n  counter &lt;- 1 # number of polygon used to extract\n  print(\"Number of polygons done:\") # information about progress printed in the console\n\n  number_of_polygons &lt;- nrow(vector_reference) # number of polygons in the reference dataset\n  extracted_values &lt;- data.frame() # empty data frame to store the results of extraction\n\n  class_column_index &lt;- which(names(vector_reference) == class_field_name) # which columns stores the class names\n\n  for (polygon_index in seq(number_of_polygons)) {\n\n    selected_polygon &lt;- vector_reference[polygon_index, ]\n    class_name &lt;- selected_polygon$class\n    polygon_pixel_values &lt;- extract(image, selected_polygon, df = TRUE) # extract pixel values from selected polygon\n    polygon_pixel_values &lt;- cbind(polygon_pixel_values[ ,2: ncol(polygon_pixel_values)], class_name, polygon_index) # add class name and polygon index to extracted values\n    extracted_values &lt;- rbind(extracted_values, polygon_pixel_values) # add the extracted values with additional information to the results data frame\n\n    print(paste0(counter, \"/\", number_of_polygons)) # information about progress printed in the console\n    counter &lt;- counter + 1 # go to the next polygon\n\n  }\n  return(extracted_values) # function results - data frame with extracted values\n}\n\n\npixel_reference &lt;- extract_pixel_values(image_data, reference_data, \"class\") # use the above function to extract pixel values from the reference dataset, set class_field_name as \"class\" (the name of the field containing desired information in the .shp loaded to reference_data variable)\n\npixel_reference &lt;- na.omit(pixel_reference)\n\n\nsaveRDS(pixel_reference, file = \"case_study_1/results/pixel_reference.RDS\") # save pixel reference\n\npixel_reference_14May &lt;- pixel_reference %&gt;%\n  select(`2018-05-14_B2`:`2018-05-14_B12`, class_name, polygon_index) # select columns with pixel values from the first term of acquisition plus class names and indices\n\nset.seed(1410)\n\ntable(pixel_reference_14May$class_name) # show in the console the amount of pixel samples for every class\n\ntable(reference_data$class) # show in the console the amount of polygons for every class\n\n\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) # randomly pick half of observations based on class field\ntraining_data &lt;- pixel_reference_14May[pixel_reference_14May$polygon_index %in% train_index, ] # select training pixel observations using polygon indexes\nvalidation_data &lt;- pixel_reference_14May[!(pixel_reference_14May$polygon_index %in% train_index), ] # select validation pixel observations using polygon indexes\n\ntable(training_data$class_name) # show in the console the amount of pixel samples in training data\ntable(validation_data$class_name) # show in the console the amount of pixel samples in validation data\n\n\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10) # set 10-fold cross validation to find the best model\n\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) # test these parameters to find the best combination\n\nmodel &lt;- train(training_data[, 1:10], training_data$class_name, method = \"svmRadial\", trControl = control, tuneGrid = tune_parameters) # train the model using SVM with radial kernel using selected parameters\n\nvarImp(model) # show predictors apparent importance\nplot(varImp(model)) # plot predictors apparent importance\n\n\n\npredictions &lt;- predict(model, validation_data[, 1:10]) # use the chosen model to predict classes in the validation dataset\nconfusion_matrix &lt;- confusionMatrix(predictions, as.factor(validation_data$class_name), mode = \"everything\") # show confusion matrix based on validation results\nprint(round(confusion_matrix$overall, 3)) # show accuracy metrics in the console\nsaveRDS(confusion_matrix, file = \"case_study_1/results/14May_confusion_matrix.RDS\") # save confusion matrix\n\n\n\nbands_14May &lt;- subset(image_data, c(1:10))\n\naoi &lt;- vect(\"case_study_1/data_exercise/aoi.shp\")\n\nbands_14May_crop &lt;- crop(bands_14May, ext(aoi)) # crop the whole image to the interesting polygon\n\ntry(terra::predict(bands_14May_crop, model, filename = \"case_study_1/results/14May_sample_classification_result.tif\", progress = \"text\", format = \"GTiff\", na.rm = TRUE), silent = TRUE)\n\n\nscene_list &lt;- list(c(11:20), c(21:30), c(31:40), c(41:50), c(51:60), c(61:70))\nscene_scenario_name &lt;- c(\"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\")\n\n\nfor (i in seq(length(scene_scenario_name))) {\n\n  scene_index &lt;- scene_list[[i]]\n  scenario_name &lt;- scene_scenario_name[i]\n\n  print(scenario_name)\n\n  scenario_pixel_reference &lt;- pixel_reference[c(scene_index, 71, 72)]\n\n  set.seed(1410)\n\n  train_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE)\n\n  training_data &lt;- scenario_pixel_reference[scenario_pixel_reference$polygon_index %in% train_index, ]\n  validation_data &lt;- scenario_pixel_reference[!(scenario_pixel_reference$polygon_index %in% train_index), ]\n\n  control &lt;- trainControl(method = \"cv\", number = 10)\n\n  tune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95))\n\n  model &lt;- train(training_data[, 1:10], training_data$class_name, method = \"svmRadial\", trControl = control,\n                 tuneGrid = tune_parameters)  \n\n  predictions &lt;- predict(model, validation_data[, 1:10])\n  confusion_matrix &lt;- confusionMatrix(predictions, as.factor(validation_data$class_name), mode = \"everything\")\n\n  print(round(confusion_matrix$overall, 3)) \n\n  saveRDS(confusion_matrix, file = paste0(\"case_study_1/results/\", scenario_name, \"_confusion_matrix.RDS\"))\n\n  scenario_bands &lt;- subset(image_data, c(scene_index))\n\n  scenario_bands_crop &lt;- crop(scenario_bands, ext(aoi))\n\n  try(terra::predict(scenario_bands_crop, model, filename = paste0(\"case_study_1/results/\", scenario_name, \"_sample_classification_result.tiff\"), progress = \"text\", format = \"GTiff\", na.rm = TRUE))\n\n}\n\n\ncm_14May &lt;- readRDS(\"case_study_1/results/14May_confusion_matrix.RDS\")\ncm_31May &lt;- readRDS(\"case_study_1/results/31May_confusion_matrix.RDS\")\ncm_7Aug &lt;- readRDS(\"case_study_1/results/7Aug_confusion_matrix.RDS\")\ncm_27Aug &lt;- readRDS(\"case_study_1/results/27Aug_confusion_matrix.RDS\")\ncm_18Sep &lt;- readRDS(\"case_study_1/results/18Sep_confusion_matrix.RDS\")\ncm_25Jun &lt;- readRDS(\"case_study_1/results/25Jun_confusion_matrix.RDS\")\ncm_30Jun &lt;- readRDS(\"case_study_1/results/30Jun_confusion_matrix.RDS\")\n\n\nlabels &lt;- c(\"BF\", \"DSV\", \"F\", \"G\", \"H\", \"NV\", \"RSV\", \"SDPS\", \"STF\")\n\nf1_14May &lt;- cm_14May$byClass[, 7]\nf1_31May &lt;- cm_31May$byClass[, 7]\nf1_7Aug &lt;- cm_7Aug$byClass[, 7]\nf1_27Aug &lt;- cm_27Aug$byClass[, 7]\nf1_18Sep &lt;- cm_18Sep$byClass[, 7]\nf1_25Jun &lt;- cm_25Jun$byClass[, 7]\nf1_30Jun &lt;- cm_30Jun$byClass[, 7]\n\n\ncolour_palette &lt;- pal_d3(\"category10\", alpha = 0.7)(9) # set colour palette for all the remaining plots\n\npng(filename = \"case_study_1/results/comp_points.png\", width = 1920, height = 1080, res = 200, pointsize = 9)\n\nplot(f1_14May, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n     ylim = c(0,1), pch = 1, col = colour_palette[1], lwd = 2, las = 2,\n     main = \"Comparison of F1 scores for different terms of data acquisition.\")\nabline(h = c(0,0.2,0.4,0.6,0.8,1), lty = 3)\npoints(f1_31May, pch = 2, col = colour_palette[2], lwd = 2)\npoints(f1_7Aug, pch = 3, col = colour_palette[3], lwd = 2)\npoints(f1_27Aug, pch = 4, col = colour_palette[4], lwd = 2)\npoints(f1_18Sep, pch = 5, col = colour_palette[5], lwd = 2)\npoints(f1_25Jun, pch = 7, col = colour_palette[6], lwd = 2)\npoints(f1_30Jun, pch = 8, col = colour_palette[7], lwd = 2)\naxis(1, at = 1:9, labels = labels)\nlegend(\"bottom\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\"),\n       col = colour_palette[1:7], pch = c(1, 2, 3, 4, 5, 6, 7), cex = 1.1,\n       title = \"Terms of data acquisition\")\ndev.off()\n\n\n\n\n# barplot\nf1_matrix &lt;- matrix(data = c(f1_14May, f1_31May, f1_7Aug, f1_27Aug, f1_18Sep, f1_25Jun, f1_30Jun),  ncol = 9, nrow = 7, byrow = TRUE)\n\npng(filename = \"case_study_1/results/comp_barplot.png\", width = 1920, height = 1080, res = 200, pointsize = 9)\n\npar(mar=c(5, 4, 4, 8), xpd = TRUE)\nbarplot(f1_matrix, beside = TRUE, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n        ylim = c(0, 1), las = 2, col = colour_palette[1:7],\n        main = \"Comparison of F1 scores for different terms of data acquisition.\")\nlegend(\"topright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\"),\n       fill = colour_palette[1:7],  title = \"Terms of\\ndata acquisition\", inset=c(-0.1, 0))\npar(xpd=FALSE)\nabline(h = c(0, 0.2, 0.4, 0.6, 0.8, 1), lty = 3)\naxis(1, at=c(4.5, 12.5, 20.5, 28.5, 36.5, 44.5, 52.5, 60.5, 68.5), labels = labels)\n\ndev.off()\n\n\n\n\nset.seed(1410)\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) # randomly pick half of observations based on class field\n\ntraining_data &lt;- pixel_reference[pixel_reference$polygon_index %in% train_index, ] # select training pixel observations using polygon indexes\nvalidation_data &lt;- pixel_reference[!(pixel_reference$polygon_index %in% train_index), ] # select validation pixel observations using polygon indexes\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10) # set 10-fold cross validation to find the best model\n\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) # test these parameters to find the best combination\n\nmodel &lt;- train(training_data[, 1:70], training_data$class_name, method = \"svmRadial\", trControl = control, tuneGrid = tune_parameters) # train the model using SVM with radial kernel using selected parameters\n\npredictions &lt;- predict(model, validation_data[, 1:70]) # use the chosen model to predict classes in the validation dataset\nconfusion_matrix &lt;- confusionMatrix(predictions, as.factor(validation_data$class_name), mode = \"everything\") # show confusion matrix based on validation results\nprint(round(confusion_matrix$overall, 3)) # show confusion metrics in the console\nsaveRDS(confusion_matrix, file = \"case_study_1/results/All_terms_confusion_matrix.RDS\") # save confusion matrix\n\n\nimage_data_crop &lt;- crop(image_data, ext(aoi)) # crop the whole image to the interesting polygon\n\ntry(terra::predict(image_data_crop, model, filename = \"case_study_1/results/All_terms_sample_classification_results.tif\", progress = \"text\", format = \"GTiff\", na.rm = TRUE), silent = TRUE)\n\n\n\ncm_all_terms &lt;- readRDS(\"case_study_1/results/All_terms_confusion_matrix.RDS\")\nf1_all_terms &lt;- cm_all_terms$byClass[, 7]\n\n\n# barplot\nf1_matrix &lt;- matrix(data = c(f1_14May, f1_31May, f1_7Aug, f1_27Aug, f1_18Sep, f1_25Jun, f1_30Jun, f1_all_terms),  ncol = 9, nrow = 8, byrow = TRUE)\n\npng(filename = \"case_study_1/results/comp2_barplot.png\", width = 1920, height = 1080, res = 200, pointsize = 9)\n\npar(mar=c(5, 4, 4, 8), xpd = TRUE)\nbarplot(f1_matrix, beside = TRUE, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n        ylim = c(0, 1), las = 2, col = colour_palette[1:8],\n        main = \"Comparison of F1 scores for different classification input datasets.\")\n\nlegend(\"topright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\", \"All_terms\"),\n       fill = colour_palette[1:8],  title = \"Classification input datasets\", bty = \"n\", xjust = 1 , inset=c(-0.1, 0))\npar(xpd = FALSE)\nabline(h = c(0, 0.2, 0.4, 0.6, 0.8, 1), lty = 3)\naxis(1, at = c(5, 14, 23, 32, 41, 50, 59, 68, 77), labels = labels)\n\ndev.off()\n\n\n\n\n\nset.seed(1410)\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) # randomly pick half of observations based on class field\n\ntraining_data &lt;- pixel_reference[pixel_reference$polygon_index %in% train_index, ] # select training pixel observations using polygon indexes\nvalidation_data &lt;- pixel_reference[!(pixel_reference$polygon_index %in% train_index), ] # select validation pixel observations using polygon indexes\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10) # set 10-fold cross validation to find the best model\n\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) # test these parameters to find the best combination\n\nmodel &lt;- train(training_data[, 1:70], training_data$class_name, method = \"svmRadial\", trControl = control, tuneGrid = tune_parameters) # train the model using SVM with radial kernel\n\n\n\nfeature_selection &lt;- varImp(model) # show predictor apparent importance\nplot(feature_selection, top = 15) # plot predictor apparent importance\n\n\nROC_scores_grasslands &lt;- feature_selection$importance$grasslands # values of apparent importance for grasslands class\nnames(ROC_scores_grasslands) &lt;- rownames(feature_selection$importance) # predictor names for best performing predictors\nROC_scores_grasslands &lt;- sort(ROC_scores_grasslands, decreasing = TRUE) # arrange predictors from best to least performing\n\n# Feature Selection barplot\n\ncolours &lt;- c(colour_palette[6], colour_palette[6], colour_palette[3], colour_palette[2], colour_palette[3], colour_palette[7], colour_palette[2], colour_palette[4], colour_palette[7], colour_palette[4], colour_palette[2], colour_palette[3], colour_palette[6], colour_palette[7], colour_palette[4], colour_palette[5], colour_palette[5], colour_palette[1], colour_palette[1], colour_palette[1], colour_palette[6], colour_palette[5], colour_palette[7], colour_palette[2], colour_palette[6], colour_palette[6], colour_palette[3], colour_palette[2], colour_palette[2], colour_palette[7], colour_palette[1], colour_palette[6], colour_palette[7], colour_palette[2], colour_palette[6])\n\nrev_colours &lt;- rev(colours)\n\nROC_scores_grasslands_increasing &lt;- sort(ROC_scores_grasslands[1:35])\n\npng(filename = \"case_study_1/results/grasslands_importance.png\", width = 1920, height = 1080, res = 200, pointsize = 9)\n\n\nbarplot(ROC_scores_grasslands_increasing, \n        names = names(ROC_scores_grasslands_increasing), \n        horiz = TRUE, \n        main = \"Grasslands class feature importance\",\n        xlab = \"Importance\", \n        las = 1,\n        border = NA,\n        cex.names = 0.5,\n        xlim = c(60, 100),\n        xpd = FALSE,\n        col = rev_colours\n)\n\nlegend(\"bottomright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\"),\n       fill = colour_palette[1:7],  title = \"Terms of data acquisition\")\n\ndev.off()\n\n\n\n\n\n\n\n\n\n\n\n\n\nfirst_10 &lt;- names(ROC_scores_grasslands)[1:10] # pull 10 best performing predictors names\nbest_10_predictors &lt;- pixel_reference[, c(which(names(pixel_reference) %in% first_10), 71, 72)] # get first most informative predictors\n\n\n\nset.seed(1410)\ntrain_index &lt;- createDataPartition(reference_data$class, p = 0.5, list = FALSE) # randomly pick half of observations based on class field\n\ntraining_data &lt;- best_10_predictors[best_10_predictors$polygon_index %in% train_index, ] # select training pixel observations using polygon indexes\nvalidation_data &lt;- best_10_predictors[!(best_10_predictors$polygon_index %in% train_index), ] # select validation pixel observations using polygon indexes\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10) # set 10-fold cross validation to find the best model\n\ntune_parameters &lt;- data.frame(C = c(10, 100, 100), sigma = c(0.1, 0.5, 0.95)) # test these parameters to find the best combination\n\nmodel &lt;- train(training_data[, 1:10], training_data$class_name, method = \"svmRadial\", trControl = control, tuneGrid = tune_parameters) # train the model using SVM with radial kernel using selected parameters\n\npredictions &lt;- predict(model, validation_data[, 1:10]) # use the chosen model to predict classes in the validation dataset\nconfusion_matrix &lt;- confusionMatrix(predictions, as.factor(validation_data$class_name), mode = \"everything\") # show confusion matrix based on validation results\nprint(round(confusion_matrix$overall, 3)) # show confusion metrics in the console\nsaveRDS(confusion_matrix, file = \"case_study_1/results/FS_confusion_matrix.RDS\") # save confusion matrix\n\n\n\nscenario_bands &lt;- subset(image_data, first_10)\n\nimage_data_crop &lt;- crop(scenario_bands, ext(aoi)) # crop the whole image to the interesting polygon\n\ntry(terra::predict(image_data_crop, model, filename = \"case_study_1/results/FS_terms_sample_classification_result.tif\", progress = \"text\", format = \"GTiff\", na.rm = TRUE), silent = TRUE)\n\n\n\n\ncm_fs &lt;- readRDS(\"case_study_1/results/FS_confusion_matrix.RDS\")\nf1_fs &lt;- cm_fs$byClass[, 7]\n\nf1_matrix &lt;- matrix(data = c(f1_14May, f1_31May, f1_7Aug, f1_27Aug, f1_18Sep, f1_25Jun, f1_30Jun, f1_all_terms, f1_fs), ncol = 9, nrow = 9, byrow = TRUE)\n\npng(filename = \"case_study_1/results/comp3_barplot.png\", width = 1920, height = 1080, res = 200, pointsize = 9)\npar(mar=c(5, 4, 4, 9), xpd = TRUE)\nbarplot(f1_matrix, beside = TRUE, xaxt = \"n\", xlab = \"Class\", ylab = \"F1\",\n        ylim = c(0, 1), las = 2, col = colour_palette[1:9],\n        main = \"Comparison of F1 scores for different classification input datasets.\")\nlegend(\"topright\", legend = c(\"14May\", \"31May\", \"7Aug\", \"27Aug\", \"18Sep\", \"25Jun\", \"30Jun\", \"All_terms\", \"FS\"),\n       fill = colour_palette[1:9],  title = \"Classification input datasets\", bty = \"n\", xjust = 1, inset = c(-0.1, 0))\npar(xpd = FALSE)\nabline(h = c(0, 0.2, 0.4, 0.6, 0.8, 1), lty = 3)\naxis(1, at = c(5.5, 15.5, 25.5, 35.5, 45.5, 55.5, 65.5, 75.5, 85.5), labels = labels)\n\ndev.off()\n\n\n</code></pre>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html","title":"Case study: Effects of air pollution: forest disturbance and recovery in the Ore Mountains (Czechia)","text":"<p>The aim of this case study is to present a methodology for analysing forest changes based on 40-year time series of Landsat imagery using two spectral indices \u2013 Integrated Forest Score (IFZ, Huang et al., 2010) and Disturbance Index (DI, Healey et al., 2005, Griffiths et al., 2014). The methodology is implemented as an online Google Earth Engine (GEE) application. The area of interest (AOI) was set to the Ore Mountains (Czechia). Both indices were also explained in Theme 5 of this Module.</p>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#study-site-and-data","title":"Study site and data","text":"<p>First, get familiar with the use case Forest disturbances in Ore Mountains and read the paper accompanying the use case and case study:</p> <p>Kupkov\u00e1, L., Pot\u016f\u010dkov\u00e1, M., Lhot\u00e1kov\u00e1, Z., Albrechtov\u00e1, J. (2018). Forest cover and disturbance changes, and their driving forces: A case study in the Ore Mountains, Czechia, heavily affected by anthropogenic acidic pollution in the second half of the 20th century. Environmental Research Letters, 13(9), 095008. https://iopscience.iop.org/article/10.1088/1748-9326/aadd2c</p>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#implementation-workflow","title":"Implementation workflow","text":"<p>The application is implemented in GEE and can be accessed from this link.</p> <p> <p>The GEE application Forest disturbance in Ore Mountains. Figure by course authors. </p> <p>The methodology consists of several steps depicted in the chart below and commented in the following text. The developed GEE script can be used for other areas with similar land cover and forest composition. The respective parts of the script that need to be altered for another AOI are marked in the following text.</p> <p> <p>General workflow applied in the GEE application Forest disturbance in Ore Mountains. Figure by course authors. </p>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#data-selection","title":"Data selection","text":"<p>The AOI is defined as a polygon and it is saved in the respective cloud repository (and shall be obviously changed if the analysis shall be carried out over another area). In our case study, the time span goes from 1984 to 2023 which corresponds to Landsat missions 4 \u2013 9. Notice, that the year 2012 is omitted. The only images available are from Landsat 7. Due to failure of the Scan Line Corrector (SLC) compensating for the forward motion of the satellite (more information is available on the USGS homepage), the images contain high amount of erroneous pixels and introduce outliers to the analysed time series. </p> <pre><code>// Create list of years\nvar startYear = 1984;\nvar endYear = 2023;\nvar yearList = ee.List.sequence(startYear, endYear)\n  .remove(2012); // Removing 2012 to remove Landsat 7, as only L7 images were available in 2012\n</code></pre> <p>Landsat Collection 2 Surface Reflectance (Level 2 Scientific Product) is used for processing.</p> <pre><code>// Define collections for Landsat 4, 5, 7, 8, and 9\nvar collections = {\n  lt4: ee.ImageCollection(\"LANDSAT/LT04/C02/T1_L2\"),\n  lt5: ee.ImageCollection(\"LANDSAT/LT05/C02/T1_L2\"),\n  lt7: ee.ImageCollection(\"LANDSAT/LE07/C02/T1_L2\"),\n  lt8: ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\"),\n  lt9: ee.ImageCollection(\"LANDSAT/LC09/C02/T1_L2\")\n};\n</code></pre> <p>To compare only images from the vegetation season, only images from May to September were selected by defining a filter <code>ee.Filter.calendarRange(5, 9, 'month')</code>.</p> <p>In addition, a new layer doy_abs defining the \u201ctime distance\u201d from the target Day Of the Year (DOY) was created in each image. After masking out the pixels contaminated with clouds and shadows, a gap-free composite will be created from pixels in the defined season span (in our case May to September) with the shortest time distance from DOY, in our case DOY = 212 (August 1st).</p>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#cloud-mask","title":"Cloud mask","text":"<p>The Landsat Collection 2 images contain a Surface Reflectance Cloud Quality Assessment Band (SR_Cloud_QA) carrying among others information on contamination of the pixels with clouds and cloud shadows. </p> <p> <p>Landsat Collection 2 Pixel Quality Assessment Bit Index. Source:  U.S. Geological Survey / Terms of use </p> <p>In the applied workflow this information is used and pixels with value 1 in the respective SR_Cloud_QA bits (1, 3, 4) are added to the cloud mask. Notice that the mask was extended of pixels containing snow that might remain at the top of the Ore mountains at the end of spring.</p> <pre><code>// Define function to mask clouds and clouds shadows with additional pixels masked around these areas.\nvar cloudMaskKernel = function(img) {\n  var dilatedCloudBitMask = 1 &lt;&lt; 1;\n  var cloudShadowBitMask = 1 &lt;&lt; 3;\n  var cloudsBitMask = 1 &lt;&lt; 4;\n  var snowBitMask = 1 &lt;&lt; 5;\n  var qa = img.select('QA_PIXEL');\n  var mask = qa.bitwiseAnd(cloudShadowBitMask)\n    .or(qa.bitwiseAnd(cloudsBitMask))\n    .or(qa.bitwiseAnd(dilatedCloudBitMask))\n    .or(qa.bitwiseAnd(snowBitMask));\n  mask = mask.focalMax(7, \"square\");\n  return qa.updateMask(mask.not());\n};\n</code></pre>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#gap-free-composites","title":"Gap-free composites","text":"<p>For each acquisition year, the pixels outside the mask are sorted according to the time distance to DOY. The final composite for the given year is composed from cloud and snow free pixels with the shortest time distance to DOY. The final image is reprojected to the WGS 84 / UTM zone 34N (EPSG:32634) with GSD of 30m. </p> <pre><code>// Function to create mosaic based on day-of-year. \n// Non-clouded pixels with lowest temporal distance from 213 day-of-year (August 1) will be present in the final composite\nvar compositeDoy = function(col) {\n  var colSort = col.sort('doy_abs', false);\n  return colSort.mosaic();\n};\n\n// Create a composite for each year\nvar yearCompList = yearList.map(function(year) {\n  var yearCol = oreCol.filter(ee.Filter.calendarRange(year, year, 'year'));\n  var yearComp = compositeDoy(yearCol);\n  yearComp = yearComp.reproject({crs: 'EPSG:32634', scale: 30}); \n  return yearComp.set({\n    'year': ee.Number(year).int(),\n    'save': ee.String(year)\n  });\n});\n</code></pre>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#disturbance-index","title":"Disturbance index","text":"<p>For the calculation of DI, the Tasselled Cap Transformation (TCT, Kauth and Thomas, 1976, Crist and Kauth, 1986) has to be carried out first. In the present implementation, the transformation coefficients from Crist and Cicone (1984) are applied for all imagery. It has two drawbacks. First, the coefficients slightly alter among Landsat sensors. Second, they differ if applied on DN or surface reflectance values (c.f., Zhai et al., 2022). Thus, in this part there is a space for further improvements and experiments.</p> <pre><code>// Coefficients after Crist and Cicone (1984) \n  var brtCoeffs = ee.Image.constant([0.3037, 0.2793, 0.4743, 0.5585, 0.5082, 0.1863]);\n  var grnCoeffs = ee.Image.constant([-0.2848, -0.2435, -0.5436, 0.7243, 0.0840, -0.1800]); \n  var wetCoeffs = ee.Image.constant([0.1509, 0.1973, 0.3279, 0.3406, -0.7112, -0.4572]); \n\n  // Sum reducer - sum band values after multiplying them by coefficients\n  var sumReducer = ee.Reducer.sum();\n  var brightness = img.multiply(brtCoeffs).reduce(sumReducer);\n  var greenness = img.multiply(grnCoeffs).reduce(sumReducer); \n  var wetness = img.multiply(wetCoeffs).reduce(sumReducer); \n  var tc = brightness.addBands(greenness)\n                     .addBands(wetness)\n                     // Rename the bands\n                     .select([0,1,2], ['TCB','TCG','TCW']); \n  return img.addBands(tc);\n};\n</code></pre> <p>The Brightness (B), Greenness (G), and Wetness (W) components are further normalised to the mean m and standard deviation \u03c3 over forest areas. In our implementation, these were calculated from pixels labelled as \u201cstable forest\u201d based on IFZ in the study Kupkov\u00e1 et al., 2018. Due to outliers present in the data, a median value and an estimated standard deviation \u03c3 based on the mean absolute difference (MAD, \u03c3 = 1.4826*MAD) were used. Differences in the m and \u03c3 values are highly possible for other areas and species and shall be check before applying elsewhere. The final value of DI is given by subtraction of the normalised G and W values from the normalised B valies.</p> <pre><code>// function for computing disturbance index\nvar disturbanceIndex = function(img) {\n  // Values based on Kupkov\u00e1 et al. (2018)\n  var brtMedian = ee.Image.constant(0.1972);\n  var brtStd = ee.Image.constant(0.05575);\n\n  var grnMedian = ee.Image.constant(0.108);\n  var grnStd = ee.Image.constant(0.03113);\n\n  var wetMedian = ee.Image.constant(0.0068);\n  var wetStd = ee.Image.constant(0.01438);\n\n  var br = img.select('TCB').subtract(brtMedian).divide(brtStd);\n  var gr = img.select('TCG').subtract(grnMedian).divide(grnStd);\n  var wr = img.select('TCW').subtract(wetMedian).divide(wetStd);\n\n  var di = br.subtract(gr).subtract(wr).rename('DI');\n\n  return img.addBands(di);\n};\n</code></pre>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#integrated-forest-score","title":"Integrated forest score","text":"<p>The calculation IFZ requires normalization of NIR, SWIR1 and SWIR2 bands using mean m and standard deviation \u03c3 of the pixels corresponding to the forest areas. The applied coefficients originate in the study Kupkov\u00e1 et al., 2018 where forest pixels were discriminated by unsupervised ISODATA classification.</p> <pre><code>//Function for computing Forest Score\nvar forestScore = function(img) {\n  // Values are based on based on Kupkov\u00e1 et al. (2018)\n  var grnMedian = ee.Image.constant(0.02411);\n  var grnStd = ee.Image.constant(0.00437);\n\n  var swir1Median = ee.Image.constant(0.06677);\n  var swir1Std = ee.Image.constant(0.01147);\n\n  var swir2Median = ee.Image.constant(0.02907);\n  var swir2Std = ee.Image.constant(0.00750);\n\n  var grnFZ = img.select('Green').subtract(grnMedian).divide(grnStd).pow(2);\n  var swir1FZ = img.select('SWIR1').subtract(swir1Median).divide(swir1Std).pow(2);\n  var swir2FZ = img.select('SWIR2').subtract(swir2Median).divide(swir2Std).pow(2);\n\n  var ifz = grnFZ.add(swir1FZ).add(swir2FZ)\n    .divide(ee.Image.constant(3))\n    .sqrt()\n    .rename('IFZ');\n</code></pre>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#analysis","title":"Analysis","text":"<p>The application provides tools for displaying IFZ and DI in a selected year over AOI. Moreover, true colour RGB and false colour SWIR1/SWRI2/R composites as well as Normalised Differential Vegetation Index (NDVI) can be visualised. You might add a function for exporting selected image layers.</p> <p>There is a possibility to display a chart of IFZ and Di values over the whole time span (1984 \u2013 2023) for preselected points or points defined by a user in a map. The preselected points correspond to areas with a stable forest and areas with forest disturbances. The charts can be enlarged and saved in svg or png formats and IFZ and DI values can be exported as csv files and processed further in R, Python, Excel or similar software tools.</p> <p>Following Kupkov\u00e1 et al., 2018, based on analysis of the IFZ time series areas of stable forest, areas of deforestation, afforestation, and combination of both can be defined. The stable forest is defined for pixels with IFZ &lt; T<sub>IFZ</sub>. Huang et al. (2010) set the T<sub>IFZ</sub> = 3. In our AOI, stands of mainly mature conifers using this threshold. Based on IFZ values from 1990, 2000, and 2006 under the forest mask derived from corresponding Corine Land Cover (CLC) databases using deciduous (code 311), coniferous (312), mixed (313) and transitional (324) forest areas, setting a higher threshold T<sub>IFZ</sub> = 5 is justifiable to discriminate all forest classes. In Kupkov\u00e1 et al., 2018 even a higher value T<sub>IFZ</sub> = 8 by adding a standard deviation \u03c3<sub>IFZ</sub> = 3. The T<sub>IFZ</sub> values shall be checked and adjusted according to the forest characteristics at the AOI.</p> <p> <p>Application of IFZ for evaluation of forest change. The upper charts show area of a stable forest with IFZ &lt; 3 in the whole time span; notice an outlier value in the year 1985. The lower chart depicts forest changes from stable forest between 1984 and 1995 with IFZ&lt;3, through its disturbance between 1996 to 2012 with IFZ &gt; 3, to forest recovery starting from 2013. The change is considered if IFZ goes over the threshold (in this case TIFZ = 3) for a period longer than 3 years to avoid variations in surface reflectance time series due to imperfections in atmospheric correction. Figure by course authors. </p> <p>The DI reacts on the fact that that recently cleared or declining forest exhibits high brightness and low greenness and wetness values in relation to undisturbed or recovering forest. A linear trend (TDI) can be calculated from the DI time series to discriminate areas where minimal change happened (TDI close to 0), where the forest condition improved in general (TDI &lt; 0), and where it got worse (TDI &gt; 0). </p> <p> <p>Example of area of a minimal change (stable forest) and improved forest condition (afforestation). Figure by course authors. </p>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#conclusions","title":"Conclusions","text":"<p>The forest changes can be evaluated based on the present methodology. The developed GEE script includes creating a time series of Landsat cloud free image composites using the shortest time distance to the selected DOY. Calculation of IFZ and DI adjusted for the area of the Ore Mountains is included. For use in other areas, the values of mean and standard deviation tasselled cap components and surface reflectance in the infrared bands, respectively, over forest areas shall be checked and possibly altered. Also, adjustment of parameters for the tasselled cap transformation to respect the surface reflectance and various sensors shall be considered. Despite to some simplifications, the method proved to have valuable results for assessment of forest condition and its development in time as showed in the study Kupkov\u00e1 et al., 2018. In the mentioned study, all calculations were done in the desktop solutions, Landsat images were downloaded from the USGS archive. Thus, only one image with the lowest cloud cover was used per each year. Due to cloud cover, some years were excluded in the study. The provided cloud solution overcomes this problem, it is much more flexible and prepares datasets for further automated processing (image layers with IFZ and DI, csv files for selected pixels). The derived dates of forest changes can be correlated with other datasets on air pollution, windstorms, clearcuts, new planting etc. to develop scenarios for sustainable forest management and future forest development. </p>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#references","title":"References","text":"<p>Crist, E. P., Cicone, R. C. (1984). A physically-based transformation of Thematic Mapper data---The TM Tasseled Cap. IEEE Transactions on Geoscience and Remote sensing, (3), 256-263.Tasseled Cap.\u201d IEEE Transactions on Geoscience and Remote Sensing GE-22: 256-263. https://ieeexplore.ieee.org/document/4157507</p> <p>Crist, E. P.,  Kauth, R. J. (1986). The tasseled cap de-mystified. Photogrammetric engineering and remote sensing, 52. https://www.asprs.org/wp-content/uploads/pers/1986journal/jan/1986_jan_81-86.pdf</p> <p>Griffiths, P., Kuemmerle, T., Baumann, M., Radeloff, V. C., Abrudan, I. V., Lieskovsky, J., \u2026 Hostert, P. (2014). Forest disturbances, forest recovery, and changes in forest types across the Carpathian ecoregion from 1985 to 2010 based on Landsat image composites. Remote Sensing of Environment, 151, 72-88. https://doi.org/10.1016/j.rse.2013.04.022</p> <p>Healey, S. P., Cohen, W. B., Zhiqiang, Y., Krankina, O. N. (2005). Comparison of Tasseled Cap-based Landsat data structures for use in forest disturbance detection. Remote sensing of environment, 97(3), 301-310. https://doi.org/10.1016/j.rse.2005.05.009</p> <p>Huang, C., Goward, S. N., Masek, J. G., Thomas, N., Zhu, Z., Vogelmann, J. E. (2010). An automated approach for reconstructing recent forest disturbance history using dense Landsat time series stacks. Remote Sensing of Environment, 114(1), 183-198. https://doi.org/10.1016/j.rse.2009.08.017</p> <p>Kauth, R. J., &amp; Thomas, G. S. (1976). The Tasseled Cap\u2014A Graphic Description of the Spectral-Temporal Development of Agricultural Crops as Seen by Landsat. LARS: Proceedings of the Symposium on Machine Processing of Remotely Sensed Data, West Lafayette, IN: Purdue University, pp. West Lafayette, IN: Purdue University: 4B-41-4B-51.</p> <p>Kupkov\u00e1, L., Pot\u016f\u010dkov\u00e1, M., Lhot\u00e1kov\u00e1, Z., Albrechtov\u00e1, J. (2018). Forest cover and disturbance changes, and their driving forces: A case study in the Ore Mountains, Czechia, heavily affected by anthropogenic acidic pollution in the second half of the 20th century. Environmental Research Letters, 13(9), 095008. https://iopscience.iop.org/article/10.1088/1748-9326/aadd2c</p> <p>Zhai, Y., Roy, D. P., Martins, V. S., Zhang, H. K., Yan, L., Li, Z. (2022). Conterminous United States Landsat-8 top of atmosphere and surface reflectance tasseled cap transformation coefficients. Remote Sensing of Environment, 274, 112992. https://doi.org/10.1016/j.rse.2022.112992</p>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#other-case-studies","title":"Other case studies","text":"<ul> <li>Monitoring tundra grasslands (Karkonosze)</li> <li>Forest disturbance detection (Tatras)</li> </ul>"},{"location":"module2/07_cs_forest_changes/07_cs_forest_changes.html#module-themes","title":"Module themes","text":"<ul> <li>Principles of multispectral imaging</li> <li>Temporal information in satellite data</li> <li>Image processing workflow</li> <li>Multitemporal classification of vegetation types</li> <li>Vegetation monitoring and disturbance detection</li> </ul>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html","title":"Case study: Forest disturbance detection (Tatra Mountains)","text":"<p>The Tatra Mountains are a unique and ecologically significant area. Monitoring forest changes there is critical to understanding environmental shifts and potential threats. The aim of this case study is to assess vegetation (forests) changes in Tatra Mountains area. If you\u2019ve gone through Theme 5 exercise you\u2019ll notice the similarit in topics.</p> <p>However, in this case study we\u2019re adopting a different approach. This time we we\u2019ll use Threshold and trend-based vegetation change monitoring algorithm (TVCMA) instead of the LandTrendr. TVCMA was developed in R language. Our work will be conducted in RStudio environment. If you haven\u2019t already done this, refer to this tutorial regarding the environment setup: R environment setup tutorial. We will use the image datasets and validation table prepared and described earlier in Theme 3 exercise and Theme 5 exercies. We\u2019ll use essential features of TVCMA, which are related to detection of time and location of forest disturbance events based on thresholding and logical rules.</p> <p>Our workflow is divided into the following key steps:</p> <ol> <li>Extraction of spectral indices values using reference data points: this initial step involves extracting spectral values from our image datasets, forming the foundation for the subsequent stages of our analysis.</li> <li>Applying the TVCMA algorithm to various variables (indices) and threshold values: the core analysis part.</li> <li>Accuracy assessment and best model selection: after acquiring the results we\u2019ll be able to assess and select best performing combination of spectral index &amp; threshold.</li> <li>Calculation of forest disturbance maps presenting detected events in the Tatra Mountains: in this part, we\u2019ll create spatial visual representations of the detected forest disturbances.</li> </ol> <p> <p></p> <p>Case Study 3 scheme of procedure. </p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#study-area-and-data","title":"Study area and data","text":"<p>Before you continue, get familiar with the use case (if you did it before in Theme 5 you can skip this step): Use case: Vegetation disturbance detection in Polish-Slovak Tatra Mountains.</p> <p>\u2026 and read the paper accompanying the use case:</p> <p>Ochtyra, A., Marcinkowska-Ochtyra, A., &amp; Raczko, E. (2020). Threshold and trend-based vegetation change monitoring algorithm based on the inter-annual multi-temporal normalized difference moisture index series: A case study of the Tatra Mountains. Remote Sensing of Environment, 249, 112026. https://doi.org/10.1016/j.rse.2020.112026</p> <p>Download data provided through Zenodo.</p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#imagery-data","title":"Imagery data","text":"<p>The image data supplied for this exercise consist of Landsat satellite imagery time series. The data preparation process is described in the Module 2 Theme 3 exercise Pipeline 2. We\u2019ll use indices time series that you can access in the <code>case_study_3/data_exercise</code> folder.</p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#reference-data","title":"Reference data","text":"<p>For this exercise, 100 reference points have been chosen to perform the assessment of TVCMA. These points were randomly spread out across the research area. Each point was assessed based on chips (method of producing chips is presented in Theme 2 exercise). The attribute table from the point layer was extracted and will be used in the second part of the exercise as a validation dataset. In table rows there are consecutive reference points recorded and in columns there are subsequent years from 1985 (the first possible disturbance observation) to 2022. In cases where no disturbance was identified, the corresponding cell was marked with a \u201c0\u201d. Conversely, if a disturbance was evident, the cell was marked with a \u201c1\u201d.</p> <p> <p></p> <p>Validation table. </p> <p>The validation process will involve overlaying the extracted TVCMA disturbance points on validation disturbance events and calculating confusion matrix statistics.</p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#getting-started","title":"Getting started","text":""},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#environment-preparation-loading-required-libraries-and-data","title":"Environment preparation: loading required libraries and data","text":"<p>To start with, we want to load necessary libraries and data and set up some initial variables, which we will use further down the line. Firstly, load required libraries into the environment: <code>terra</code>, <code>sf</code>, <code>dplyr</code>, <code>readxl</code>, <code>writexl</code> and <code>tools</code>. Functions included in these packages will be used further in this case study.</p> <pre><code># raster I/o and processing\nlibrary(terra) \n\n# vector I/O and processing\nlibrary(sf) \n\n# tabular data manipulation\nlibrary(dplyr) \n\n# reading .xlsx files\nlibrary(readxl) \n\n# writing .xlsx files\nlibrary(writexl) \n\n# miscellaneous tools\nlibrary(tools)\n</code></pre> <p>We can move to loading up the required data. We will start with the raster data which are time series of vegetation indices and products of Tasseled Cap transformation calculated on Landsat cloud-free composites from 1984 to 2022 prepared in Theme 3 exercise, pipeline 2. Data can be found in a package provided through Zenodo. We\u2019ll load 8 multiband rasters to a list and name them appropriately.</p> <pre><code># load relative paths to .tif files\n# the first file is forest mask, which we want to load separately\nindices_list &lt;- list.files(\"case_study_3/data_exercise/\", \n                           pattern = \"*.tif\", \n                           full.names = TRUE)[2:9]\n\n# load images as terra rast objects\nindices_imgs &lt;- lapply(indices_list, terra::rast)\n\n# get indices names\nindices_names &lt;- file_path_sans_ext(list.files(\"case_study_3/data_exercise/\", \n                                               pattern = \"*.tif\")[2:9])\n\n# rename loaded images\nnames(indices_imgs) &lt;- indices_names\n</code></pre> <p>After we\u2019ve loaded image data, we can now can also read in 100 reference/validation points.</p> <pre><code># read reference points\npoints &lt;- st_read(\"case_study_3/data_exercise/CS3_points.shp\")\n</code></pre> <p>To have all the data needed later we\u2019ll also load validation table at this moment. We\u2019ll get rid of the redundant <code>id</code> column.</p> <pre><code># read validation table\nval_table &lt;- read_xlsx(\"case_study_3/data_exercise/CS3_validation_table.xlsx\") %&gt;%\n  select(2:39)\n</code></pre> <p>Finally, we\u2019ll use forest mask in the final parts of the exercise. Our focus is detection disturbances in the forested areas, so this mask will reduce visual changes in, for example, crops in the disturbance maps.</p> <pre><code># read forest mask raster\nforest_mask &lt;- rast(\"case_study_3/data_exercise/CS3_forest_mask.tif\")\n</code></pre>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#reference-values-extraction","title":"Reference values extraction","text":"<p>TVCMA algorithm needs spectral indices values to apply sets of rules to them to indicate the disturbance events. We want to extract these values for all time series rasters we loaded. We\u2019ll use <code>terra::extract</code> function and apply it to the <code>points</code> layer to produce a list containing extracted values for each index.</p> <pre><code># extract indices values to points\nindices_values &lt;- lapply(indices_imgs, terra::extract, y = points, ID = FALSE)\n</code></pre> <p>This is the result of that function. Each data frame contains 100 rows (one per point) with 39 columns (one per year from 1984 to 2022).</p> <p> <p></p> <p></p> <p>We can create a single spatial points file, with attribute table populated with extracted values. In order to do that we\u2019ll bind the columns from all extracted data frames and then bind these columns to the attribute table of <code>points</code> shapefile.</p> <pre><code># bind columns to point layer\npoints_all_values &lt;- bind_cols(points, do.call(bind_cols, indices_values))\n</code></pre> <p>We can save the results to shapefile.</p> <pre><code># save shapefile with extracted values\nst_write(points_all_values, \"case_study_3/results/CS3_points_indices_values.shp\")\n</code></pre> <p>You may see this warning.</p> <pre><code># Warning message:\n#   In CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options),  :\n#                      GDAL Message 1: Creating a 256th field, but some DBF readers might only support 255 fields\n</code></pre> <p>To make sure the data is saved correctly, save the points to geopackage.</p> <pre><code># save geopackage with extracted values\nst_write(points_all_values, \"case_study_3/results/CS3_points_indices_values.gpkg\")\n</code></pre>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#analysis","title":"Analysis","text":"<p>We are now ready to advance to the analysis part of this Case Study. We\u2019ve acquired a dataset on which we can apply TVCMA conditions to indicate disturbance events. In short, TVCMA checks three conditions in order to classify a point as a disturbance event. You can see a visualised example in Theme 5 Lesson.</p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#tvcma-conditions-function","title":"TVCMA conditions function","text":"<p>We\u2019ll prepare a function which will take two inputs. The first one <code>ind</code> is a data frame with 100 rows and 39 columns representing extracted values for one index. The second argument <code>threshold</code> is the numerical value which is used to assess whether the conditions are met (resulting in a <code>TRUE</code> value) or not (<code>FALSE</code> value).</p> <pre><code># function to check TVCMA conditions on values in a data frame\napply_tvcma_conditions_points &lt;- function(ind, threshold) {\n\n  # prepare a result matrix with dimensions matching input data frame\n  res &lt;- matrix(data = FALSE, nrow = nrow(ind), ncol = ncol(ind))\n\n  # loop to check TVCMA conditions for years from third (in this case 1986)\n  # to second to last (2021)\n  for (i in 1:nrow(ind)) {\n    for (j in 3:(ncol(ind) - 1)) {\n      if (threshold &gt;= 0) {\n        cond_1 &lt;- (ind[i, j] - ind[i, j - 1]) &gt; threshold\n        cond_2 &lt;- (ind[i, j + 1] - ind[i, j - 1]) &gt; threshold\n        cond_3 &lt;- (ind[i, j] - ind[i, j - 2]) &gt; threshold\n      } else {\n        cond_1 &lt;- (ind[i, j] - ind[i, j - 1]) &lt; threshold\n        cond_2 &lt;- (ind[i, j + 1] - ind[i, j - 1]) &lt; threshold\n        cond_3 &lt;- (ind[i, j] - ind[i, j - 2]) &lt; threshold\n      }\n      res[i, j] &lt;- cond_1 &amp; cond_2 &amp; cond_3\n    }\n\n    # for the second observation (1985), check conditions 1 and 2\n    if (threshold &gt;= 0) {\n      cond_1_second &lt;- (ind[i, 2] - ind[i, 1]) &gt; threshold\n      cond_2_second &lt;- (ind[i, 3] - ind[i, 1]) &gt; threshold\n    } else {\n      cond_1_second &lt;- (ind[i, 2] - ind[i, 1]) &lt; threshold\n      cond_2_second &lt;- (ind[i, 3] - ind[i, 1]) &lt; threshold\n    }\n    res[i, 2] &lt;- cond_1_second &amp; cond_2_second\n\n    # For the last observation (2022), check conditions 1 and 3\n    if (threshold &gt;= 0) {\n      cond_1_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 1]) &gt; threshold\n      cond_3_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 2]) &gt; threshold\n    } else {\n      cond_1_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 1]) &lt; threshold\n      cond_3_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 2]) &lt; threshold\n    }\n    res[i, ncol(ind)] &lt;- cond_1_last &amp; cond_3_last\n  }\n\n\n  # Remove the first column (1984) from resulting matrix \n  # because no conditions can be applied to the first year in time series\n  res &lt;- res[,2:ncol(res)]\n\n  # return the resulting 1/0 data frame\n  return(res)\n}\n</code></pre>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#accuracy-statistics-function","title":"Accuracy statistics function","text":"<p>We want to test two main variables to assess the performance of TVCMA: input spectral index and threshold. To calculate statistics for each pair we can prepare a function (similar to that in Theme 5 exercise where we assessed LandTrendr results.)</p> <pre><code># prepare a function to calculate statistics\ncalculate_confusion_matrix_stats &lt;- function(matrix, validation_matrix, set_name) {\n\n  # Calculate TP, FP, TN, FN\n  TP &lt;- sum(matrix == 1 &amp; validation_matrix == 1)\n  FP &lt;- sum(matrix == 1 &amp; validation_matrix == 0)\n  TN &lt;- sum(matrix == 0 &amp; validation_matrix == 0)\n  FN &lt;- sum(matrix == 0 &amp; validation_matrix == 1)\n\n  # Calculate the metrics\n  Accuracy &lt;- (TP + TN) / (TP + FP + FN + TN)\n  Precision &lt;- TP / (TP + FP)\n  Sensitivity &lt;- TP / (TP + FN)\n  Specificity &lt;- TN / (TN + FP)\n  F1_Score &lt;- 2 * (Sensitivity * Precision) / (Sensitivity  + Precision)\n\n  # Create a data frame with the confusion matrix stats\n  stats &lt;- data.frame(Set = set_name, TP, FP, TN, FN, Accuracy, Precision, Sensitivity, Specificity, F1_Score)\n\n  return(stats)\n}\n</code></pre>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#applying-functions-to-data","title":"Applying functions to data","text":"<p>We have prepared necessary data and functions to progress the analysis.</p> <p>One additional issue we haven\u2019t considered so far is the direction of change indicating disturbances. In order to detect a disturbance with spectral index values we need to know what is the sign (positive or negative) of delta if the disturbance occurs. Let\u2019s consider as an example NDVI. If NDVI pixel value in 1985 is <code>0.8</code> and in 1986 is <code>0.3</code> then delta is equal to <code>-0.5</code>. That means that disturbances can be detected when delta is negative. On the other hand, indices like NDWI or TCB behave the other way round - positive delta indicates disturbance. In our case these are the lists of positive and negative delta indices.</p> <pre><code># direction of index delta when considering disturbances\npos_indices &lt;- c(\"NDWI\", \"TCB\")\nneg_indices &lt;- c(\"NBR\", \"NBR2\", \"NDMI\", \"NDVI\", \"TCG\", \"TCW\")\n</code></pre> <p>For these two groups we also want to prepare a set of thresholds we\u2019ll test. Each time we\u2019ll consider 50 thresholds with increments of 0.01 each time.</p> <pre><code>pos_thresholds &lt;- seq(0.01, 0.5, by = 0.01)\nneg_thresholds &lt;- seq(-0.01, -0.5, by = -0.01)\n</code></pre> <p>We are finally ready to run TVCMA algorithm and calculate the accuracy results. In the code below we\u2019ll prepare an empty list to store the results separately per index. Each loop will produce an element - accuracy table with 50 rows (one per each threshold) - that will be added to <code>results_list</code>. Each accuracy table will also be separately exported to <code>.xlsx</code> file.</p> <pre><code># prepare empty results list\nresults_list &lt;- list()\n\n# loop through indices_values \nfor (i in seq(1, length(indices_values))) {\n\n  # select values for one index\n  ind = indices_values[[i]]\n\n  # get the name of the index\n  ind_name &lt;- names(indices_values)[i]\n\n  # check whether to apply positive or negative thresholds\n  if ((ind_name %in% pos_indices) == TRUE) {\n\n    thresholds &lt;- pos_thresholds\n\n  } else {\n\n    thresholds &lt;- neg_thresholds\n  }\n\n  # prepare a names list for each row in the resulting table containing threshold\n  # value with \".\" replaced by \"_\"\n  names_list &lt;- paste0(\"Threshold_\", gsub(\"\\\\.\", \"_\", thresholds))\n\n  # apply the functions where conditions are checked for each threshold value\n  # in other words: take index values and for each threshold value in the \n  # thresholds list check the conditions in apply_tvcma_conditions_points\n  thresholds_results &lt;- lapply(thresholds, \n                               apply_tvcma_conditions_points, \n                               ind = ind)\n\n  # set names for each separate result\n  thresholds_results &lt;- setNames(thresholds_results, names_list)\n\n  # apply calculate_confusion_matrix_stats for each result\n  # each time take separate result (one result for one threshold tested)\n  # calculate the statistics by comparing the result to validation table\n  stats_list &lt;- lapply(seq_along(thresholds_results), \n                       function(i) calculate_confusion_matrix_stats(\n                         matrix = thresholds_results[[i]],\n                         validation_matrix = val_table,\n                         set_name = names(thresholds_results)[i]))\n\n  # bind all the results for one index together\n  # 50 rows, one per threshold, containing calculated statistics\n  stats_df &lt;- do.call(rbind, stats_list)\n\n  # save the results to .xlsx file\n  write_xlsx(stats_df, paste0(\"case_study_3/results/\", ind_name, \"_th_min_\", min(thresholds), \"_max_\", max(thresholds), \".xlsx\"))\n\n  # add the table for one index to a list\n  results_list[[i]] &lt;- stats_df\n\n}\n</code></pre>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#accuracy-assessment","title":"Accuracy assessment","text":"<p>In your <code>results</code> folder you should now be able to see 8 <code>.xlsx</code> files. Each file should contain 50 rows with data (+ 1 heading row). See the example below.</p> <p> <p></p> <p></p> <p>The goal of our Case Study is to indicate the best spectral index and threshold combination to detect disturbance events with the highest accuracy. We can do that based on the achieved accuracy metrics. For the purpose of our Case Study we\u2019ll choose F1 as a measure to choose the best combination. Take a couple of minutes to assess the results and come back to the contents to see if your assessment is in line with our findings.</p>  The highest F1 was measured for\u2026   NDMI with threshold `0.09`. With F1 value equal to `0.65` this combination allowed us to correctly identify **42** cases of disturbances (True Positive, TP). Our results falsely indicate **17** disturbance events (False Positive, FP). There were also **27** disturbances events omitted in the TVCMA results, which were present in our validation dataset (False Negative, FN).   <p>If you\u2019ve evaluated the results you can proceed to the next part of the Case Study.</p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#disturbance-mapping","title":"Disturbance mapping","text":"<p>With the best performing index and threshold identified we can calculate disturbance maps for each year in the analysed time series.</p> <p>First, we want to set the threshold and choose index to apply the calculations to.</p>  Set the best performing threshold to threshold variable and best performing image (index) data to multiband_raster variable.  <pre><code># best index image data\nmultiband_raster &lt;- indices_imgs$NDMI\n\n# best threshold\nthreshold &lt;- -0.09\n</code></pre> <p>Next up we\u2019ll prepare an array to store the results. We\u2019ll set the dimensions of the array to be equal to input images, except number of layers, which needs to be shorter by one, since we cannot indicate disturbances for the first year in the series. We\u2019ll produce disturbance maps from 1985 to 2022. At the start the 1193x2255x38 array will be populated with zeroes.</p> <pre><code># empty array to store the results\nresult_array &lt;- array(0, dim = c(nrow(multiband_raster), \n                                 ncol(multiband_raster), \n                                 nlyr(multiband_raster) - 1))\n</code></pre> <p>We are now ready to start calculating maps of disturbances based on TVCMA algorithm using NDMI and 0.09 threshold value as inputs. The input raster must be open for reading in order for us to be able to extract values from it. In a loop we\u2019ll apply TVCMA conditions for each row of the input image. Results for each row (value 1 for detected disturbance, value 0 for no disturbance) will be pasted into <code>results_array</code>.</p> <p>This process may take a couple minutes to finish. Observe the progress printing in the console.</p> <pre><code># open a file for reading\nreadStart(multiband_raster)\n\n# for each row in input image\nfor (i in 1:nrow(multiband_raster)) {\n\n  # extract raster values from the image row \n  # each row contains 2255 x 39 values\n  row_values &lt;- readValues(multiband_raster, \n                           row = i, \n                           nrows = 1, \n                           col = 1, \n                           ncols = ncol(multiband_raster), \n                           mat = TRUE)\n\n  # prepare empty vector to store the TVCMA result for one row\n  results &lt;- rep(NA, ncol(multiband_raster) * (nlyr(multiband_raster) - 1))\n\n  # apply TVCMA conditions to extracted raster values\n  for (k in 1:nrow(row_values)) {\n\n    values &lt;- row_values[k, ]\n    val_len &lt;- length(values)\n\n    if (threshold &gt;= 0) {\n\n      cond_1 &lt;- (values[3:((val_len) - 1)] - values[2:(val_len - 2)]) &gt; threshold\n      cond_2 &lt;- (values[4:(val_len)] - values[2:(val_len - 2)]) &gt; threshold\n      cond_3 &lt;- (values[3:((val_len) - 1)] - values[1:(val_len - 3)]) &gt; threshold\n\n      r1 &lt;- cond_1 &amp; cond_2 &amp; cond_3\n      r2 &lt;- (values[2] - values[1]) &gt; threshold &amp; (values[3] - values[1]) &gt; threshold\n      r3 &lt;- (values[val_len] - values[val_len - 1]) &gt; threshold &amp; (values[val_len] - values[val_len - 2]) &gt; threshold\n\n    } else {\n\n      cond_1 &lt;- (values[3:((val_len) - 1)] - values[2:(val_len - 2)]) &lt; threshold\n      cond_2 &lt;- (values[4:(val_len)] - values[2:(val_len - 2)]) &lt; threshold\n      cond_3 &lt;- (values[3:((val_len) - 1)] - values[1:(val_len - 3)]) &lt; threshold\n\n      r1 &lt;- cond_1 &amp; cond_2 &amp; cond_3\n      r2 &lt;- (values[2] - values[1]) &lt; threshold &amp; (values[3] - values[1]) &lt; threshold\n      r3 &lt;- (values[val_len] - values[val_len - 1]) &lt; threshold &amp; (values[val_len] - values[val_len - 2]) &lt; threshold\n    }\n\n    result &lt;- c(r2, r1, r3)\n    results[(((k - 1) * (val_len - 1)) + 1):(k * (val_len - 1))] &lt;- result\n\n  }\n\n  # number of output layers (38)\n  n_layers_minus_one &lt;- nlyr(multiband_raster) - 1\n\n  # insert TVCMA results to result array\n  for (j in 1:ncol(multiband_raster)) {\n\n    index_start &lt;- (j - 1) * n_layers_minus_one + 1\n    index_end &lt;- j * n_layers_minus_one\n    result_array[i, j, ] &lt;- results[index_start:index_end]\n  }\n\n  # print progress\n  print(paste0(\"Row: \", i, \"/\", nrow(multiband_raster)))\n\n}\n# close a file from reading\nreadStop(multiband_raster)\n</code></pre> <p>The result of this loop is an array populated with values 0 (no disturbance) and 1 (disturbance). Our goal is to create a multiband raster to indicate the spatial distribution of detected disturbances. We can achieve that by transforming an array into raster with coordinate reference system. We will also rename the bands to match years of detection of disturbances.</p> <pre><code># transform array into raster with coordinate reference system\nresults_img &lt;- rast(result_array,\n                    extent = ext(multiband_raster),\n                    crs = crs(multiband_raster))\n\n\n# name the bands\nnames(results_img) &lt;- as.character(paste0(\"TVCMA_\", seq(1985, 2022)))\n</code></pre> <p>Once that\u2019s done we can also mask the resulting image with forest mask. This will limit the area of analysis to mainly forests with some patches of other land cover classes also present (mainly to show that they are volatile and sensitive to TVCMA algorithm, i.e.\u00a0lake in the SW part of the image).</p> <pre><code># mask the results with forest mask\nresults_img &lt;- mask(results_img, forest_mask)\n</code></pre> <p>To conclude the Case Study we can export the multiband image in a couple variants.</p> <p>First, let\u2019s export the image \u2018as is\u2019, so the disturbances are marked with pixel value 1 and no disturbances are 0. The masked areas have \u2018NA\u2019 pixel values.</p> <pre><code># write 1/0 TVCMA image\nwriteRaster(results_img, \n            filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_1_0.tif\",\n            datatype = \"INT1U\")\n</code></pre> <p>This is the result after loading it to QGIS. This image shows disturbances in yellow detected in 2005 and no disturbances in purple.</p> <p> <p></p> <p></p> <p>We can change the 0 values to NA for pixels where no disturbances were detected to get a clearer view of areas where some negative change occurred.</p> <pre><code># write 1/NA TVCMA image\nresults_img_1_na &lt;- subst(results_img, from = 0, to = NA,\n                         filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_1_NA.tif\",\n                         datatype = \"INT1U\")\n</code></pre> <p> <p></p> <p></p> <p>Finally, we can prepare two single band images presenting the earliest and latest detected disturbance for each pixel. In order to do that we\u2019ll reclassify values indicating disturbance - 1 - in each band to year of detection value. This will allow us to use <code>min</code> and <code>max</code> functions to extract minimum (earliest) and maximum (latest) disturbance year for each pixel.</p> <pre><code># get year of detection \nyod &lt;- seq(1985, 2022)\n\n# iterate over each band and replace values of 1 with the year of detection value\nresults_yod &lt;- lapply(1:length(yod), function(i) {\n  band &lt;- results_img_1_na[[i]]\n  band &lt;- band * yod[i]\n  return(band)\n})\n\n# make a raster out of year of detection list\nresults_yod_rast &lt;- rast(results_yod)\n\n\n# create a raster showing the earliest detected disturbance\nresults_earliest &lt;- min(results_yod_rast, na.rm = TRUE)\n\nwriteRaster(results_earliest, \n            filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_earliest.tif\",\n            datatype = \"INT2U\")\n\n\n\n# create a raster showing the latest detected disturbance \nresults_latest &lt;- max(results_yod_rast, na.rm = TRUE)\n\n\nwriteRaster(results_latest, \n            filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_latest.tif\",\n            datatype = \"INT2U\")\n</code></pre> <p> <p></p> <p>Year of earliest detected disturbance <p></p> <p>Year of earliest latest disturbance </p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#discussion","title":"Discussion","text":"<p>Points to discuss:</p> <ul> <li>comparison of usefulness of each variable/index in disturbance detection based on accuracy metrics</li> <li>finer best threshold tuning after establishing the overall valid range of thresholds</li> <li>what is the possible reason of False Negatives occurrence</li> <li>what is the possible reason of False Positives occurrence</li> <li>compare disturbance maps with image composites for corresponding years and think about its correctness</li> </ul>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#references","title":"References","text":"<p>Ochtyra, A., Marcinkowska-Ochtyra, A., &amp; Raczko, E. (2020). Threshold and trend-based vegetation change monitoring algorithm based on the inter-annual multi-temporal normalized difference moisture index series: A case study of the Tatra Mountains. Remote Sensing of Environment, 249, 112026. https://doi.org/10.1016/j.rse.2020.112026</p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#other-case-studies","title":"Other case studies","text":"<ul> <li>Monitoring tundra grasslands (Karkonosze)</li> <li>Effects of pollution (Ore Mountains)</li> </ul>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#module-themes","title":"Module themes","text":"<ul> <li>Principles of multispectral imaging</li> <li>Temporal information in satellite data</li> <li>Image processing workflow</li> <li>Multitemporal classification of vegetation types</li> <li>Vegetation monitoring and disturbance detection</li> </ul>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#data","title":"Data","text":"<p>Landsat 4, 5, 7, 8 and 9 imagery courtesy of the U.S. Geological Survey/ Terms of use processed in and downloaded from Google Earth Engine by Gorelick et al., 2017</p>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#software","title":"Software","text":"<ul> <li>QGIS Development Team (2022). QGIS Geographic Information System. Open Source Geospatial Foundation Project. http://qgis.osgeo.org</li> <li>R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.</li> <li>Hijmans R (2023). terra: Spatial Data Analysis. R package version 1.7-39, https://CRAN.R-project.org/package=terra</li> <li>Ooms J (2023). writexl: Export Data Frames to Excel \u2018xlsx\u2019 Format. R package version 1.4.2, https://CRAN.R-project.org/package=writexl.</li> <li>Pebesma, E., &amp; Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016</li> <li>Pebesma, E., 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10 (1), 439-446, https://doi.org/10.32614/RJ-2018-009</li> <li>Wickham H, Fran\u00e7ois R, Henry L, M\u00fcller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.2, https://CRAN.R-project.org/package=dplyr</li> <li>Wickham H, Bryan J (2023). readxl: Read Excel Files. https://readxl.tidyverse.org, https://github.com/tidyverse/readxl.</li> </ul>"},{"location":"module2/08_cs_disturbance_detection/08_cs_disturbance_detection.html#source-code","title":"Source code","text":"You can find the entire code used in this exercise here  <pre><code># raster I/o and processing\nlibrary(terra) \n\n# vector I/O and processing\nlibrary(sf) \n\n# tabular data manipulation\nlibrary(dplyr) \n\n# reading .xlsx files\nlibrary(readxl) \n\n# writing .xlsx files\nlibrary(writexl) \n\n# miscellaneous tools\nlibrary(tools)\n\n# load relative paths to .tif files\nindices_list &lt;- list.files(\"case_study_3/data_exercise/\", \n                           pattern = \"*.tif\", \n                           full.names = TRUE)[2:9]\n\n# load images as terra rast objects\nindices_imgs &lt;- lapply(indices_list, terra::rast)\n\n# get indices names\nindices_names &lt;- file_path_sans_ext(list.files(\"case_study_3/data_exercise/\", \n                                               pattern = \"*.tif\")[2:9])\n\n# rename loaded images\nnames(indices_imgs) &lt;- indices_names\n\n# read validation points\npoints &lt;- st_read(\"case_study_3/data_exercise/CS3_points.shp\")\n\n# read validation table\nval_table &lt;- read_xlsx(\"case_study_3/data_exercise/CS3_validation_table.xlsx\") %&gt;%\n  select(2:39)\n\n# read fprest mask raster\nforest_mask &lt;- rast(\"case_study_3/data_exercise/CS3_forest_mask.tif\")\n\n\n# extract indices values to points\nindices_values &lt;- lapply(indices_imgs, terra::extract, y = points, ID = FALSE)\n\n# bind columns to point layer\npoints_all_values &lt;- bind_cols(points, do.call(bind_cols, indices_values))\n\n# save shapefile with extracted values\nst_write(points_all_values, \"case_study_3/results/CS3_points_indices_values.shp\")\n\n# Warning message:\n#   In CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options),  :\n#                      GDAL Message 1: Creating a 256th field, but some DBF readers might only support 255 fields\n\n# save geopackage with extracted values\nst_write(points_all_values, \"case_study_3/results/CS3_points_indices_values.gpkg\")\n\n\n# function to check TVCMA conditions on values in a data frame\napply_tvcma_conditions_points &lt;- function(ind, threshold) {\n\n  # prepare a result matrix with dimensions matching input data frame\n  res &lt;- matrix(data = FALSE, nrow = nrow(ind), ncol = ncol(ind))\n\n  # loop to check TVCMA conditions for years from third (in this case 1986)\n  # to second to last (2021)\n  for (i in 1:nrow(ind)) {\n    for (j in 3:(ncol(ind) - 1)) {\n      if (threshold &gt;= 0) {\n        cond_1 &lt;- (ind[i, j] - ind[i, j - 1]) &gt; threshold\n        cond_2 &lt;- (ind[i, j + 1] - ind[i, j - 1]) &gt; threshold\n        cond_3 &lt;- (ind[i, j] - ind[i, j - 2]) &gt; threshold\n      } else {\n        cond_1 &lt;- (ind[i, j] - ind[i, j - 1]) &lt; threshold\n        cond_2 &lt;- (ind[i, j + 1] - ind[i, j - 1]) &lt; threshold\n        cond_3 &lt;- (ind[i, j] - ind[i, j - 2]) &lt; threshold\n      }\n      res[i, j] &lt;- cond_1 &amp; cond_2 &amp; cond_3\n    }\n\n    # for the second observation (1985), check conditions 1 and 2\n    if (threshold &gt;= 0) {\n      cond_1_second &lt;- (ind[i, 2] - ind[i, 1]) &gt; threshold\n      cond_2_second &lt;- (ind[i, 3] - ind[i, 1]) &gt; threshold\n    } else {\n      cond_1_second &lt;- (ind[i, 2] - ind[i, 1]) &lt; threshold\n      cond_2_second &lt;- (ind[i, 3] - ind[i, 1]) &lt; threshold\n    }\n    res[i, 2] &lt;- cond_1_second &amp; cond_2_second\n\n    # For the last observation (2022), check conditions 1 and 3\n    if (threshold &gt;= 0) {\n      cond_1_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 1]) &gt; threshold\n      cond_3_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 2]) &gt; threshold\n    } else {\n      cond_1_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 1]) &lt; threshold\n      cond_3_last &lt;- (ind[i, ncol(ind)] - ind[i, ncol(ind) - 2]) &lt; threshold\n    }\n    res[i, ncol(ind)] &lt;- cond_1_last &amp; cond_3_last\n  }\n\n\n  # Remove the first column (1984 for which it is not possible to check any\n  # conditions) from resulting matrix\n  res &lt;- res[,2:ncol(res)]\n\n  # return the resulting 0/1 data frame\n  return(res)\n}\n\n\n# prepare a function to calculate statistics\ncalculate_confusion_matrix_stats &lt;- function(matrix, validation_matrix, set_name) {\n\n  # Calculate TP, FP, TN, FN\n  TP &lt;- sum(matrix == 1 &amp; validation_matrix == 1)\n  FP &lt;- sum(matrix == 1 &amp; validation_matrix == 0)\n  TN &lt;- sum(matrix == 0 &amp; validation_matrix == 0)\n  FN &lt;- sum(matrix == 0 &amp; validation_matrix == 1)\n\n  # Calculate the metrics\n  Accuracy &lt;- (TP + TN) / (TP + FP + FN + TN)\n  Precision &lt;- TP / (TP + FP)\n  Sensitivity &lt;- TP / (TP + FN)\n  Specificity &lt;- TN / (TN + FP)\n  F1_Score &lt;- 2 * (Sensitivity * Precision) / (Sensitivity  + Precision)\n\n  # Create a data frame with the confusion matrix stats\n  stats &lt;- data.frame(Set = set_name, TP, FP, TN, FN, Accuracy, Precision, Sensitivity, Specificity, F1_Score)\n\n  return(stats)\n}\n\n\n# direction of index delta when considering disturbances\npos_indices &lt;- c(\"NDWI\", \"TCB\")\nneg_indices &lt;- c(\"NBR\", \"NBR2\", \"NDMI\", \"NDVI\", \"TCG\", \"TCW\")\n\n\npos_thresholds &lt;- seq(0.01, 0.5, by = 0.01)\nneg_thresholds &lt;- seq(-0.01, -0.5, by = -0.01)\n\n\n\n# prepare empty results list\nresults_list &lt;- list()\n\n# loop through indices_values \nfor (i in seq(1, length(indices_values))) {\n\n  # select values for one index\n  ind = indices_values[[i]]\n\n  # get the name of the index\n  ind_name &lt;- names(indices_values)[i]\n\n  # check whether to apply positive or negative thresholds\n  if ((ind_name %in% pos_indices) == TRUE) {\n\n    thresholds &lt;- pos_thresholds\n\n  } else {\n\n    thresholds &lt;- neg_thresholds\n  }\n\n  # prepare a names list to be applied for each result \n  # name is containing threshold value with \".\" replaced by \"_\"\n  names_list &lt;- paste0(\"Threshold_\", gsub(\"\\\\.\", \"_\", thresholds)) \n\n  # apply the functions where conditions are checked for each threshold value\n  # in other words: take index values and for each threshold value in the \n  # thresholds list check the conditions in apply_tvcma_conditions_points\n  thresholds_results &lt;- lapply(thresholds, \n                               apply_tvcma_conditions_points, \n                               ind = ind)\n\n  # set names for each separate result\n  thresholds_results &lt;- setNames(thresholds_results, names_list)\n\n  # apply calculate_confusion_matrix_stats for each result\n  # each time take separate result (one result for one threshold tested)\n  # calculate the statistics by comparing the result to validation table\n  stats_list &lt;- lapply(seq_along(thresholds_results), \n                       function(i) calculate_confusion_matrix_stats(\n                         matrix = thresholds_results[[i]],\n                         validation_matrix = val_table,\n                         set_name = names(thresholds_results)[i]))\n\n  # bind all the results for one index together\n  # 50 rows, one per threshold, containing calculated statistics\n  stats_df &lt;- do.call(rbind, stats_list)\n\n  # save the results to .xlsx file\n  write_xlsx(stats_df, paste0(\"case_study_3/results/\", ind_name, \"_th_min_\", min(thresholds), \"_max_\", max(thresholds), \".xlsx\"))\n\n  # add the table for one index to a list\n  results_list[[i]] &lt;- stats_df\n\n}\n\n\n\nmultiband_raster &lt;- indices_imgs$NDMI\nthreshold &lt;- -0.09\n\n# empty array to store the results\nresult_array &lt;- array(0, dim = c(nrow(multiband_raster), ncol(multiband_raster), nlyr(multiband_raster) - 1))\n\n# open a file for reading\nreadStart(multiband_raster)\n# for each row in input image\nfor (i in 1:nrow(multiband_raster)) {\n\n  # extract raster values from the image row \n  # each row contains 2255 x 39 values\n  row_values &lt;- readValues(multiband_raster, row = i, nrows = 1, col = 1, ncols = ncol(multiband_raster), mat = TRUE)\n\n  # prepare empty vector to store the TVCMA result\n  results &lt;- rep(NA, ncol(multiband_raster) * (nlyr(multiband_raster) - 1))\n\n  # apply TVCMA conditions to extracted raster values\n  for (k in 1:nrow(row_values)) {\n\n    values &lt;- row_values[k, ]\n    val_len &lt;- length(values)\n\n    if (threshold &gt;= 0) {\n\n      cond_1 &lt;- (values[3:((val_len) - 1)] - values[2:(val_len - 2)]) &gt; threshold\n      cond_2 &lt;- (values[4:(val_len)] - values[2:(val_len - 2)]) &gt; threshold\n      cond_3 &lt;- (values[3:((val_len) - 1)] - values[1:(val_len - 3)]) &gt; threshold\n\n      r1 &lt;- cond_1 &amp; cond_2 &amp; cond_3\n      r2 &lt;- (values[2] - values[1]) &gt; threshold &amp; (values[3] - values[1]) &gt; threshold\n      r3 &lt;- (values[val_len] - values[val_len - 1]) &gt; threshold &amp; (values[val_len] - values[val_len - 2]) &gt; threshold\n\n    } else {\n\n      cond_1 &lt;- (values[3:((val_len) - 1)] - values[2:(val_len - 2)]) &lt; threshold\n      cond_2 &lt;- (values[4:(val_len)] - values[2:(val_len - 2)]) &lt; threshold\n      cond_3 &lt;- (values[3:((val_len) - 1)] - values[1:(val_len - 3)]) &lt; threshold\n\n      r1 &lt;- cond_1 &amp; cond_2 &amp; cond_3\n      r2 &lt;- (values[2] - values[1]) &lt; threshold &amp; (values[3] - values[1]) &lt; threshold\n      r3 &lt;- (values[val_len] - values[val_len - 1]) &lt; threshold &amp; (values[val_len] - values[val_len - 2]) &lt; threshold\n    }\n\n    result &lt;- c(r2, r1, r3)\n    results[(((k - 1) * (val_len - 1)) + 1):(k * (val_len - 1))] &lt;- result\n\n  }\n\n  # number of output layers (38)\n  n_layers_minus_one &lt;- nlyr(multiband_raster) - 1\n\n  # insert TVCMA results to result array\n  for (j in 1:ncol(multiband_raster)) {\n\n    index_start &lt;- (j - 1) * n_layers_minus_one + 1\n    index_end &lt;- j * n_layers_minus_one\n    result_array[i, j, ] &lt;- results[index_start:index_end]\n  }\n\n  # print progress\n  print(paste0(\"Row: \", i, \"/\", nrow(multiband_raster)))\n\n}\n# close a file from reading\nreadStop(multiband_raster)\n\n\n# transform array into raster with coordinate reference system\nresults_img &lt;- rast(result_array,\n                    extent = ext(multiband_raster),\n                    crs = crs(multiband_raster))\n\n# name the bands\nnames(results_img) &lt;- as.character(paste0(\"TVCMA_\", seq(1985, 2022)))\n\n# mask the results with forest mask\nresults_img &lt;- mask(results_img, forest_mask)\n\n# write 1/0 TVCMA image\nwriteRaster(results_img, \n            filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_1_0.tif\",\n            datatype = \"INT1U\")\n\n# write 1/NA TVCMA image\nresults_img_1_na &lt;- subst(results_img, from = 0, to = NA,\n                         filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_1_NA.tif\",\n                         datatype = \"INT1U\")\n\n# get year of detection \nyod &lt;- seq(1985, 2022)\n\n# iterate over each band and replace values of 1 with the year of detection value\nresults_yod &lt;- lapply(1:length(yod), function(i) {\n  band &lt;- results_img_1_na[[i]]\n  band &lt;- band * yod[i]\n  return(band)\n})\n\n# make a raster out of year of detection list\nresults_yod_rast &lt;- rast(results_yod)\n\n# create a raster showing the latest detected disturbance \nresults_latest &lt;- max(results_yod_rast, na.rm = TRUE)\n\n\nwriteRaster(results_latest, \n            filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_latest.tif\",\n            datatype = \"INT2U\")\n\n# create a raster showing the earliest detected disturbance\nresults_earliest &lt;- min(results_yod_rast, na.rm = TRUE)\n\nwriteRaster(results_earliest, \n            filename = \"case_study_3/results/TVCMA_NDMI_th_-0.09_earliest.tif\",\n            datatype = \"INT2U\")\n\n</code></pre>"},{"location":"module3/module3.html","title":"3D/4D Geographic Point Cloud Time Series Analysis","text":"<p>Surface dynamics within a local landscape occur on a large range of spatiotemporal scales. The analysis of surface activities and structural dynamics in 4D point cloud data has therefore become an integral part of Earth observation. These data contain detailed 3D information of the topography with time as additional dimension. Information derived on surface dynamics from 3D time series can provide new insights into Earth surface processes and human-environment interaction. Therefore, in this module you will learn to:</p> <ul> <li>handle 3D/4D point cloud data and perform fundamental operations </li> <li>perform point cloud analysis with Python and automate workflows</li> <li>differentiate concepts of change analysis</li> <li>apply methods for change detection and analysis in 3D point clouds</li> <li>derive change information from multitemporal point clouds and 3D time series </li> <li>analyze 4D point clouds with automatic workflows using open-source Python tools</li> <li>apply machine learning methods for 3D/4D point cloud analysis (supervised and unsupervised classification, supervised regression)</li> <li>conduct case study analyses with 4D point clouds in different use cases.</li> </ul>"},{"location":"module3/module3.html#structure","title":"Structure","text":"<p>The module is structured into the following themes:</p> <ul> <li>Principles of 3D/4D geographic point clouds</li> <li>Programming for point cloud analysis with Python</li> <li>Principles and basic algorithms of 3D change detection and analysis</li> <li>Time series analysis of 3D point clouds</li> <li>Machine learning-based 3D/4D point cloud analysis</li> <li>Case study: Multitemporal 3D change analysis at an active rock glacier</li> <li>Case study: Time series-based change analysis of sandy beach dynamics</li> </ul>"},{"location":"module3/module3.html#prerequisites-to-perform-this-module","title":"Prerequisites to perform this module","text":"<p>The following skills and background knowledge are required for this module.</p> <ul> <li>Basics of statistics</li> <li>Basics of geoinformation systems and handling raster/vector data</li> <li>Principles of remote sensing</li> <li>Basic programming skills (Python will be used here)</li> </ul> <p>Follow this link for an overview of the listed prerequisites and recommendations on external material for preparation.</p>"},{"location":"module3/module3.html#software","title":"Software","text":"<p>For this module, you will need the software listed below. Follow the links to the individual software or tools, for help in setting them up.</p> <ul> <li>CloudCompare for point cloud visualization and editing</li> <li>QGIS for visualization and editing of results (e.g., Digital Terrain Models)</li> <li>Python for programming of point cloud analysis</li> </ul>"},{"location":"module3/module3.html#use-cases-and-data","title":"Use Cases and Data","text":"<p>In the research-oriented case studies, this module uses multitemporal 3D point clouds of an active rock glacier and 4D point clouds of a sandy beach.</p>"},{"location":"module3/module3.html#start-the-module","title":"Start the module","text":"<p>... by proceeding to the first theme on Principles of 3D/4D geographic point clouds.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html","title":"Principles of 3D/4D geographic point clouds","text":"<p>In this theme, you will learn about 3D/4D point cloud data, fundamental point cloud operations and concepts of change analysis:</p> <ul> <li>Introduction to Geographic 3D/4D Point Clouds</li> <li>Point Cloud Model and Fundamental Operations</li> <li>Data Management of 4D Point Clouds</li> </ul> <p>The theme will conclude with:</p> <ul> <li>Self-evaluation quiz</li> <li>Exercise</li> <li>References</li> </ul> <p>Afterwards, you will understand a broad set of topics required for analyzing topographic change in point cloud time series. You will apply this knowledge in the exercise, and require it for all subsequent themes.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#introduction-to-geographic-3d4d-point-clouds","title":"Introduction to Geographic 3D/4D Point Clouds","text":""},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#importance-of-3d4d-point-clouds-in-geography","title":"Importance of 3D/4D Point Clouds in Geography","text":"<p>The third dimension of geospatial data is essential for a multitude of research and applications fields. 3D geodata contains information about the geometry and structure of the surface, and dynamics therein. This information is useful for researching the physical environment or impacts of climate change. For example, changes to a glacier or the coast indicate effects of environmental processes and can ultimately be linked to climate change. Processes of vegetation growth and disturbance are inherently dynamic and benefit from observation at different temporal scales (e.g., regarding processes with day-night patterns, seasonal variations, and developments over multiple years) (Guo et al., 2020). 3D geodata can further be useful in natural hazard monitoring and disaster management, e.g. for the observation of areas that are affected by rockfalls, avalanches or landslides. For a broad variety of applications, 3D point clouds together with temporal information help us to observe landscape evolution and to understand and monitor spatiotemporal dynamics of Earth surface phenomena (Eitel et al., 2016)</p> <p>Analysis of surface or structural dynamics in 3D/4D geodata provides information in different domains:</p> <ul> <li> <p>Mapping of objects or landforms. With regards to monitoring, mapping can regard certain landscape processes which are characterized by specific spatiotemporal properties. An example is visualized with avalanches in a snow-covered scene in the figure below (left). The process, e.g. an avalanche, can be mapped based on the change information, rather than on geometric or spectral properties of the scene (Anders et al., 2022).</p> </li> <li> <p>Characterization of surface dynamics can describe their properties, i.e. how the surface or an object changed. The figure below (middle) illustrates changes related to sand transport at the surface of a mega dune form (Herzog et al., 2022). The volume and direction of change allow an interpretation on drivers of sand transport and mechanisms of dune evolution.</p> </li> <li> <p>Modelling of Earth surface processes can make use of information on surface or structural dynamics (as model input or for improved parametrization). Tree structure dynamics (see figure below on the right) can be important, for example, for assessing forest biomass and its dynamics (Wang et al., 2022).</p> </li> </ul> <p> <p>Examples of different domains supported by change analysis of surface or structure dynamics in 3D/4D point clouds. Figures by K. Anders (left, middle) and Wang et al. (2022) / CC BY-NC-ND 4.0 (right). </p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#multitemporal-and-4d-point-clouds","title":"Multitemporal and 4D Point Clouds","text":"<p>For capturing and analyzing dynamics in the topography, the temporal dimension can be added to 3D point cloud data through repeat acquisition. Each point cloud acquired at one point in time represents a so-called epoch. Such repeat measurements provide multitemporal point clouds and changes can be assessed between epochs. We refer to time series of point clouds, if the acquisition is repeated to obtain a sequence of data. These time series are often acquired at more or less regular intervals, e.g., hourly, monthly, or annually (there is no strict definition). </p> <p>Multitemporal or time series of point clouds are widely available through repeat surveying at the national level, for example. Airborne laser scanning (ALS) point clouds of several years are openly available, e.g., for The Netherlands (https://www.ahn.nl/) or Norway (https://kartverket.no/en). Further open data is made available on geographic data repositories such as Open Topography or PANGAEA, often by research projects.</p> <p>When point clouds are acquired at higher frequencies, the data may represent surface dynamics in their evolution rather than at single points in time (e.g., before and after an event or surface process). For such near-continuous temporal sampling, we speak of 4D point clouds, as they contain a full temporal domain in addition to the 3D spatial domain (3D space + 1D time).</p> <p>The figure below illustrates the additional information that can be obtained from 4D point clouds, where surface processes are represented near-continuously. The example is taken from the use case of beach monitoring. The map visualizes surface changes due to sand transport over three weeks. In this timespan, a sand bar formed which becomes apparent in the elongated shape of sand volume increase. As hourly point cloud data is available at this site, we can inspect the time series of topographic measurements, for example, at one location on the sand bar. From this, we can identify when the sand bar started forming via the onset of surface increase, and when it disappears at a later point (after Feb-27). We can derive further temporal properties, such as the change rate and also determine the highest magnitude of the sand bar during its existence. Overall, the example demonstrates how surface dynamics can be characterized in their spatiotemporal properties using 4D point clouds, compared to more limited information with sparse time series or multitemporal point clouds of few epochs. </p> <p> <p>Temporal surface information in bitemporal change representation (map on the left) and as near-continuous time series at a location in 4D point clouds (right). Figure by K. Anders, modified after Anders et al. (2019). </p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#concepts-of-change-in-3d4d-point-clouds","title":"Concepts of Change in 3D/4D Point Clouds","text":"<p>This section gives a brief overview on concepts of change in multitemporal and 4D point clouds and on a typical change analysis workflow. Details on principles and methods of 3D/4D change analysis are part of Theme 3.</p> <p>Using topographic 3D point clouds, we are typically looking for changes in the geometry of the surface or objects. In general, a change analysis requires the following typical series of steps:</p> <ol> <li>Data acquisition: Acquiring 3D/4D point clouds for the observation of surface dynamics includes considerations of typical aspects such as the geometric accuracy, spatial resolution and coverage (choice of sensors, platforms, and survey strategy). Regarding monitoring applications, an additional important consideration is the temporal resolution, i.e. acquisition frequency, with respect to the observed phenomena. An overview of different modes of point cloud acquisition will be given in the following section.</li> <li>Co-registration: Epochs in multitemporal 3D/4D datasets need to be aligned in a common coordinate reference frame to enable change measurements. This can be solved by accurate georeferencing of acquired data, but often additional fine alignment is applied to further increase the detectability of 'real' change, opposed to small offsets between surface positions in the point cloud. </li> <li>Change detection and quantification: This step is the core of any change analysis. Depending on the geographic setting and types of input data, there are various approaches to change analysis, and different methods using point clouds and rasterized data. Specific approaches and algorithms will be introduced in Theme 3. </li> <li>Uncertainty assessment: Assessment of uncertainties associated to change detection and quantification is essential to interpret and communicate results. Different components contributing to uncertainty in change analysis and how to quantify them are part of Theme 3.</li> <li>Change representation: The final step in change analysis is the representation of changes, which is closely linked to the characteristics of observed surface dynamics and the analysis objective. Depending on the objective, the representation of uncertainties can even be essential to decision making, for example in the case of natural hazard monitoring.</li> </ol> <p>To further familiarize yourself with approaches and applications of 3D change analysis, look into the review article by Qin et al. (2016). For a first interactive experience of topographic change analysis from point cloud data, check out this on-demand tool by OpenTopography, coming with an informative tutorial video (Scott et al., 2021). It will perform simple vertical differencing of the elevation, and nicely visualize topographic change, in this example, due to dune migration:</p> <p> <p>Result of on-demand topographic differencing in OpenTopography showing the hillshades of terrain elevation for each LiDAR survey (left and middle), and resulting vertical differences (right). Data: White Sands National Monument, NM: LiDAR Survey of Dune Fields (Sept 2009). Lidar data acquisition and processing completed by the National Center for Airborne Laser Mapping (NCALM). NCALM funding provided by NSF's Division of Earth Sciences, Instrumentation and Facilities Program (EAR-1043051). Distributed by OpenTopography / CC BY 4.0. </p> <p>The remainder of this theme will focus on basic principles of point clouds acquisition and processing for change analysis or monitoring in general.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#point-cloud-model-and-fundamental-operations","title":"Point Cloud Model and Fundamental Operations","text":"<p>This section introduces sources and characteristics of point clouds, the corresponding data model, and fundamental point cloud operations as a basis to processing steps in 3D/4D point cloud analysis.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#point-cloud-acquisition","title":"Point Cloud Acquisition","text":"<p>3D point clouds can be acquired with different sensing techniques and survey strategies. The most common source of point cloud data is laser scanning (Otepka et al., 2013; Telling et al., 2017) and photogrammetry (via dense image matching techniques; Passalacqua et al., 2015). A broad overview of Earth observation sensors, platforms, and their properties is given in Theme 1 of Module 1. Regarding 3D/4D point clouds, the acquisition mode spans terrestrial, UAV-borne, and airborne (i.e. aircraft-based) platforms. The acquisition platform is decisive for the spatial and temporal scales at which data can be captured.</p> <p>Airborne acquisition is usually not performed more than once a year, due to the high cost and effort. UAV-based acquisition can be repeated more frequently, but still requires operators to be on site, which also poses a cost factor for each repetition of a survey. The repetition interval of terrestrial laser scanning (TLS), at the moment, is limited only by the duration of individual scans themselves. A scan can be repeated as soon as the acquisition of a current scan is finished, if the instrument remains at one, fixed position. </p> <p>Automatic TLS has become possible with the availability of programmable laser scanning instruments. By setting up TLS instruments in fixed positions for several days to years, permanent 3D observation systems have been developed, which acquire the surrounding scene at scheduled intervals from minutes to days (Eitel et al., 2016). An alternative to laser scanning are fixed installations of time-lapse 3D photogrammetry. A collection of selected examples for both are given with the figures below:</p> <p> <p>Selected examples of automatic TLS for 4D point cloud acquisition in different geographic settings. Figures by Kromer et al. (2017) / CC BY 3.0, Campos et al. (2021) / CC BY 4.0, Stumvoll et al. (2020) / CC BY 4.0, Williams et al. (2018) / CC BY 4.0, Winiwarter et al. (2022a) / CC BY 4.0, Vos et al. (2022) / CC BY 4.0, and Voordendag et al. (2021) / CC BY 4.0.  </p> <p> <p>Example of automatic time-lapse photogrammetry for 4D point cloud acquisition for rockfall monitoring. Figure by Kromer et al. (2019) / CC BY 4.0. </p> <p>Accordingly, permanent TLS and 3D photogrammetry has been used to observe vegetation dynamics (Campos et al., 2021), landslides (Kromer et al., 2017; Stumvoll et al., 2020), rockfalls (Kromer et al., 2019; Williams et al., 2018), glaciers (Voordendag et al., 2021), beach environments (O'Dea et al., 2019; Vos et al., 2022), and soil erosion (Eltner et al., 2017), among others. More information on the requirements and special characteristics of permanent 3D acquisition is given in Theme 4 of this module.</p> <p>An alternative strategy for generating point clouds is virtual laser scanning (VLS), which has gained increasing importance in recent years. VLS simulates the acquisition of LiDAR point clouds by scanning virtual 3D scenes by any combination of input 3D models, platforms, and sensors. The suitability of simulated laser scanning data mostly depends on the application, and simulations that are more realistic come with stricter requirements on input data quality and higher computational costs (Winiwarter et al., 2022b). VLS can be useful in different use cases: survey planning, method development, and generation of training data for machine learning.</p> <ul> <li>Survey planning: The objective of data acquisition is typically to minimize cost and effort while obtaining data that is fit for the purpose of the planned analysis, e.g. characterization of specific surface processes. With VLS, a 3D point cloud can be generated and tested for its usability in the planned application, by applying the planned analysis method. </li> <li>Method development: In 3D/4D point cloud analysis, we are often developing methods to extract certain features describing our objects or surface processes of interest. Real point cloud data to calibrate algorithms and validate results are costly to acquire and not error free. Here, VLS can be used to generate data with perfectly know reference values, e.g., if synthetic changes are applied to a virtual scene in case of 4D applications.</li> <li>Generation of training data for machine learning algorithms: Recently, several methods for deep learning on point clouds have been presented. However, such methods require immense amounts of training data to achieve acceptable performance. We present how VLS can be used to generate training data in machine learning classifiers, and how different sensor settings influence the classification results.</li> </ul> <p>A pioneer study using 4D VLS point cloud for change detection is presented by Winiwarter et al. (2022a). In this study, VLS point clouds of different ALS acquisition settings are generated to evaluate at which flight altitude a specific target change can be detected using an available analysis method. This consideration has high practical relevance, since it supports decision making of stakeholders, for example, in the context of natural hazard management. </p> <p>VLS data can be acquired using the scientific open source software HELIOS++ (Winiwarter et al., 2022b). HELIOS++ is implemented in C++ for optimized runtimes, and provides bindings in Python to allow integration into scripting environments (e.g., GIS plugins, Jupyter Notebooks). A variety of model types to represent 3D scenes are supported: terrain models, voxel models, and mesh models. As platforms, four options are currently supported: airplane, multicopter, ground vehicle and static tripod. In the figure below, a schematic diverging laser beam and its corresponding waveform (magenta) is shown being emitted from the airplane and interacting with a mesh model tree and the rasterized ground surface.</p> <p> <p>Schematic concept of virtual laser scanning with HELIOS++, showcasing platforms (tripod, car, UAV, airplane) and 3D object models (mesh, point cloud, voxels, raster) composing a scene. Figure by Winiwarter et al. (2022b) / CC BY 4.0. </p> <p>In this module, you will learn to create VLS point clouds with HELIOS++ for your own analyses (exercise in Theme 2). For alternative implementations of LiDAR simulation and their specific purposes and differences, see the detailed literature review in Winiwarter et al. (2022b)</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#point-cloud-data-model","title":"Point Cloud Data Model","text":"<p>In this section, we have a closer look at the data model of point clouds and their characteristics depending on the data source and acquisition mode. Aspects beyond the point cloud itself, such as metadata and supplementary data, will be considered in a later section on data management of 4D point clouds.</p> <p>A point cloud is an unorganized set of points in three-dimensional cartesian space (Otepka et al., 2013). Each point has three coordinates X, Y, and Z, and can have an arbitrary number of attributes (or none). The composition of a point cloud is illustrated in the figure below, with the corresponding data entries in tabular format. Therein, one point is stored in each row with attributes in columns in addition to the xyz coordinate.</p> <p> </p> <p> <p>Visual and tabular representation of a 3D point cloud with three coordinates (XYZ) and attributes classification (ID), LiDAR backscatter (Intensity), and true color (RGB). Figure by K. Anders, modified after Ghamisi et al. (2019). </p> <p>Typical attributes for LiDAR point clouds (i.e. acquired via laser scanning) are the LiDAR intensity and information on multiple returns. LiDAR measurements do not include color information, but RGB information can be added from images that are taken in addition to scanning (Ghamisi et al., 2019). This step belongs to the concept of data fusion. Point clouds which are acquired with photogrammetric techniques directly contain RGB as attribute from the original photographs, which are used for 3D reconstruction through dense image matching (Westoby et al., 2012).</p> <p> <p>Fusion of photographic data with a LiDAR point cloud by assigning color values as RGB attributes to the point cloud. Figure by K. Anders, modified after Ghamisi et al. (2019). </p> <p>Point clouds can be used to derive other data models or GIS layers as result of analysis. The most common derivative of point clouds are rasters. They can be 2D projections of any attribute information onto raster cells. Also the geometric information can be gridded to represent the topography. Deriving, for example, a Digital Terrain Model (DTM) from point clouds results in a 2.5D represenation of the topography: 2D raster locations with elevation values. This typically entails decisions for a fixed resolution and interpolation of data, e.g. when gridding the terrain in a DTM (Pfeifer &amp; Mandlburger, 2008). The unorganized nature of the point cloud allows to have different point spacing within a scene, i.e. a denser representation of the terrain by 3D points in some parts than in others. In contrast, a rasterized terrain representation either has exactly one value or no value (NoData) per cell:</p> <p> <p>Comparison of terrain representation as point cloud and raster. Figure by course authors. </p> <p>The geometry of full 3D objects, such as trees, cannot be represented in rasters. Especially when analyzing complex geometries or surface dynamics, conducting analyses in full 3D has the important advantage of avoiding information loss compared to raster-based analysis. </p> <p> <p>Representation of terrain, surface and object structure in 3D point clouds. Figure by course authors. </p> <p>In this module, you will learn about change analysis methods using the full 3D data provided by point clouds, as well as raster-based methods when deriving rasterized terrain models from point cloud data (especially in Themes 2 and 3). If you would like to get deeper into concepts of terrain interpolation from 3D point clouds, take a look at Pfeifer &amp; Mandlburger (2008).</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#multisource-and-multitemporal-data-fusion","title":"Multisource and Multitemporal Data Fusion","text":"<p>3D point clouds of a scene are often available from multiple sources. Particularly in case of multitemporal data, point clouds may have been acquired from various sensors and platforms throughout a full observation period of a site throughout many years. For change analysis, it is often desirable to use as many point cloud epochs as possible, to assess the state of the topography at many points in time. Combining data from multiple sources and/or multiple points in time is referred to as data fusion (Ghamisi et al., 2019).</p> <p>For point clouds, an example of multisensor fusion is the combination of laser scanning and photogrammetric point cloud. This can regard attributes, which are assigned from one point cloud to the other based on spatial proximity of points (we will have a look at neighborhood operations in the next section). Apart from an increase in attributes, fusion can yield an increase in coverage. For example, terrestrially acquired point clouds which suffer from occlusion can be complemented by UAV-borne point clouds with higher completeness in certain areas of the scene. Multitemporal fusion regards the combination of measurements which were acquired at different points in time.</p> <p>Temporally, point clouds can be fused to obtain one multitemporal point cloud dataset or time series to perform change analysis on (Ghamisi et al., 2019). With respect to the data model, temporal fusion can yield a time series where each epoch is a separate point cloud. Accordingly, the spatial sampling between epochs can differ. This applies even for repeat acquisitions from a single, fixed TLS position, as no point in a scene is ever measured twice by 3D surveys. Alternatively, temporal fusion can add measurements per epoch as attribute to one single point cloud, thereby storing the change in a feature (incl. geometric features) for the same set of 3D points. Assigning multitemporal measurements, possibly from different data sources, to the same 3D points, requires spatial neighborhood operations - a fundamental step in any 3D data analysis - which are introduced in the following section.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#neighborhood-operations-and-neighborhood-based-attributes","title":"Neighborhood Operations and Neighborhood-Based Attributes","text":"<p>Calculations based on the neighborhood are among the most important operations when working with point cloud data, besides standard geospatial operations, such as attribute-based calculations, coordinate transformations and distribution measures. They are required for almost any filtering, smoothing or interpolation step, and for (surface) modeling and information extraction. Neighborhoods mainly regard the spatial neighborhood of 3D points, but can also include the temporal neighborhood.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#spatial-neighborhood","title":"Spatial Neighborhood","text":"<p>Spatial neighborhoods in a point cloud are subsets of all points in a point cloud defined either by a fixed distance or by a fixed number of nearest neighbors (Weinmann et al., 2015).  The fixed distance can be in 2D. It is then called a cylindrical neighborhood. This means that all 3D points belong to a local neighborhood whose 2D projection on a plane are within a circle of given radius. For a horizontal plane, this means that all points whose XY coordinate is within the radius around the search point belong to the neighborhood of that point - irrespective of their height (Z value). Fixed-distance neighborhoods can also be 3D. In this case they are spherical neighborhoods, as the neighborhood radius is considered in all three dimensions.</p> <p> <p>Cylindrical point cloud neighborhood of a 3D point (black) with a fixed 2D distance of radius r in a schematic view from the side (left) and above (right). Figure by course authors. </p> <p>Besides radius-based distances, one can also use box neighborhoods in either 2D or 3D. In 2D, a neighborhood is then similar to points lying within a raster cell. In 3D, the 2D cell becomes a box and the vertical point position is considered.</p> <p>In contrast to a fixed distance, a point cloud neighborhood can be defined by a fixed number of neighbors. The number of neighbors is conventionally specified as k, so that the concept is known as k nearest neighbors (kNN). The search of kNN around a point can, again, be performed in 2D or in 3D Euclidian distances. The size of kNN neighborhoods can be highly different within a neighborhood, depending on the local point density. In the same way, the number of neighbors in the fixed-distance neighborhood varies depending on the point density.</p> <p> <p>Point cloud neighborhood around a 3D search point (black) with a fixed number of k nearest neighbors in 2D (left) and 3D (right). When considering the 3D Euclidian distance, the point neighborhood may change, because a point may have a vertically large distance compared to another point which is horizontally further away (indicated by the red connection lines). Figure by course authors. </p> <p>We generally need to consider two aspects when deciding on the definition of the spatial neighborhood in a point cloud:</p> <ol> <li>Neighborhoods may be empty when using a fixed distance, if there is no other point in the point cloud at the specified distance around a search point.</li> <li>Neighborhoods may not be unique when using a fixed number of neighbors, for example if a group of 3D points is spatially isolated so that each of the points shares the same neighborhood including each other (i.e., same set of kNN).</li> </ol> <p>For some applications, it may be helpful to combine a fixed distance neighborhood with a fixed number of kNN to overcome drawbacks of each. Accordingly, one may avoid empty neighborhoods by setting a large distance radius, but restricting the number of neighbors to the k nearest points. Depending on the purpose of the operation, there are even more advanced methods for neighborhood selection, such as quadrant- or octant-based kNN. If you would like to learn more about this, you may look into Pfeifer et al. (2014).</p> <p>So what can the neighborhood of a point tell us? A basic attribute to be derived from the spatial neighborhood is the point density within a point cloud. The 3D point density, for example, can be derived as the number of points (kNN) within the radius of a local sphere:</p> \\[  D = {k+1 \\over {4 \\over 3} \\pi r{^3}_{kNN}}  \\] <p>By calculating the point density like this for every point in a point cloud, we can tell for the entire point cloud how many points are contained within a radius of, e.g., 1.0 m, in a certain area of the scene. This information can also be used to identify isolated points and removing outliers. A common method for this is the statistical outlier removal (SOR; Rusu &amp; Cousins, 2011). A selection of further useful point cloud features to be derived from the spatial neighborhood are: * normal vectors * principal components on local point distribution * the local surface roughness</p> <p>We will have a look at these in the remainder of this theme. But first, since we are dealing with time series, let's have a look at the concept of temporal neighbors.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#temporal-neighborhood","title":"Temporal Neighborhood","text":"<p>The temporal neighborhood regards measurements of a variable at the same spatial location prior or successively to a certain point in time. For time series, the neighborhood is typically defined as a temporal window of a certain timespan. For 3D/4D point clouds, most times a combination of spatial and temporal neighborhoods is used. In the same way that spatial neighbors are searched among the 3D points within one point cloud, they may be searched in another, separate point cloud. You may note: looking for spatially close points in the temporal neighborhood (i.e., another point cloud epoch), yields information on how properties of these neighboring points change over time, e.g. through attribute differences or height differences. Being aware of possible neighborhood operations in multitemporal point clouds, gives you already one point cloud-based approach of change analysis (with more to come in Theme 3 of this module).</p> <p>The spatial-temporal neighborhood is used in an approach for 4D point cloud processing by Kromer et al. (2015) to filter noise from the data. The concept uses the redundancy of frequent and spatially dense measurements in 4D point clouds and averages each point to the median of points which are neighboring in space and time:</p> <p> <p>Spherical spatial neighborhoods (left) and temporal window neighborhood (right) are combined to a spatiotemporal neighborhood (middle) to perform noise filtering in 4D point clouds, with number of neighbors NN, reference point cloud T<sub>ref</sub> and temporal window T<sub>step</sub>. Figure modified from Kromer et al. (2015) / CC BY 4.0. </p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#normal-vector-and-3d-structure-tensor","title":"Normal vector and 3D structure tensor","text":"<p>Coming back to the spatial neighborhood, this section introduces the concept of the normal vector in 3D point clouds. The normal vector is the surface normal of a tangent plane. Fitting a plane to a set of points therefore allows us to characterize point neighborhoods as local surfaces, even though each 3D point itself has no dimensionality.</p> <p>So if you think of a surface in a scene which is sampled by a 3D point cloud and fit a tangent plane to these measurements, the normal vector is perpendicular to the plane and reflects the direction into which the plane is oriented:</p> <p> <p>Normal vector of a tangent plane fit to a set of 3D points, which are sampled on a surface in the scene. Figure by course authors. </p> <p>The normal vector of a plane in 3D has three direction components \\(n_x\\), \\(n_y\\), and \\(n_z\\), which describe the direction in three-dimensional space.</p> <p>The normal vector of a point in a point cloud is estimated by adjusting a plane to all points in its local neighborhood. A plane can be adjusted to a set of point using least-squares regression. In 3D space, this means minimizing the orthogonal distances of points to the (unknown plane), i.e. finding the best set of plane parameters. The result of this plane adjustment are the plane parameters (see below) and the residual distance of the adjustment based on point distances to the fitted plane (Dorninger &amp; Nothegger, 2007).</p> <p>The plane parameters derived with a plane fit in 3D cartesian space are based on the Hessian normal form with the following equation:</p> \\[  0 = d + n_x + n_x + n_z  \\] <p>with \\(n_x\\), \\(n_y\\), \\(n_z\\) representing the normal vector, and \\(d\\) representing the normal distance of the plane to the origin of the Cartesian coordinate system. </p> <p> <p>Definition of a plane with parameters according to the Hessian normal form. Figure by Quartl / CC BY-SA 3.0. </p> <p>As surfaces in point clouds scenes are rarely perfectly planar, a simple least-squares plane fit is not robust to outliers or edges, where part of the neighboring points strongly deviate from the actual planar surface. Robust local regression planes can be derived by considering outliers using weighting of points based on their residual distances, and excluding outliers from the final plane adjustment (Dorninger &amp; Nothegger, 2007).</p> <p>In the context of the local regression plane, there is more information to be obtained about local spatial point cloud characteristics to describe the scene. Principal component analysis (PCA) allows us to assess how points are dispersed around their center (search point or centroid). By spanning the covariance matrix, we can derive the 3D structure tensor formed by eigenvalues and eigenvectors. These describe the principal components of directions into which 3D points are dispersed. If you are not familiar with the concept of PCA and eigenvalue decomposition, you may have a look at this tutorial (VanderPlas, 2016).</p> <p> <p>Schematic of principal components for two sets of 3D point. Figure by course authors. </p> <p>If we look at the box in the figure above, the points are dispersed into all three directions. Accordingly, the eigenvectors are pointing into all directions with similar magnitude, i.e. the eigenvalues have the same scale. The example of a plane (right figure above) demonstrates how the components of the eigenvector on the plane are very large (red and green arrows), whereas the component perpendicular to the plane is small (blue arrow). Thereby, PCA provides information about the planarity of a point neighborhood: if you take the smallest eigenvalue, denoted by \\(\\lambda\\), this represents the direction where there is least dispersion of points. The corresponding eigenvector represents the surface normal of the plane. If you'd like to go further into normal estimation using PCA, look into this tutorial by the Point Cloud Library (PCL).</p> <p>Further information on the 3D structure of points can be expressed based on the eigenvalues. For example, the ratio to the sum of eigenvalues describes the change in curvature, which corresponds to local surface variation:</p> \\[ \\lambda_3 \\over {\\lambda_1 + \\lambda_2 + \\lambda_3} \\] <p>with $$ \\lambda_3 &lt; \\lambda_2 &lt; \\lambda_1 $$</p> <p>Find out more about geometric interpretations of the 3D structure tensor and shape features based on eigenvalues in Weinmann et al. (2015).</p> <p>Another point feature to be exploited from the local plane adjustment is the roughness. The surface roughness as point feature expresses the discrepancy between an ideal surface (i.e. plane fit) and the actual surface (point distribution). This roughness can therefore be derived as the standard deviation of residual distances of the plane fit, or via the smallest eigenvalue of the 3D structure tensor (as described for the figure above).</p> <p>For all geometric operations and features, it is important to bear in mind that they are scale-dependent, i.e. always depending on the size of the spatial neighborhood. Consider a plane fit to point neighborhoods in the following figure, with a large radius on the left and smaller radius on the right - resulting in different orientations of the local surfaces. This must be considered when parametrizing any analysis and should be determined regarding the respective objective.</p> <p> <p>Scale-dependent roughness where the local plane fit depends on the size of the spatial neighborhood. Figure by course authors. </p> <p>The following figure shows how different radii affect the derived roughness of a surface. Here you can see the roughness derived for three different radii on a small extent of a laser scanning point cloud of a rock glacier (you will use this use case in the exercise and throughout the module).</p> <p> <p>Roughness derived for radii r of 0.25 m, 5 m and 10 m (left to right) for an area of a rock glacier point cloud, derived as standard deviation of residual distances of points to a local plane fit. Figure by course authors. </p> <p>A smaller radius (0.25 m) yields lower roughness values here, as the planes can be well fit to the local surfaces of boulder faces composing the rock glacier surface. Larger radii yield higher roughness values, as the surface is composed of multiple boulders at these scales, and plane fits represent the overall topography of the rock glacier tongue.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#transformation-of-3d-point-cloud-data","title":"Transformation of 3D point cloud data","text":"<p>Transformation of point cloud data is another fundamental task in many analysis. 3D coordinates of points need to be transformed, for example, to register the data in coordinate reference systems (i.e., georeferencing). Transformation is further used to improve the co-registration between datasets, particularly multitemporal point clouds. There is a dedicated section on alignment for 3D change analysis in the corresponding theme on principles of 3D change analysis. Here, we get familiar with some principles:</p> <p>In general, transformation can be split into three components: translation, rotation, and scaling. Translation regards the shift of the coordinates in X, Y, and/or Z direction. Rotation is conducted around the X-, Y-, and/or Z-axis respectively. applying a rotation, it is crucial to know what the rotation center of the point cloud is. Without any prior translation, the rotation center is at the origin (0, 0, 0) of the Cartesian coordinate system. Translation and rotation represent a rigid transformation, whereas additional scaling would represent affine transformation. Scaling is usually not applied LiDAR measurements, which are accurate in (relative) metric space. Scaling may be required for photogrammetric data, which inherently contain no information on absolute measures in the scene. </p> <p> <p>Components of transformation representing rigid and affine transformation. Figure by course authors. </p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#data-management-of-4d-point-clouds","title":"Data Management of 4D Point Clouds","text":""},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#data-formats","title":"Data formats","text":"<p>Point cloud data can be stored in many formats, including tabular representations of plain-text ascii. The best practice is to adhere to the open binary LAS format, which is the de-facto standard following the specification of the American Society for Photogrammetry and Remote Sensing (ASPRS). With the LAZ format, there is a losslessly compressed version LAS, which strongly reduces file sizes. For repeat acquisitions, or 4D point clouds, data is usually available as one point cloud file per epoch, i.e. point in time of acquisition. </p> <p>In practice, 4D data is often made accessible by appending the observed variables or change attributes to a set of core points (more on that in Themes 3 and 4 of this module). By this, for each location in a 3D scene, the full temporal information on each variable is available (corresponding to a 1D time series).</p> <p> <p>Point cloud model with XYZ coordinates and attributes of a variable A for each point in time t of T epochs in the point cloud time series. Figure by K. Anders, modified after Ghamisi et al. (2019). </p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#metadata-and-supplementary-datasets","title":"Metadata and Supplementary Datasets","text":"<p>As with any type of geodata, metadata on 3D/4D point clouds are essential for their analysis and determining the fitness of purpose for certain tasks. When working with point cloud data, it should always be documented by which instrument the data was acquired (including the sensor, model, platform, measurement accuracy and other specifications). Further metadata regards survey information, so for example the trajectory of aerial/mobile acquisitions or scan positiopns of terrestrial laser scanning, which provides insight into the survey geometry (e.g., measurement range and incidence angle, if not available as point cloud attributes). Other metadata that is required is the coordinate reference system (CRS) of the 3D points in the point clouds. Since LAS format version 1.4 it is required to define the coordinate system in the LAS file. But up to now, CRS information is often not contained in the point cloud file itself and must be separately documented.</p> <p>When point clouds are provided at a certain processing level (cf. Otepka et al., 2013), a full documentation of this processing must be provided and wherever useful, also the files or scripts used to apply the processing. In the context of 4D point clouds, this shall specifically mention registration information. Multitemporal point clouds are often registered to fit some reference point cloud, improving single-epoch georeferencing for change analysis. Any of these point cloud transformations should be provided with additional files containing the matrices. The rationale of this is to always allow reverting modifications of the data. This is relevant, for example, if improved methods become available in the future.</p> <p>And the most important when working with temporal information is to always provide timestamps corresponding to the acquisition of each epoch. Only this time information allows any inference about, for example, change rates or other process properties observed in the data.</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"<p>You have now completed the theme contents. Use the following quiz to evaluate yourself - do you remember the important aspects, did you learn about Principles of 3D/4D geographic point clouds? If you are struggling with the following questions, revisit some of the topics or consider looking into additional resources (e.g., linked in the theme contents).</p>  What is the standard data format to store point cloud files?    LAS    Which of the following attributes is not contained in laser scanning point clouds without performing fusion operations?   XYZ (coordinate)  Intensity (LIDAR backscatter)  RGB (color)   RGB (color)    Check the correct answer: Virtual laser scanning provides a means to   operate a laser scanner in an augmented reality, but no data is generated of the surrounding scene  simulate laser scanning acquisitions of virtual 3D scenes, for example, to test acquisition strategies or new methods  increase the spatial detail of point clouds acquired in real-world scenes by adding virtual point measurements   simulate laser scanning acquisitions of virtual 3D scenes, for example, to test acquisition strategies or new methods    Sort the following typical steps of 3D change analysis by their order. If you do not know what a step means, revisit the contents: Co-registration, Change detection and quantification, Data acquisition, Change representation, Uncertainty assessment    Data acquisition &gt; Co-registration &gt; Change detection and quantification &gt; Uncertainty assessment &gt; Change representation    What useful metadate or supplementary files should be provided with 4D point clouds? Name at least 3.    Instrument used for acquisition, sensor settings, acquisition platform, timestamp of each point cloud epoch, transformation matrices for co-registration, details about applied processing,...    What are the two possibilities to define a spatial neighborhood in 3D point clouds?    Spatial neighborhoods in 3D point clouds can be defined via a fixed distance or a fixed number of neighbors.    Which one of these local point cloud properties cannot be described with the normal vector?   surface orientation  surface roughness  surface density   surface density    Which transformation type is included in affine transformation and not in rigid transformation?   scaling  translation  rotation   scaling    Sort the following 3D acquisitions strategies by time interval, at which acquisitions can be repeated (with reasonable effort and cost):  UAV photogrammetry, Airborne laser scanning, Time-lapse photogrammetry, Terrestrial laser scanning    Time-lapse photogrammetry (seconds to minutes from static cameras) &gt; Terrestrial laser scanning (minutes to hours from static scan position) &gt; UAV photogrammetry (daily to monthly, piloted) &gt; Airborne laser scanning (seasonally to annually, piloted and high cost)    What is the special property of 4D point clouds compared to multitemporal point clouds in general?    With 4D point clouds, the observed variable is represented near-continuously. the acquisition interval is so high that the change process in itself can be described from the data, not only the before and after state as with standard multitemporal point clouds of two or few epochs"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#exercise","title":"Exercise","text":"<p>Made it through the quiz? Then you are ready for the exercise, where you will be getting your hands on multisource, multitemporal point cloud data of an active rock glacier.</p> <p>Exercise</p>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#references","title":"References","text":""},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#key-literature","title":"Key literature","text":"<ol> <li>Otepka, J., Ghuffar, S., Waldhauser, C., Hochreiter, R., &amp; Pfeifer, N. (2013). Georeferenced Point Clouds: A Survey of Features and Point Cloud Management. ISPRS International Journal of Geo-Information, 2 (4), pp. 1038-1065. doi: 10.3390/ijgi2041038.</li> <li>Eitel, J. U. H., H\u00f6fle, B., Vierling, L. A., Abell\u00e1n, A., Asner, G. P., Deems, J. S., Glennie, C. L., Joerg, P. C., LeWinter, A. L., Magney, T. S., Mandlburger, G., Morton, D. C., M\u00fcller, J., &amp; Vierling, K. T. (2016). Beyond 3-D: The new spectrum of lidar applications for earth and ecological sciences. Remote Sensing of Environment, 186, pp. 372-392. doi: 10.1016/j.rse.2016.08.018.</li> </ol>"},{"location":"module3/01_pointcloud_principles/01_pointcloud_principles.html#further-articles-and-referenced-literature","title":"Further articles and referenced literature","text":"<ul> <li>Anders, K., Winiwarter, L., &amp; H\u00f6fle, B. (2022). Improving Change Analysis From Near-Continuous 3D Time Series by Considering Full Temporal Information. IEEE Geoscience and Remote Sensing Letters, 19, pp. 1-5. doi: 10.1109/LGRS.2022.3148920.</li> <li>Campos, M. B. , Litkey, P. , Wang, Y. , Chen, Y. , Hyyti, H. , Hyypp\u00e4, J. &amp; Puttonen, E. (2021). A long-Term Terrestrial Laser Scanning Measurement Station to Continuously Monitor Structural and Phenological Dynamics of Boreal Forest Canopy. Frontiers in Plant Science, 11: 606752. doi: 10.3389/fpls.2020.606752.</li> <li>Czerwonka-Schr\u00f6der, D., Anders, K., Winiwarter L. &amp; Wujanz, D.(2022):Permanent terrestrial LiDAR monitoring in mining, natural hazard prevention and infrastructure protection \u2013 Chances, risks, and challenges: A case study of a rockfall in Tyrol, Austria. 5th Joint International Symposium on Deformation Monitoring (JISDM). doi: 10.4995/JISDM2022.2022.13649.</li> <li>Dorninger, P. &amp; Nothegger, C. (2007). 3D segmentation of unstructured point clouds for building modelling. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 35(3/W49A), pp. 191-196. </li> <li>Eitel, J. U. H., H\u00f6fle, B., Vierling, L. A., Abell\u00e1n, A., Asner, G. P., Deems, J. S., Glennie, C. L., Joerg, P. C., LeWinter, A. L., Magney, T. S., Mandlburger, G., Morton, D. C., M\u00fcller, J., &amp; Vierling, K. T. (2016). Beyond 3-D: The new spectrum of lidar applications for earth and ecological sciences. Remote Sensing of Environment, 186, pp. 372-392. doi: 10.1016/j.rse.2016.08.018.</li> <li>Eltner, A., Baumgart, P., Domula, A. R., Barkleit, A. &amp; Faust, D. (2017): Scale dependent soil erosion dynamics in a fragile loess landscape. Zeitschrift f\u00fcr Geomorphologie Supplementary Issues, 61(3), pp. 191-206. doi: 10.1127/zfg/2017/0409.</li> <li>Ghamisi, P., Rasti, B., Yokoya, N., Wang, Q., H\u00f6fle, B., Bruzzone, L., Bovolo, F., Chi, M., Anders, K., Gloaguen, R., Atkinson, P. M., &amp; Benediktsson, J. A. (2019). Multisource and Multitemporal Data Fusion in Remote Sensing: A Comprehensive Review of the State of the Art. IEEE Geoscience and Remote Sensing Magazine, 7 (1), pp. 6-39. doi: 10.1109/MGRS.2018.2890023.</li> <li>Guo, Q., Su, Y., Hu, T., Guan, H., Jin, S., Zhang, J., Zhao, X., Xu, K., Wei, D., Kelly, M., &amp; Coops, N. C. (2020). Lidar boosts 3d ecological observations and modelings: A review and perspective. IEEE Geoscience and Remote Sensing Magazine, 9(1), pp. 232-257. doi: 10.1109/MGRS.2020.3032713.</li> <li>Herzog, M., Anders, K., H\u00f6fle, B., Bubenzer, O. (2022). Capturing complex star dune dynamics \u2010 Repeated highly accurate surveys combining multitemporal 3D topographic measurements and local wind data. Earth Surface Processes and Landforms, 47(11), pp. 2726-2739. doi: 10.1002/esp.5420.</li> <li>Kromer, R., Abell\u00e1n, A., Hutchinson, D., Lato, M., Edwards, T., &amp; Jaboyedoff, M. (2015). A 4D Filtering and Calibration Technique for Small-Scale Point Cloud Change Detection with a Terrestrial Laser Scanner. Remote Sensing, 7 (10), pp. 13029-13052. doi: 10.3390/rs71013029</li> <li>Kromer, R. A., Abell\u00e1n, A., Hutchinson, D. J., Lato, M., Chanut, M.-A., Dubois, L. &amp; Jaboyedoff, M. (2017). Automated terrestrial laser scanning with near-real-time change detection \u2013 monitoring of the S\u00e9chilienne landslide. Earth Surface Dynamics, 5, pp. 293\u2013310. doi: 10.5194/esurf-5-293-2017.</li> <li>Kromer, R., Walton, G., Gray, B., Lato, M. &amp; Group, R. (2019): Development and Optimization of an Automated Fixed-Location Time Lapse Photogrammetric Rock Slope Monitoring System. Remote Sensing, 11(16). doi: 10.3390/rs11161890.</li> <li>O'Dea, A., Brodie, K. L. &amp; Hartzell, P. (2019). Continuous Coastal Monitoring with an Automated Terrestrial Lidar Scanner. Journal of Marine Science and Engineering, 7(2):37. doi: 10.3390/jmse7020037.</li> <li>Otepka, J., Ghuffar, S., Waldhauser, C., Hochreiter, R., &amp; Pfeifer, N. (2013). Georeferenced Point Clouds: A Survey of Features and Point Cloud Management. ISPRS International Journal of Geo-Information, 2 (4), pp. 1038-1065. doi: 10.3390/ijgi2041038.</li> <li>Passalacqua, P., Belmont, P., Staley, D. M., Simley, J. D., Arrowsmith, J. R., Bode, C. A., Crosby, C., DeLong, S. B., Glenn, N. F., Kelly, S. A., Lague, D., Sangireddy, H., Schaffrath, K., Tarboton, D. G., Wasklewicz, T., &amp; Wheaton, J. M. (2015). Analyzing high resolution topography for advancing the understanding of mass and energy transfer through landscapes: A review. Earth-Science Reviews, 148, pp. 174-193. doi: 10.1016/j.earscirev.2015.05.012.</li> <li>Pfeifer &amp; Mandlburger (2008). LiDAR Data Filtering and DTM Generation. In: Shan &amp; Toth (Edts): Topographic Laser Ranging and Scanning: Principles and Processing. Chapter 11. </li> <li>Pfeifer, N., Mandlburger, G., Otepka, J., &amp; Karel, W. (2014). OPALS - A framework for Airborne Laser Scanning data analysis. Computers, Environment and Urban Systems, 45, pp. 125-136. doi: 10.1016/j.compenvurbsys.2013.11.002.</li> <li>Qin, R., Tian, J., &amp; Reinartz, P. (2016). 3D change detection \u2013 Approaches and applications. ISPRS Journal of Photogrammetry and Remote Sensing, 122, pp. 41-56. doi: 10.1016/j.isprsjprs.2016.09.013.</li> <li>Riegl LMS GmbH: Retrieved from: http://www.riegl.com/.</li> <li>Rusu, R. B. &amp; Cousins, S. (2011). 3D is here: Point Cloud Library (PCL). 2011 IEEE International Conference on Robotics and Automation, pp. 1-4. doi: 10.1109/ICRA.2011.5980567.</li> <li>Schr\u00f6der, D., Anders, K., Winiwarter, L., &amp; Wujanz, D. (2022). Permanent terrestrial LiDAR monitoring in mining, natural hazard prevention and infrastructure protection \u2013 Chances, risks, and challenges: A case study of a rockfall in Tyrol, Austria. 5th Joint International Symposium on Deformation Monitoring (JISDM 2022), doi: 10.4995/JISDM2022.2022.13649.</li> <li>Scott, C., Phan, M., Nandigam, V., Crosby, C. &amp; Arrowsmith, R. (2021). Measuring change at Earth\u2019s surface: On-demand vertical and three- dimensional topographic differencing implemented in OpenTopography. Geosphere, 17 (4), pp. 1318-1332. doi: 10.1130/GES02259.1.</li> <li>Stumvoll, M. J., Canli, E., Engels, A., Thiebes, B., Groiss, B., Glade, T., Schweigl, J. &amp; Bertagnoli, M. (2020): The \u201cSalcher\u201d landslide observatory\u2014experimental long-term monitoring in the Flysch Zone of Lower Austria. Bulletin of Engineering Geology and the Environment, 79, pp. 1831-1848. doi: 10.1007/s10064-019-01632-w.</li> <li>Telling, J., Lyda, A., Hartzell, P., &amp; Glennie, C. (2017): Review of Earth science research using terrestrial laser scanning. Earth-Science Reviews, 169, pp. 35-68. doi: 10.1016/j.earscirev.2017.04.007.</li> <li>VanderPlas, J. (2016). Python Data Science Handbook. O'Reilly Media, Inc.</li> <li>Voordendag, A. B., Goger, B., Klug, C., Prinz, R., Rutzinger, M. &amp; Kaser, G. (2021). Automated and permanent long-range terrestrial laser scanning in a high mountain environment: Setup and first results. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V-2-2021, pp. 153-160. doi: 10.5194/isprs-annals-V-2-2021-153-2021.</li> <li>Vos, S., Anders, K., Kuschnerus, M., Lindenberg, R., H\u00f6fle, B., Aarnikhof, S. &amp; Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands.  Scientific Data, 9:191. doi: 10.1038/s41597-022-01291-9.</li> <li>Wang, D., Puttonen, E., &amp; Casella, E. (2022). PlantMove: A tool for quantifying motion fields of plant movements from point cloud time series. International Journal of Applied Earth Observation and Geoinformation, 110, pp. 102781. doi: 10.1016/j.jag.2022.102781.</li> <li>Weinmann, M., Jutzi, B., Hinz, S., &amp; Mallet, C. (2015). Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers. ISPRS Journal of Photogrammetry and Remote Sensing, 105, pp. 286-304. doi: 10.1016/j.isprsjprs.2015.01.016.</li> <li>Westoby, M. J., Brasington, J., Glasser, N. F., Hambrey, M. J., &amp; Reynolds, J. M. (2012). \u2018Structure-from-Motion\u2019 photogrammetry: A low-cost, effective tool for geoscience applications. Geomorphology, 179, pp. 300-314. doi: 10.1016/j.geomorph.2012.08.021.</li> <li>Williams, J. G., Rosser, N. J., Hardy, R. J., Brain, M. J. &amp; Afana, A. A. (2018). Optimising 4-D surface change detection: an approach for capturing rockfall magnitude\u2013frequency. Earth Surface Dynamics, 6, pp. 101\u2013119. doi: 10.5194/esurf-6-101-2018.</li> <li>Winiwarter, L., Anders, K., Schr\u00f6der, D. &amp; H\u00f6fle, B. (2022a). Full 4D Change Analysis of Topographic Point Cloud Time Series using Kalman Filtering. Earth Surface Dynamics, Discussion (preprint). doi: 10.5194/esurf-2021-103.</li> <li>Winiwarter, L., Pena, A. M. E., Weiser, H., Anders, K., Sanchez, J. M., Searle, M. &amp; H\u00f6fle, B. (2022b). Virtual laser scanning with HELIOS++: A novel take on ray tracing-based simulation of topographic full-waveform 3D laser scanning. Remote Sensing of Environment, 269. doi: 10.1016/j.rse.2021.112772.</li> </ul>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1.html","title":"Exercise: Principles of 3D/4D geographic point clouds","text":"<p>In this exercise, you will get familiar with 3D point clouds in open, graphical software. You will further visually explore multitemporal point clouds and assess changes using visual tools and manual measurements.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1.html#software-and-data","title":"Software and data","text":"<p>Use the software CloudCompare for this exercise. For help with the basic usage of CloudCompare, you may refer to the tutorial videos on the CloudCompare website or on this workshop-website.</p> <p>The dataset will be point clouds of a rock glacier acquired by UAV photogrammetry and laser scanning. See the introduction to the use case and dataset here.</p> <p>Use the data from the directory <code>ahk</code> in the course data repository.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1.html#explore-the-point-clouds","title":"Explore the point clouds","text":"<p>Load a UAV laser scanning point cloud of 2021 into CloudCompare. Look at both the photogrammetric and laser scanning point cloud, but use either to work on the following questions. Later on, you will compare them.</p> <ul> <li>How many points does the point cloud have? </li> <li>What are the measures of the site (width and length of the bounding box)? </li> <li>What is the approximate length of the acquired part of the rock glacier tongue (manual measurement)?</li> <li>How many attributes (scalar field in CloudCompare) and which does the point cloud have? Note that attributes were removed to reduce the data volume of the course, so you may miss some typical LiDAR attributes.</li> </ul> <p>Load the photogrammetric point cloud (<code>ahk_2021_photo.laz</code>) and compare:</p> <ul> <li>What are the different available attributes?</li> <li>How do the point clouds differ visually regarding point density and coverage?</li> <li>How does the representation of individual boulders on the rock glacier differ visually?</li> </ul> <p>Prepare a visualization comparing both point clouds at the same close-up view of few boulders - either as two images or as an animation. Do not forget to add the basic elements of scale bar and axes orientation.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1.html#manual-editing-of-the-point-cloud","title":"Manual editing of the point cloud","text":"<p>Use the <code>Segment</code> tool in CloudCompare to cut out individual boulders from the rock glacier point cloud. Then <code>merge</code> the individual boulders and have CloudCompare \"add indexes of the original point clouds\". This provides you (a part of) the point cloud with a point attribute of individual boulder indices - a manual boulder segmentation.</p> <p>Load the file ULS point cloud from 2020 into your CloudCompare project (<code>ahk_2020_uls.laz</code>). Check the extract of your boulders or some local area visually, can you see any changes 2020 to 2021? </p> <ul> <li>What challenges do you face when cutting out a boulder manually?</li> <li>Do you have an idea how boulder segmentation could be done automatically? Think of the features and decisions you would implement in an algorithm.</li> </ul> <p>Share your result visually \u2013 make a nice image of your segmented boulders using some tricks CloudCompare\u2019s visualization options.  Some suggestions:</p> <ul> <li>Adjust the color of the background (e.g., white and no shading). Don\u2019t forget to adjust the colors of the bounding box, labels, etc. then, too. </li> <li>Color the boulders by single colors to make the segmentation nicely visible. </li> <li>Use the GL Shader to make the geometry better visible and have a \u201cplastic touch\u201d. </li> <li>Can you think of more?</li> </ul> <p>Render the view to an image.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1.html#derivation-of-roughness","title":"Derivation of roughness","text":"<p>For the derivation of surface roughness, use the 2021 ULS point cloud. Cut out a subset of a small area covering part of the rock glacier and part of the adjacent area.</p> <p>Calculate the surface roughness. As you know, roughness is scale-dependent. Can you find a suitable radius to (i) distinguish individual boulder faces, and to (ii) distinguish the rock glacier (composed of boulders) from the grass and rock areas next to the rock glacier tongue?</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1.html#inspection-of-topographic-change","title":"Inspection of topographic change","text":"<p>Methods of change detection and quantification are covered by subsequent themes. However, you may already visually identify movement and deformation of the rock glacier body. Compare the 2020 and 2021 ULS point clouds of the rock glacier. </p> <p>Check changes of the rock glacier in the point clouds with the following approaches:</p> <ul> <li>Color each point cloud by single, distinct colors (e.g., blue and red).</li> <li>Examine a profile through both point clouds along the rock glacier tongue.</li> <li>Measure the distance from one point cloud to the corresponding surface of the other.</li> </ul> <p>Is the change measurement straight-forward, can you be sure to measure the same surfaces, e.g., of corresponding boulders everywhere?</p> <p>In which direction(s) do you measure change: vertically, horizontally, surface orientation, ...?</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1.html#wrap-up","title":"Wrap up","text":"<p>Did you make it through the exercise? Discuss with peers and consider the solution provided in the course.</p> <p>You are now familiar with point cloud editing and analysis in a graphical user interface. Next, you should try to process point clouds using programming, e.g., with Python.</p> <p>Further, you should now get into 3D change analysis of this complex natural phenomenon with the available multitemporal point cloud data!</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html","title":"Exercise: Principles of 3D/4D geographic point clouds","text":"<p>In this exercise, you will get familiar with 3D point clouds in open, graphical software. You will further visually explore multitemporal point clouds and assess changes using visual tools and manual measurements.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html#software-and-data","title":"Software and data","text":"<p>Use the software CloudCompare for this exercise. For help with the basic usage of CloudCompare, you may refer to the tutorial videos on the CloudCompare website or on this workshop-website. For those needing further assistance with CloudCompare, there is a detailed solution video at the end.</p> <p>The dataset will be point clouds of a rock glacier acquired by UAV photogrammetry and laser scanning. See the introduction to the use case and dataset here.</p> <p>Use the data from the directory <code>ahk</code> in the course data repository.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html#explore-the-point-clouds","title":"Explore the point clouds","text":"<p>Load the UAV laser scanning point cloud of 2021 into CloudCompare (<code>ahk_2021_uls.laz</code>). Get familiar with the point cloud:</p> <ul> <li>How many points does the point cloud have? </li> </ul> <p>45,662,285 points</p> <p></p> <ul> <li>What are the measures of the point cloud area (width and length of the bounding box)?</li> </ul> <p>The info can be found in the point cloud properties via the table of contents.</p> <p></p> <ul> <li>What is the approximate length of the acquired part of the rock glacier tongue (manual measurement)?</li> </ul> <p></p> <p>The approximate length is 608 m.</p> <ul> <li>How many attributes (scalar field in CloudCompare) and which does the point cloud have? Note that attributes were removed to reduce the data volume of the course, so you may miss some typical LiDAR attributes.</li> </ul> <p>Only one attribute is available: intensity.</p> <p>Load the photogrammetric point cloud (<code>ahk_2021_photo.laz</code>) and compare:</p> <ul> <li>What are the different available attributes?</li> </ul> <p>The photogrammetric point cloud only contains RGB values (true color).</p> <ul> <li>How do the point clouds differ visually regarding point density and coverage?</li> </ul> <p>The photogrammetric point cloud has a lower point density than the ULS point cloud (note: both are subsampled to reduce data volumes in the course, this relation does not necessarily reflect the original acquisitions and depends on the survey settings). Even though the point density is lower, the photogrammetric point cloud is easier to interpret visually, due to the photo-realistic effect of the coloring.</p> <ul> <li>How does the representation of individual boulders on the rock glacier differ visually?</li> </ul> <p>The photogrammetric point cloud looks more \"realistic\", the coloring and shadows provides texture that is easier to interpret than the ULS point cloud colored by intensity. Further, the points in the ULS point cloud seem more scattered, which may suggest a higher quality of the photogrammetric point cloud to represent the local surface.</p> <p>Prepare a visualization comparing both point clouds at the same close-up view of few boulders - either as two images or as an animation. Do not forget to add the basic elements of scale bar and axes orientation.</p> <p>In order to be able to see the structure of the point cloud better on the same background, both point clouds are visualized without color (None in the Colors tab of CloudCompare).</p> <p></p> <p>Laser scanning point cloud </p> <p></p> <p>Photogrammetric point cloud </p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html#manual-editing-of-the-point-cloud","title":"Manual editing of the point cloud","text":"<p>Use the <code>Segment</code> tool in CloudCompare to cut out individual boulders from the rock glacier point cloud. Then merge the individual boulders and have CloudCompare \"add indexes of the original point clouds\". This provides you (a part of) the point cloud with a point attribute of individual boulder indices - a manual boulder segmentation.</p> <p></p> <p>Load the file ULS point cloud from 2020 into your CloudCompare project (<code>ahk_2020_uls.laz</code>). Check the extract of your boulders, can you see any changes from 2020 to 2021? </p> <ul> <li>What challenges do you face when cutting out a boulder manually?</li> </ul> <p>It is often not easy to distinguish single boulders, depending on their size and geometry, and how they are set relative to one another. Furthermore, manual segmenting in 3D is laborious, since the object needs to be cut from multiple perspectives, given the cmoplex 3D geometry of the objects. </p> <ul> <li>Do you have an idea how boulder segmentation could be done automatically? Think of the features and decisions you would implement in an algorithm.</li> </ul> <p>Approaches for automatic segmentation could make use of knowledge about the object geometry. For example, an algorithm may search for planes which represent the faces of a boulder, and then compose the object from the primitive. Another approach could be looking for local maxima, i.e. the tip of a boulder, and then grow the object by adding points downward from the starting point (seed).</p> <p>Share your result visually \u2013 make a nice image of your segmented boulders using some tricks CloudCompare\u2019s visualization options.  Some suggestions:</p> <ul> <li>Adjust the color of the background (e.g., white and no shading). Don\u2019t forget to adjust the colors of the bounding box, labels, etc. then, too. </li> <li>Color the boulders by single colors to make the segmentation nicely visible. </li> <li>Use the GL Shader to make the geometry better visible and have a \u201cplastic touch\u201d. </li> <li>Can you think of more?</li> </ul> <p>Render the view to an image or animation.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html#derivation-of-roughness","title":"Derivation of roughness","text":"<p>For the derivation of surface roughness, use the 2021 ULS point cloud. Cut out a subset of a small area covering part of the rock glacier and part of the adjacent area.</p> <p>Calculate the surface roughness. As you know, roughness is scale-dependent. Can you find a suitable radius to distinguish the rock glacier (composed of boulders) from the grass and rock areas next to the rock glacier tongue?</p> <p></p> <p>Roughness values derived for r=0.5 m show low roughness on the grass and rock area outside the rock glacier tongue (right part in the image above), and higher values on the rock glacier surface, composed of individual, large boulders.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html#inspection-of-topographic-change","title":"Inspection of topographic change","text":"<p>Methods of change detection and quantification are covered by subsequent themes. However, you may already visually identify movement and deformation of the rock glacier body. Compare the 2020 and 2021 ULS point clouds of the rock glacier. </p> <p>Check changes of the rock glacier in the point clouds with the following approaches:</p> <ul> <li>Color each point cloud by single, distinct colors (e.g., blue and red).</li> <li>Examine a profile through both point clouds along the rock glacier tongue.</li> <li>Measure the distance from one point cloud to the corresponding surface of the other.</li> </ul> <p>Is the change measurement straight-forward, can you be sure to measure the same surfaces, e.g., of corresponding boulders everywhere?</p> <p>In which direction(s) do you measure change: vertically, horizontally, surface orientation, ...?</p> <p></p> <p>The red profile shows the 2021 pointcloud and the blue one the 2020 pointcloud. In these profiles, a distinct structure that can be matched between epochs is recognizable and shows the movement of the rock glacier. In the visualized area, the rock glacier moved approximately 15.57 m from 2020 to 2021 according to the manual measurement.</p>"},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html#solution-video","title":"Solution Video","text":""},{"location":"module3/01_pointcloud_principles/exercise/m3_theme1_exercise1_solution1.html#wrap-up","title":"Wrap up","text":"<p>You are now familiar with point cloud editing and analysis in a graphical user interface. Next, you should try to process point clouds using programming, e.g., with Python.</p> <p>Further, you should now get into 3D change analysis of this complex natural phenomenon with the available multitemporal point cloud data!</p> <p>Proceed with the next theme on \"Programming for Point Cloud Analysis with Python\".</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html","title":"Lesson","text":"Metadata     title: \"E-TRAINEE Programming for point cloud analysis with Python\"     description: \"This is the second theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2022-03     authors: Katharina Anders, Bernhard H\u00f6fle     contributors: Sina Zumstein     estimatedTime: 1.5 hrs <p>This theme provides an introduction how point cloud analyses can be performed using Python. Scripting in Python enables to automate the processing and analysis of 3D/4D point clouds. By this, workflows can be easily reproduced and transferred to other data or geographic regions. Python scripts can be easily shared with colleagues or the overall community (just like this Jupyter notebook), independent of the operating system. Parameters of the analysis can be adapted, and analysis results can be directly output in the form of new output layers (geodata files), statistics, or visualizations.</p> <p>In this theme, you will learn about:</p> <ul> <li>Reading, handling, and writing point cloud data</li> <li>Plotting and statistics</li> <li>Neighborhood operations</li> <li>Rasterizing point clouds</li> </ul> <p>After finishing this theme you will be able to:</p> <ul> <li>read and write point cloud data,</li> <li>rasterize point cloud information,</li> <li>perform simple comparisons of multitemporal point clouds,</li> <li>derive statistical information about point cloud properties and derived changes,</li> <li>create visualizations from the results, and</li> <li>... and to include virtual laser scanning in the analysis (in the exercise)</li> </ul> <p></p> <p>In order to process point cloud data in Python, the data needs to be read from files into the working memory. Results of the processing should be written to point cloud files again. Point cloud data can be stored in different file formats. Throughout this theme, we will focus on the LAS format, as it is a binary format with low file sizes and the de-facto standard of point cloud data. For this, the laspy package will be used (version &gt;= 2.0).</p> In\u00a0[1]: Copied! <pre># define path to point cloud file\ndata_dir = 'path-to-data/hellstugubrean'\n\n# check if the specified data path exists\nimport os\nif not os.path.isdir(data_dir):\n    print(f'ERROR: {data_dir} does not exist')\n    print('Please specify the correct path to the data directory by replacing &lt;path-to-data&gt; above.')\n\npc_filename_in = 'hellstugubrean_2009.las'\npc_file_in = f'{data_dir}/{pc_filename_in}'\n</pre> # define path to point cloud file data_dir = 'path-to-data/hellstugubrean'  # check if the specified data path exists import os if not os.path.isdir(data_dir):     print(f'ERROR: {data_dir} does not exist')     print('Please specify the correct path to the data directory by replacing  above.')  pc_filename_in = 'hellstugubrean_2009.las' pc_file_in = f'{data_dir}/{pc_filename_in}' <p>As reading of point cloud data is a repetitive task, we will create a function <code>read_las</code> for this. The function performs all general aspects required when reading point cloud data for analysis:</p> <ul> <li>Obtaining coordinate information (xyz) for each point</li> <li>Obtaining attribute information for each point, i.e. the name and value of each additional attribute</li> </ul> <p>The main input parameter is the specification of the point cloud file to be read. The function returns a 2D array of the XYZ coordinates of all points. The array has the shape <code>(N,3)</code>, where <code>N</code> is the number of points in the point cloud. The function can additionally return the point attribute data. The attributes contained in point clouds can be very different, depending on the sensor, processing level, and application. Therefore, all available attributes will be read and returned in a dictionary, where the keys are the attribute names and the values are the attribute values as 1D array of shape <code>(N,1)</code>.</p> <p>As many tasks need only the geometric information, i.e. coordinates, of the point cloud, we will add an optional parameter <code>get_attributes</code> (boolean, default: <code>False</code>). If we require the attribute information, the parameter must be set to <code>True</code>. Additionally, we add the parameter <code>use_every</code> to provide an easy way of subsampling the point cloud if required. Accordingly, the point cloud is subsampled by only returning every n-th point of the full dataset. The default value shall be <code>1</code>, i.e. returning the full point cloud data.</p> In\u00a0[2]: Copied! <pre># import required modules\nimport laspy # import the laspy package for handling point cloud files\nimport numpy as np # import numpy for array handling\n\ndef read_las(infile,get_attributes=False,use_every=1):\n    \"\"\"\n    Function to read coordinates and optionally attribute information of point cloud data from las/laz file.\n\n    :param infile: specification of input file (format: las or laz)\n    :param get_attributes: if True, will return all attributes in file, otherwise will only return coordinates (default is False)\n    :param use_every: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)\n    :return: Array of point coordinates of shape (N,3) with N number of points in input file (or subsampled by 'use_every')\n    \"\"\"\n\n    # read the file using the laspy read function\n    indata = laspy.read(infile)\n\n    # get the coordinates (XYZ) and stack them in a 3D array\n    coords = np.vstack((indata.x, indata.y, indata.z)).transpose()\n\n    # subsample the point cloud, if use_every = 1 will remain the full point cloud data\n    coords = coords[::use_every, :]\n\n    # read attributes if get_attributes is set to True\n    if get_attributes == True:\n        # get all attribute names in the las file as list\n        las_fields= list(indata.points.point_format.dimension_names)\n\n        # create a dictionary to store attributes\n        attributes = {}\n\n        # loop over all available fields in the las point cloud data\n        for las_field in las_fields[3:]: # skip the first three fields, which contain coordinate information (X,Y,Z)\n            attribute = np.array(indata.points[las_field]) # transpose shape to (N,1) to fit coordinates array\n            if np.sum(attribute)==0: # if field contains only 0, it is empty\n                continue\n            # add the attribute to the dictionary with the name (las_field) as key\n            attributes[las_field] = attribute[::use_every] # subsample by use_every, corresponding to point coordinates\n\n        # return coordinates and attribute data\n        return coords, attributes\n\n    else: # get_attributes == False\n        return coords  # return coordinates only\n</pre> # import required modules import laspy # import the laspy package for handling point cloud files import numpy as np # import numpy for array handling  def read_las(infile,get_attributes=False,use_every=1):     \"\"\"     Function to read coordinates and optionally attribute information of point cloud data from las/laz file.      :param infile: specification of input file (format: las or laz)     :param get_attributes: if True, will return all attributes in file, otherwise will only return coordinates (default is False)     :param use_every: value specifies every n-th point to use from input, i.e. simple subsampling (default is 1, i.e. returning every point)     :return: Array of point coordinates of shape (N,3) with N number of points in input file (or subsampled by 'use_every')     \"\"\"      # read the file using the laspy read function     indata = laspy.read(infile)      # get the coordinates (XYZ) and stack them in a 3D array     coords = np.vstack((indata.x, indata.y, indata.z)).transpose()      # subsample the point cloud, if use_every = 1 will remain the full point cloud data     coords = coords[::use_every, :]      # read attributes if get_attributes is set to True     if get_attributes == True:         # get all attribute names in the las file as list         las_fields= list(indata.points.point_format.dimension_names)          # create a dictionary to store attributes         attributes = {}          # loop over all available fields in the las point cloud data         for las_field in las_fields[3:]: # skip the first three fields, which contain coordinate information (X,Y,Z)             attribute = np.array(indata.points[las_field]) # transpose shape to (N,1) to fit coordinates array             if np.sum(attribute)==0: # if field contains only 0, it is empty                 continue             # add the attribute to the dictionary with the name (las_field) as key             attributes[las_field] = attribute[::use_every] # subsample by use_every, corresponding to point coordinates          # return coordinates and attribute data         return coords, attributes      else: # get_attributes == False         return coords  # return coordinates only <p>With this function, we can read the data from our point cloud file, including the point cloud attributes:</p> In\u00a0[3]: Copied! <pre>pc_coords, pc_attributes = read_las(pc_file_in, get_attributes=True)\n</pre> pc_coords, pc_attributes = read_las(pc_file_in, get_attributes=True) <p>We can check the properties of the point cloud by inspecting the returned coordinates <code>pc_coords</code> and attributes <code>pc_attributes</code>. For example, we can obtain the number of points and spatial extent from the coordinates array:</p> In\u00a0[4]: Copied! <pre># get number of points from array shape\nnum_points = pc_coords.shape[0]\nprint(f'The point cloud consists of {num_points} points.')\n\n# get spatial extent from minimum and maximum X,Y,Z coordinate values\nmin_coord_vals = np.min(pc_coords, axis=0)\nmax_coord_vals = np.max(pc_coords, axis=0)\nprint(f'The point cloud has a vertical extent of {max_coord_vals[2]-min_coord_vals[2]:.1f} m.')\n</pre> # get number of points from array shape num_points = pc_coords.shape[0] print(f'The point cloud consists of {num_points} points.')  # get spatial extent from minimum and maximum X,Y,Z coordinate values min_coord_vals = np.min(pc_coords, axis=0) max_coord_vals = np.max(pc_coords, axis=0) print(f'The point cloud has a vertical extent of {max_coord_vals[2]-min_coord_vals[2]:.1f} m.') <pre>The point cloud consists of 1694840 points.\nThe point cloud has a vertical extent of 583.3 m.\n</pre> <p>We can check the available attributes via the keys of the dictionary <code>pc_attributes</code>:</p> In\u00a0[5]: Copied! <pre>pc_attributes.keys()\n</pre> pc_attributes.keys() Out[5]: <pre>dict_keys(['intensity', 'return_number', 'number_of_returns', 'scan_direction_flag', 'classification', 'scan_angle_rank', 'user_data', 'gps_time'])</pre> <p>For a specific attribute, we may check which values occur in our dataset:</p> In\u00a0[6]: Copied! <pre>my_attr = 'number_of_returns'\nprint(f'The following values occur in the attribute {my_attr}: {np.unique(pc_attributes[my_attr])}')\n</pre> my_attr = 'number_of_returns' print(f'The following values occur in the attribute {my_attr}: {np.unique(pc_attributes[my_attr])}') <pre>The following values occur in the attribute number_of_returns: [1 2 3 4]\n</pre> <p></p> In\u00a0[7]: Copied! <pre># dictionary of possible echo type values\necho_types_dict = {0:'single', 1:'first', 2:'intermediate', 3:'last'}\n\n# derive per-point echo types using list comprehension based on attributes return_number and number_of_returns\necho_types = [0 if num_rets == 1 else 1 if ret_num == 1 else 3 if ret_num == num_rets else 2 for num_rets, ret_num in zip(pc_attributes['number_of_returns'],pc_attributes['return_number'])]\n\n# add new list echo types to attributes as array to allow calculations\npc_attributes['echo_type'] = np.array(echo_types)\n\n# check the occurrence and count of different echo types\necho_type_occ, echo_type_occ_count = np.unique(pc_attributes['echo_type'], return_counts = True)\n\nprint('The following echo types (count) occur in the attribute echo_type:')\nfor et, etc in zip(echo_type_occ, echo_type_occ_count):\n    print(f'\\t{echo_types_dict[et]} ({etc})')\n</pre> # dictionary of possible echo type values echo_types_dict = {0:'single', 1:'first', 2:'intermediate', 3:'last'}  # derive per-point echo types using list comprehension based on attributes return_number and number_of_returns echo_types = [0 if num_rets == 1 else 1 if ret_num == 1 else 3 if ret_num == num_rets else 2 for num_rets, ret_num in zip(pc_attributes['number_of_returns'],pc_attributes['return_number'])]  # add new list echo types to attributes as array to allow calculations pc_attributes['echo_type'] = np.array(echo_types)  # check the occurrence and count of different echo types echo_type_occ, echo_type_occ_count = np.unique(pc_attributes['echo_type'], return_counts = True)  print('The following echo types (count) occur in the attribute echo_type:') for et, etc in zip(echo_type_occ, echo_type_occ_count):     print(f'\\t{echo_types_dict[et]} ({etc})')  <pre>The following echo types (count) occur in the attribute echo_type:\n\tsingle (1687948)\n\tfirst (3388)\n\tintermediate (94)\n\tlast (3410)\n</pre> <p>We now remove all points with echo type first (<code>echo_type</code>=1) and intermediate (<code>echo_type</code>=2), i.e. maintaining only single and last echos.</p> In\u00a0[8]: Copied! <pre># for every coordinate and attribute in point cloud, select only those where corresponding echo type is 0 or 3\npc_coords_filtered = pc_coords[(pc_attributes['echo_type']==0) | (pc_attributes['echo_type']==3)]\npc_attributes_filtered = {}\nfor k, v in pc_attributes.items():\n    pc_attributes_filtered[k] = v[(pc_attributes['echo_type']==0) | (pc_attributes['echo_type']==3)]\n    \n# free working memory by setting original point cloud variables to None\npc_coords = None\npc_attributes = None\n</pre> # for every coordinate and attribute in point cloud, select only those where corresponding echo type is 0 or 3 pc_coords_filtered = pc_coords[(pc_attributes['echo_type']==0) | (pc_attributes['echo_type']==3)] pc_attributes_filtered = {} for k, v in pc_attributes.items():     pc_attributes_filtered[k] = v[(pc_attributes['echo_type']==0) | (pc_attributes['echo_type']==3)]      # free working memory by setting original point cloud variables to None pc_coords = None pc_attributes = None In\u00a0[9]: Copied! <pre># 2D bounding box of area of interest (minimum X, minimum Y, maximum X, maximum Y)\naoi_bb = [470325.0, 6826505.0, 470815.0, 6827240.0]\n\n# for every coordinate and attribute in point cloud select only those where corresponding XY coordinate is within minimum and maximum XY\nsel_coords = (pc_coords_filtered[:,0]&gt;=aoi_bb[0]) &amp; (pc_coords_filtered[:,0]&lt;=aoi_bb[2]) &amp; (pc_coords_filtered[:,1]&gt;=aoi_bb[1]) &amp; (pc_coords_filtered[:,1]&lt;=aoi_bb[3])\npc_coords_bb = pc_coords_filtered[sel_coords]\npc_attributes_bb = {}\nfor k, v in pc_attributes_filtered.items():\n    pc_attributes_bb[k] = v[sel_coords]\n</pre> # 2D bounding box of area of interest (minimum X, minimum Y, maximum X, maximum Y) aoi_bb = [470325.0, 6826505.0, 470815.0, 6827240.0]  # for every coordinate and attribute in point cloud select only those where corresponding XY coordinate is within minimum and maximum XY sel_coords = (pc_coords_filtered[:,0]&gt;=aoi_bb[0]) &amp; (pc_coords_filtered[:,0]&lt;=aoi_bb[2]) &amp; (pc_coords_filtered[:,1]&gt;=aoi_bb[1]) &amp; (pc_coords_filtered[:,1]&lt;=aoi_bb[3]) pc_coords_bb = pc_coords_filtered[sel_coords] pc_attributes_bb = {} for k, v in pc_attributes_filtered.items():     pc_attributes_bb[k] = v[sel_coords] <p>In geographic analyses, spatial subsetting will often be based on polygon shapes. These can be defined, for example, in GIS as a shapefile. In the following, we will read a shapefile (<code>hellstugubrean.shp</code> provided with exercise data) that was created as area of interest and select only points located within the contained polygon. First, we will read the polygon geometry from the shapefile:</p> In\u00a0[10]: Copied! <pre># define path to shapefile\nshp_filename_in = 'hellstugubrean_2009.shp'\nshp_file_in = f'{data_dir}/{shp_filename_in}'\n\n#read the file and check the contained Polygon\nimport geopandas as gpd\n\ndatasource = gpd.read_file(shp_file_in)\n\nif datasource is None:\n    print(f'Could not open shapefile {shp_file_in}, skipping cell content.')\nelse:\n    polygons = gpd.GeoSeries(datasource['geometry'])\n    print(polygons)\n</pre> # define path to shapefile shp_filename_in = 'hellstugubrean_2009.shp' shp_file_in = f'{data_dir}/{shp_filename_in}'  #read the file and check the contained Polygon import geopandas as gpd  datasource = gpd.read_file(shp_file_in)  if datasource is None:     print(f'Could not open shapefile {shp_file_in}, skipping cell content.') else:     polygons = gpd.GeoSeries(datasource['geometry'])     print(polygons) <pre>0    POLYGON ((470316.119 6826481.929, 470334.269 6...\nName: geometry, dtype: geometry\n</pre> <p>We will now loop over all coordinates in the point cloud and check if the respective point lies within the polygon. For this, we create an array which is used as boolean mask to subsequently select the inlier points from the full point cloud.</p> In\u00a0[11]: Copied! <pre># create boolean array to mask points (initialize all entries as False)\npc_mask_polygon = np.full(len(pc_coords_filtered), fill_value=False, dtype=bool)\n\nimport shapely\nfrom shapely.geometry import Point, Polygon\n\n# loop over xy coordinates of all points\nfor pt_id, (x,y) in enumerate(pc_coords_filtered[:,:2]):\n    point=Point([x,y])\n    \n    # check distance of this point to all polygons\n    for poly in polygons:\n        distance= poly.distance(point)\n        # set mask to True, if point is within polygon\n        if distance &lt;= 0.0:\n            pc_mask_polygon[pt_id] = True \n            \npc_coords_poly = pc_coords_filtered[pc_mask_polygon]\npc_attributes_poly = {}\nfor k, v in pc_attributes_filtered.items():\n    pc_attributes_poly[k] = v[pc_mask_polygon]\n</pre> # create boolean array to mask points (initialize all entries as False) pc_mask_polygon = np.full(len(pc_coords_filtered), fill_value=False, dtype=bool)  import shapely from shapely.geometry import Point, Polygon  # loop over xy coordinates of all points for pt_id, (x,y) in enumerate(pc_coords_filtered[:,:2]):     point=Point([x,y])          # check distance of this point to all polygons     for poly in polygons:         distance= poly.distance(point)         # set mask to True, if point is within polygon         if distance &lt;= 0.0:             pc_mask_polygon[pt_id] = True               pc_coords_poly = pc_coords_filtered[pc_mask_polygon] pc_attributes_poly = {} for k, v in pc_attributes_filtered.items():     pc_attributes_poly[k] = v[pc_mask_polygon] <p>We now obtained two different spatial point cloud subsets and can check, e.g., their point counts:</p> In\u00a0[12]: Copied! <pre>print(f'The attribute-filtered point cloud (single and last echos) contains {len(pc_coords_filtered)} points.')\nprint(f'The spatial subset using a 2D bounding box contains {len(pc_coords_bb)} points.')\nprint(f'The spatial subset using a polygon from a shapefile contains {len(pc_coords_poly)} points.')\n</pre> print(f'The attribute-filtered point cloud (single and last echos) contains {len(pc_coords_filtered)} points.') print(f'The spatial subset using a 2D bounding box contains {len(pc_coords_bb)} points.') print(f'The spatial subset using a polygon from a shapefile contains {len(pc_coords_poly)} points.') <pre>The attribute-filtered point cloud (single and last echos) contains 1691358 points.\nThe spatial subset using a 2D bounding box contains 477524 points.\nThe spatial subset using a polygon from a shapefile contains 397098 points.\n</pre> <p>To visualize our results, we will subsequently go into plotting of point cloud data and derived information directly with Python. But first, the topic of point cloud handling will be concluded by writing out edited point cloud data to output files.</p> <p></p> <p>Once editing of the point cloud is concluded, it can be stored to a new las file as final output. We write point clouds using a function which takes points and attributes as an input and writes them to a specified file path. Attributes to be written are optional, the default <code>attribute_dict</code> is empty.</p> In\u00a0[13]: Copied! <pre>def write_las(outpoints,outfilepath,attribute_dict={}):\n\n    \"\"\"\n    :param outpoints: 3D array of points to be written to output file\n    :param outfilepath: specification of output file (format: las or laz)\n    :param attribute_dict: dictionary of attributes (key: name of attribute; value: 1D array of attribute values in order of points in 'outpoints'); if not specified, dictionary is empty and nothing is added\n    :return: None\n    \"\"\"\n\n    # create a header for new las file\n    hdr = laspy.LasHeader(version=\"1.4\", point_format=6)\n\n    # create the las data\n    las = laspy.LasData(hdr)\n\n    # write coordinates into las data\n    las.x = outpoints[:, 0]\n    las.y = outpoints[:, 1]\n    las.z = outpoints[:, 2]\n    \n    # add all dictionary entries to las data (if available)\n    for key,vals in attribute_dict.items():\n        try:\n            las[key] = vals\n        except:\n            las.add_extra_dim(laspy.ExtraBytesParams(\n                name=key,\n                type=type(vals[0])\n                ))\n            las[key] = vals\n\n    # write las file\n    las.write(outfilepath)\n\n    return\n</pre> def write_las(outpoints,outfilepath,attribute_dict={}):      \"\"\"     :param outpoints: 3D array of points to be written to output file     :param outfilepath: specification of output file (format: las or laz)     :param attribute_dict: dictionary of attributes (key: name of attribute; value: 1D array of attribute values in order of points in 'outpoints'); if not specified, dictionary is empty and nothing is added     :return: None     \"\"\"      # create a header for new las file     hdr = laspy.LasHeader(version=\"1.4\", point_format=6)      # create the las data     las = laspy.LasData(hdr)      # write coordinates into las data     las.x = outpoints[:, 0]     las.y = outpoints[:, 1]     las.z = outpoints[:, 2]          # add all dictionary entries to las data (if available)     for key,vals in attribute_dict.items():         try:             las[key] = vals         except:             las.add_extra_dim(laspy.ExtraBytesParams(                 name=key,                 type=type(vals[0])                 ))             las[key] = vals      # write las file     las.write(outfilepath)      return  <p>A point cloud, for example the filtered and spatial subset <code>pc_coords_poly</code> with attributed <code>pc_attributes_poly</code>, can now be saved to a new output file using the above function:</p> In\u00a0[14]: Copied! <pre># define the output path\npc_subset_file = f'{data_dir}/hellstugubrean_2009_filtered_poly.las'\n\n# write point cloud data to file with attributes\nwrite_las(pc_coords_poly, pc_subset_file, attribute_dict=pc_attributes_poly)\n</pre> # define the output path pc_subset_file = f'{data_dir}/hellstugubrean_2009_filtered_poly.las'  # write point cloud data to file with attributes write_las(pc_coords_poly, pc_subset_file, attribute_dict=pc_attributes_poly) <p>You can now check your written point cloud files in CloudCompare. You will find that the newly created attribute <code>echo_type</code> is now available in the point cloud, which was contained in the attribute dictionary. The figure below shows the original point cloud <code>hellstugubrean_2009.las</code> (colored in blue) and the filtered and spatially subset point cloud <code>hellstugubrean_2009_filtered_poly.las</code> (colored by intensity in grayscale):</p> Visualization of output point cloud in CloudCompare (colored by intensity) compared to input point cloud with full spatial extent (colored in blue). The newly added echo type is available as attribute. Figure by course authors. <p>Having finished this first part and written the output to file, we will release the point cloud data from memory. In the following part, we will use the full-size, filtered point cloud. Since we are going to include a second epoch (point cloud acquired in 2017) in the following, we will store this point cloud data in a new variable here, which explicitly contains the time information for easier processing.</p> In\u00a0[15]: Copied! <pre># store point cloud data in new variables (make explicit copy and remove old ones from working memory)\npc_2009_coords = pc_coords_filtered.copy()\npc_2009_attributes = pc_attributes_filtered.copy()\n\n# free working memory from all other point cloud objects\npc_coords_filtered = None\npc_attributes_filtered = None\npc_coords_bb = None\npc_attributes_bb = None\npc_coords_poly = None\npc_attributes_poly = None\n</pre> # store point cloud data in new variables (make explicit copy and remove old ones from working memory) pc_2009_coords = pc_coords_filtered.copy() pc_2009_attributes = pc_attributes_filtered.copy()  # free working memory from all other point cloud objects pc_coords_filtered = None pc_attributes_filtered = None pc_coords_bb = None pc_attributes_bb = None pc_coords_poly = None pc_attributes_poly = None <p></p> In\u00a0[16]: Copied! <pre># importing required modules\nimport matplotlib.pyplot as plt\n</pre> # importing required modules import matplotlib.pyplot as plt <p>Usually one of the first steps in data analysis is the exploration of (attribute) properties. The data may be easily checked via a histogram, which visualizes the frequency of occurrence of single values (in so-called bins). We will now create a histogram for the intensity attribute of our example point cloud (here using the spatial subset in the 2D bounding box).</p> In\u00a0[17]: Copied! <pre># define which point cloud data to use (already in memory from previous part)\nmy_attr = 'intensity'\ndata = pc_2009_attributes[my_attr]\n\n# plot a simple histogram\nplt.hist(data, bins=30)\n\n# add axes labels\nplt.xlabel('Intensity [dB]')\nplt.ylabel('Frequency')\n\n# add title\nplt.title(f'Histogram of {my_attr} values')\n\n# show the plot in interactive mode\nplt.show() # or use this command to save the figure to file: plt.savefig('hist.png', dpi=300)\n</pre> # define which point cloud data to use (already in memory from previous part) my_attr = 'intensity' data = pc_2009_attributes[my_attr]  # plot a simple histogram plt.hist(data, bins=30)  # add axes labels plt.xlabel('Intensity [dB]') plt.ylabel('Frequency')  # add title plt.title(f'Histogram of {my_attr} values')  # show the plot in interactive mode plt.show() # or use this command to save the figure to file: plt.savefig('hist.png', dpi=300) <p>Any element of the matplotlib plot can be tweaked, such as the color, axis labels, tick format and labels, adding a legend, and many more. Check out the matplotlib gallery for many examples of how to adjust your plot layout: https://matplotlib.org/stable/gallery/index</p> <p>We may further explore the data via descriptive statistics. Properties such as the mean and standard deviation, or percentiles of attribute values provide us insight into the distribution of attribute values. Here, we can directly make use of the powerful <code>numpy</code> array operations and functions.</p> In\u00a0[18]: Copied! <pre>data_mean = np.mean(data)\ndata_stdev = np.std(data)\nprint(f'The data ({my_attr}) has a mean of {data_mean:.1f} and standard deviation of {data_stdev:.1f}.')\n</pre> data_mean = np.mean(data) data_stdev = np.std(data) print(f'The data ({my_attr}) has a mean of {data_mean:.1f} and standard deviation of {data_stdev:.1f}.') <pre>The data (intensity) has a mean of 71.3 and standard deviation of 25.6.\n</pre> <p>We may add this information directly into our plots, visually or as text. This time, we will further adjust the layout of the figure:</p> In\u00a0[19]: Copied! <pre># create the figure object\nfig, ax = plt.subplots(figsize=(6,3))\n\n# plot the histogram of data with keyword arguments to add a label (legend) and adapt the layout\nax.hist(data, bins=50, label='Value occurrence', facecolor='red', alpha=.75, edgecolor='darkred')\n\n# add title and axis labels\nax.set_title(f'Histogram of LiDAR intensity values')\nax.set_xlabel('Intensity [dB]')\nax.set_ylabel('Frequency')\n\n# add vertical lines of mean and standard deviation\nax.axvline(data_mean, color='black', linewidth=1.0, label='Mean')\nax.axvline(data_mean-data_stdev, color='black', linewidth=1.0, linestyle='--', label='Standard deviation')\nax.axvline(data_mean+data_stdev, color='black', linewidth=1.0, linestyle='--')\n\n# add statistical values as text \nfrom matplotlib.offsetbox import AnchoredText # use anchored text for automatic positioning in the figure\nanchored_text = AnchoredText(f'Mean: {data_mean:.1f}\\nStd. dev.: {data_stdev:.1f}\\nMedian: {np.median(data):.1f}\\n25th perc.: {np.percentile(data_mean,25):.1f}', loc='lower right')\nax.add_artist(anchored_text)\n\n# add legend\nplt.legend()\n\n# cleanly fit plots within figure\nplt.tight_layout()\n\n# show plot in interactive mode\nplt.show()\n</pre> # create the figure object fig, ax = plt.subplots(figsize=(6,3))  # plot the histogram of data with keyword arguments to add a label (legend) and adapt the layout ax.hist(data, bins=50, label='Value occurrence', facecolor='red', alpha=.75, edgecolor='darkred')  # add title and axis labels ax.set_title(f'Histogram of LiDAR intensity values') ax.set_xlabel('Intensity [dB]') ax.set_ylabel('Frequency')  # add vertical lines of mean and standard deviation ax.axvline(data_mean, color='black', linewidth=1.0, label='Mean') ax.axvline(data_mean-data_stdev, color='black', linewidth=1.0, linestyle='--', label='Standard deviation') ax.axvline(data_mean+data_stdev, color='black', linewidth=1.0, linestyle='--')  # add statistical values as text  from matplotlib.offsetbox import AnchoredText # use anchored text for automatic positioning in the figure anchored_text = AnchoredText(f'Mean: {data_mean:.1f}\\nStd. dev.: {data_stdev:.1f}\\nMedian: {np.median(data):.1f}\\n25th perc.: {np.percentile(data_mean,25):.1f}', loc='lower right') ax.add_artist(anchored_text)  # add legend plt.legend()  # cleanly fit plots within figure plt.tight_layout()  # show plot in interactive mode plt.show() <p>We can further explore our topographic laser scanning point cloud by checking the spatial distribution of an attribute. In 2D, points can be plotted in a scatter plot. To not introduce distortion between the x- and y-axis, we need to set the aspect ratio to <code>equal</code>. We then use the intensity attribute as color, which is set via the argument <code>c</code> of the <code>scatter()</code> function. We select a colormap (here: <code>cmap='copper'</code>) and decrease the size of the markers to render the points better visible (<code>s=1</code>).</p> In\u00a0[20]: Copied! <pre># create the figure object\nfig, ax = plt.subplots()\nax.axis('equal') # make axis ratio equal to avoid distortion between x and y\n\n# scatter the x and y coordinates and color by intensity\nnth = 10 # use only every n-th point to improve visibility\nsp = ax.scatter(pc_2009_coords[:,0][::nth], pc_2009_coords[:,1][::nth], c=pc_2009_attributes['intensity'][::nth], cmap='copper', s=1)\n\n# add title and axis labels\nax.set_title(f'Spatial representation of LiDAR intensity')\nax.set_xlabel('X coordinate [m]')\nax.set_ylabel('Y coordinate [m]')\n\n# add a colorbar legend\ncb = plt.colorbar(sp)\ncb.set_label('Intensity [dB]')\n\n# cleanly fit plots within figure\nplt.tight_layout()\n\n# show the plot in interactive mode\nplt.show()\n</pre> # create the figure object fig, ax = plt.subplots() ax.axis('equal') # make axis ratio equal to avoid distortion between x and y  # scatter the x and y coordinates and color by intensity nth = 10 # use only every n-th point to improve visibility sp = ax.scatter(pc_2009_coords[:,0][::nth], pc_2009_coords[:,1][::nth], c=pc_2009_attributes['intensity'][::nth], cmap='copper', s=1)  # add title and axis labels ax.set_title(f'Spatial representation of LiDAR intensity') ax.set_xlabel('X coordinate [m]') ax.set_ylabel('Y coordinate [m]')  # add a colorbar legend cb = plt.colorbar(sp) cb.set_label('Intensity [dB]')  # cleanly fit plots within figure plt.tight_layout()  # show the plot in interactive mode plt.show()  <p>Since we are interested in observing changes through multitemporal or time series of 3D geodata, at this point we will load a second point cloud of the same area, which was acquired in 2017, and visually compare it to the 2009 dataset. Methods of change detection and quantification in the topography, are part of the subsequent theme on Principles and basic algorithms of 3D change detection and analysis.</p> In\u00a0[21]: Copied! <pre># read point cloud data of 2017 epoch using the function defined above\npc_filename_in = 'hellstugubrean_2017.las'\npc_file_in = f'{data_dir}/{pc_filename_in}'\npc_2017_coords, pc_2017_attributes = read_las(pc_file_in, get_attributes=True)\n\n# note: we are not applying the same echo type filtering here\n# in real analysis, you might make this filtering step a function and apply it to each of your epochs\n</pre> # read point cloud data of 2017 epoch using the function defined above pc_filename_in = 'hellstugubrean_2017.las' pc_file_in = f'{data_dir}/{pc_filename_in}' pc_2017_coords, pc_2017_attributes = read_las(pc_file_in, get_attributes=True)  # note: we are not applying the same echo type filtering here # in real analysis, you might make this filtering step a function and apply it to each of your epochs <p>Having read both epochs of point clouds, we will now create a plot with two columns, where one epoch will be displayed on each side with same formatting and layout for visual comparison.</p> In\u00a0[22]: Copied! <pre># create the figure object with one row, two columns\nfig, axs = plt.subplots(1, 2, sharey=True, figsize=(9,4)) # the x and y axis extent should be exactly the same\nfor ax in axs:\n    ax.axis('equal') # make axis ratio equal to avoid distortion between x and y\n\nax1, ax2 = axs # one axis to plot each epoch\n    \n# scatter the x and y coordinates and color by intensity\nnth = 10 # use only every n-th point to improve visibility\nsp1 = ax1.scatter(pc_2009_coords[:,0][::nth], pc_2009_coords[:,1][::nth], c=pc_2009_attributes['intensity'][::nth], cmap='copper', s=1)\nsp2 = ax2.scatter(pc_2017_coords[:,0][::nth], pc_2017_coords[:,1][::nth], c=pc_2017_attributes['intensity'][::nth], cmap='copper', s=1)\n\n# add title and axis labels\nax1.set_title(f'Epoch 2009')\nax1.set_xlabel('X coordinate [m]')\nax1.set_ylabel('Y coordinate [m]')\nax2.set_title(f'Epoch 2017')\nax2.set_xlabel('X coordinate [m]')\n\n# make sure both plots have the same x extent\nax1.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0]))\nax2.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0]))\nax1.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1]))\nax2.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1]))\n\n\n# add a colorbar legend (to the second axis to be right of the figure)\ncb = plt.colorbar(sp2)\ncb.set_label('Intensity')\n\nplt.show()\n</pre> # create the figure object with one row, two columns fig, axs = plt.subplots(1, 2, sharey=True, figsize=(9,4)) # the x and y axis extent should be exactly the same for ax in axs:     ax.axis('equal') # make axis ratio equal to avoid distortion between x and y  ax1, ax2 = axs # one axis to plot each epoch      # scatter the x and y coordinates and color by intensity nth = 10 # use only every n-th point to improve visibility sp1 = ax1.scatter(pc_2009_coords[:,0][::nth], pc_2009_coords[:,1][::nth], c=pc_2009_attributes['intensity'][::nth], cmap='copper', s=1) sp2 = ax2.scatter(pc_2017_coords[:,0][::nth], pc_2017_coords[:,1][::nth], c=pc_2017_attributes['intensity'][::nth], cmap='copper', s=1)  # add title and axis labels ax1.set_title(f'Epoch 2009') ax1.set_xlabel('X coordinate [m]') ax1.set_ylabel('Y coordinate [m]') ax2.set_title(f'Epoch 2017') ax2.set_xlabel('X coordinate [m]')  # make sure both plots have the same x extent ax1.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0])) ax2.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0])) ax1.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1])) ax2.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1]))   # add a colorbar legend (to the second axis to be right of the figure) cb = plt.colorbar(sp2) cb.set_label('Intensity')  plt.show() <p>We can now visually identify differences in the data properties as well as changes in the scene. The glacier outline becomes much better visible in the 2017 data, likely because the data quality improved regarding the processing and range-dependent correction of intensity data from the different airborne laser scanning strips. Making out the glacier outline in 2009, we can also determine that the glacier retreated between these two epochs. To better visualize this retreat, we will add the glacier outline of 2009 to the plots. The outline is contained as polygon in the shapefile <code>hellstugubrean.shp</code>. The polygon geometry is still contained in our <code>all_geoms</code> object created in the first part of this theme.</p> In\u00a0[23]: Copied! <pre>%matplotlib inline\n# get the outline of the polygon\nglacier_poly= polygons[0]\nglacier_poly_outring = glacier_poly.boundary\n\nx,y = glacier_poly.exterior.xy\n\n# create the figure object with one row, two columns\nfig, axs = plt.subplots(1, 2, sharey=True, figsize=(9,4)) # the x and y axis extent should be exactly the same\nfor ax in axs:\n    ax.axis('equal') # make axis ratio equal to avoid distortion between x and y\n\nax1, ax2 = axs # one axis to plot each epoch\n    \n# scatter the x and y coordinates and color by intensity\nnth = 10 # use only every n-th point to improve visibility\nsp1 = ax1.scatter(pc_2009_coords[:,0][::nth], pc_2009_coords[:,1][::nth], c=pc_2009_attributes['intensity'][::nth], cmap='copper', s=1)\nsp2 = ax2.scatter(pc_2017_coords[:,0][::nth], pc_2017_coords[:,1][::nth], c=pc_2017_attributes['intensity'][::nth], cmap='copper', s=1)\n\n\n# add title and axis labels\nax1.set_title(f'Epoch 2009')\nax1.set_xlabel('X coordinate [m]')\nax1.set_ylabel('Y coordinate [m]')\nax2.set_title(f'Epoch 2017')\nax2.set_xlabel('X coordinate [m]')\n\n# make sure both plots have the same x extent\nax1.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0]))\nax2.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0]))\nax1.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1]))\nax2.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1]))\n\n# add a colorbar legend (to the second axis to be right of the figure)\ncb = plt.colorbar(sp2)\ncb.set_label('Intensity')\n\n# add the glacier outline to the plot\nax1.plot(x,y, color='blue', lw=1.5, label = 'Glacier outline 2009')\nax2.plot(x,y, color='blue', lw=1.5, label = 'Glacier outline 2009')\n\n# add a legend\nax2.legend(loc='upper right')\n\nplt.show()\n</pre> %matplotlib inline # get the outline of the polygon glacier_poly= polygons[0] glacier_poly_outring = glacier_poly.boundary  x,y = glacier_poly.exterior.xy  # create the figure object with one row, two columns fig, axs = plt.subplots(1, 2, sharey=True, figsize=(9,4)) # the x and y axis extent should be exactly the same for ax in axs:     ax.axis('equal') # make axis ratio equal to avoid distortion between x and y  ax1, ax2 = axs # one axis to plot each epoch      # scatter the x and y coordinates and color by intensity nth = 10 # use only every n-th point to improve visibility sp1 = ax1.scatter(pc_2009_coords[:,0][::nth], pc_2009_coords[:,1][::nth], c=pc_2009_attributes['intensity'][::nth], cmap='copper', s=1) sp2 = ax2.scatter(pc_2017_coords[:,0][::nth], pc_2017_coords[:,1][::nth], c=pc_2017_attributes['intensity'][::nth], cmap='copper', s=1)   # add title and axis labels ax1.set_title(f'Epoch 2009') ax1.set_xlabel('X coordinate [m]') ax1.set_ylabel('Y coordinate [m]') ax2.set_title(f'Epoch 2017') ax2.set_xlabel('X coordinate [m]')  # make sure both plots have the same x extent ax1.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0])) ax2.set_xlim(min(pc_2009_coords[:,0]), max(pc_2009_coords[:,0])) ax1.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1])) ax2.set_ylim(min(pc_2009_coords[:,1]), max(pc_2009_coords[:,1]))  # add a colorbar legend (to the second axis to be right of the figure) cb = plt.colorbar(sp2) cb.set_label('Intensity')  # add the glacier outline to the plot ax1.plot(x,y, color='blue', lw=1.5, label = 'Glacier outline 2009') ax2.plot(x,y, color='blue', lw=1.5, label = 'Glacier outline 2009')  # add a legend ax2.legend(loc='upper right')  plt.show() <p>In the final plot above, it becomes clearly visible how much the glacier retreated.</p> <p>To include information on the elevation in our visualization, we may advance our figure to a 3D plot. In the following, we will plot the point cloud data of the 2017 epoch in a 3D scatter plot.</p> In\u00a0[24]: Copied! <pre># create a figure with 3D axis (two columns for different coloring)\nfig, axs = plt.subplots(1,2,subplot_kw={\"projection\": \"3d\"},figsize=(10,5))\nax1, ax2 = axs\n\nnth=500\n\n# plot the point cloud colored by height (z values)\nsurf = ax1.scatter(pc_2017_coords[::nth,0], pc_2017_coords[::nth,1], pc_2017_coords[::nth,2], s=5,\n                   c=pc_2017_coords[::nth,2],cmap=plt.cm.terrain)\n\n# plot the point cloud colored by intensity\nintens = ax2.scatter(pc_2017_coords[::nth,0], pc_2017_coords[::nth,1], pc_2017_coords[::nth,2], s=5,\n                   c=pc_2017_attributes['intensity'][::nth],cmap='copper')\n\n\n# label axes and add title\nfor ax in axs:\n    ax.set_xlabel('X [m]')\n    ax.set_ylabel('Y [m]')\n    ax.set_zlabel('Z [m]')\n    \n    # set initial view of 3D plot\n    ax.view_init(elev=40., azim=130.)\n\n# add colorbars\nfig.colorbar(surf, shrink=.5, aspect=10, label='Height [m]', ax=ax1, pad=0.2)\nfig.colorbar(intens, shrink=.5, aspect=10, label='Intensity', ax=ax2, pad=0.2)\n\n# show the plot\nplt.tight_layout()\nplt.show()\n</pre> # create a figure with 3D axis (two columns for different coloring) fig, axs = plt.subplots(1,2,subplot_kw={\"projection\": \"3d\"},figsize=(10,5)) ax1, ax2 = axs  nth=500  # plot the point cloud colored by height (z values) surf = ax1.scatter(pc_2017_coords[::nth,0], pc_2017_coords[::nth,1], pc_2017_coords[::nth,2], s=5,                    c=pc_2017_coords[::nth,2],cmap=plt.cm.terrain)  # plot the point cloud colored by intensity intens = ax2.scatter(pc_2017_coords[::nth,0], pc_2017_coords[::nth,1], pc_2017_coords[::nth,2], s=5,                    c=pc_2017_attributes['intensity'][::nth],cmap='copper')   # label axes and add title for ax in axs:     ax.set_xlabel('X [m]')     ax.set_ylabel('Y [m]')     ax.set_zlabel('Z [m]')          # set initial view of 3D plot     ax.view_init(elev=40., azim=130.)  # add colorbars fig.colorbar(surf, shrink=.5, aspect=10, label='Height [m]', ax=ax1, pad=0.2) fig.colorbar(intens, shrink=.5, aspect=10, label='Intensity', ax=ax2, pad=0.2)  # show the plot plt.tight_layout() plt.show() <p>We may additionally plot the glacier outline as polygon here. To make it visible on the 3D surface, we need to add elevation (Z) values to the XY coordinates. We may grab these values from the point cloud data. This is a neighborhood operation, which can be efficiently done using suitable data structures (e.g., a kd-tree) and corresponding search functions. This will be introduced in the next section of this theme.</p> <p>As a last part of visualization, we would like to hint to the Python package <code>polyscope</code>, a viewer and user interface for 3D data (such as meshes and point clouds). Interactive 3D visualizations in a user interface (UI) can be created and customized with few lines of code:</p> In\u00a0[25]: Copied! <pre>import polyscope as ps\n\nps.init() # initialize polyscope\n\nps_cloud = ps.register_point_cloud(\"Glacier 2017\", pc_2017_coords) # register a point cloud\n\nps.set_up_dir(\"z_up\") # orient the point cloud correctly\n\nps.show() # view the point cloud and mesh we just registered in the 3D UI\n</pre> import polyscope as ps  ps.init() # initialize polyscope  ps_cloud = ps.register_point_cloud(\"Glacier 2017\", pc_2017_coords) # register a point cloud  ps.set_up_dir(\"z_up\") # orient the point cloud correctly  ps.show() # view the point cloud and mesh we just registered in the 3D UI <p>If you'd like to use polyscope, find further examples and usage guidelines in the polyscope documentation.</p> <p></p> In\u00a0[26]: Copied! <pre># import required module\nfrom scipy.spatial import KDTree\n\n# build kd-tree from 3D coordinates\ntree2017 = KDTree(pc_2017_coords)\n</pre> # import required module from scipy.spatial import KDTree  # build kd-tree from 3D coordinates tree2017 = KDTree(pc_2017_coords) <p>The kd-tree can be used to search the neighborhood of 3D points efficiently. The k nearest neighbors of a set of search points (same number of dimensions as the kd-tree) can be queried using the scipy function <code>query</code>. The neighbors in the spatial neighborhood can be queried using the function <code>query_ball_point</code> for a specified radius.</p> In\u00a0[27]: Copied! <pre># create a 2D kd-tree from the point cloud\ntree2017_2d = KDTree(pc_2017_coords[:,:2])\n\n# create a grid of XY coordinates with 1.0 m resolution and spatial extent of the point cloud\nX = np.arange(int(np.min(pc_2017_coords[:,0])), int(np.max(pc_2017_coords[:,0]) + 0.5), 1.0)\nY = np.arange(int(np.min(pc_2017_coords[:,1])), int(np.max(pc_2017_coords[:,1]) + 0.5), 1.0)\nX, Y = np.meshgrid(X, Y)\nsearch_locs = np.vstack((X.flatten(),Y.flatten())).transpose()\n\n# determine the neighboring points within radius r of each point using a 2D kd-tree\n# use return_length to return the number of points, instead of their indices\nnum_neighs = tree2017_2d.query_ball_point(search_locs, r=0.5, return_length=True)\n</pre> # create a 2D kd-tree from the point cloud tree2017_2d = KDTree(pc_2017_coords[:,:2])  # create a grid of XY coordinates with 1.0 m resolution and spatial extent of the point cloud X = np.arange(int(np.min(pc_2017_coords[:,0])), int(np.max(pc_2017_coords[:,0]) + 0.5), 1.0) Y = np.arange(int(np.min(pc_2017_coords[:,1])), int(np.max(pc_2017_coords[:,1]) + 0.5), 1.0) X, Y = np.meshgrid(X, Y) search_locs = np.vstack((X.flatten(),Y.flatten())).transpose()  # determine the neighboring points within radius r of each point using a 2D kd-tree # use return_length to return the number of points, instead of their indices num_neighs = tree2017_2d.query_ball_point(search_locs, r=0.5, return_length=True) <p>The 1D array <code>num_neighs</code>contains the number of neighboring points per input search location. From this we can derive the average as point density per square meter (appr. due to search radius of 0.5 m):</p> In\u00a0[28]: Copied! <pre>pdens_2d = np.mean(num_neighs)\nprint(f'The mean point density in the point cloud is {pdens_2d:.1f} pts/m\u00b2.')\n</pre> pdens_2d = np.mean(num_neighs) print(f'The mean point density in the point cloud is {pdens_2d:.1f} pts/m\u00b2.') <pre>The mean point density in the point cloud is 3.9 pts/m\u00b2.\n</pre> In\u00a0[29]: Copied! <pre># query indices of nearest neighbors of 2009 coordinates in 2017 kd-tree\nnn_dists = tree2017.query(pc_2009_coords, k=1)\n</pre> # query indices of nearest neighbors of 2009 coordinates in 2017 kd-tree nn_dists = tree2017.query(pc_2009_coords, k=1) <p>The <code>query()</code>function returns both the distances to the nearest neighbors and their index in the search point cloud. Therefore, the distances are directly available and can be plotted for visual inspection:</p> In\u00a0[30]: Copied! <pre># obtain distances as first element in tuple returned by query above\ndistances = nn_dists[0]\n\n# create a figure with 3D axis (two columns for different coloring)\nfig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))\n\nnth=100\n\n# plot the point cloud colored by height (z values)\ns = ax.scatter(pc_2009_coords[::nth,0], pc_2009_coords[::nth,1], pc_2009_coords[::nth,2], s=5,\n                   c=distances[::nth])\n\n# label axes and add title\nax.set_xlabel('X [m]')\nax.set_ylabel('Y [m]')\nax.set_zlabel('Z [m]')\n\n# set initial view of 3D plot\nax.view_init(elev=40., azim=130.)\n\n# add a colorbar\nfig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.2)\n\n# show the plot\nplt.tight_layout()\nplt.show()\n</pre> # obtain distances as first element in tuple returned by query above distances = nn_dists[0]  # create a figure with 3D axis (two columns for different coloring) fig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))  nth=100  # plot the point cloud colored by height (z values) s = ax.scatter(pc_2009_coords[::nth,0], pc_2009_coords[::nth,1], pc_2009_coords[::nth,2], s=5,                    c=distances[::nth])  # label axes and add title ax.set_xlabel('X [m]') ax.set_ylabel('Y [m]') ax.set_zlabel('Z [m]')  # set initial view of 3D plot ax.view_init(elev=40., azim=130.)  # add a colorbar fig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.2)  # show the plot plt.tight_layout() plt.show() <p>We now obtained our first 3D change information: The distances between points from one epoch to the nearest points in the other epoch show us large distances (up to 30 m) on the glacier. This indicates that there were strong changes of glacier retreat and/or volume loss. In contrast, distance values are small to zero on the surrounding rock slopes, which are hence mostly stable or unchanged.</p> <p>To further assess and interpret changes in such a 3D scene, concepts of 3D change analysis including types and directions of changes, and associated uncertainties are introduced in detail in the next theme on Principles and basic algorithms of 3D change detection and analysis.</p> <p>The following section regards the derivation of rasters from 3D point cloud data.</p> <p></p> <p>Many workflows of point cloud analysis yield a GIS layer in the form of rasters as result. Rasterizing 3D point clouds means to assign one value to each raster pixel. For example, creating a raster of Z values representing the terrain surface yields a Digital Terrain Model (DTM), or Digital Elevation Model (DEM) in more general terms (cf. previous theme on Principles of 3D/4D geographic point clouds).</p> <p>To further process point clouds and deriving rasters, we are going to include PDAL, the open source \"Point Data Abstraction Library\" which offers functionality for processing point cloud data. It offers many common and special methods and algorithms to process point clouds. In this course, we are going to make use of PDAL's Python support.</p> In\u00a0[31]: Copied! <pre>import pdal\n</pre> import pdal <p>Our aim is to rasterize the terrain of our glacier point clouds to obtain a DTM. A simple approach is to assign each raster pixel the minimum Z value that occurs in the area of the pixel (of defined spatial resolution).</p> In\u00a0[32]: Copied! <pre># define the point cloud paths for both epochs\npc_2009 = f'{data_dir}/hellstugubrean_2009.las'\npc_2017 = f'{data_dir}/hellstugubrean_2017.las'\n\n# replacing backslashes by forward slashes is required for some Windows paths\npc_2009 = pc_2009.replace(\"\\\\\", \"/\")\npc_2017 = pc_2017.replace(\"\\\\\", \"/\")\n\n# define the DTM raster file name based on the input file name\ndtm_2009 = pc_2009.replace(\".las\", \"_dtm.tif\")\ndtm_2017 = pc_2017.replace(\".las\", \"_dtm.tif\")\n</pre> # define the point cloud paths for both epochs pc_2009 = f'{data_dir}/hellstugubrean_2009.las' pc_2017 = f'{data_dir}/hellstugubrean_2017.las'  # replacing backslashes by forward slashes is required for some Windows paths pc_2009 = pc_2009.replace(\"\\\\\", \"/\") pc_2017 = pc_2017.replace(\"\\\\\", \"/\")  # define the DTM raster file name based on the input file name dtm_2009 = pc_2009.replace(\".las\", \"_dtm.tif\") dtm_2017 = pc_2017.replace(\".las\", \"_dtm.tif\") <p>PDAL works in so-called stages, divided into readers, filters and writers. The processing of data in PDAL is defined in a pipeline, which may consist of different stages. In our case, we will perform two steps: (1) reading the point cloud from las, and (2) writing the elevation to a raster. The pipeline is formatted in json:</p> In\u00a0[33]: Copied! <pre>json_dtm = \"\"\"[\n    \"%s\",\n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":5.0,\n        \"window_size\":8\n    }\n]\"\"\"  \n</pre> json_dtm = \"\"\"[     \"%s\",     {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":5.0,         \"window_size\":8     } ]\"\"\"   <p>In the above definition of the pipeline as json, we did not yet define the variable of the input point cloud file and the output raster file. They are represented by <code>%s</code> and can be used for string formatting. By this, we can use the same pipeline template for different inputs and outputs.</p> In\u00a0[34]: Copied! <pre>json_dtm_2009 = json_dtm % (pc_2009, dtm_2009)\njson_dtm_2017 = json_dtm % (pc_2017, dtm_2017)\n</pre> json_dtm_2009 = json_dtm % (pc_2009, dtm_2009) json_dtm_2017 = json_dtm % (pc_2017, dtm_2017) <p>Using the json format, we can construct a PDAL pipeline and execute it:</p> In\u00a0[35]: Copied! <pre>pipeline = pdal.Pipeline(json_dtm_2009)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_dtm_2009) exe = pipeline.execute() <p>We create the same DTM for the 2017 epoch:</p> In\u00a0[36]: Copied! <pre>pipeline = pdal.Pipeline(json_dtm_2017)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_dtm_2017) exe = pipeline.execute() <p>In your data directory, you will now find the generated raster files in GeoTiff format. Check it out in QGIS:</p> Digital terrain models (DTMs) in QGIS with render type 'Hillshade', which visualizes the terrain and especially glacier surface changes between the two epochs. Figure by course authors. <p>To assess raster-based changes in the terrain, we can simply subtract the two DTMs. This will provide the difference in local elevation (Z values) for each pixel (more on that in the next theme). We will read the raster data into a numpy array using rasterio.</p> In\u00a0[37]: Copied! <pre>import rasterio as rio\n\nwith rio.open(dtm_2009) as src:\n    dtm_2009_data = src.read(1, masked=True)\n\nwith rio.open(dtm_2017) as src:\n    dtm_2017_data = src.read(1, masked=True)\n</pre> import rasterio as rio  with rio.open(dtm_2009) as src:     dtm_2009_data = src.read(1, masked=True)  with rio.open(dtm_2017) as src:     dtm_2017_data = src.read(1, masked=True) <p>The difference between the two is obtained by simply subtracting one epoch from the other:</p> In\u00a0[38]: Copied! <pre>dtm_diff = dtm_2017_data - dtm_2009_data\n</pre> dtm_diff = dtm_2017_data - dtm_2009_data <p>We use Python plots to visualize the result:</p> In\u00a0[39]: Copied! <pre># create a figure \nfig, ax = plt.subplots(1,1)\n\n# plot the difference values in the 2D array\nm = ax.imshow(dtm_diff, aspect='equal')\n\n# add a colorbar\nfig.colorbar(m, shrink=.5, aspect=10, label='DTM difference [m]', ax = ax, pad=0.2)\n\n# show the plot\nplt.tight_layout()\nplt.show()\n</pre> # create a figure  fig, ax = plt.subplots(1,1)  # plot the difference values in the 2D array m = ax.imshow(dtm_diff, aspect='equal')  # add a colorbar fig.colorbar(m, shrink=.5, aspect=10, label='DTM difference [m]', ax = ax, pad=0.2)  # show the plot plt.tight_layout() plt.show() <p>We see a similar result as for the temporal neighbor search in the point cloud above. By subtracting the DTMs (later minus earlier epochs), we already obtained a correct sign of distances. Where the surface decreased - on the glacier - the values are negative. However, there are important advantages of conducting the change analysis in 3D on the point cloud. You will learn about them in the next theme, including approaches which account for the direction of changes. But first, finish this theme by proceeding with the exercise!</p> <p></p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#programming-for-point-cloud-analysis-with-python","title":"Programming for point cloud analysis with Python\u00b6","text":""},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#requirements-for-this-lesson","title":"Requirements for this lesson\u00b6","text":""},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#software-and-skills","title":"Software and skills\u00b6","text":"<p>To follow this theme hands-on and to perform the exercise, a Python installation and environment with specific packages is required. You can set up the Python environment for this course using the provided requirements file. Follow the instructions in the software material.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#data","title":"Data\u00b6","text":"<p>The data used in this exercise are two point clouds of the Hellstugubrean glacier in the Jotunheimen nasjonalpark, Norway 61.5759\u00b0 N, 8.4459\u00b0 E. Data is provided by the Norwegian Mapping Authority under the CC BY 4.0 License: \u00a9 Kartverket. The data was downloaded from H\u00f8ydedata and cropped to an area of interest.</p> <p>We use two acquisitions, an airborne laser scanning point cloud from 2009 with an average point density of 0.3 $pts/m^2$ and a dense image matching point cloud from 2017 with an average point density of 4 $pts/m^2$. Find the data in the directory <code>hellstugubrean</code> of the course data repository.</p> <p>Point cloud of Hullstugubrean glacier in Norway (epoch 2017) used for exercise throughout this theme. Figure by course authors. Data \u00a9 Kartverket / CC BY 4.0.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#reading-handling-and-writing-point-cloud-data","title":"Reading, handling, and writing point cloud data\u00b6","text":""},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#reading-point-cloud-data","title":"Reading point cloud data\u00b6","text":"<p>For the task of reading and writing point cloud data with Python, we will use the exercise point cloud <code>hellstugubrean_2009.las</code>. Specify the data locations in your local computer environment:</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#handling-point-cloud-data","title":"Handling point cloud data\u00b6","text":"<p>In the following, we will use the attribute and coordinate information for filtering and spatial subsetting of the data, before writing the edited point cloud to a new output file.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#filtering-by-attribute","title":"Filtering by attribute\u00b6","text":"<p>Editing of the point cloud data can be based on attribute information, for example using their value ranges or derived properties for filtering.</p> <p>When performing analyses of the terrain, we may wish to remove points that do not represent solid object surfaces. In a simple approach, such points can be determined as measurements stemming from multiple returns wihin a single laser pulse. We therefore filter our point cloud data to contain only points which originate as single returns or last returns in case of multiple echos. This removes points which are first or intermediate echos of multiple returns and thereby represent penetrable objects, such as vegetation, or stem from edges which are hit by the beam while the pulse travels on until it hits the ultimate target surface.</p> <p>As we know from the attribute dictionary, our point cloud contains information on each point's <code>return_number</code>and <code>number_of_returns</code>. From this, we can derive if a point is a single echo (<code>number_of_returns</code>=1) or one of multiple echos (<code>number_of_returns</code>&gt;1). If a point originates from a beam with multiple echoes, we can derive if it is a first, intermediate or last echo by incorporating its <code>return_number</code>. To obtain this information for each point, we will create a new, additional attribute <code>echo_type</code> with values <code>{0:'single', 1:'first', 2:'intermediate', 3:'last'}</code>.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#spatial-subsetting","title":"Spatial subsetting\u00b6","text":"<p>We may further edit our point cloud data via spatial subsetting, i.e. maintaining only points whose coordinate are located in a certain area of interest. This area of interest can be defined via a simple 2D bounding box.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#writing-point-cloud-data","title":"Writing point cloud data\u00b6","text":""},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#plotting-and-statistics","title":"Plotting and statistics\u00b6","text":"<p>In addition to writing outputs to point cloud files, the data can be visualized directly using Python functionality. Visualization can regard the point cloud data, i.e. the topography of the scene, but also information derived from the point cloud data, for example in the form of statistics. In the following, we use matplotlib for plotting point cloud information.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#neighborhood-operations","title":"Neighborhood operations\u00b6","text":"<p>Neighborhood operations are key to many processing steps of 3D geodata analysis. The concepts of neighborhood operations are part of Theme 1. Now we will consider them in practice.</p> <p></p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#spatial-data-structure-for-neighborhood-operations-kd-tree","title":"Spatial data structure for neighborhood operations: kd-tree\u00b6","text":"<p>How would you find the nearest neighbor of a point in a point cloud? By checking the distance of each point and selecting the one with the shortest distance. Checking each point for every neighborhood search takes a lot of time. But we can make use of spatial indexing to speed up the process. This means, we can immediately exclude a large portion of points, which are not in the same spatial area as our search point. We will use a kd-tree as data structure for efficient neighborhood searches.</p> <p>A kd-tree is a binary tree that splits a dataset by using nodes. When no more splitting is possible the created splitted datasets are called leaf cells. The dataset is divided at any non-leaf node into two equally sized subsets of the data. If you want to learn more about kd-trees, you  may have a look at this video. In this course, we will use the kd-tree in in the Python package <code>scipy.spatial</code>.</p> <p>We start by building a kd-tree for the point cloud epoch of 2017:</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#estimating-point-density","title":"Estimating point density\u00b6","text":"<p>A commonly provided metric to describe point cloud data is the 2D point density, i.e. information about the average number of points per square meter. We can derive these, by searching all points in the 2D neighborhood of XY coordinates. To obtain a meaningful estimate, we need to compute the point density using a regular grid of 2D locations (XY coordinates) as search points.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#nearest-neighbor-search-in-multitemporal-point-clouds","title":"Nearest neighbor search in multitemporal point clouds\u00b6","text":"<p>The neighborhood search allows us to perform a first simple change analysis in multitemporal point clouds. Using the two epochs of 2009 and 2017, for each point in one epoch we can determine the closest point in the other epoch and derive their Euclidian distance. We will use the 3D kdtree <code>tree2017</code> created above and search the nearest neighbor (<code>k=1</code>) for each point in the 2009 point cloud epoch.</p>"},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#rasterizing-point-cloud-data","title":"Rasterizing point cloud data\u00b6","text":""},{"location":"module3/02_pointcloud_programming_python/02_pointcloud_programming_python.html#exercise","title":"Exercise\u00b6","text":"<p>Since this theme is highly practical, there is no self-evaluation quiz. You will assess your achieved skills in practice, by performing the exercise. In the exercise, you will use virtual laser scanning with HELIOS++ in Python to assess how the acquisition strategy - airborne instead of UAV laser scanning - influences the changes derived for rock glacier monitoring.</p> <p>Proceed with the exercise by going to the next page below or directly download the Jupyter Notebook.</p>"},{"location":"module3/02_pointcloud_programming_python/exercise/m3_theme2_exercise1.html","title":"Exercise","text":"Metadata      title: \"E-TRAINEE HELIOS++ exercise for point cloud change analysis\"     description: \"This is an exercise in the second theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2022-08     authors: Mark Searle     contributors: Katharina Anders, Bernhard H\u00f6fle      estimatedTime: 1.5 hrs  <p>In the following cell, we indicate our HELIOS++ root directory from which we will be working. Then, we import the remaining libraries.</p> <p>Important: All data paths need to be indicated with forward slashes</p> In\u00a0[1]: Copied! <pre>import os\n\nhelios_path = \"your/helios/path\"\nos.chdir(helios_path)\n</pre> import os  helios_path = \"your/helios/path\" os.chdir(helios_path) In\u00a0[2]: Copied! <pre># Import required modules\nimport pdal \nimport pyhelios\nfrom pyhelios import SimulationBuilder\nfrom pyhelios.util import flight_planner, scene_writer\n%matplotlib qt\nimport matplotlib.pyplot as plt\nimport matplotlib\n#import xml.etree.ElementTree as ET\nfrom ipywidgets import interact\nimport numpy as np\nimport time\n</pre> # Import required modules import pdal  import pyhelios from pyhelios import SimulationBuilder from pyhelios.util import flight_planner, scene_writer %matplotlib qt import matplotlib.pyplot as plt import matplotlib #import xml.etree.ElementTree as ET from ipywidgets import interact import numpy as np import time In\u00a0[6]: Copied! <pre>uls_t1 = \"path-to-data/ahk_2020_uls.laz\"\nuls_t2 = \"path-to-data/ahk_2021_uls.laz\"\n</pre> uls_t1 = \"path-to-data/ahk_2020_uls.laz\" uls_t2 = \"path-to-data/ahk_2021_uls.laz\" <p>1.1 Create raster from ULS 2020:</p> In\u00a0[4]: Copied! <pre>uls_t1_dtm = uls_t1.replace(\".laz\", \"_dtm_1m.tif\")\n</pre> uls_t1_dtm = uls_t1.replace(\".laz\", \"_dtm_1m.tif\") In\u00a0[5]: Copied! <pre>json_dtm = \"\"\"[\n    \"%s\", \n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":1.0,\n        \"window_size\":8\n    }\n]\"\"\"%(uls_t1, uls_t1_dtm)\n</pre> json_dtm = \"\"\"[     \"%s\",      {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":1.0,         \"window_size\":8     } ]\"\"\"%(uls_t1, uls_t1_dtm)  In\u00a0[6]: Copied! <pre>pipeline = pdal.Pipeline(json_dtm)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_dtm) exe = pipeline.execute() <p>1.1.1 Getting raster dimensions:</p> <p>For this we will use the python package rasterio, which allows us to read and write TIF files.</p> In\u00a0[7]: Copied! <pre>import rasterio as rio\n</pre> import rasterio as rio In\u00a0[8]: Copied! <pre>#getting raster dimensions so the dimensions of the second raster can be adapted accordingly\n\nwith rio.open(uls_t1_dtm) as src:\n    uls_t1_data = src.read(1, masked=True)\n    t1_tf = src.transform\n    t1_bounds = src.bounds\n    dsm_meta = src.profile\n    width= src.width\n    height = src.height\n    \norigin_left, origin_bottom, origin_right, origin_top = t1_bounds\ndsm_width = dsm_meta['width']\ndsm_height = dsm_meta['height']\n</pre> #getting raster dimensions so the dimensions of the second raster can be adapted accordingly  with rio.open(uls_t1_dtm) as src:     uls_t1_data = src.read(1, masked=True)     t1_tf = src.transform     t1_bounds = src.bounds     dsm_meta = src.profile     width= src.width     height = src.height      origin_left, origin_bottom, origin_right, origin_top = t1_bounds dsm_width = dsm_meta['width'] dsm_height = dsm_meta['height']  <p>1.2 Create raster from ULS 2021:</p> In\u00a0[9]: Copied! <pre>uls_t2_dtm = uls_t2.replace(\".laz\", \"_dtm_1m.tif\")\n</pre> uls_t2_dtm = uls_t2.replace(\".laz\", \"_dtm_1m.tif\") In\u00a0[10]: Copied! <pre>json_dtm = \"\"\"[\n    \"%s\",\n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":1.0,\n        \"window_size\":8,\n        \"origin_x\":\"%.3f\",\n        \"origin_y\":\"%.3f\",\n        \"width\":\"%i\",\n        \"height\":\"%i\"\n    }\n]\"\"\"% (uls_t2, uls_t2_dtm, origin_left, origin_bottom, dsm_width, dsm_height)\n</pre> json_dtm = \"\"\"[     \"%s\",     {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":1.0,         \"window_size\":8,         \"origin_x\":\"%.3f\",         \"origin_y\":\"%.3f\",         \"width\":\"%i\",         \"height\":\"%i\"     } ]\"\"\"% (uls_t2, uls_t2_dtm, origin_left, origin_bottom, dsm_width, dsm_height) In\u00a0[11]: Copied! <pre>pipeline = pdal.Pipeline(json_dtm)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_dtm) exe = pipeline.execute() <p>1.3 Calculate difference of rasters:</p> In\u00a0[12]: Copied! <pre>with rio.open(uls_t1_dtm) as src:\n    uls_t1_data = src.read(1, masked=True)\n    \nwith rio.open(uls_t2_dtm) as src:\n    uls_t2_data = src.read(1, masked=True)\n    \n    \n</pre> with rio.open(uls_t1_dtm) as src:     uls_t1_data = src.read(1, masked=True)      with rio.open(uls_t2_dtm) as src:     uls_t2_data = src.read(1, masked=True)           In\u00a0[13]: Copied! <pre>uls_diff = uls_t1_data - uls_t2_data\n</pre> uls_diff = uls_t1_data - uls_t2_data In\u00a0[14]: Copied! <pre>plt.imshow(uls_diff, cmap='pink')\nplt.show()\n</pre> plt.imshow(uls_diff, cmap='pink') plt.show() <p>We can clearly identify the rock glacier by the areas of surface change that we see in the image. Let's save the ULS surface change to a file.</p> In\u00a0[15]: Copied! <pre>uls_diff_file = 'uls_diff.tif'\n\nwith rio.open(uls_diff_file, 'w', **dsm_meta) as ff:\n    ff.write(uls_diff, 1)\n</pre> uls_diff_file = 'uls_diff.tif'  with rio.open(uls_diff_file, 'w', **dsm_meta) as ff:     ff.write(uls_diff, 1) <p>2.1 Create HELIOS++ scenes</p> <p>First we must create the XML scenes for our two simulations. The scene files allow you to define the 3d models which will be loaded into HELIOS++ and recorded by the virtual laser scanner. You can read up on scene files in the HELIOS++ wiki entry. If you are not familiar with XML files it might also be sensible to have a look at the XML wikipedia article. Each component of a HELIOS++ simulation is defined in an XML file.</p> <p>In our case, the scenes will consist of one TIF file each, representing the surface of the rock glacier at the two points in time. We will write the scenes using the scene_writer, which is part of the util-sub package of pyhelios. You can read up on the scene writer in the corresponding wiki entry.</p> <p>2.1.1 Create sceneparts from DTMs</p> <p>We first create python instances of out two sceneparts, the DTMs of the rock glacier in 2020 and 2021. To do so, we use the <code>create_scenepart_tiff</code> function which is demonstrated in the corresponding wiki entry. Click on the plus sign to the left of the cell if you need help.</p> In\u00a0[63]: Copied! <pre>sp1 = scene_writer.create_scenepart_tiff()\n\nsp2 = scene_writer.create_scenepart_tiff()\n</pre> sp1 = scene_writer.create_scenepart_tiff()  sp2 = scene_writer.create_scenepart_tiff() <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In [63], line 1\n----&gt; 1 sp1 = scene_writer.create_scenepart_tiff()\n      3 sp2 = scene_writer.create_scenepart_tiff()\n\nTypeError: create_scenepart_tiff() missing 1 required positional argument: 'filepath'</pre> In\u00a0[16]: Copied! <pre>sp1 = scene_writer.create_scenepart_tiff(uls_t1_dtm,\n                               matfile=r\"data/sceneparts/basic/groundplane/groundplane.mtl\",\n                               matname=\"None\")\n\nsp2 = scene_writer.create_scenepart_tiff(uls_t2_dtm,\n                               matfile=r\"data/sceneparts/basic/groundplane/groundplane.mtl\",\n                               matname=\"None\")\n</pre> sp1 = scene_writer.create_scenepart_tiff(uls_t1_dtm,                                matfile=r\"data/sceneparts/basic/groundplane/groundplane.mtl\",                                matname=\"None\")  sp2 = scene_writer.create_scenepart_tiff(uls_t2_dtm,                                matfile=r\"data/sceneparts/basic/groundplane/groundplane.mtl\",                                matname=\"None\") <p>2.1.2 Build scenes</p> <p>Next, we build our two scenes. Each scene contains one of the sceneparts created in the previous step, corresponding to the rock glacier in 2020 and 2021. The <code>build_scene</code> function allows us to do so. Click on the plus sign to the left of the cell if you need help.</p> In\u00a0[22]: Copied! <pre>scenes = [scene_writer.build_scene(), \n          scene_writer.build_scene()]\n</pre> scenes = [scene_writer.build_scene(),            scene_writer.build_scene()] <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In [22], line 1\n----&gt; 1 scenes = [scene_writer.build_scene(), \n      2           scene_writer.build_scene()]\n\nTypeError: build_scene() missing 2 required positional arguments: 'scene_id' and 'name'</pre> In\u00a0[17]: Copied! <pre>scenes = [scene_writer.build_scene(scene_id=\"uls_t1\", name=\"Rock Glacier T1\", sceneparts=[sp1]), \n          scene_writer.build_scene(scene_id=\"uls_t2\", name=\"Rock Glacier T2\", sceneparts=[sp2])]\n</pre> scenes = [scene_writer.build_scene(scene_id=\"uls_t1\", name=\"Rock Glacier T1\", sceneparts=[sp1]),            scene_writer.build_scene(scene_id=\"uls_t2\", name=\"Rock Glacier T2\", sceneparts=[sp2])] <p>2.1.3 Write scenefiles</p> <p>Finally, we write the scenes to XML files which can be read by HELIOS++ to execute the simulations. We give a name to each file and write the content of the scenes we built in the previous step to the files. Click on the plus sign to the left of the cell if you need help.</p> In\u00a0[24]: Copied! <pre>scenefiles = ['', '']\n\nfor i in range(2):\n    pass\n</pre> scenefiles = ['', '']  for i in range(2):     pass In\u00a0[18]: Copied! <pre>scenefiles = ['data/scenes/scene_t1.xml', 'data/scenes/scene_t2.xml']\n\nfor i in range(2):\n    with open(scenefiles[i], 'w') as f:\n        f.write(scenes[i])\n</pre> scenefiles = ['data/scenes/scene_t1.xml', 'data/scenes/scene_t2.xml']  for i in range(2):     with open(scenefiles[i], 'w') as f:         f.write(scenes[i]) <p>2.2 Create HELIOS++ surveys</p> <p>The HELIOS++ XML survey file defines the movement and type of scanner used, as well as the settings of the scanner and the scene to be scanned. In the cell below you can choose an appropriate scanning platform and scanner model for our acquisitions. You can also choose the settings for the surveys, as well as names for the surveys.</p> <p>Take a look at the wiki page for the scanner and the platform to help you choose the correct platform and scanner ids for ALS acquisitions. The possible scanner settings can be read from the entries for each scanner in the scanner xml file. If you need some help, click on the plus sign to the left of the cell.</p> In\u00a0[\u00a0]: Copied! <pre>scanner_xml = 'data/scanners_als.xml'\nplatform_xml = 'data/platforms.xml'\nscanner_id = ''\nplatform_id = ''\npulse_freq = 0\nscan_freq = 0\nflight_v = 0\nalt = 0\nscan_angle = 0\nsurveys = [\"\", \"\"]\n</pre> scanner_xml = 'data/scanners_als.xml' platform_xml = 'data/platforms.xml' scanner_id = '' platform_id = '' pulse_freq = 0 scan_freq = 0 flight_v = 0 alt = 0 scan_angle = 0 surveys = [\"\", \"\"] In\u00a0[19]: Copied! <pre>scanner_xml = 'data/scanners_als.xml'\nplatform_xml = 'data/platforms.xml'\nscanner_id = 'leica_als50-ii'\nplatform_id = 'sr22'\npulse_freq = 100000\nscan_freq = 90\nflight_v = 60\nalt = 4000\nscan_angle = 30\nsurveys = [\"data/surveys/als_t1.xml\", \"data/surveys/als_t2.xml\"]\n</pre> scanner_xml = 'data/scanners_als.xml' platform_xml = 'data/platforms.xml' scanner_id = 'leica_als50-ii' platform_id = 'sr22' pulse_freq = 100000 scan_freq = 90 flight_v = 60 alt = 4000 scan_angle = 30 surveys = [\"data/surveys/als_t1.xml\", \"data/surveys/als_t2.xml\"] <p>Explanation: The specifications above mean that we will use the Leica ALS 50-ii which will be mounted on our airborne laser scanning mount, the SR-22. The scanner will operate at a pulse frequency of 100000 Hz and a scan frequency of 90 Hz at a scan angle of +- 30\u00b0. The aicraft will be flying at an altitude of 4000 m (above sea level) with a speed of 60 m/s.</p> <p>You will find the ALS 50-ii and its specifications in the file <code>data/scanners_als.xml</code> relative to your HELIOS++ directory. The SR-22 mount can be found in the file <code>data/platforms_als.xml</code></p> <p>2.2.1 Set flight paths</p> <p>To define our surveys we must determine the flight path of the aircraft. Excecute the following code to set the two flight passes manually.</p> In\u00a0[20]: Copied! <pre>def tellme(s):\n    plt.title(s, fontsize=12)\n    plt.draw()\n    \nn_pos = 4\n\nplt.figure()\n\nzoom_out = 600\n\nplt.imshow(uls_diff, cmap='pink')\n\nplt.axis('equal')\n\nax = plt.gca()\nax.set_xlim([-zoom_out, np.shape(uls_diff)[1]+zoom_out])\nax.set_ylim([np.shape(uls_diff)[0]+zoom_out, -zoom_out])\n\ntellme('Now you may select the trajectory of the aircraft carrying the virtual laser scanner.')\n\nplt.waitforbuttonpress()\n\nwhile True:\n    pt = []\n    while len(pt) &lt; n_pos:\n        tellme(f'Please choose {n_pos} four positions with the mouse.')\n        pt = np.asarray(plt.ginput(n_pos, timeout=-1))\n    line_1 = plt.plot([pt[0][0], pt[1][0]], [pt[0][1], pt[1][1]], label='Pass 1', color='steelblue')\n    line_2 = plt.plot([pt[2][0], pt[3][0]], [pt[2][1], pt[3][1]], label='Pass 2', color='firebrick')\n    \n    ar_length = 10000\n    arrow_1 = plt.arrow(pt[1][0], pt[1][1], -(pt[0][0]-pt[1][0])/ar_length, (pt[1][1]-pt[0][1])/ar_length, head_width=50, edgecolor='none', facecolor='steelblue')\n    arrow_2 = plt.arrow(pt[3][0], pt[3][1], -(pt[2][0]-pt[3][0])/ar_length, (pt[3][1]-pt[2][1])/ar_length, head_width=50, edgecolor='none', facecolor='firebrick')\n    plt.legend()\n    \n    tellme('Happy?\\nKeypress for \"Yes\", mouseclick for \"No\"')\n\n    if plt.waitforbuttonpress(timeout=-1):\n        plt.close()\n        break\n    \n    ax = plt.gca()\n\n    for art in ax.get_children():\n        if isinstance(art, matplotlib.patches.FancyArrow) or isinstance(art, matplotlib.lines.Line2D):\n            art.remove()\n\nx1, y1 = t1_tf * (pt[0][0], pt[0][1])\nx2, y2 = t1_tf * (pt[1][0], pt[1][1])\nprint('You have chosen your flight lines.\\nCoordinates P1: x={}, y={}\\nCoordinates P2: x={}, y={}'.format(x1, y1, x2, y2))\n</pre> def tellme(s):     plt.title(s, fontsize=12)     plt.draw()      n_pos = 4  plt.figure()  zoom_out = 600  plt.imshow(uls_diff, cmap='pink')  plt.axis('equal')  ax = plt.gca() ax.set_xlim([-zoom_out, np.shape(uls_diff)[1]+zoom_out]) ax.set_ylim([np.shape(uls_diff)[0]+zoom_out, -zoom_out])  tellme('Now you may select the trajectory of the aircraft carrying the virtual laser scanner.')  plt.waitforbuttonpress()  while True:     pt = []     while len(pt) &lt; n_pos:         tellme(f'Please choose {n_pos} four positions with the mouse.')         pt = np.asarray(plt.ginput(n_pos, timeout=-1))     line_1 = plt.plot([pt[0][0], pt[1][0]], [pt[0][1], pt[1][1]], label='Pass 1', color='steelblue')     line_2 = plt.plot([pt[2][0], pt[3][0]], [pt[2][1], pt[3][1]], label='Pass 2', color='firebrick')          ar_length = 10000     arrow_1 = plt.arrow(pt[1][0], pt[1][1], -(pt[0][0]-pt[1][0])/ar_length, (pt[1][1]-pt[0][1])/ar_length, head_width=50, edgecolor='none', facecolor='steelblue')     arrow_2 = plt.arrow(pt[3][0], pt[3][1], -(pt[2][0]-pt[3][0])/ar_length, (pt[3][1]-pt[2][1])/ar_length, head_width=50, edgecolor='none', facecolor='firebrick')     plt.legend()          tellme('Happy?\\nKeypress for \"Yes\", mouseclick for \"No\"')      if plt.waitforbuttonpress(timeout=-1):         plt.close()         break          ax = plt.gca()      for art in ax.get_children():         if isinstance(art, matplotlib.patches.FancyArrow) or isinstance(art, matplotlib.lines.Line2D):             art.remove()  x1, y1 = t1_tf * (pt[0][0], pt[0][1]) x2, y2 = t1_tf * (pt[1][0], pt[1][1]) print('You have chosen your flight lines.\\nCoordinates P1: x={}, y={}\\nCoordinates P2: x={}, y={}'.format(x1, y1, x2, y2)) <pre>You have chosen your flight lines.\nCoordinates P1: x=652857.1074354838, y=5189495.766032258\nCoordinates P2: x=653181.8614677419, y=5188936.887\n</pre> <p>2.2.2 Write survey files</p> <p>Now we can write the selected scan lines to our survey files. In the step below, we write the string containing the simulation legs, that define the movement of the scanner and the settings at each point in time. Have a look at the wiki entry on simulation legs to find out more.</p> <p>In our case, we take the points selected in the plot in the last cell.</p> In\u00a0[21]: Copied! <pre>legs = ''\nactive = True\nfor i in range(4):\n    if i%2==0:\n        active = True\n    else:\n        active = False\n    \n    x, y = t1_tf * (pt[i][0], pt[i][1])\n    \n    legs+=f'''\n    &lt;leg stripId=\"0\"&gt;\n            &lt;platformSettings x=\"{x}\" y=\"{y}\" template=\"platform_als\"/&gt;\n            &lt;scannerSettings template=\"scanner_als\" active=\"{active}\"/&gt;\n    &lt;/leg&gt;'''\n</pre> legs = '' active = True for i in range(4):     if i%2==0:         active = True     else:         active = False          x, y = t1_tf * (pt[i][0], pt[i][1])          legs+=f'''      ''' <p>The main survey content contains templates for the scan settings as well as the platform settings which can then be applied to each leg. We also define the name of the platform to be used in the survey as well as the scanner model and the scene.</p> In\u00a0[22]: Copied! <pre>survey_content = f'''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;document&gt;\n    &lt;platformSettings id=\"platform_als\" movePerSec_m=\"{flight_v}\" z=\"{alt}\" /&gt;\n    &lt;scannerSettings active=\"true\" id=\"scanner_als\" pulseFreq_hz=\"{pulse_freq}\" scanAngle_deg=\"{scan_angle}\" scanFreq_hz=\"{scan_freq}\" trajectoryTimeInterval_s=\"0.01\"/&gt;\n    &lt;survey name=\"als_rock_glacier\" platform=\"{platform_xml}#{platform_id}\" scanner=\"{scanner_xml}#{scanner_id}\" scene=\"\"&gt;\n    &lt;FWFSettings beamSampleQuality=\"5\" winSize_ns=\"1.5\"/&gt;\n    {legs}\n    &lt;/survey&gt;\n&lt;/document&gt;\n\n''' \n</pre> survey_content = f'''      {legs}        '''  <p>To distinguish our surveys from each other we must assign them each their corresponding scene file and a unique survey id. In this case <code>als_rock_glacier_t1</code> and <code>als_rock_glacier_t2</code>.</p> In\u00a0[23]: Copied! <pre>for i in range(2):\n    scene_name = scenefiles[i] + '#' + 'uls_t{}'.format(i+1)\n    survey_id = 'als_rock_glacier_t{}'.format(i+1)\n    \n    print('Writing survey...\\nFilename: {}\\nSurvey ID: {}\\nScene: {}\\n'.format(surveys[i], survey_id, scene_name))\n    \n    with open(surveys[i], 'w') as f:\n        f.write(survey_content.replace('scene=\"\"', 'scene=\"{}\"'.format(scene_name)).replace(\n        'als_rock_glacier', survey_id))\n</pre> for i in range(2):     scene_name = scenefiles[i] + '#' + 'uls_t{}'.format(i+1)     survey_id = 'als_rock_glacier_t{}'.format(i+1)          print('Writing survey...\\nFilename: {}\\nSurvey ID: {}\\nScene: {}\\n'.format(surveys[i], survey_id, scene_name))          with open(surveys[i], 'w') as f:         f.write(survey_content.replace('scene=\"\"', 'scene=\"{}\"'.format(scene_name)).replace(         'als_rock_glacier', survey_id)) <pre>Writing survey...\nFilename: data/surveys/als_t1.xml\nSurvey ID: als_rock_glacier_t1\nScene: data/scenes/scene_t1.xml#uls_t1\n\nWriting survey...\nFilename: data/surveys/als_t2.xml\nSurvey ID: als_rock_glacier_t2\nScene: data/scenes/scene_t2.xml#uls_t2\n\n</pre> <p>2.3 Run HELIOS++ surveys</p> <p>Next, we run our simulations, using the HELIOS++ python bindings, <code>pyhelios</code>. There is a wiki entry designated to pyhelios where you can read up on the basic usage.</p> <p>Since our working directory is already the helios root directory, we can get started right away.</p> <p>2.3.1 Set HELIOS++ environment</p> <p>First we set the desired logging verbosity as well as a randomness seed.</p> In\u00a0[24]: Copied! <pre># Set logging.\n#pyhelios.loggingSilent()\n#pyhelios.loggingQuiet()\npyhelios.loggingDefault()\n#pyhelios.loggingVerbose()\n#pyhelios.loggingVerbose2()\n\n# Set seed for random number generator.\npyhelios.setDefaultRandomnessGeneratorSeed(\"123\")\n</pre> # Set logging. #pyhelios.loggingSilent() #pyhelios.loggingQuiet() pyhelios.loggingDefault() #pyhelios.loggingVerbose() #pyhelios.loggingVerbose2()  # Set seed for random number generator. pyhelios.setDefaultRandomnessGeneratorSeed(\"123\") <p>2.3.2 Build simulations and run surveys</p> <p>Now we build and run the simulation for each of the two survey files we created in the previous steps. The for-loop iterates over both our simulations and builds and executes them consecutively.</p> <p>Note: If you are running jupyter from a console window you can watch the output of the HELIOS++ simulations there.</p> In\u00a0[25]: Copied! <pre>outfiles = []\n\nfor survey in surveys:\n    # use SimulationBuilder to configure simulation\n    simB = SimulationBuilder(str(survey), \"assets/\", \"output/\")\n    simB.setLasOutput(True)\n    simB.setZipOutput(True)\n    simB.setCallbackFrequency(10000)\n    \n    # build the simulation\n    sim = simB.build()\n    \n    # Start the simulation.\n    start_time = time.time()\n    sim.start()\n\n    if sim.isStarted():\n        print('Simulation has started!\\nSurvey Name: {survey_name}\\n{scanner_info}'.format(\n            survey_name = sim.sim.getSurvey().name,\n            scanner_info = sim.sim.getScanner().toString()))\n\n    while sim.isRunning():\n        duration = time.time()-start_time\n        mins = duration // 60\n        secs = duration % 60\n        print(\"\\r\"+\"Simulation has been running for {} min and {} sec. Please wait.\".format(int(mins), int(secs)), end=\"\")\n        time.sleep(1)\n\n    if sim.isFinished():\n        print(\"\\n\"+\"Simulation has finished!\")\n    \n    output = sim.join()\n        \n    outfiles.append(output.filepath)\n</pre> outfiles = []  for survey in surveys:     # use SimulationBuilder to configure simulation     simB = SimulationBuilder(str(survey), \"assets/\", \"output/\")     simB.setLasOutput(True)     simB.setZipOutput(True)     simB.setCallbackFrequency(10000)          # build the simulation     sim = simB.build()          # Start the simulation.     start_time = time.time()     sim.start()      if sim.isStarted():         print('Simulation has started!\\nSurvey Name: {survey_name}\\n{scanner_info}'.format(             survey_name = sim.sim.getSurvey().name,             scanner_info = sim.sim.getScanner().toString()))      while sim.isRunning():         duration = time.time()-start_time         mins = duration // 60         secs = duration % 60         print(\"\\r\"+\"Simulation has been running for {} min and {} sec. Please wait.\".format(int(mins), int(secs)), end=\"\")         time.sleep(1)      if sim.isFinished():         print(\"\\n\"+\"Simulation has finished!\")          output = sim.join()              outfiles.append(output.filepath) <pre>SimulationBuilder is building simulation ...\nSimulationBuilder built simulation in 13.413351799999987 seconds\nSimulation has started!\nSurvey Name: als_rock_glacier_t1\nScanner: leica_als50-ii Power: 4.000000 W Divergence: 0.220000 mrad Wavelength: 1064 nm Visibility: 23.000000 km\nSimulation has been running for 0 min and 30 sec. Please wait.\nSimulation has finished!\nSimulationBuilder is building simulation ...\nSimulationBuilder built simulation in 13.161589699999922 seconds\nSimulation has started!\nSurvey Name: als_rock_glacier_t2\nScanner: leica_als50-ii Power: 4.000000 W Divergence: 0.220000 mrad Wavelength: 1064 nm Visibility: 23.000000 km\nSimulation has been running for 0 min and 30 sec. Please wait.\nSimulation has finished!\n</pre> In\u00a0[26]: Copied! <pre>from pathlib import PurePath\nals_t1 = str(PurePath(outfiles[0])).replace('\\\\', '\\\\\\\\')\nals_t2 = str(PurePath(outfiles[1])).replace('\\\\', '\\\\\\\\')\n\nprint(f'ALS point clouds located at:\\n\\n{als_t1}\\nand\\n{als_t2}')\n</pre> from pathlib import PurePath als_t1 = str(PurePath(outfiles[0])).replace('\\\\', '\\\\\\\\') als_t2 = str(PurePath(outfiles[1])).replace('\\\\', '\\\\\\\\')  print(f'ALS point clouds located at:\\n\\n{als_t1}\\nand\\n{als_t2}') <pre>ALS point clouds located at:\n\noutput\\\\Survey Playback\\\\als_rock_glacier_t1\\\\2022-12-13_17-26-45\\\\points\\\\strip000_point.laz\nand\noutput\\\\Survey Playback\\\\als_rock_glacier_t2\\\\2022-12-13_17-27-30\\\\points\\\\strip000_point.laz\n</pre> <p>3.1 Create raster from ALS 2020:</p> In\u00a0[27]: Copied! <pre>als_t1_dtm = als_t1.replace(\".laz\", \"_dtm_1m.tif\")\n</pre> als_t1_dtm = als_t1.replace(\".laz\", \"_dtm_1m.tif\") <p>Execute the following code in order to fix the global encoding of the HELIOS-generated point cloud.</p> In\u00a0[28]: Copied! <pre>filename = als_t1\nf = open(filename, \"rb+\")\nf.seek(6)\nf.write(bytes([17, 0, 0, 0]));\nf.close()\n</pre> filename = als_t1 f = open(filename, \"rb+\") f.seek(6) f.write(bytes([17, 0, 0, 0])); f.close()  In\u00a0[29]: Copied! <pre>json_dtm = \"\"\"[\n    \"%s\",\n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":1.0,\n        \"window_size\":8,\n         \"origin_x\":\"%.3f\",\n        \"origin_y\":\"%.3f\",\n        \"width\":\"%i\",\n        \"height\":\"%i\"\n    }\n]\"\"\"% (als_t1, als_t1_dtm, origin_left, origin_bottom, dsm_width, dsm_height)\n\n#using the extent defined by the uls data, so the difference calculation is working\n</pre> json_dtm = \"\"\"[     \"%s\",     {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":1.0,         \"window_size\":8,          \"origin_x\":\"%.3f\",         \"origin_y\":\"%.3f\",         \"width\":\"%i\",         \"height\":\"%i\"     } ]\"\"\"% (als_t1, als_t1_dtm, origin_left, origin_bottom, dsm_width, dsm_height)  #using the extent defined by the uls data, so the difference calculation is working  In\u00a0[30]: Copied! <pre>pipeline = pdal.Pipeline(json_dtm)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_dtm) exe = pipeline.execute() <p>3.2 Create raster from ALS 2021:</p> In\u00a0[31]: Copied! <pre>als_t2_dtm = als_t2.replace(\".laz\", \"_dtm_1m.tif\")\n</pre> als_t2_dtm = als_t2.replace(\".laz\", \"_dtm_1m.tif\") In\u00a0[32]: Copied! <pre>filename = als_t2\nf = open(filename, \"rb+\")\nf.seek(6)\nf.write(bytes([17, 0, 0, 0]));\nf.close()\n</pre> filename = als_t2 f = open(filename, \"rb+\") f.seek(6) f.write(bytes([17, 0, 0, 0])); f.close()  In\u00a0[33]: Copied! <pre>json_dtm = \"\"\"[\n    \"%s\",\n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":1.0,\n        \"window_size\":8,\n        \"origin_x\":\"%.3f\",\n        \"origin_y\":\"%.3f\",\n        \"width\":\"%i\",\n        \"height\":\"%i\"\n    }\n]\"\"\"% (als_t2, als_t2_dtm, origin_left, origin_bottom, dsm_width, dsm_height) \n</pre> json_dtm = \"\"\"[     \"%s\",     {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":1.0,         \"window_size\":8,         \"origin_x\":\"%.3f\",         \"origin_y\":\"%.3f\",         \"width\":\"%i\",         \"height\":\"%i\"     } ]\"\"\"% (als_t2, als_t2_dtm, origin_left, origin_bottom, dsm_width, dsm_height)   In\u00a0[34]: Copied! <pre>pipeline = pdal.Pipeline(json_dtm)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_dtm) exe = pipeline.execute() <p>3.3 Calculate difference of rasters:</p> In\u00a0[35]: Copied! <pre>with rio.open(als_t1_dtm) as src:\n    als_t1_data = src.read(1, masked=True)\n    dsm_meta = src.profile\n    \nwith rio.open(als_t2_dtm) as src:\n    als_t2_data = src.read(1, masked=True)\n    dsm_meta = src.profile\n</pre> with rio.open(als_t1_dtm) as src:     als_t1_data = src.read(1, masked=True)     dsm_meta = src.profile      with rio.open(als_t2_dtm) as src:     als_t2_data = src.read(1, masked=True)     dsm_meta = src.profile In\u00a0[36]: Copied! <pre>als_diff = als_t1_data - als_t2_data\n</pre> als_diff = als_t1_data - als_t2_data In\u00a0[37]: Copied! <pre>plt.imshow(als_diff, cmap='pink')\nplt.show()\n</pre> plt.imshow(als_diff, cmap='pink') plt.show() In\u00a0[38]: Copied! <pre>als_diff_file = 'als_diff.tif'\n\nwith rio.open(als_diff_file, 'w', **dsm_meta) as ff:\n    ff.write(als_diff, 1)\n</pre> als_diff_file = 'als_diff.tif'  with rio.open(als_diff_file, 'w', **dsm_meta) as ff:     ff.write(als_diff, 1) <p>First, we open and plot the two DTMs representing the amount of surface change next to eachother for a visual comparison.</p> In\u00a0[39]: Copied! <pre>with rio.open(uls_diff_file) as src:\n    uls_diff = src.read(1, masked=True)\n    dsm_meta = src.profile\n\nwith rio.open(als_diff_file) as src:\n    als_diff = src.read(1, masked=True)\n    dsm_meta = src.profile\n</pre> with rio.open(uls_diff_file) as src:     uls_diff = src.read(1, masked=True)     dsm_meta = src.profile  with rio.open(als_diff_file) as src:     als_diff = src.read(1, masked=True)     dsm_meta = src.profile In\u00a0[40]: Copied! <pre>fig, (ax1, ax2) = plt.subplots(1, 2)\n\nfig.suptitle('Surface Change Comparison')\n\nax1.imshow(uls_diff, cmap='pink')\nax1.set_title('ULS Surface Change')\nax2.imshow(als_diff, cmap='pink')\nax2.set_title('ALS Surface Change')\n</pre> fig, (ax1, ax2) = plt.subplots(1, 2)  fig.suptitle('Surface Change Comparison')  ax1.imshow(uls_diff, cmap='pink') ax1.set_title('ULS Surface Change') ax2.imshow(als_diff, cmap='pink') ax2.set_title('ALS Surface Change') Out[40]: <pre>Text(0.5, 1.0, 'ALS Surface Change')</pre> <p>The visual comparison did not reveal any major differences between the surface change detection. To gain a more detailed insight, we can calculate the difference of between the ULS difference and the ALS difference rasters. Thereby we can find out exactly by how much the ALS change detection differs from the change detection with the ULS data.</p> In\u00a0[41]: Copied! <pre>change_det_diff = uls_diff - als_diff\n</pre> change_det_diff = uls_diff - als_diff In\u00a0[42]: Copied! <pre>plt.imshow(change_det_diff, cmap='pink')\nplt.show()\n</pre> plt.imshow(change_det_diff, cmap='pink') plt.show() <p>Next, we can calculate some statistics for the differences between the ULS and ALS change detection.</p> In\u00a0[49]: Copied! <pre>print('Maximum difference in detected surface change: {}m'.format(np.nanmax(np.abs(change_det_diff))))\nprint('Median absolute difference in detected surface change: {}m'.format(np.nanmedian(np.abs(change_det_diff))))\nprint('Arithmetic mean of absolute difference in detected surface change: {}m'.format(np.nanmean(np.abs(change_det_diff))))\n#print('Q1 quantile of difference in detected surface change: {}m'.format(np.nanquantile(change_det_diff, 0.25)))\nprint('Q2 quantile of difference in detected surface change: {}m'.format(np.nanquantile(change_det_diff, 0.75)))\n</pre> print('Maximum difference in detected surface change: {}m'.format(np.nanmax(np.abs(change_det_diff)))) print('Median absolute difference in detected surface change: {}m'.format(np.nanmedian(np.abs(change_det_diff)))) print('Arithmetic mean of absolute difference in detected surface change: {}m'.format(np.nanmean(np.abs(change_det_diff)))) #print('Q1 quantile of difference in detected surface change: {}m'.format(np.nanquantile(change_det_diff, 0.25))) print('Q2 quantile of difference in detected surface change: {}m'.format(np.nanquantile(change_det_diff, 0.75))) <pre>Maximum difference in detected surface change: 112.87696249999954m\nMedian absolute difference in detected surface change: 0.3359375m\nArithmetic mean of absolute difference in detected surface change: 0.24289509890524133m\nQ2 quantile of difference in detected surface change: 0.027932175735031706m\n</pre> <p>As we can see, small overall differences in the detected magnitude do exist between the ALS and ULS surface change detection.</p> <p>We found that the lower density point clouds from ALS deliver a similar amount of information on the areas of surface change but yielded inaccuracies regarding the magnitude of the surface changes. Thus we  can conclude that, to apply this approach in future, point clouds with lower point densities as delivered by ALS acquisitions of the rock glacier would be sufficient to detect the areas where surface change occurred, whereas the ULS data with a higher spatial resoution would be required to accurately measure the magnitude of the change. Since ALS acquisitions can cover large areas relative to ULS, this could prove to be a more cost-efficient acquisition method when monitoring areas of surface change, especially since ALS datasets are often available online via national inventory databases.</p> <p>Bonus questions:</p> <ol> <li><p>Can you think of other methods of surface change detection?</p> </li> <li><p>Do you think ALS point clouds would be sufficient for these methods?</p> </li> </ol>"},{"location":"module3/02_pointcloud_programming_python/exercise/m3_theme2_exercise1.html#exercise-virtual-laser-scanning-in-helios-for-point-cloud-change-analysis","title":"Exercise: Virtual Laser Scanning in HELIOS++ for Point Cloud Change Analysis\u00b6","text":"<p>In this excercise we will be conducting an experiment of surface change analysis using UAV-borne laser scanning (ULS) point clouds and virtually generated airborne laser scanning (ALS) point coulds. Our area of interest (AOI) is a rock glacier in Austria of which UAV point clouds have been acquired repeatedly by the 3DGeo Research Group in Heidelberg over the past years. By analysing and comparing the point clouds from different points in time (epochs), it is possible to analyse the movement of the rock glacier.</p> <p>The aim of this exercise is to analyse the difference in surface change detection when acquiring the rock glacier using ULS vs ALS. ALS point clouds generally have lower point densities and therefore lower spatial resolutions as well as lower overall ranging accuracy and may therefore lead to unwanted noise in the change detection. However, ALS can prove to be a more cost-efficient method due to the large coverage areas. For many regions, ALS datasets are available online via national inventory databases. Therefore the option of using ALS data may be considered if the acquired data meets the requirements for any given analysis. Virtual laser scanning (VLS) is a valuable tool that can help us determine whether point clouds from certain LiDAR acquisitions meet the analysis requirements. In this case, we will be analysing whether ALS point clouds of the rock glacier in Austria would deliver valuable results when applying a rudimentary method of surface change analysis.</p> <p>To do so, we will first perform surface change detection between our two epochs of ULS data. Then, we will generate ALS data for each epoch, using the Heidelberg LiDAR Operations Simulator (HELIOS++) and the ULS data as the 3D models which are recorded by the virtual airborne laser scanner. After performing surface change analysis on the ALS data for each epoch, we can compare the results of the ULS and the ALS surface change analysis and determine if there is a noteworthy difference in the results.</p> <p>Note to start: HELIOS++ must be installed on your device for this exercise. You can download HELIOS++ from the HELIOS++ GitHub repo. Additionally, to enable hidden solution cells, you must install the jupyter notebook extension Exercise. Instructions to install jupyter extensions can be found here.</p> <p>The data for the exercise is located in the directory <code>ahk</code> of the course data repository.</p>"},{"location":"module3/02_pointcloud_programming_python/exercise/m3_theme2_exercise1.html#1-uls-data-surface-change-analysis","title":"1. ULS data surface change analysis\u00b6","text":"<p>The ULS data can be seen as the 'ground truth' data for the rock glacier. By conducting a change detection analysis on the ULS data, we get a good estimate of the surface change that occurred between 2020 and 2021.</p> <p>Method: For a rudementary form of change detection we will convert both point clouds to height rasters and calculate the difference between the rasters.</p> <p>Please indicate the path of the ULS datasets at points T1 and T2 below.</p>"},{"location":"module3/02_pointcloud_programming_python/exercise/m3_theme2_exercise1.html#2-generate-als-data","title":"2. Generate ALS data\u00b6","text":"<p>Using HELIOS++, we will now run two virtual laser scanning acquisitions over our 3d-model of the rock glacier at both points in time (2020 &amp; 2021). We will be conducting airborne laser scanning (ALS) acquisitions. Comparing the two virtual ALS datasets will give us an estimate for the change we would have detected in the rock glacier if we had flown two ALS acquisitions. Comparing the detected change by ALS with the change by ULS will indicate the precision/accuracy of the ALS change detection.</p> <p>To make yourself familiar with HELIOS++ and how it works, please consult the wiki.</p>"},{"location":"module3/02_pointcloud_programming_python/exercise/m3_theme2_exercise1.html#3-als-data-surface-change-analysis","title":"3. ALS data surface change analysis\u00b6","text":"<p>The HELIOS++ acquisitions have left us with two point clouds, representing ALS acquisitions of the rock glacier in 2020 and 2021 respectively. The filenames of the ouptut were stored in the <code>outfiles</code> python list after completion of each simulation.</p> <p>As we did for the ULS data, we will now convert both point clouds to height rasters, and calculate the difference of the two rasters as a rudimentary form of surface change analysis.</p>"},{"location":"module3/02_pointcloud_programming_python/exercise/m3_theme2_exercise1.html#4-compare-results-of-surface-change-detection-uls-vs-als","title":"4. Compare results of surface change detection: ULS vs ALS\u00b6","text":"<p>We have now calculated the detected change in the rock glacier from ULS data and estimated the detected change if we had flown an ALS acquisition. By comparing the amount of change detected between ULS and ALS, we can determine whether an ALS acquisition would have been sufficient to detect surface change using our rudimentary method.</p>"},{"location":"module3/02_pointcloud_programming_python/exercise/m3_theme2_exercise1.html#5-results","title":"5. Results\u00b6","text":"<p>Is ALS data sufficient for conducting our surface change analysis?</p> <p>Repeat: what is the advantage of conducting ALS vs ULS acquisitions?</p> <p>Have another look at the plots and the statistics and try to answer the questions. For our answer, click on the plus symbol to the left of the cell.</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html","title":"Lesson","text":"Metadata      title: \"E-TRAINEE Principles and basic algorithms of 3D change detection and analysis\"     description: \"This is the third theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2023-02-28     authors: Katharina Anders, Bernhard H\u00f6fle     contributors: Sina Zumstein     estimatedTime: 1.5 hours  <p>This theme is about principles of change detection and analysis in 3D point clouds and introduces algorithms for deriving change information from multitemporal point clouds and 3D time series.</p> <p>You will learn about:</p> <ul> <li>Terminology of 3D change analysis</li> <li>Approaches to 3D change analysis</li> <li>A change analysis workflow in methods<ul> <li>Point cloud alignment</li> <li>Change detection and quantification (surface change analysis)</li> <li>Uncertainty of change analysis</li> <li>Representation of change analysis results</li> </ul> </li> </ul> <p></p> <p>Map of building changes derived from airborne laser scanning (ALS) in the Netherlands. Figure from http://skynet.elte.hu/tudelft/vgc2016.html by Cser\u00e9p &amp; Lindenbergh (2023) / CC BY 4.0.</p> <p>Accordingly, change analysis comprises the process of quantifying the magnitude of change and other properties, e.g. deriving the change rate. It can further comprise the attribution of semantics, i.e. the type of change. For building change, this may be categories of \"newly built\", \"demolished\", \"extended\", etc. In the example of building change above, the changes are derived in vertical direction, i.e. as local changes in height. One-dimensional changes in topographic applications are typically oriented in Z direction to reflect a surface increase or lowering. Changes may be derived in 2D, for example, as change in location, i.e. horizontal displacement. 3D changes regard the occupancy of 3D space (and therein distances between two states) or changes in sizes and/or shapes. Changes may be higher-dimensional (i.e., &gt;3D), if they comprise all of the above aspects and additionally changes in feature space (Eitel et al., 2016).</p> <p>Change analysis is straightforward, if we are dealing with simple objects, a high signal-to-noise ratio, a low number of epochs in multitemporal data, and if processes are known or certain types of changes expected (i.e., specific magnitudes or locations). In remote sensing and geographic applications, many of these aspects usually do not apply. We are rather dealing with changes at small scales compared to associated uncertainties (cf. section on uncertainty of change analyis), objects or features in a scene may be difficult to recognize (even for human interpreters), and objects are irregular, i.e. not following simple geometric primitives that are easily identifiable by algorithms. Further, we are typically dealing with large and detailed scenes and many epochs to assess for changes (even time series). Furthermore, data of multiple epochs may originate from different sources with different properties of, e.g., coverage, spatial resolution, and uncertainty.</p> <p>This theme will introduce you to these different aspects and how to approach change analysis of natural, geographic phenomena using 3D point clouds.</p> <p></p> <p></p> <p>Object-based change analysis of trees using terrestrial and UAV LiDAR point clouds. Individual tree objects are identified in the point cloud to derive change in tree height and diameter at breast height. Figure by Chen et al. (2022) / CC BY 4.0.</p> <p>Object-based change analysis typically follows this workflow:</p> <ol> <li>Object extraction: Objects or their components (segments) are extracted from single point cloud epochs.</li> <li>Object identification: Objects are re-identified between epochs, e.g., the same tree, typically based on location if there is no strong movement involved.</li> <li>Object characterization: The state of individual objects is derived, e.g. their shape, size, etc.</li> <li>Object monitoring: Steps 1-3 are repeated for each epoch to assess their change over time.</li> </ol> <p>The result of the analysis may be binary detection if the object is still there or gone, or the change in derived properties, such as shape, size, or even location for moving objects.</p> <p>Accordingly, objects may also be components of geomorphic phenomena, such as a landslide scarp, whose size, shape and location are monitored over time (e.g., Mayr et al. 2017):</p> <p>Object-based change analysis of landslide components in terrestrial laser scanning point clouds. Figure modified from Mayr et al. (2018) / CC BY 4.0.</p> <p></p> <p></p> <p>To maintain the advantages of point cloud analysis and the full 3D representation of surfaces, approaches of direct point cloud comparison are available. A simple approach to point cloud-based surface change analysis is so-called direct cloud-to-cloud comparison (C2C; Girardeau-Montaut et al., 2005). C2C distances are derived as Hausdorff distances, so the distance to the closest neighboring 3D point in the compared epoch for each point of a reference epoch:</p> <p>Schematic of changes derived as cloud-to-cloud (C2C) distances between two epochs of point clouds compared to the true distance from a point at epoch t2 to the true surface in epoch t1. Figure by course authors.</p> <p>Caveats for the C2C method are that it is sensitive to roughness of surfaces in the point cloud, sensitive to outliers, and to an uneven point distribution. Accordingly, the nearest point in the compared point cloud does not necessarily represent the same surface or similar surface position (see true distance vs. C2C distance in the schematic figure above).</p> <p>To overcome this and better ensure that similar surfaces are compared, an advanced method of point cloud-based surface change analysis is provided by the multiscale model-to-model cloud comparison (M3C2) (Lague et al., 2013). The M3C2 algorithm calculates the distance between point clouds of two epochs in the direction of the local surface normal at specific 3D locations called \u2019core points\u2019. Accordingly, a plane is fit to the points in a neighborhood of defined radius around a core point. The surface position is averaged for points within a cylinder with a diameter defined by the \u2019projection scale\u2019 which is oriented along the normal direction. The distance is then computed along this normal direction to the averaged surface position of points in the compared point cloud, i.e. where the projection cylinder intersects the point cloud of the second epoch. Please read the article by Lague et al. (2013) to understand the algorithm in detail.</p> <p>Schematic of point cloud distance computation with the M3C2 algorithm (Lague et al., 2013). Figure by V. Zahs modified from Zahs et al. (2022).</p> <p>With this concept, the M3C2 is less sensitive to effects of surface roughness: Local planarity is assumed, so that a plane can be adjusted to the point neighborhood, rather than using each point position directly as in the C2C distances. With the surface normal orientation, a further benefit is that signed distances are provided. We thereby derive not only a change value, but can also infer surface increase or decrease (e.g. accumulation and erosion) from the resulting positive or negative change values. Deriving the direction of movement normal (i.e. orthogonally) to the local surface is useful for many applications (but not all, see e.g. Williams et al., 2021).</p> <p>There is another special capability of the M3C2 algorithm we should look at, namely the multi-scale aspect. We have learnt that the M3C2 assumes local planarity, to derive surface change in the direction of local surface orientation. As you know about point cloud characteristics, planarity depends on the surface roughness and is thereby scale-dependent (you may revisit this aspect in the first theme of this module). Accordingly, the ideal radius for normal estimation, at which the surface is most planar, depends on the roughness of the local surface, which can be highly variable within a scene. To account for this, the M3C2 takes a set or a range of radii as input, and calculates the normal vector for each of these radii. Finally, the normal radius at each core point is selected as such that the adjusted plane has the lowest residuals (i.e., low roughness or high planarity). You make use of this multi-scale aspect in the exercise of this theme.</p> <p>The M3C2 is therefore a widely established algorithm of 3D change analysis in geoscientific applications using conventional multitemporal point cloud data (usually much less than 100 epochs) of complex scenes, such as rock glaciers (Zahs et al., 2019), glaciers (Rossini et al., 2018), and landslides (Mayr et al., 2018) in high-mountain landscapes, thaw subsidence (Marx et al., 2017) and slumping (Barnhart and Crosby, 2013) in Arctic permafrost landscapes, or cliff erosion (G\u00f3mez-Guti\u00e9rrez and Gon\u00e7alves, 2020) and beach dynamics (Corb\u00ed et al., 2018) in coastal environments.</p> <p></p> <p>The measurement uncertainty of single epochs directly propagates into change analysis. Imagine a simple example: If a specific point in your scene is measured as a 3D coordinate with an (in practice unknown) error of, say, 1 mm and the same unchanged position is measured again at the same error of 1 mm, the distance between the two coordinates is derived with 2 mm even though no change occurred. In another scenario, the actual point position may have moved by 2 mm. If the coordinate measurements are off by 1 mm 'towards each other', their distance will be quantified with 0 mm, i.e. no change could be identified. This is a highly fictious example, but should emphasize the importance to consider sources of error and using means to assess uncertainty in a quantitative way. Before we have a look at these methods, some further important uncertainty sources shall be introduced in the following.</p> <p>The scanning mechanism as well as the scanning geometry further influence the point sampling on the target surface. Importantly, this sampling will always be at least slightly different when repeating the acquisition, even from the very same instrument position, e.g. in permanent laser scanning. A distance from a point in one epoch to the nearest point in the other epoch may therefore partly or solely stem from this sampling effect, while the points are sampled from the same, unchanged surface.</p> <p>Differing sampling of points on a surface in two epochs due to effects of the scanning mechanism and influence of the scanning geometry (roughness). Figure by course authors, following Williams (2017).</p> <p>Another source of uncertainty is the survey geometry. We know about the influence of ranging accuracy, incidence angles, and occlusion effects in single point cloud acquisitions. When repeating a measurement, we also need to consider that the surface change itself influences how we sample the surface and what parts of a scene (cf. Girardeau-Montau et al., 2005):</p> <p>Survey geometry as a source of uncertainty if occluded areas change due to changes (surface displacement) in the target scene. Figure by Girardeau-Montaut et al. (2005) / CC BY 3.0.</p> <p>This can influence your change analysis, because the occluded areas change, so that you may not find corresponding surfaces in all parts of the data. If you are not aware of this effect in a scene, it can even lead to errors in change assessment. This is the case, for example, if a surface in one epoch is compared to another surface in the other epoch, which is not corresponding in reality.</p> <p>Another influence of scene dynamics can be seasonal effects, such as leaf-on and leaf-off conditions for vegetation, the dryness of a river bed, or snow cover, all depending on the target scene and the variable of interest (Qin et al., 2016).</p> <p>Of course, any data acquisition setup will be designed to minimize errors and influences as much as possible. In practice, the acquisition design is always constrained by the circumstances, which instrument is available, what are the scene characteristics, etc.</p> <p>For all remaining uncertainty, there are methods to assess the uncertainty in a quantifiable manner. Based on this, we can test the significance of derived changes regarding the quality and properties of our data.</p> <p></p> <p>With this, you know the theoretical concepts of change analysis and uncertainty assessment in 3D/4D change analysis of point clouds. In the following, we will continue by working through a change analysis workflow, detailing the required steps in a practical example.</p> <p></p> <p>Visualize the data colored by elevation (because we did not load any attribute information, just XYZ coordinates):</p> In\u00a0[\u00a0]: Copied! <pre># add the script assets folder to the path, so that we can import the functions created there\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))\n\n# import point cloud functions\nimport pointcloud_functions as pcfuncs\n\n# define path to point cloud files\ndata_path = 'path-to-data'\n\n# read data\nlas_data2017 = f'{data_path}/hellstugubrean_2017.las'\nlas_data2009 = f'{data_path}/hellstugubrean_2009_prealignment.las'\ndata_2009 = pcfuncs.read_las(f'{data_path}/hellstugubrean_2009_prealignment.las')\ndata_2017 = pcfuncs.read_las(las_data2017)\n</pre> # add the script assets folder to the path, so that we can import the functions created there import sys from pathlib import Path sys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))  # import point cloud functions import pointcloud_functions as pcfuncs  # define path to point cloud files data_path = 'path-to-data'  # read data las_data2017 = f'{data_path}/hellstugubrean_2017.las' las_data2009 = f'{data_path}/hellstugubrean_2009_prealignment.las' data_2009 = pcfuncs.read_las(f'{data_path}/hellstugubrean_2009_prealignment.las') data_2017 = pcfuncs.read_las(las_data2017) In\u00a0[2]: Copied! <pre># importing required modules\nimport matplotlib.pyplot as plt\n\n# allow interactive rotation in notebook\n#%matplotlib notebook\n\n# create the figure object\nfig, ax = plt.subplots()\nax.axis('equal') # make axis ratio equal to avoid distortion between x and y\n\n# scatter the x and y coordinates and color by intensity\nnth = 10 # use only every n-th point to improve visibility\nsp = ax.scatter(data_2009[:,0][::nth], data_2009[:,1][::nth], c=data_2009[:,2][::nth], cmap='terrain', s=1)\n\n# set axis to be equally scaled\nax.set_aspect('equal')  \n\n# add title and axis labels\nax.set_xlabel('X coordinate [m]')\nax.set_ylabel('Y coordinate [m]')\n\n# add a colorbar legend\ncb = plt.colorbar(sp)\ncb.set_label('Elevation [m]')\n\n# cleanly fit plots within figure\nplt.tight_layout()\n\n# show the plot in interactive mode\nplt.show()\n</pre> # importing required modules import matplotlib.pyplot as plt  # allow interactive rotation in notebook #%matplotlib notebook  # create the figure object fig, ax = plt.subplots() ax.axis('equal') # make axis ratio equal to avoid distortion between x and y  # scatter the x and y coordinates and color by intensity nth = 10 # use only every n-th point to improve visibility sp = ax.scatter(data_2009[:,0][::nth], data_2009[:,1][::nth], c=data_2009[:,2][::nth], cmap='terrain', s=1)  # set axis to be equally scaled ax.set_aspect('equal')    # add title and axis labels ax.set_xlabel('X coordinate [m]') ax.set_ylabel('Y coordinate [m]')  # add a colorbar legend cb = plt.colorbar(sp) cb.set_label('Elevation [m]')  # cleanly fit plots within figure plt.tight_layout()  # show the plot in interactive mode plt.show() <p></p> <p>For our exercise dataset, we will make a simple separation into changed and unchanged areas using the polygon of the glacier outline provided with <code>hellstugubrean_2009.shp</code>. Note that this is not necessarily ideal, as also other parts surrounding the glacier may have changed during the analysis timespan. For a thorough analysis, it would be better to narrow down the areas, e.g. of rock outcrop, where we are more confident that they are stable than, e.g., debris- or snow-covered surfaces. To revisit spatial subsetting of point clouds, you may revisit the respective part in Theme 2.</p> In\u00a0[3]: Copied! <pre>import numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\n# define path to shapefile\nshp_file = f'{data_path}/hellstugubrean_2009.shp'\n\ndatasource = gpd.read_file(shp_file)\n\nif datasource is None:\n    print(f'Could not open shapefile {shp_file}.')\nelse:\n    polygons = gpd.GeoSeries(datasource['geometry'])\n\n# create boolean array to mask points (initialize all entries as False)\nmask_polygon_2009 = np.full(len(data_2009), fill_value=False, dtype=bool)\n\n# loop over xy coordinates of all points\nfor pt_id, (x,y) in enumerate(data_2009[:,:2]):\n    point=Point([x,y])\n    \n    # check distance of this point to all polygons\n    for poly in polygons:\n        distance= poly.distance(point)\n         \n        # set mask to True, if point is inside polygon\n        if distance &lt;= 0.0:\n            mask_polygon_2009[pt_id] = True \n\n# mask the point cloud array where NOT true \ndata_2009_noglacier = data_2009[~mask_polygon_2009]\n\n# do the same masking for the other epoch\nmask_polygon_2017 = np.full(len(data_2017), fill_value=False, dtype=bool)\nfor pt_id, (x,y) in enumerate(data_2017[:,:2]):\n    point=Point([x,y])\n    for poly in polygons:\n        distance= poly.distance(point)\n        if distance &lt;= 0.0:\n            mask_polygon_2017[pt_id] = True\ndata_2017_noglacier = data_2017[~mask_polygon_2017]\n</pre> import numpy as np import geopandas as gpd from shapely.geometry import Point, Polygon  # define path to shapefile shp_file = f'{data_path}/hellstugubrean_2009.shp'  datasource = gpd.read_file(shp_file)  if datasource is None:     print(f'Could not open shapefile {shp_file}.') else:     polygons = gpd.GeoSeries(datasource['geometry'])  # create boolean array to mask points (initialize all entries as False) mask_polygon_2009 = np.full(len(data_2009), fill_value=False, dtype=bool)  # loop over xy coordinates of all points for pt_id, (x,y) in enumerate(data_2009[:,:2]):     point=Point([x,y])          # check distance of this point to all polygons     for poly in polygons:         distance= poly.distance(point)                   # set mask to True, if point is inside polygon         if distance &lt;= 0.0:             mask_polygon_2009[pt_id] = True   # mask the point cloud array where NOT true  data_2009_noglacier = data_2009[~mask_polygon_2009]  # do the same masking for the other epoch mask_polygon_2017 = np.full(len(data_2017), fill_value=False, dtype=bool) for pt_id, (x,y) in enumerate(data_2017[:,:2]):     point=Point([x,y])     for poly in polygons:         distance= poly.distance(point)         if distance &lt;= 0.0:             mask_polygon_2017[pt_id] = True data_2017_noglacier = data_2017[~mask_polygon_2017] <p>Visualize the data after masking out the glacier area:</p> In\u00a0[4]: Copied! <pre># create the figure object\nfig, ax = plt.subplots()\nax.axis('equal') # make axis ratio equal to avoid distortion between x and y\n\n# scatter the x and y coordinates and color by intensity\nnth = 10 # use only every n-th point to improve visibility\nsp = ax.scatter(data_2009_noglacier[:,0][::nth], data_2009_noglacier[:,1][::nth], c=data_2009_noglacier[:,2][::nth], cmap='terrain', s=1)\n\n# set axis to be equally scaled\nax.set_aspect('equal')  \n\n# add title and axis labels\nax.set_xlabel('X coordinate [m]')\nax.set_ylabel('Y coordinate [m]')\n\n# add a colorbar legend\ncb = plt.colorbar(sp)\ncb.set_label('Elevation [m]')\n\n# cleanly fit plots within figure\nplt.tight_layout()\n\n# show the plot in interactive mode\nplt.show()\n</pre> # create the figure object fig, ax = plt.subplots() ax.axis('equal') # make axis ratio equal to avoid distortion between x and y  # scatter the x and y coordinates and color by intensity nth = 10 # use only every n-th point to improve visibility sp = ax.scatter(data_2009_noglacier[:,0][::nth], data_2009_noglacier[:,1][::nth], c=data_2009_noglacier[:,2][::nth], cmap='terrain', s=1)  # set axis to be equally scaled ax.set_aspect('equal')    # add title and axis labels ax.set_xlabel('X coordinate [m]') ax.set_ylabel('Y coordinate [m]')  # add a colorbar legend cb = plt.colorbar(sp) cb.set_label('Elevation [m]')  # cleanly fit plots within figure plt.tight_layout()  # show the plot in interactive mode plt.show() <p>For this area outside the glacier, where we expect little to no changes, we can use a nearest neighbor search between the two point cloud epochs as a simple approach to check the distances:</p> In\u00a0[5]: Copied! <pre># import required module\nfrom scipy.spatial import KDTree\n\n# build kd-tree from 3D coordinates\ntree2017 = KDTree(data_2017_noglacier)\n\n# query indices of nearest neighbors of 2009 coordinates in 2017 kd-tree\nnn_dists = tree2017.query(data_2009_noglacier, k=1)\n\n# obtain distances as first element in tuple returned by query above\ndistances = nn_dists[0]\n\n# create a figure with 3D axis (two columns for different coloring)\nfig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))\n\nnth=100\n\n# plot the point cloud colored by height (z values)\ns = ax.scatter(data_2009_noglacier[::nth,0], data_2009_noglacier[::nth,1], data_2009_noglacier[::nth,2], s=5,\n                   c=distances[::nth], vmax=15.0)\n\n# set axis to be equally scaled\nax.set_aspect('equal')          \n\n# label axes and add title\nax.set_xlabel('X [m]')\nax.set_ylabel('Y [m]')\nax.set_zlabel('Z [m]')\n\n# set initial view of 3D plot\nax.view_init(elev=40., azim=130.)\n\n# add a colorbar\nfig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.1, extend='max')\n\n# show the plot\nplt.tight_layout()\nplt.show()\n\nprint(f'Median distances: {np.median(distances):.2f} m')\nprint(f'Stdev. of distances: {np.std(distances):.2f} m')\n</pre> # import required module from scipy.spatial import KDTree  # build kd-tree from 3D coordinates tree2017 = KDTree(data_2017_noglacier)  # query indices of nearest neighbors of 2009 coordinates in 2017 kd-tree nn_dists = tree2017.query(data_2009_noglacier, k=1)  # obtain distances as first element in tuple returned by query above distances = nn_dists[0]  # create a figure with 3D axis (two columns for different coloring) fig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))  nth=100  # plot the point cloud colored by height (z values) s = ax.scatter(data_2009_noglacier[::nth,0], data_2009_noglacier[::nth,1], data_2009_noglacier[::nth,2], s=5,                    c=distances[::nth], vmax=15.0)  # set axis to be equally scaled ax.set_aspect('equal')            # label axes and add title ax.set_xlabel('X [m]') ax.set_ylabel('Y [m]') ax.set_zlabel('Z [m]')  # set initial view of 3D plot ax.view_init(elev=40., azim=130.)  # add a colorbar fig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.1, extend='max')  # show the plot plt.tight_layout() plt.show()  print(f'Median distances: {np.median(distances):.2f} m') print(f'Stdev. of distances: {np.std(distances):.2f} m') <pre>Median distances: 9.44 m\nStdev. of distances: 8.71 m\n</pre> <p>From the statistics, we can see that most distances are around 9.5 m (median) with a standard deviation of 8.7 m. The corresponding plot shows that some parts of the scene have much lower distances (&lt; 8.0 m). Probably, we are dealing with some areas which exhibit actual change of the surface - otherwise we would see a more systematic spatial distribution of distances.</p> <p>For a real analysis, it would be advisable to further restrict the stable areas during transformation, e.g. by selecting rock outcrops and excluding debris- or snow-covered surfaces. In this lesson, for simplicity, we will input the entire 'no glacier' point clouds into the ICP point cloud alignment.</p> <p>We use the ICP method in PDAL to derive the transformation matrix of our point cloud. For an introduction on the use of PDAL in Python, see the lesson contents of Theme 2. As input, the ICP filter in PDAL requires two <code>las</code> point cloud files: one epoch that is fixed (reference), and one epoch that is moving (to be aligned).</p> In\u00a0[6]: Copied! <pre># store the no glacier point clouds as las files\nlas_data2009_noglacier = f'{data_path}/hellstugubrean_2009_prealignment_noglacier.las'\npcfuncs.write_las(data_2009_noglacier, las_data2009_noglacier)\n\nlas_data2017_noglacier = f'{data_path}/hellstugubrean_2017_noglacier.las'\npcfuncs.write_las(data_2017_noglacier, las_data2017_noglacier)\n</pre> # store the no glacier point clouds as las files las_data2009_noglacier = f'{data_path}/hellstugubrean_2009_prealignment_noglacier.las' pcfuncs.write_las(data_2009_noglacier, las_data2009_noglacier)  las_data2017_noglacier = f'{data_path}/hellstugubrean_2017_noglacier.las' pcfuncs.write_las(data_2017_noglacier, las_data2017_noglacier) <p>We need to specify which epoch to use as reference, and which to align (i.e. determine the transformation for). Generally, the choice of reference point cloud can be based on some metric of data quality. One important factor is to fix the epoch with the best possible georeferencing accuracy. A typical scenario could be that high-accuracy GNSS measurements were only taken during one survey, and all other epochs are co-registered to this global reference.</p> <p>We will use the 2017 epoch as reference in the alignment, as this point cloud has a higher point density. Accordingly, we construct and execute the pipeline in PDAL:</p> In\u00a0[7]: Copied! <pre>import pdal\nimport json\n\n# construct the pdal pipeline\npipeline = \"\"\"[\n    \"%s\",\n    \"%s\",\n    {\n        \"type\":\"filters.icp\"\n    }\n]\"\"\" % (las_data2017_noglacier.replace(\"\\\\\", \"/\"),las_data2009_noglacier.replace(\"\\\\\", \"/\"))\n</pre> import pdal import json  # construct the pdal pipeline pipeline = \"\"\"[     \"%s\",     \"%s\",     {         \"type\":\"filters.icp\"     } ]\"\"\" % (las_data2017_noglacier.replace(\"\\\\\", \"/\"),las_data2009_noglacier.replace(\"\\\\\", \"/\")) <p>Now we run the ICP method - which takes some time (around 10-15 min for the exercise data), so if you are running the method yourself, you may use this opportunity for a short coffee break ;-)</p> In\u00a0[8]: Copied! <pre># execute the pipeline\np = pdal.Pipeline(pipeline)\np.execute()\n</pre> # execute the pipeline p = pdal.Pipeline(pipeline) p.execute() Out[8]: <pre>1299322</pre> <p>The ICP has provided a 4x4 transformation matrix, which we can access in the metadata returned by PDAL. The transofrmation matrix contains the 3x3 rotation, 3x1 translation (last column), and scaling (last row) which is 1, because we do not apply any scaling in the rigid transformation. We also need to consider the centroid of the transformation, which is the rotation center of the alignment, to be able to apply the transformation correctly to the full dataset.</p> In\u00a0[9]: Copied! <pre># capture the metadata, which contains the ICP transformation (matrix and centroid)\nmeta = p.metadata\ntrafo_string = meta['metadata']['filters.icp']['transform']\ntrafo = np.fromstring(trafo_string,sep=' ').reshape(4,4)\ncentroid_string = meta['metadata']['filters.icp']['centroid']\ncentroid = np.fromstring(centroid_string,sep=' ')\n\n# derived transformation matrix:\nprint('Transformation matrix:')\nprint(trafo)\nprint('Centroid for applying the transformation:')\nprint(centroid)\n</pre> # capture the metadata, which contains the ICP transformation (matrix and centroid) meta = p.metadata trafo_string = meta['metadata']['filters.icp']['transform'] trafo = np.fromstring(trafo_string,sep=' ').reshape(4,4) centroid_string = meta['metadata']['filters.icp']['centroid'] centroid = np.fromstring(centroid_string,sep=' ')  # derived transformation matrix: print('Transformation matrix:') print(trafo) print('Centroid for applying the transformation:') print(centroid)  <pre>Transformation matrix:\n[[ 0.99763569  0.06787693 -0.01075909  9.69457057]\n [-0.06752148  0.9972506   0.03053023 -1.38164554]\n [ 0.01280181 -0.02973158  0.99947594 -9.12546695]\n [ 0.          0.          0.          1.        ]]\nCentroid for applying the transformation:\n[4.70711945e+05 6.82696584e+06 1.66776003e+03]\n</pre> <p>We now apply this transformation to the full point cloud dataset of 2009, to rigidly align the entire scene - including the changed glacier - to the dataset of 2017. For this, we define a function which takes as input a set of points (XYZ coordinates) and returns the transformed coordinates. The reduction point will be the centroid returned by the ICP, so that the coordinates can be shifted to this location before applying the transformation.</p> In\u00a0[10]: Copied! <pre># create function to transform points using a 3x4 transformation matrix with 3x3 rotation and 3x1 translation\ndef transform_points(points, trafomat, reduction_point = [.0,.0,.0]):\n\t'''\n\tApplies a rigid transformation, i.e. rotation and translation, to 3D point data.\n\t:param points: 2D array of 3D points with shape (N,3)\n\t:param trafomat: 2D array of rigid transformation matrix with 3x3 rotation and 1x3 translation parameters\n\t:return: transformed points in 2D array of shape (N,3)\n\t'''\n\t\n\trotation = np.array(trafomat)[:, :3]\n\ttranslation = np.array(trafomat)[:, 3].flatten()\n\n\tpoints -= centroid\n\tpts_rot = points.dot(rotation.T)\n\tpts_trafo = pts_rot + translation\n\tpoints_out = pts_trafo[:,:3] + centroid\n\t\n\treturn points_out\n\n# apply the function to point coordinates of 2009 epoch\ndata_2009_aligned = transform_points(data_2009.copy(), trafo, reduction_point=centroid)\n</pre> # create function to transform points using a 3x4 transformation matrix with 3x3 rotation and 3x1 translation def transform_points(points, trafomat, reduction_point = [.0,.0,.0]): \t''' \tApplies a rigid transformation, i.e. rotation and translation, to 3D point data. \t:param points: 2D array of 3D points with shape (N,3) \t:param trafomat: 2D array of rigid transformation matrix with 3x3 rotation and 1x3 translation parameters \t:return: transformed points in 2D array of shape (N,3) \t''' \t \trotation = np.array(trafomat)[:, :3] \ttranslation = np.array(trafomat)[:, 3].flatten()  \tpoints -= centroid \tpts_rot = points.dot(rotation.T) \tpts_trafo = pts_rot + translation \tpoints_out = pts_trafo[:,:3] + centroid \t \treturn points_out  # apply the function to point coordinates of 2009 epoch data_2009_aligned = transform_points(data_2009.copy(), trafo, reduction_point=centroid) <p>To assess how the alignment improved, we can check the distances of the 2017 epoch (no glacier points) to the aligned 2009 epoch:</p> In\u00a0[11]: Copied! <pre>data_2009_aligned_noglacier = data_2009_aligned[~mask_polygon_2009]\n\n# query indices of nearest neighbors of aligned 2009 coordinates in 2017 kd-tree\nnn_dists = tree2017.query(data_2009_aligned_noglacier, k=1)\n\n# obtain distances as first element in tuple returned by query above\ndistances = nn_dists[0]\n\n# create a figure with 3D axis (two columns for different coloring)\nfig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))\n\nnth=100\n\n# plot the point cloud colored by distances\ns = ax.scatter(data_2009_aligned_noglacier[::nth,0], data_2009_aligned_noglacier[::nth,1], data_2009_aligned_noglacier[::nth,2], s=5,\n                   c=distances[::nth], vmax=3.0)\nax.set_aspect('equal')\n\n# label axes and add title\nax.set_xlabel('X [m]')\nax.set_ylabel('Y [m]')\nax.set_zlabel('Z [m]')\n\n# set initial view of 3D plot\nax.view_init(elev=40., azim=130.)\n\n# add a colorbar\nfig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.1, extend='max')\n\n# show the plot\nplt.tight_layout()\nplt.show()\n\nprint(f'Median distances: {np.median(distances):.2f} m')\nprint(f'Stdev. of distances: {np.std(distances):.2f} m')\n</pre> data_2009_aligned_noglacier = data_2009_aligned[~mask_polygon_2009]  # query indices of nearest neighbors of aligned 2009 coordinates in 2017 kd-tree nn_dists = tree2017.query(data_2009_aligned_noglacier, k=1)  # obtain distances as first element in tuple returned by query above distances = nn_dists[0]  # create a figure with 3D axis (two columns for different coloring) fig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))  nth=100  # plot the point cloud colored by distances s = ax.scatter(data_2009_aligned_noglacier[::nth,0], data_2009_aligned_noglacier[::nth,1], data_2009_aligned_noglacier[::nth,2], s=5,                    c=distances[::nth], vmax=3.0) ax.set_aspect('equal')  # label axes and add title ax.set_xlabel('X [m]') ax.set_ylabel('Y [m]') ax.set_zlabel('Z [m]')  # set initial view of 3D plot ax.view_init(elev=40., azim=130.)  # add a colorbar fig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.1, extend='max')  # show the plot plt.tight_layout() plt.show()  print(f'Median distances: {np.median(distances):.2f} m') print(f'Stdev. of distances: {np.std(distances):.2f} m') <pre>Median distances: 0.49 m\nStdev. of distances: 1.31 m\n</pre> <p>The statistics show that the overall alignment has strongly improved, in agreement with the visualization above. Larger distances remain at the edge of the glacier (yellow, &gt;3.0 m). We can assume that this is cause by actual surface distances, as we expect changes of the glacier body - parts of which may have been contained in the masked point cloud. Another area of large distances (yellow path in the upper right of the plot) is probably also an area where considerable surface change occurred. Since the majority of our 'no glacier' points show small distances (median of 0.5 m), we can visually confirm that the alignment worked out quite well (within the expected accuracy of these data). Be aware that the performance of the ICP largely depends on how many points of 'changed areas' influence the derived transformation. If too many change points are contained, the ICP will overfit to these, we obtain larger distances in actually stable parts and an overall poor alignment result.</p> <p>By writing the point clouds to las files, we can assess the alignment visually, e.g. in CloudCompare. Such a check is always advisable, because it can uncover unwanted effects, where the alignment deteriorates in parts where we are sure that they should well-align (e.g. some rock surface or other distinct, solid morphology). It is therefore important not to rely only on the statistical descriptors of ICP performance, which may be fairly low for a local optimum (point subsets of selected stable areas) but not represent the global optimum of point distances.</p> In\u00a0[12]: Copied! <pre>las_data2009_aligned = f'{data_path}/hellstugubrean_2009_aligned.las'\npcfuncs.write_las(data_2009_aligned, las_data2009_aligned)\n</pre> las_data2009_aligned = f'{data_path}/hellstugubrean_2009_aligned.las' pcfuncs.write_las(data_2009_aligned, las_data2009_aligned) <p>Visual assessment of point cloud alignment in CloudCompare by comparing the point cloud epochs in single colors (red: reference (2017), blue: aligned (2009)). The points fit well on the surfaces on the mountain flanks, a large gap is obvious at the glacier surface. This part is the target of change analysis and was excluded from the ICP-based derivation of the rigid transformation. Another area of offset becomes apparent in the right subfigure, which may stem from snow cover in this part in the 2009 epoch. Since the remaining points on the mountain flank fit well, we can assume that the influence of this area on the ICP alignment was acceptably small. Figure by course authors.</p> <p>Note that in our exercise we are using the same area ('no glacier') to derive the alignment and assess the alignment accuracy. In real analysis, it is recommended to use two separate subsets of the data, i.e. one set of stable areas to derive the transformation using ICP, and another set of stable areas to derive the (residual) distances between the aligned epochs. Both subsets are ideally spatially distributed in the scene, so that a bias towards certain areas is avoided and a stable solution is found for the target (changed) area.</p> <p></p> <p>After co-registration and fine alignment of our point cloud epochs, we can perform the actual surface change analysis. We will use the M3C2 algorithm (Lague et al., 2013) to quantify change (i.e. distances) between surfaces in the Hellstugubrean point clouds from 2009 to 2017.</p> <p>The M3C2 algorithm is available in CloudCompare with a graphical user interface. Since you already got familiar with point cloud processing in Python and we are interested in automating the analysis for potentially large series of point clouds, we will use the M3C2 implementation in the open source Python library <code>py4dgeo</code>. The package is contained in the Python environment of this course, or can be installed via pip - see the documentation for further information on installation and basic usage: https://py4dgeo.readthedocs.io/en/latest/index.html.</p> <p><code>py4dgeo</code> works with <code>Epoch</code> objects, which can be created from las files or directly input as <code>numpy</code> arrays of shape <code>n x 3</code> (containing the XYZ coordinates).</p> In\u00a0[\u00a0]: Copied! <pre># import the library for M3C2 distance calculation\nimport py4dgeo\n\n# load point clouds as epoch objects\nepoch_2009, epoch_2017 = py4dgeo.read_from_las(\n    las_data2009_aligned, las_data2017\n)\n</pre> # import the library for M3C2 distance calculation import py4dgeo  # load point clouds as epoch objects epoch_2009, epoch_2017 = py4dgeo.read_from_las(     las_data2009_aligned, las_data2017 ) <p>As core points, we could use a subsampled version of one of the input point clouds or a regular sampling of the scene. In this exercise, we will use all points of the sparser point cloud in 2009, for which the distances to the 2017 epoch shall be derived:</p> In\u00a0[14]: Copied! <pre>corepoints = epoch_2009.cloud[::]\n</pre> corepoints = epoch_2009.cloud[::] <p>In the next step, we instantiate the <code>M3C2</code> algorithm class, where we specify the parameters of the algorithm. With the <code>epochs</code>, we specify the two point clouds (arrays with XYZ coordinates) for change quantification. Therein, the first epoch acts as the reference epoch, and the second as the compared epoch. In our case, changes are hence derived from 2009 to 2017. The <code>corepoints</code> parameters takes as input the 3D coordinates at which the M3C2 distance should be computed (considering all original points of the input epochs). <code>normal_radii</code> specifies the radius for estimating the local surface normals. According to the M3C2 algorithm, a multi-scale normal estimation is possible, for which the normal radius with highest planarity of the locally adjusted surface is selected at each core point. For our glacier dataset, we specify a minimum radius of <code>2.0 m</code> with steps of <code>1.0 m</code> radius increase up to a maximum radius of <code>8.0 m</code>. We define a separate radius for the cylinder spanned in normal direction with <code>cyl_radii</code>. All points within this radius from the respective core point are used to average the position for subsequent distance quantification. The <code>max_distance</code> parameter sets the length of the cylinder, and thereby the maximum distance to search for points in the other epoch. If not set, the distance defaults to the cylinder radius (which is too small for most expected use cases, so it is important to set this parameter). Finally, we specify the <code>registration_error</code> in our algorithm, from which the level of detection is derived. We set the registration error to the standard deviation of cloud-to-cloud distances in unchanged areas of our scene (see above). If not set, no registration error is considered (i.e. default is <code>0.0</code>).</p> In\u00a0[15]: Copied! <pre>m3c2 = py4dgeo.M3C2(\n    epochs=(epoch_2009, epoch_2017),\n    corepoints=corepoints,\n    normal_radii=(2.0, 1.0, 8.0),\n    cyl_radii=(2.0,),\n    max_distance=(50.0),\n    registration_error=(1.31)\n)\n</pre> m3c2 = py4dgeo.M3C2(     epochs=(epoch_2009, epoch_2017),     corepoints=corepoints,     normal_radii=(2.0, 1.0, 8.0),     cyl_radii=(2.0,),     max_distance=(50.0),     registration_error=(1.31) ) <p>As a final step, we just need to run the distance calculation, which returns the <code>distances</code> and a structured array of all variables relating to <code>uncertainties</code>.</p> In\u00a0[16]: Copied! <pre>m3c2_distances, uncertainties = m3c2.run()\n</pre> m3c2_distances, uncertainties = m3c2.run() <pre>[2023-02-17 16:10:17][INFO] Building KDTree structure with leaf parameter 10\n[2023-02-17 16:10:17][INFO] Building KDTree structure with leaf parameter 10\n</pre> <p>The calculated result is an array with one distance per core point. The order of distances corresponds exactly to the order of input core points. Corresponding to the derived distances, an uncertainty array is returned which contains several quantities that can be accessed individually: The level of detection <code>lodetection</code>, the spread of the distance across points in either cloud (<code>spread1</code> and <code>spread2</code>, by default measured as the standard deviation of distances) and the total number of points taken into consideration in either cloud (<code>num_samples1</code> and <code>num_samples2</code>).</p> In\u00a0[17]: Copied! <pre>m3c2_distances\n</pre> m3c2_distances Out[17]: <pre>array([-3.56735986, -3.54197136, -3.5456269 , ..., -0.88864044,\n       -0.92088742, -0.96017669])</pre> In\u00a0[18]: Copied! <pre>uncertainties['lodetection']\n</pre> uncertainties['lodetection'] Out[18]: <pre>array([2.59091504, 2.58815035, 2.5887849 , ..., 2.61745518, 2.62151081,\n       2.59732118])</pre> <p>We can now visualize the distances and corresponding level of detection in the scene:</p> In\u00a0[19]: Copied! <pre># create the figure\nfig, ax = plt.subplots(1, 1, figsize=(7,4), subplot_kw={\"projection\": \"3d\"})\n\n# plot the distances\nd = ax.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-50, vmax=50, s=1) \nplt.colorbar(d, format=('%.1f'), label='Distance [m]', shrink=.5, pad=.1)\n\n# add plot elements\nax.set_xlabel('Easting [m]')\nax.set_ylabel('Northing [m]')\nax.set_aspect('equal')\nax.view_init(elev=40., azim=130.)\n\nplt.tight_layout()\nplt.show()\n</pre> # create the figure fig, ax = plt.subplots(1, 1, figsize=(7,4), subplot_kw={\"projection\": \"3d\"})  # plot the distances d = ax.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-50, vmax=50, s=1)  plt.colorbar(d, format=('%.1f'), label='Distance [m]', shrink=.5, pad=.1)  # add plot elements ax.set_xlabel('Easting [m]') ax.set_ylabel('Northing [m]') ax.set_aspect('equal') ax.view_init(elev=40., azim=130.)  plt.tight_layout() plt.show() <p>The M3C2 distances show highest values in the area of the glacier, which sticks out in red, i.e. negative surface change. From 2009 to 2017, this represents surface decrease and you will immediately associate it to glacier mass loss. Two more red patches of surface decrease are visible on both flanks adjacent to the glacier area. In the eastern part, we already assumed presence of snow in the 2009 epoch during the coregistration process. This snow not being present in 2017 yields negative surface change. On the opposite flank, the surface decrease may similarly be caused by the changing presence of snow, but further possibilities are erosion of surface material or rock falls. Relating the derived surface change to underlying processes requires more in-depth examination of the local situation, either in the data, from complementary measurements, or from in-situ knowledge.</p> <p></p> <p>We will now look into the uncertainties associated to our change analysis results. As introduced in theory above, the M3C2 provides a spatially variable estimate of the uncertainty, which considers the global registration error of two epochs (i.e. our alignment accuracy), as well as the local surface roughness and point density. Based on a statistical test, this yields the level of detection at a location. If the quantified change is larger than the level of detection, we consider the change statistically significant, otherwise it is not significant.</p> <p>Let's visualize the level of detection compared to our change quantification, and derive the significance of change per point from our results:</p> In\u00a0[20]: Copied! <pre># create a binary mask of significant change\nchange_sign = np.where(abs(m3c2_distances) &gt; uncertainties['lodetection'], True, False)\n\n# create the figure\nfig, axs = plt.subplots(1, 3, figsize=(14,6), subplot_kw={\"projection\": \"3d\"})\nax1,ax2,ax3 = axs\n\n# plot the distances\nd = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-50, vmax=50, s=1) \nplt.colorbar(d, format=('%.1f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)\n\n# plot the level of detection values\nl = ax2.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=uncertainties['lodetection'], cmap='viridis', vmax=2.8, s=1) \nplt.colorbar(l, format=('%.1f'), label='Level of detection [m]', ax=ax2, extend='max', shrink=.5, pad=.15)\n\n# plot the significant change values (boolean)\nax3.scatter(corepoints[~change_sign][:,0], corepoints[~change_sign][:,1], corepoints[~change_sign][:,2], label='Not significant change', c='blue', s=1) \nax3.scatter(corepoints[change_sign][:,0], corepoints[change_sign][:,1], corepoints[change_sign][:,2], label='Significant change', c='red', s=1) \nax3.legend()\n\n# add plot elements\nfor ax in axs:\n    ax.set_xlabel('Easting [m]')\n    ax.set_ylabel('Northing [m]')\n    ax.set_aspect('equal')\n    ax.view_init(elev=40., azim=130.)\n\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> # create a binary mask of significant change change_sign = np.where(abs(m3c2_distances) &gt; uncertainties['lodetection'], True, False)  # create the figure fig, axs = plt.subplots(1, 3, figsize=(14,6), subplot_kw={\"projection\": \"3d\"}) ax1,ax2,ax3 = axs  # plot the distances d = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-50, vmax=50, s=1)  plt.colorbar(d, format=('%.1f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)  # plot the level of detection values l = ax2.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=uncertainties['lodetection'], cmap='viridis', vmax=2.8, s=1)  plt.colorbar(l, format=('%.1f'), label='Level of detection [m]', ax=ax2, extend='max', shrink=.5, pad=.15)  # plot the significant change values (boolean) ax3.scatter(corepoints[~change_sign][:,0], corepoints[~change_sign][:,1], corepoints[~change_sign][:,2], label='Not significant change', c='blue', s=1)  ax3.scatter(corepoints[change_sign][:,0], corepoints[change_sign][:,1], corepoints[change_sign][:,2], label='Significant change', c='red', s=1)  ax3.legend()  # add plot elements for ax in axs:     ax.set_xlabel('Easting [m]')     ax.set_ylabel('Northing [m]')     ax.set_aspect('equal')     ax.view_init(elev=40., azim=130.)  plt.axis('equal') plt.tight_layout() plt.show() <p>From the level of detection, we can derive that the main area of significant change is the glacier tongue (right plot showing <code>level of detection</code> &gt; <code>change</code>). This is an intuitive result, as we quantified surface decrease in ranges of -10 to -40 m, whereas the level of detection is well below 3 m. Notably, the area of the glacier tongue yields homogeneously low level of detection values (middle plot above). This is likely related mainly to the surface roughness in our point cloud data, which is low at our planarity scales of several meters compared to the rougher surface on the mountain flanks, featuring debris and topographic features. The plot visualizes nicely how the surface roughness - in terms of level of detection - is fairly high at the glacier boundary, where planarity decreases due to the change in morphologic units.</p> <p>The relation of derived changes to the associated uncertainties is an important aspect for the representation of change analysis results, and ultimately for their interpretation. This will be elaborated in the subsequent section. For now, we conclude by storing the results to point cloud files:</p> In\u00a0[21]: Copied! <pre># name of output las file\nlas_results = f'{data_path}/hellstugubrean_changes_2009_2017.las'\n\n# dictionary with attributes from M3C2 change analysis\nchange_data = {'m3c2_distance':m3c2_distances, 'lodetection':uncertainties['lodetection'], 'sig_change': change_sign.astype(int)}\n\n# write corepoints and attributes to las using our write function\npcfuncs.write_las(corepoints, las_results, attribute_dict=change_data)\n</pre> # name of output las file las_results = f'{data_path}/hellstugubrean_changes_2009_2017.las'  # dictionary with attributes from M3C2 change analysis change_data = {'m3c2_distance':m3c2_distances, 'lodetection':uncertainties['lodetection'], 'sig_change': change_sign.astype(int)}  # write corepoints and attributes to las using our write function pcfuncs.write_las(corepoints, las_results, attribute_dict=change_data) <p></p> In\u00a0[22]: Copied! <pre>directions = m3c2.directions()\ndirections\n</pre> directions = m3c2.directions() directions Out[22]: <pre>array([[ 0.20199503,  0.10329517,  0.97392408],\n       [ 0.19820585,  0.10382951,  0.97464551],\n       [ 0.20006672,  0.1033419 ,  0.97431707],\n       ...,\n       [-0.50224166, -0.28419147,  0.81669365],\n       [-0.49335208, -0.28829136,  0.82066547],\n       [-0.49720595, -0.29223511,  0.81693628]])</pre> <p>We visualize the directions using an HSV color wheel, so that we can derive unique coloring for a combination of aspect and slope of the surface directions. We use pre-defined functions to obtain the aspect and slope from the normal vectors, which are contained in the supplementary script <code>vector_functions.py</code> (have a look at the code to understand how it works)..</p> In\u00a0[23]: Copied! <pre>import math\nimport colorsys\n\n# add the assets folder to the path, so that we can import supplementary Python functions \nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets/python_functions\").resolve()))\nimport vector_functions as vfuncs\n\n# derive aspect and slope values of directions (normal vector) per corepoint\nslopes = np.array([vfuncs.getSlope(d) for d in directions])\naspects = np.array([vfuncs.getAspect(d) for d in directions])\n\n# normalize values for hsv/rgb derivation\nslope_max = np.max(slopes)\nslopes = slopes/slope_max\naspects = aspects/np.max(aspects)\n\n# derive hsv values for aspect (h) and slope (s), v is constant\nvs = np.ones_like(aspects).flatten().tolist() # value is constant\nhsv_dirs = np.array([colorsys.hsv_to_rgb(*x) for x in zip(aspects,slopes,vs)])\n\n# initialize the figure\nfig = plt.figure(figsize=(12,7))\nax1 = fig.add_subplot(121, projection='3d')\n#ax2 = plt.subplot()\nax2 = fig.add_subplot(324, projection='polar')\n\n# plot the 3D directions and color by HSV\nd = ax1.scatter(corepoints[::10,0], corepoints[::10,1], corepoints[::10,2], c=hsv_dirs[::10], s=1) #, vmin=-50, vmax=50, s=1\n\nax1.set_xlabel('Easting [m]')\nax1.set_ylabel('Northing [m]')\nax1.set_aspect('equal')\nax1.view_init(elev=40., azim=130.)\n\n\n# plot the HSV color wheel (legend)\nrho = np.linspace(0,1,100) # Radius of 1, distance from center to outer edge\nphi = np.linspace(0, math.pi*2.,1000 ) # in radians, one full circle\n\nRHO, PHI = np.meshgrid(rho,phi) # get every combination of rho and phi\n\nh = (PHI-PHI.min()) / (PHI.max()-PHI.min()) # use angle to determine hue, normalized from 0-1\nh = np.flip(h)        \ns = RHO               # saturation is set as a function of radias\nv = np.ones_like(RHO) # value is constant\n\n# convert the np arrays to lists. This actually speeds up the colorsys call\nh,s,v = h.flatten().tolist(), s.flatten().tolist(), v.flatten().tolist()\nc = np.array([colorsys.hsv_to_rgb(*x) for x in zip(h,s,v)])\n\nax2.set_theta_zero_location('N')\nax2.set_theta_direction(-1)\nax2.scatter(PHI, RHO*slope_max, c=c)\nax2.set_xlabel('Aspect')\nax2.text(0.2,100,'Slope [\u00b0]')\n\nplt.tight_layout()\n</pre> import math import colorsys  # add the assets folder to the path, so that we can import supplementary Python functions  import sys from pathlib import Path sys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets/python_functions\").resolve())) import vector_functions as vfuncs  # derive aspect and slope values of directions (normal vector) per corepoint slopes = np.array([vfuncs.getSlope(d) for d in directions]) aspects = np.array([vfuncs.getAspect(d) for d in directions])  # normalize values for hsv/rgb derivation slope_max = np.max(slopes) slopes = slopes/slope_max aspects = aspects/np.max(aspects)  # derive hsv values for aspect (h) and slope (s), v is constant vs = np.ones_like(aspects).flatten().tolist() # value is constant hsv_dirs = np.array([colorsys.hsv_to_rgb(*x) for x in zip(aspects,slopes,vs)])  # initialize the figure fig = plt.figure(figsize=(12,7)) ax1 = fig.add_subplot(121, projection='3d') #ax2 = plt.subplot() ax2 = fig.add_subplot(324, projection='polar')  # plot the 3D directions and color by HSV d = ax1.scatter(corepoints[::10,0], corepoints[::10,1], corepoints[::10,2], c=hsv_dirs[::10], s=1) #, vmin=-50, vmax=50, s=1  ax1.set_xlabel('Easting [m]') ax1.set_ylabel('Northing [m]') ax1.set_aspect('equal') ax1.view_init(elev=40., azim=130.)   # plot the HSV color wheel (legend) rho = np.linspace(0,1,100) # Radius of 1, distance from center to outer edge phi = np.linspace(0, math.pi*2.,1000 ) # in radians, one full circle  RHO, PHI = np.meshgrid(rho,phi) # get every combination of rho and phi  h = (PHI-PHI.min()) / (PHI.max()-PHI.min()) # use angle to determine hue, normalized from 0-1 h = np.flip(h)         s = RHO               # saturation is set as a function of radias v = np.ones_like(RHO) # value is constant  # convert the np arrays to lists. This actually speeds up the colorsys call h,s,v = h.flatten().tolist(), s.flatten().tolist(), v.flatten().tolist() c = np.array([colorsys.hsv_to_rgb(*x) for x in zip(h,s,v)])  ax2.set_theta_zero_location('N') ax2.set_theta_direction(-1) ax2.scatter(PHI, RHO*slope_max, c=c) ax2.set_xlabel('Aspect') ax2.text(0.2,100,'Slope [\u00b0]')  plt.tight_layout() <p>All these visualizations are strongly depending on the information to be communicated. Based on all these information components (change values, uncertainties, directions, ...), a next analysis could be a classification of observed changes, for example, into the types of changes, which can ultimately be related to the underlying processes. For our glacier example, a simple approach could be to classifiy changes with large magnitude and strong vertical direction component as glacier mass loss change. A stronger horizontal change component is contained on the mountain flanks, where change (if significant) could be associated to presence of snow cover in one epoch. As you can imagine, this post-classification can be very complex and challenging in real scenarios with scenes features (typically a variety of) natural phenomena.</p> <p>We will conclude the lesson part of this theme at this point. It is time to proceed with the self-evaluation, so that you are (hopefully) ready to proceed with the exercise and getting your hands on your own analysis of geomorphic change in multitemporal 3D point clouds.</p> <p></p>  What main properties can change in point clouds and are usually assessed in change analysis?  <p>Geometry and any attributes; assessment of biophysical, chemical, geometric, morphologic properties</p> <p></p>  What is the difference between change detection and change analysis?  <p>Change detection is a binary assessment if change occurred or not. Change analysis regards the quantification of change and/or assessment of change properties.</p> <p></p>  Assign the following examples of change analysis to the respective dimensionality:  A B C D  \thorizontal displacement of objects or features in, e.g., object tracking A B C D  \tsurface increase or lowering in, e.g., topographic monitoring A B C D  \tgeometric change in size and/or shape, object or surface deformation in, e.g. geomorphic monitoring A B C D  \tchanges in geometry and feature space of, e.g. vegetation A. 1D changes  B. 2D changes   C. 3D changes  D. higher-dimensional (&gt;3D) changes  <p>B A C D</p> <p></p>  List the typical steps in object-based change analysis  <p>Object extraction, object identification (between epochs), object characterization, monitoring</p> <p></p>  Which are the main methods for surface change analysis using multitemporal point clouds?  <p>DEM differencing, cloud-to-cloud distances (C2C), multi-scale model-to-model cloud comparison (M3C2)</p> <p></p>  What is the main disadvantage of DEM differencing over full 3D change analysis?   analysis of rasters is computationally complex and requires long processing times  change direction is restricted to one dimension, usually vertically  vegetation needs to be removed before analyzing terrain changes <p>change direction is restricted to one dimension, usually vertically</p> <p></p>  What is the central underlying assumption of point cloud distance computation with the M3C2 algorithm?   local planarity is assumed to derive change in direction of the surface orientation  homogeneous density is assumed to reduce uncertainty due to effects of point sampling change directions are assumed mainly parallel to the surface, using distinct features for deriving distances <p>local planarity is assumed to derive change in direction of the surface orientation</p> <p></p>  Which variables are considered by the spatially variable uncertainty modelled in the M3C2 (Level of Detection)?   LiDAR intensity, roughness, point density  Roughness, point density, registration error Point density, registration error, incidence angle <p>Roughness, point density, registration error</p> <p></p>  Sort the following change representations by the contained level of information (least (1) to most (4)):  1 2 3 4  \tTriple change mask (positive, negative, and no change) 1 2 3 4  \tTriple change mask (positive, negative, and no change) 1 2 3 4  \tBinary change 1 2 3 4  \tType change (post-classification) <p>2 3 1 4</p> <p></p>  What is an important caveat when using the level of detection to filter out non-significant changes from the result?  <p>The level of detection describes a confidence interval at which the change quantification is statistically significant. Change values below the corresponding level of detection may nonetheless represent actual changes, but would be discarded as \u2018no change\u2019 by filtering. It is therefore recommended to consider the level of detection value as associated uncertainty, rather than removing such changes from the subsequent analysis.</p> <p></p> <p></p> <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#principles-and-basic-algorithms-of-3d-change-detection-and-analysis","title":"Principles and basic algorithms of 3D change detection and analysis\u00b6","text":""},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#terminology-of-3d-change-analysis","title":"Terminology of 3D change analysis\u00b6","text":"<p>Change in 3D point cloud data can regard any attribute or property. Since we are dealing with topographic data, we will focus on geometric change in this course, i.e. changes in the elevation or morphology of the terrain surface. However, the same principles apply to other attributes, for example the intensity or color of the point cloud data, geometric properties, such as the roughness or shape, or object properties, such as the vitality of vegetation (e.g., NDVI) or the temperature (e.g., thermal measurements).</p> <p>In general, we distinguish the terms of change detection and change analysis. Change detection is the process of identifying the existence of change in its location and maybe extent. It regards binary change information, i.e. a location or object has changed or not (0/1). For example, this web service provides information if a building was added or removed (positive or negative values), but already provides a quantification of the corresponding height change (Cser\u00e9p &amp; Lindenbergh, 2023):</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#approaches-to-3d-change-analysis","title":"Approaches to 3D change analysis\u00b6","text":"<p>A variety of approaches exist in 3D change analysis to detect and quantify changes in multitemporal point clouds, typically using pairs of epochs acquired at two different points in time. These approaches can be broadly categorized into object-based change analysis, feature-based analysis and tracking, and surface change analysis.</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#object-based-change-analysis","title":"Object-based change analysis\u00b6","text":"<p>Object-based approaches of change analysis commonly detect objects or meaningful features within each epoch of a scene for assessing changes in their state. This may regard, e.g., their location, geometry, or size. For this, objects or segments are first extracted in single point cloud epochs, and therein identified (semantically) and characterized (e.g., in their shape or other properties that are monitored). Monitoring of each object's state is realized by repeating this object identification for each new epoch. Object-based change analysis in environmental and geographic applications may target, for example, trees regarding changes in height, crown extent or trunk size (e.g., Chen et al., 2022, Xiao et al., 2016):</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#feature-based-change-analysis","title":"Feature-based change analysis\u00b6","text":"<p>Feature-based change analysis uses features in the 3D scene to derive their change or displacement. Features may be meaningful, such as breaklines representing a landslide scarp, or ridges representing dune crests. In this case, feature-based approaches are similar to object-based analysis. Features may also be simple geometric primitives, such as planes or lines. Feature descriptors can also be derived per point, based on local neighborhood properties. As such, features are often handcrafted, meaning they are defined by the analyst based on knowledge about the application domain. Therein, a combination of geometric and spectral features (e.g. multispectral/hyperspectral image texture) may be used (cf. Qin et al., 2016).</p> <p>As geometric features, the aim is typically to obtain features that are translation- and rotation-independent, such as local surface normals, curvature, and others. To be re-identifiable between epochs, we need to assume a certain degree of rigidity, so that the features do not change with the deforming object. Recently, also learned features are derived for change analysis, e.g., using deep learning approaches (e.g., Gojcic et al., 2021).</p> <p>Feature tracking can then be realized by matching features in different epochs of a scene. Gojcic et al. (2021) present such a method to assess landslide displacements. They establish feature correspondences between epochs by performing a nearest neighbor search in feature space. From these correspondences, 3D displacement vectors can be derived from the coordinate locations of the features:</p> <p>Feature-based change analysis by establishing correspondences between geometric features and deriving their displacement as 3D vectors between coordinate locations in two epochs. Figure modified from Gojcic et al. (2021) / CC BY 4.0.</p> <p>Another approach to feature-based change analysis is provided with the so-called baseline method by Shen et al. (2017). The concept is to establish baselines between features within on single point cloud epoch. Subsequently, the baselines connecting corresponding points are compared between epochs in order to derive changes. Also here, features may be geometric neighborhood features (virtual points) or correspond to object features that can be identified in the scene. In their paper, the method is demonstrated to assess structural damage on a building after an earthquake (with data generated in a lab experiment). The (virtual) feature points are located at the center of bricks, and by establishing the baseline changes, finally 3D displacement vectors can be derived:</p> <p>Feature-based change analysis with the baseline method by (A) establishing baselines between features within each epoch and subsequently assessing their changes. From this, (B) displacement vectors of feature points can be derived, independent from accurate co-registration. Figure modified from Shen et al. (2017) / CC BY 4.0.</p> <p>The advantage of the baseline method is that the change analysis is not depending on the accuracy of co-registration, compared to other approaches which derive change values as point or object distances between epochs (Shen et al., 2017).</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#surface-change-analysis","title":"Surface change analysis\u00b6","text":"<p>Surface change analysis is performed by comparing a model of a scene\u2019s surface, commonly the terrain, between pairs of epochs. A basic method is to derive the difference between digital elevation models (DEMs) (James et al.,  2012) derived from each point cloud epoch. Therein, point cloud data is gridded into a DEM. The elevation value within each raster cell can then be subtracted, and the difference between elevations represents the change in height at each location. The resulting DEMs of difference (so-called DoDs) provide changes that are directed purely orthogonally to the (commonly vertical) orientation of a rasterized scene. The advantages of this method are that the analysis is simple and computationally efficient (i.e. fast raster-based analysis). DEM differencing is therefore useful for planar surfaces without a strong 3D morphology.</p> <p>Schematic of changes derived from differencing two epochs of digital elevation models (DEMs), which are derived vertically. Depending on the direction of change, DEM differences do not necessarily represent the true distance. Figure by course authors.</p> <p>The drawback of DEM differencing is visualized in the schematic figure above: if the surface and the direction of change are not directed vertically (i.e. orthogonally to the surface orientation in the raster), the difference value will not represent the true distance of surfaces between two epochs. Derived changes are then not topographically meaningful, for example, at slopes or on the side of objects. In general terms, the direction of differencing is always sensitive to the rotation of the point cloud.</p> <p>Further, seeing that DEM differencing is a raster-based approach, the same general limitations compared to point cloud analysis apply (cf. Theme 1): The requirement to choose a fixed cell size is challenging where surfaces are rough or point densities vary across a scene. The level of detail is thereby fixed to a certain user-defined resolution.</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#uncertainty-in-change-analysis","title":"Uncertainty in change analysis\u00b6","text":""},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#sources-of-uncertainty","title":"Sources of uncertainty\u00b6","text":"<p>Change analysis - as any analysis or quantification in our data - is always associated with some degree of uncertainty stemming from different sources. One source of uncertainty influencing the analysis is the geometric accuracy of single epochs. This accuracy is influenced by instrument errors, such as the angular resolution of a laser scanner or the positional accuracy of LiDAR measurements. LiDAR measurements are further influenced by the target geometry, for example if a very rough surface of edge is hit. This influence increases with the range and incidence angle of the diverging laser beam due to its increasing footprint. Also the surface characteristics themselves influence the measurement, e.g. presence of moisture or weathering. Further, atmospheric conditions influence the LiDAR measurement depending on the temperature, pressure, and humidity affecting the speed and refraction of light travel. This effect is especially difficult to model, but does become notable with highly frequent repetitions of scans, as we introduce with the LiDAR time series in the subsequent theme. For more details on these aspects of point cloud accuracy, have a look at Soudarissanane et al. (2011) and watch this lecture video (by Christoph Holst).</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#assessment-of-uncertainty","title":"Assessment of uncertainty\u00b6","text":"<p>The analysis and interpretation of change requires us to assess - considering all uncertainties - the accuracy of the change detection and quantification. A key concept here is the minimum detectable change, so the question: Considering the sources of uncertainties, what is the minimum change value at which we can confidently detect change, i.e. separate it from noise or other effects? A common term used for the determined minimum detectable change value is the level of detection (Lague et al., 2013, Wheaton et al., 2010).</p> <p>A simple way of assessing uncertainty in terms of a minimum detectable change is to look at the distribution of change values in areas that are presumed stable (i.e. actually unchanged). Any change values, i.e. distances between stable surfaces, can be assumed to stem from a registration error and noise (wherein noise here includes all sources of uncertainty in the measurements). In this way, the minimum detectable change can be derived via a statistical test, for example, a t-test using as threshold the 95th percentile of change values across stable surfaces (Wheaton et al., 2010). This provides a single level of detection value for the entire scene, which can be used, e.g., to mask out all change values that are not statistically significant.</p> <p>[comment]: &lt;&gt; (Spatially variable uncertainty quantification) As we are aware of spatially variable properties of our data in the scene, such as variable point density or variable measurement uncertainty due to different surface characteristics, it is desirable to account for this in a spatially variable uncertainty assessment. If considered, this means that we may be able to detect smaller changes in certain areas that are, e.g. highly planar and densely sampled, whereas we can confidently detect only larger changes in areas that are sparsely sampled and/or exhibit high surface roughness.</p> <p>An approach to quantify spatially variable uncertainty for 3D point cloud comparison is included in the M3C2 algorithm (Lague et al., 2013). This level of detection is based on the registration accuracy for a pair of point clouds as one global value for the scene, and the local surface roughness for each point where change is derived. Since the M3C2 distance is derived for local estimates of 3D planar surfaces, also the surface roughness and point density are estimated at this local scale and surface orientation. As you remember from the concept of the M3C2 algorithm above), it considers a cylindrical neighborhood around each core point for an averaged distance calculation along the normal direction of the locally adjusted plane. According to Lague et al. (2013), the Level of Detection for each core point at which distance is calculated is derived as follows:</p> <p>$$  LoD_{95\\%} = \\pm 1.96 \\left(\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} + reg \\right) $$</p> <p>where reg denotes the global registration error, i.e. a uniform value for the entire scene for two point cloud epochs. How this can be derived, you will learn later in this theme as part of the change analysis workflow. $\\sigma_1^2$ and $\\sigma_2^2$ denote the point position variances in epochs 1 and 2, respectively, which represent the local roughness. $n_1$ and $n_2$ are the number of points in each epoch, i.e. the size of the distribution. In this formulation, the two samples are assumed to be two independet Gaussian distributions (see schematic figure below). The factor of $1.96$ is used to derive the level of detection $LoD_{95\\%}$ of these distributions in combination with the registration error at 95 % confidence. For stricter or looser confidence intervals, the value may be adapted. But the given formulation has become the de-facto standard in level of detection quantification in point cloud change analysis.</p> <p>Estimation of level of detection in the M3C2 algorithm as the distribution of points of epochs 1 and 2 along the cylinder axis oriented in normal direction. Figure by L. Winiwarter, modified from Winiwarter et al. (2021) / CC BY 4.0.</p> <p>The level of detection value derived at each core point can now be used to determine if the corresponding change value (specifically, e.g., M3C2 distance value) is statistically significant. If ${M3C2\\:distance} &gt; LoD_{95\\%}$, we can consider it significant change. If ${M3C2\\:distance} \\leq LoD_{95\\%}$, the change is not statistically significant, i.e. cannot be confidently separated from noise.</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#a-change-analysis-workflow-in-methods","title":"A change analysis workflow in methods\u00b6","text":"<p>So far, we have looked at approaches and theoretical concepts to assessing change. In the following, specific methods will be introduced along the steps of a typical change analysis workflow (cf. Theme 1):</p> <ol> <li>Data acquisition</li> <li>Co-registration</li> <li>Change detection and quantification</li> <li>Uncertainty assessment</li> <li>Change representation</li> </ol> <p>Multitemporal data acquisition is introduced in Theme 1. So we start at the typical task of co-registration, i.e. the alignment of multitemporal point clouds.</p> <p>We start the change analysis workflow by reading two epochs of input point clouds. For the lesson in this theme, we will continue using the data of Hellstugubrean (data directory <code>hellstugubrean</code>), which you worked with in the previous theme. For reading point clouds from las, we use the same function defined which is stored in a supplementary script file <code>pointcloud_funcs</code>.</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#point-cloud-alignment","title":"Point cloud alignment\u00b6","text":"<p>To ensure that we are only analyzing actual change between surface in the data, in this step we need to ensure that the two point clouds are well aligned. Even though both datasets are georeferenced, small offsets (translation and rotation) are still possible. Since the measurement accuracy of our instruments (here mm-scale laser scanning) is higher than the georeferencing accuracy (usually cm-scale GNSS), we can use the dense spatial sampling of the surface to improve the alignment (or co-registration). This is based on the principle that stable, unchanged surfaces in the scene can be used to minimize their distances. In other words: we align those parts of the data, where we expect no change and thus a highly similar surface representation and position in the point cloud epochs.</p> <p>As the relative representation of the scene (i.e. scaling) in our laser scanning point clouds is of high metric accuracy, we apply a rigid body transformation. For this, we use the so-called iterative closest point (ICP) algorithm (Besl &amp; McKay, 1992). This algorithm keeps one set of points (point cloud epoch 1) fixed and iteratively transforms another set of points (point cloud epoch 2) rigidly to minimize the distances between points in the point sets to their closest neighbors. Therein, the transformation usually starts with an initial guess and then proceeds as long as the distances are reduced by further transformation (here, translation and rotation of the point set). Once an optimum is reached, the final transformation is applied and/or a transformation matrix is yielded as result. To learn more about the ICP algorithm in the context of point cloud registration, follow this lecture video (by Cyrill Stachniss).</p> <p>By the way, an ICP method is available in CloudCompare, as in many other point cloud processing tools. In this session, we will make use of an automatic procedure in PDAL - so let's continue with the analysis.</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#change-detection-and-quantification-surface-change-analysis","title":"Change detection and quantification (surface change analysis)\u00b6","text":""},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#uncertainty-of-change-analysis","title":"Uncertainty of change analysis\u00b6","text":""},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#representation-of-change-analysis-results","title":"Representation of change analysis results\u00b6","text":"<p>There are different approaches to representing, i.e. communicating, results of change analysis, depending on the objective and required information of the analysis task. The simplest form of change representation is binary change. This separation into changed and unchanged areas is equivalent to displaying significant change resulting from our M3C2 analysis.</p> <p>One more level of information is added via a triple change mask, which distinguishes positive and negative change, and no change. Accordingly, you could choose three colors to plot the change map in a manner similar to the significant/non-significant change above. If you are running the exercise notebook by yourself, go for it!</p> <p>The full information directly resulting from the change quantification would be to plot the change values themselves, as we did above with our M3C2 distances. Here especially, it is important to always provide additionally the associated uncertainties. We may observe quite high change values but also high level of detection values in some areas. This must be put into context by the human interpreter. Therein, it is recommended not to solely rely on the level of detection as threshold, but rather considering the actual uncertainty values and their spatial distribution. To get further into this aspect, you may read Anderson (2019).</p> <p>Seeing that we are dealing with 3D change analysis, we actually have even more information about the change properties of our surfaces. Where the scene geometry is three-dimensional (i.e. no flat, predominant change direction), the direction of changes can be of particular interest. For our M3C2 change analysis, we can derive the change direction from the normals estimation. In <code>py4dgeo</code>, the normal vector directions can be accessed in the <code>M3C2</code> algorithm object:</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#self-evaluation-quiz","title":"Self-evaluation quiz\u00b6","text":""},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#exercise","title":"Exercise\u00b6","text":"<p>Made it through the quiz? Then you are ready for the exercise - proceed with the next part using the button at the bottom of this page.</p>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#literature","title":"Literature\u00b6","text":""},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#key-papers-to-read","title":"Key papers to read\u00b6","text":"<ol> <li>Qin, R., Tian, J., &amp; Reinartz , P. (2016). 3D change detection Approaches and applications. ISPRS Journal of Photogrammetry and Remote Sensing, 122 , pp. 41 56. doi: 10.1016/j.isprsjprs.2016.09.013.</li> <li>Lague, D., Brodu, N., &amp; Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82, pp. 10-26. doi: 10.1016/j.isprsjprs.2013.04.009.</li> <li>James, M. R., Robson, S., &amp; Smith, M. W. (2017). 3D uncertainty based topographic change detection with structure from motion photogrammetry: precision maps for ground control and directly georeferenced surveys. Earth Surface Processes and Landforms, 42 (12), pp. 1769 1788. doi: 10.1002/esp.4125.</li> </ol>"},{"location":"module3/03_3d_change_analysis/03_3d_change_analysis.html#references","title":"References\u00b6","text":"<ul> <li>Anderson, S. W. (2019). Uncertainty in quantitative analyses of topographic change: error propagation and the role of thresholding. Earth Surface Processes and Landforms, 44 (5), pp. 1015-1033. doi: 10.1002/esp.4551.</li> <li>Barnhart, T. &amp; Crosby, B. (2013). Comparing Two Methods of Surface Change Detection on an Evolving Thermokarst Using High-Temporal-Frequency Terrestrial Laser Scanning, Selawik River, Alaska. Remote Sensing, 5(6), pp. 2813\u20132837. doi: 10.3390/rs5062813.</li> <li>Besl, P. J., &amp; McKay, N. D. (1992). A Method for Registration of 3-D Shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14 (2), pp. 239-256. doi: 10.1109/34.121791.</li> <li>Chen, J., Chen, Y., &amp; Liu, Z. (2022). Extraction of Forestry Parameters Based on Multi-Platform LiDAR. IEEE Access, 10, pp. 21077-21094. doi: 10.1109/ACCESS.2022.3151685.</li> <li>Corb\u00ed, H., Riquelme, A., Meg\u00edas-Ba\u00f1os, C., &amp; Abellan, A. (2018). 3-D Morphological Change Analysis of a Beach with Seagrass Berm Using a Terrestrial Laser Scanner. ISPRS International Journal of Geo-Information, 7 (234), pp. 15. doi: 10.3390/ijgi7070234.</li> <li>Cser\u00e9p, M., &amp; Lindenbergh, R. (2023). Distributed processing of Dutch AHN laser altimetry changes of the built-up area. International Journal of Applied Earth Observation and Geoinformation, 116, pp. 103174. doi: 10.1016/j.jag.2022.103174.</li> <li>Eitel, J. U. H., H\u00f6fle, B., Vierling, L. A., Abell\u00e1n, A., Asner, G. P., Deems,J. S., Glennie, C. L., Joerg, P. C., LeWinter, A. L., Magney, T. S., Mandlburger, G., Morton, D. C., M\u00fcller, J., &amp; Vierling, K. T. (2016). Beyond 3D: The new spectrum of lidar applications for earth and ecological sciences. Remote Sensing of Environment, 186 , pp. 372-392. doi: 10.1016/j.rse.2016.08.018.</li> <li>Girardeau-Montaut, D., Roux, M., Marc, R., &amp; Thibault, G. (2005). Change detection on points cloud data acquired with a ground laser scanner. ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XXXVI-3/W19, pp. 30-35.</li> <li>G\u00f3mez-Guti\u00e9rrez, A. &amp; Gon\u00e7alves G. R. (2020). Surveying coastal cliffs using two UAV platforms (multirotor and fixed-wing) and three different approaches for the estimation of volumetric changes. International Journal of Remote Sensing, 41(21), pp. 8143-8175. doi: 10.1080/01431161.2020.1752950.</li> <li>Gojcic, Z., Schmid, L., &amp; Wieser, A. (2021). Dense 3D displacement vector fields for point cloud-based landslide monitoring. Landslides, 18(12), 3821-3832. doi: 10.1007/s10346-021-01761-y.</li> <li>James, M. R., Robson, S., &amp; Smith, M. W. (2017). 3D uncertainty based topographic change detection with structure from motion photogrammetry: precision maps for ground control and directly georeferenced surveys. Earth Surface Processes and Landforms, 42 (12), pp. 1769 1788. doi: 10.1002/esp.4125.</li> <li>Lague, D., Brodu, N., &amp; Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82, pp. 10-26. doi: 10.1016/j.isprsjprs.2013.04.009.</li> <li>Marx, S., Anders, K., Antonova, S., Beck, I., Boike, J., Marsh, P., Langer, M., &amp; H\u00f6fle, B. (2017). Terrestrial laser scanning for quantifying small-scale vertical movements of the ground surface in Artic permafrost regions. Earth Surf. Dynam. Discuss., 2017, pp. 1-31. doi: 10.5194/esurf-2017-49.</li> <li>Mayr, A., Rutzinger, M., Bremer, M., Oude Elberink, S., Stumpf, F., &amp; Geitner, C. (2017). Object-based classification of terrestrial laser scanning point clouds for landslide monitoring. The Photogrammetric Record, 32 (160), pp. 377-397. doi: 10.1111/phor.12215.</li> <li>Mayr, A., Rutzinger, M., &amp; Geitner, C. (2018). Multitemporal Analysis of Objects in 3D Point Clouds for Landslide Monitoring. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, XLII-2, pp. 691-697. doi: 10.5194/isprs-archives-XLII-2-691-2018.</li> <li>Rossini, M., Di Mauro, B., Garzonio, R. &amp; Baccolo, G. (2018). Rapid melting dynamics of an alpine glacier with repeated UAV photogrammetry. Geomorphology, 304, pp. 159-172. doi: 10.1016/j.geomorph.2017.12.039.</li> <li>Shen, Y., Lindenbergh, R., &amp; Wang, J. (2017). Change analysis in structural laser scanning point clouds: The baseline method. Sensors, 17(1), 26. doi: 10.3390/s17010026.</li> <li>Soudarissanane, S., Lindenbergh, R., Menenti, M., &amp; Teunissen, P. (2011). Scanning geometry: Influencing factor on the quality of terrestrial laser scanning points. ISPRS Journal of Photogrammetry and Remote Sensing, 66 (4), pp. 389-399. doi: 10.1016/j.isprsjprs.2011.01.005.</li> <li>Wheaton, J. M., Brasington, J., Darby, S. E., &amp; Sear, D. A. (2010). Accounting for uncertainty in DEMs from repeat topographic surveys: improved sediment budgets. Earth Surface Processes and Landforms, 35 (2), pp. 136-156. doi: 10.1002/esp.1886.</li> <li>Williams, J. G. (2017). Insights into Rockfall from Constant 4D Monitoring. PhD thesis, Durham University. URL: http://etheses.dur.ac.uk/12172/.</li> <li>Williams, J. G., Anders, K., Winiwarter, L., Zahs, V., &amp; H\u00f6fle, B. (2021). Multi-directional change detection between point clouds. ISPRS Journal of Photogrammetry and Remote Sensing, 172, pp. 95-113. doi: 10.1016/j.isprsjprs.2020.12.002.</li> <li>Winiwarter, L., Anders, K., &amp; H\u00f6fle, B. (2021). M3C2-EP: Pushing the limits of 3D topographic point cloud change detection by error propagation. ISPRS Journal of Photogrammetry and Remote Sensing, 178, pp. 240-258. doi: 10.1016/j.isprsjprs.2021.06.011.</li> <li>Xiao, W., Xu, S., Elberink, S. O. &amp; Vosselman, G. (2016). Individual Tree Crown Modeling and Change Detection From Airborne Lidar Data. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 9(8), pp. 3467-3477. doi: 10.1109/JSTARS.2016.2541780.</li> <li>Zahs, V., H\u00e4mmerle, M., Anders, K., Hecht, S., Sailer, R., Rutzinger, M., Williams, J. G., &amp; H\u00f6fle, B. (2019). Multi-temporal 3D point cloud-based quantification and analysis of geomorphological activity at an alpine rock glacier using airborne and terrestrial LiDAR. Permafrost and Periglacial Processes, 30 (3), pp. 222-238. doi: 10.1002/ppp.2004.</li> <li>Zahs, V., Winiwarter, L., Anders,K. Williams, J. ,G., Rutzinger, M. &amp; H\u00f6fle, B. (2022). Correspondence-driven plane-based M3C2 for lower uncertainty in 3D topographic change quantification. ISPRS Journal of Photogrammetry and Remote Sensing, 183, pp. 541 - 559. doi: 10.1016/j.isprsjprs.2021.11.018.</li> </ul>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html","title":"Exercise: Principles and basic algorithms of 3D change detection and analysis","text":""},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#3d-change-analysis-at-an-active-rock-glacier-using-multitemporal-point-clouds","title":"3D Change Analysis at an Active Rock Glacier using Multitemporal Point Clouds","text":"<p>In this exercise, you will perform a surface change analysis on TLS point clouds of the \u00c4u\u00dferes Hochebenkar rock glacier (46\u00b050\u201911''N, 11\u00b000\u201920\u2018\u2019E) for two consecutive years. See the introduction to the case study and dataset here.</p> <p>The objective is to perform a full workflow of 3D change analysis with</p> <ul> <li>Assessment of alignment uncertainty</li> <li>Change analysis using the M3C2 algorithm </li> <li>Change representation and assessment of results</li> <li>For the fast ones: Comparison to change analysis via DEM differencing</li> </ul> <p>Look into the article by Zahs et al., 2019 for comparison of possible surface dynamics at the site and help for deciding on suitable parameters, etc.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#software-and-data","title":"Software and data","text":"<p>This exercise can be solved fully in CloudCompare or using Python with the <code>py4dgeo</code> library. If you have the Python skills, we recommend to work on the exercise with Python in a Jupyter notebook, hence obtaining a sharable and reproducible workflow.</p> <p>Use CloudCompare and GIS Software (e.g., QGIS) to check and visualize your results.</p> <p>In any case, make use of the software documentations!</p> <p>The dataset will be two epochs of point clouds acquired by UAV laser scanning in 2020 and 2021: <code>ahk_2020_uls.laz</code> and <code>ahk_2021_uls.laz</code>.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#assessment-of-alignment-uncertainty","title":"Assessment of alignment uncertainty","text":"<p>The epochs are georeferenced, i.e., multitemporal point clouds are registered in a common coordinate reference frame. The point clouds have further been fine aligned using an ICP method on stable parts outside the rock glacier area. You may assume that the best possible alignment between the multitemporal data has been achieved.</p> <p>For change analysis, it is important to assess the uncertainty of the point cloud alignment for information on the minimum detectable change. Do this, by cutting out some stable rock surfaces outside the rock glacier and checking the cloud-to-cloud distances for these subsets. You may cut out point cloud subsets manually in CloudCompare, or define polygons (e.g., in QGIS) to extract the parts from the full point cloud in the Python workflow.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#3d-change-analysis-via-point-cloud-distance-computation","title":"3D change analysis via point cloud distance computation","text":"<p>Calculate the distance between point clouds of the two epochs using the M3C2 algorithm Lague et al., 2013. </p> <p>Think about a suitable parametrization: * Normal scale D: diameter of point neighborhood to use for normal vector computation. Hint: Aim for overall, larger-scale surface change, e.g. due to rock glacier creep or heave/thaw (instead of individual boulders). * Projection scale d: diameter of cylindrical point neighborhood to obtain average position. * Maximum cylinder depth: Maximum distance along normal at which to obtain distances to the other point cloud. * Preferred normal orientation. * Registration error ~ alignment accuracy</p> <p>Consider using the CloudCompare Wiki and py4dgeo documentation.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#change-representation-and-assessment-of-results","title":"Change representation and assessment of results","text":"<p>Visualize the obtained point cloud distances and corresponding information layers, such as the level of detection and the normal vectors representing the direction of changes.</p> <p>Prepare the result assessment for interpretation by analysts. For example, you may rasterize the different layers and create a map, e.g., of the derived surface changes in GIS software. Tip: Use the Web Map Service (WMS) of Tyrol as a basemap: https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer?, which can be implemented in QGIS (Layer &gt; add Layer &gt; add WMS/WMTS Layer).</p> <p>What are the different properties of visible changes and to which types of surface activity would you attribute them? Think of heave and subsidence processes, individual boulder movement, and rock glacier creep (cf. Zahs et al., 2019).</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#for-the-fast-ones-change-analysis-via-dem-differencing","title":"For the fast ones: Change analysis via DEM differencing","text":"<p>Generate a DEM from the point cloud of each epoch and subsequently difference them. Think about a suitable parametrization, particularly the interpolation method and neighborhood definition. Visualize the raster of change information in GIS software.</p> <p>Compare the DEM of difference values to the M3C2 distances and their directions. Where do you see differences and how does the analysis hence benefit from a full 3D method of change quantification?</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#solution","title":"Solution","text":"<p>This Exercise can be solved with Python or in CloudCompare. Find the solution for the Python approach here and for the CloudCompare/QGIS approach here.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1.html#references","title":"References","text":"<ul> <li> <p>Lague, D., Brodu, N., &amp; Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82, pp. 10-26. doi: 10.1016/j.isprsjprs.2013.04.009.</p> </li> <li> <p>Zahs, V., H\u00e4mmerle, M., Anders, K., Hecht, S., Sailer, R., Rutzinger, M., Williams, J. G., &amp; H\u00f6fle, B. (2019). Multi-temporal 3D point cloud-based quantification and analysis of geomorphological activity at an alpine rock glacier using airborne and terrestrial LiDAR. Permafrost and Periglacial Processes, 30 (3), pp. 222-238. doi: 10.1002/ppp.2004.</p> </li> </ul>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html","title":"M3 theme3 exercise1 solution1","text":"Metadata      title: \"E-TRAINEE Exercise for 3D point cloud change analysis\"     description: \"This is an exercise in the third theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2022-08     authors: Mark Searle     contributors: Katharina Anders, Bernhard H\u00f6fle      estimatedTime: 1.5 hrs  In\u00a0[1]: Copied! <pre># import required modules\nimport os\nimport numpy as np\nimport py4dgeo\n\n# add the script assets folder to the path, so that we can import the functions created there\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))\n\n# import point cloud functions\nimport pointcloud_functions as pcfuncs\n</pre> # import required modules import os import numpy as np import py4dgeo  # add the script assets folder to the path, so that we can import the functions created there import sys from pathlib import Path sys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))  # import point cloud functions import pointcloud_functions as pcfuncs <p>Specify the path to the data and names of input files, and read in the point cloud data:</p> In\u00a0[2]: Copied! <pre># specify the path to the data directory\ndata_dir ='path-to-data' # /ahk\nif not os.path.isdir(data_dir):\n    print(f'Directory with data not found: {data_dir}')\n    print('Please check the specified path')\n</pre> # specify the path to the data directory data_dir ='path-to-data' # /ahk if not os.path.isdir(data_dir):     print(f'Directory with data not found: {data_dir}')     print('Please check the specified path') In\u00a0[3]: Copied! <pre># derive the file paths and read in the data\npc_file_2020 = f'{data_dir}/ahk_2020_uls.laz'\npc_file_2021 = f'{data_dir}/ahk_2021_uls.laz'\n\n# read the point clouds into numpy arrays\npc_2020 = pcfuncs.read_las(pc_file_2020)\npc_2021 = pcfuncs.read_las(pc_file_2021)\n</pre> # derive the file paths and read in the data pc_file_2020 = f'{data_dir}/ahk_2020_uls.laz' pc_file_2021 = f'{data_dir}/ahk_2021_uls.laz'  # read the point clouds into numpy arrays pc_2020 = pcfuncs.read_las(pc_file_2020) pc_2021 = pcfuncs.read_las(pc_file_2021) <p>We check the alignment uncertainty by cutting out some stable rock surfaces outside the rock glacier. In this solution, we have derived polygons of stable parts for one point cloud epoch (<code>ahk_2021_uls_stableparts.shp</code>). Using this, we can extract the points within stable parts (2D query) and subsequently derive the point cloud distances between epochs.</p> In\u00a0[4]: Copied! <pre># specify input shapefile\nshp_file = f'{data_dir}/ahk_2021_uls_stableparts.shp'\n\n# import modules for vector/shapefile processing\nimport geopandas as gpd\nimport shapely\nfrom shapely.geometry import Point, Polygon\n\n# read the shapefile \ndatasource = gpd.read_file(shp_file)\npolygons = gpd.GeoSeries(datasource['geometry'])\n\n# to speed up the polygon selection, we make a coarse subsetting based on the XY bounding box of the polygons\nminy = polygons.bounds['miny'].values[0]\nmaxy = polygons.bounds['maxy'].values[0]\nminx = polygons.bounds['minx'].values[0]\nmaxx = polygons.bounds['maxx'].values[0]\npc_2020_subs = pc_2020[(pc_2020[:,1]&gt;=miny) &amp; (pc_2020[:,1]&lt;=maxy) &amp; (pc_2020[:,0]&gt;=minx) &amp; (pc_2020[:,0]&lt;=maxx)]\npc_2021_subs = pc_2021[(pc_2021[:,1]&gt;=miny) &amp; (pc_2021[:,1]&lt;=maxy) &amp; (pc_2021[:,0]&gt;=minx) &amp; (pc_2021[:,0]&lt;=maxx)]\n\nprint(f'Remaining points: {pc_2020_subs.shape[0]}/{pc_2020.shape[0]} for 2020; {pc_2021_subs.shape[0]}/{pc_2021.shape[0]} for 2021')\n</pre> # specify input shapefile shp_file = f'{data_dir}/ahk_2021_uls_stableparts.shp'  # import modules for vector/shapefile processing import geopandas as gpd import shapely from shapely.geometry import Point, Polygon  # read the shapefile  datasource = gpd.read_file(shp_file) polygons = gpd.GeoSeries(datasource['geometry'])  # to speed up the polygon selection, we make a coarse subsetting based on the XY bounding box of the polygons miny = polygons.bounds['miny'].values[0] maxy = polygons.bounds['maxy'].values[0] minx = polygons.bounds['minx'].values[0] maxx = polygons.bounds['maxx'].values[0] pc_2020_subs = pc_2020[(pc_2020[:,1]&gt;=miny) &amp; (pc_2020[:,1]&lt;=maxy) &amp; (pc_2020[:,0]&gt;=minx) &amp; (pc_2020[:,0]&lt;=maxx)] pc_2021_subs = pc_2021[(pc_2021[:,1]&gt;=miny) &amp; (pc_2021[:,1]&lt;=maxy) &amp; (pc_2021[:,0]&gt;=minx) &amp; (pc_2021[:,0]&lt;=maxx)]  print(f'Remaining points: {pc_2020_subs.shape[0]}/{pc_2020.shape[0]} for 2020; {pc_2021_subs.shape[0]}/{pc_2021.shape[0]} for 2021') <pre>Remaining points: 8497831/37426631 for 2020; 10759711/45662285 for 2021\n</pre> <p>In the next part, we check each remaining point for its position in the polygon/stable area. You may take a coffee break while it is running!</p> In\u00a0[5]: Copied! <pre># create boolean array to mask points (initialize all entries as False)\npc_mask_2020 = np.full(len(pc_2020_subs), fill_value=False, dtype=bool)\npc_mask_2021 = np.full(len(pc_2021_subs), fill_value=False, dtype=bool)\n\nprint('Mask stable parts via polygons for 2020 points...')\n# loop over xy coordinates of all points\nfor pt_id, (x,y) in enumerate(pc_2020_subs[:,:2]):\n    point=Point([x,y])\n    \n    # check distance of this point to all polygons\n    for poly in polygons:\n        distance= poly.distance(point)\n        # set mask to True, if point is within polygon\n        if distance &lt;= 0.0:\n            pc_mask_2020[pt_id] = True \n            break # stop polygon checks as soon as point lies within one of them\n            \npc_stable_2020 = pc_2020_subs[pc_mask_2020]\n\nprint('Mask stable parts via polygons for 2021 points...')\n# loop over xy coordinates of all points\nfor pt_id, (x,y) in enumerate(pc_2021_subs[:,:2]):\n    point=Point([x,y])\n    \n    # check distance of this point to all polygons\n    for poly in polygons:\n        distance= poly.distance(point)\n        # set mask to True, if point is within polygon\n        if distance &lt;= 0.0:\n            pc_mask_2021[pt_id] = True\n            break # stop polygon checks as soon as point lies within one of them\n            \npc_stable_2021 = pc_2021_subs[pc_mask_2021]\n\nprint(f'Remaining points: {pc_stable_2020.shape[0]}/{pc_2020.shape[0]} for 2020; {pc_stable_2021.shape[0]}/{pc_2021.shape[0]} for 2021')\n</pre> # create boolean array to mask points (initialize all entries as False) pc_mask_2020 = np.full(len(pc_2020_subs), fill_value=False, dtype=bool) pc_mask_2021 = np.full(len(pc_2021_subs), fill_value=False, dtype=bool)  print('Mask stable parts via polygons for 2020 points...') # loop over xy coordinates of all points for pt_id, (x,y) in enumerate(pc_2020_subs[:,:2]):     point=Point([x,y])          # check distance of this point to all polygons     for poly in polygons:         distance= poly.distance(point)         # set mask to True, if point is within polygon         if distance &lt;= 0.0:             pc_mask_2020[pt_id] = True              break # stop polygon checks as soon as point lies within one of them              pc_stable_2020 = pc_2020_subs[pc_mask_2020]  print('Mask stable parts via polygons for 2021 points...') # loop over xy coordinates of all points for pt_id, (x,y) in enumerate(pc_2021_subs[:,:2]):     point=Point([x,y])          # check distance of this point to all polygons     for poly in polygons:         distance= poly.distance(point)         # set mask to True, if point is within polygon         if distance &lt;= 0.0:             pc_mask_2021[pt_id] = True             break # stop polygon checks as soon as point lies within one of them              pc_stable_2021 = pc_2021_subs[pc_mask_2021]  print(f'Remaining points: {pc_stable_2020.shape[0]}/{pc_2020.shape[0]} for 2020; {pc_stable_2021.shape[0]}/{pc_2021.shape[0]} for 2021')  <pre>Mask stable parts via polygons for 2020 points...\nMask stable parts via polygons for 2021 points...\nRemaining points: 330254/37426631 for 2020; 409231/45662285 for 2021\n</pre> <p>Create a kd tree and compute the distances:</p> In\u00a0[6]: Copied! <pre># import module to build kdtree\nfrom scipy.spatial import KDTree\n\n# create a 2D kd-tree from the point cloud\ntree2021 = KDTree(pc_stable_2021[:,:3])\n\n# query indices of nearest neighbors of 2009 coordinates in 2017 kd-tree\nnn_dists = tree2021.query(pc_stable_2020[:,:3], k=1)\n\n# obtain distances as first element in tuple returned by query above\ndistances = nn_dists[0]\n</pre> # import module to build kdtree from scipy.spatial import KDTree  # create a 2D kd-tree from the point cloud tree2021 = KDTree(pc_stable_2021[:,:3])  # query indices of nearest neighbors of 2009 coordinates in 2017 kd-tree nn_dists = tree2021.query(pc_stable_2020[:,:3], k=1)  # obtain distances as first element in tuple returned by query above distances = nn_dists[0] <p>We assess the distances visually, and derive the statistics:</p> In\u00a0[7]: Copied! <pre># import plotting module\nimport matplotlib.pyplot as plt\n\n# allow interactive rotation in notebook\n%matplotlib inline\n\n# create a figure with 3D axis\nfig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))\n\nnth=100\n\n# plot the point cloud colored by height (z values)\ns = ax.scatter(pc_stable_2020[::nth,0], pc_stable_2020[::nth,1], pc_stable_2020[::nth,2], s=1, c=distances[::nth])\n\n# label axes and add title\nax.set_xlabel('X [m]')\nax.set_ylabel('Y [m]')\nax.set_zlabel('Z [m]')\n\n# set initial view of 3D plot\nax.view_init(elev=40., azim=130.)\n\n# add a colorbar\nfig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.2)\n\n# show the plot\nplt.tight_layout()\nplt.show()\n</pre> # import plotting module import matplotlib.pyplot as plt  # allow interactive rotation in notebook %matplotlib inline  # create a figure with 3D axis fig, ax = plt.subplots(1,1,subplot_kw={\"projection\": \"3d\"},figsize=(7,5))  nth=100  # plot the point cloud colored by height (z values) s = ax.scatter(pc_stable_2020[::nth,0], pc_stable_2020[::nth,1], pc_stable_2020[::nth,2], s=1, c=distances[::nth])  # label axes and add title ax.set_xlabel('X [m]') ax.set_ylabel('Y [m]') ax.set_zlabel('Z [m]')  # set initial view of 3D plot ax.view_init(elev=40., azim=130.)  # add a colorbar fig.colorbar(s, shrink=.5, aspect=10, label='NN distance [m]', ax = ax, pad=0.2)  # show the plot plt.tight_layout() plt.show() In\u00a0[8]: Copied! <pre># print statistics of nn distances\nprint(f'Median distances: {np.median(distances):.3f} m')\nprint(f'Std. dev. of distances: {np.std(distances):.3f} m')\n</pre> # print statistics of nn distances print(f'Median distances: {np.median(distances):.3f} m') print(f'Std. dev. of distances: {np.std(distances):.3f} m') <pre>Median distances: 0.060 m\nStd. dev. of distances: 0.151 m\n</pre> <p>The distances are quite large according to the statistics. We would assume lower values for the (visual) quality of the co-registration and the measurement accuracy of UAV laser scanning in this scene. An explanation for the large values could be the influence of point sampling, so you see here an effect of the drawbacks in simple cloud-to-cloud (C2C) distance computation. For a potentially more robust result, we derive the M3C2 distances in these stable parts (for a detailed solution how to establish the M3C2 change analysis, see the remainder of the solution).</p> In\u00a0[9]: Copied! <pre># load the stable parts as py4dgeo Epochs\nepoch2020_stable = py4dgeo.epoch.as_epoch(pc_stable_2020)\nepoch2021_stable = py4dgeo.epoch.as_epoch(pc_stable_2021)\n\n# instantiate the M3C2 algorithm object, using all points of 2020 as corepoints (no subsampling)\nm3c2 = py4dgeo.M3C2(\n    epochs=(epoch2020_stable, epoch2021_stable),\n    corepoints=epoch2020_stable.cloud[::],\n    normal_radii=(0.5,),\n    cyl_radii=(0.5,),\n    max_distance=(5.0),\n    registration_error=(0.0)\n)\n\n# run the distance computation\nm3c2_distances_stableparts, uncertainties_stableparts = m3c2.run()\n\n# print statistics of nn distances\nprint(f'Median M3C2 distances: {np.nanmedian(m3c2_distances_stableparts):.3f} m')\nprint(f'Std. dev. of M3C2 distances: {np.nanstd(m3c2_distances_stableparts):.3f} m')\n</pre> # load the stable parts as py4dgeo Epochs epoch2020_stable = py4dgeo.epoch.as_epoch(pc_stable_2020) epoch2021_stable = py4dgeo.epoch.as_epoch(pc_stable_2021)  # instantiate the M3C2 algorithm object, using all points of 2020 as corepoints (no subsampling) m3c2 = py4dgeo.M3C2(     epochs=(epoch2020_stable, epoch2021_stable),     corepoints=epoch2020_stable.cloud[::],     normal_radii=(0.5,),     cyl_radii=(0.5,),     max_distance=(5.0),     registration_error=(0.0) )  # run the distance computation m3c2_distances_stableparts, uncertainties_stableparts = m3c2.run()  # print statistics of nn distances print(f'Median M3C2 distances: {np.nanmedian(m3c2_distances_stableparts):.3f} m') print(f'Std. dev. of M3C2 distances: {np.nanstd(m3c2_distances_stableparts):.3f} m') <pre>[2023-06-08 11:46:43][INFO] Initializing Epoch object from given point cloud\n[2023-06-08 11:46:43][INFO] Initializing Epoch object from given point cloud\n[2023-06-08 11:46:43][INFO] Building KDTree structure with leaf parameter 10\n[2023-06-08 11:46:44][INFO] Building KDTree structure with leaf parameter 10\nMedian M3C2 distances: -0.013 m\nStd. dev. of M3C2 distances: 0.037 m\n</pre> <p>As the M3C2 distances provide a more robust etimate of point distances in the stable parts, we use their standard deviation as measure of alignment accuracy in our change analysis. This measure we can later on use as registration error for deriving the level of detection.</p> In\u00a0[10]: Copied! <pre>reg_2020_2021 = np.nanstd(m3c2_distances_stableparts)\nprint(f'Registration error based on point distances in stable parts is {reg_2020_2021:.3f} m.')\n</pre> reg_2020_2021 = np.nanstd(m3c2_distances_stableparts) print(f'Registration error based on point distances in stable parts is {reg_2020_2021:.3f} m.') <pre>Registration error based on point distances in stable parts is 0.037 m.\n</pre> <p>This value already provides as an estimate of change magnitudes which can be confidently derived from point cloud comparison of these two epochs. In the M3C2 distance calculation, we will derive the per-point level of detection considering the local (i.e. spatially variable) roughness and point density.</p> <p>By the way, we can free up some working memory by removing the point cloud arrays here. In the following, we continue with <code>py4dgeo</code> and therefore load the data into (new) <code>Epoch</code>objects.</p> In\u00a0[11]: Copied! <pre>pc_2020 = None\npc_2021 = None\npc_2020_subs = None\npc_2021_subs = None\npc_stable_2020 = None\npc_stable_2021 = None\n</pre> pc_2020 = None pc_2021 = None pc_2020_subs = None pc_2021_subs = None pc_stable_2020 = None pc_stable_2021 = None <p>To perform the M3C2 change analyis with py4dgeo, we need to read the data into <code>Epoch</code> objects. This can be done from the <code>Nx3</code> array of coordinates using the function <code>py4dgeo.epoch.as_epoch()</code>, or directly from file using <code>py4dgeo.read_from_las()</code>.</p> In\u00a0[12]: Copied! <pre># load epochs into py4dgeo objects\nepoch2020 = py4dgeo.read_from_las(pc_file_2020)\nepoch2021 = py4dgeo.read_from_las(pc_file_2021)\n</pre> # load epochs into py4dgeo objects epoch2020 = py4dgeo.read_from_las(pc_file_2020) epoch2021 = py4dgeo.read_from_las(pc_file_2021) <pre>[2023-06-08 11:46:48][INFO] Reading point cloud from file 'D:\\data_etrainee\\module3\\ahk/ahk_2020_uls.laz'\n[2023-06-08 11:46:52][INFO] Reading point cloud from file 'D:\\data_etrainee\\module3\\ahk/ahk_2021_uls.laz'\n</pre> <p>As core points, we use a subsampling of coordinates in the epoch of 2020 (which will be the reference for deriving M3C2 distances to the later epoch of 2021), e.g. using every 100th point.</p> In\u00a0[13]: Copied! <pre># use every nth point as core point for distance calculation\ncorepoints = epoch2020.cloud[::100]\n</pre> # use every nth point as core point for distance calculation corepoints = epoch2020.cloud[::100] <p>We parametrize the M3C2 algorithm object according to the parameters in Zahs et al., 2019, and use the registration error we derived above. The default normal orientation in <code>py4dgeo</code> is upwards (``), so we do not need to adapt/set this parameter.</p> In\u00a0[14]: Copied! <pre># instantiate and parametrize the M3C2 algorithm object\nm3c2 = py4dgeo.M3C2(\n    epochs=(epoch2020, epoch2021),\n    corepoints=corepoints,\n    normal_radii=(4.0, 0.5, 6.0),\n    cyl_radii=(0.5,),\n    max_distance=(15.0),\n    registration_error=(reg_2020_2021)\n)\n</pre> # instantiate and parametrize the M3C2 algorithm object m3c2 = py4dgeo.M3C2(     epochs=(epoch2020, epoch2021),     corepoints=corepoints,     normal_radii=(4.0, 0.5, 6.0),     cyl_radii=(0.5,),     max_distance=(15.0),     registration_error=(reg_2020_2021) ) <p>Now the analysis can be run - and another small break would be suitable for the waiting time ;-)</p> In\u00a0[15]: Copied! <pre>m3c2_distances, uncertainties = m3c2.run()\n</pre> m3c2_distances, uncertainties = m3c2.run() <pre>[2023-06-08 11:47:00][INFO] Building KDTree structure with leaf parameter 10\n[2023-06-08 11:47:23][INFO] Building KDTree structure with leaf parameter 10\n</pre> <p>All change information (distances and uncertainties) are now contained in the returned objects.</p> <p>Using the level of detection computed by the M3C2, we derive for each point if the quantified change is significant:</p> In\u00a0[16]: Copied! <pre># create a binary mask of significant change\nchange_sign = np.where(abs(m3c2_distances) &gt; uncertainties['lodetection'], True, False)\n</pre> # create a binary mask of significant change change_sign = np.where(abs(m3c2_distances) &gt; uncertainties['lodetection'], True, False) <p>Now we create a composite figure of the change values (distances), change directions (vertical component), level of detection and significant change:</p> In\u00a0[17]: Copied! <pre># create the figure\nfig, axs = plt.subplots(2, 2, figsize=(14,14), subplot_kw={\"projection\": \"3d\"})\n(ax1,ax2),(ax3,ax4) = axs\n\n# plot the distances\nd = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-5.0, vmax=5.0, s=1) \nplt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)\n\n# plot the directions\ndirections = m3c2.directions()\ndz = ax2.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=directions[:,2], cmap='cool', s=1) \nplt.colorbar(dz, format=('%.2f'), label='Normal Z [0,1]', ax=ax2, shrink=.5, pad=.15)\n\n# plot the level of detection values\nl = ax3.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=uncertainties['lodetection'], cmap='viridis', vmax=0.15, s=1) \nplt.colorbar(l, format=('%.2f'), label='Level of detection [m]', ax=ax3, extend='max', shrink=.5, pad=.15)\n\n# plot the significant change values (boolean)\n# ax4.scatter(corepoints[~change_sign][:,0], corepoints[~change_sign][:,1], corepoints[~change_sign][:,2], label='Not significant change', c='blue', s=1) # if added, visibility of significant change areas is poor\nax4.scatter(corepoints[change_sign][:,0], corepoints[change_sign][:,1], corepoints[change_sign][:,2], label='Significant change', c='red', s=1) \nax4.legend()\n\n# add plot elements\nfor ax_set in axs:\n    for ax in ax_set:\n        ax.set_xlabel('Easting [m]')\n        ax.set_ylabel('Northing [m]')\n        ax.set_aspect('equal')\n        ax.view_init(elev=30., azim=120.)\n\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> # create the figure fig, axs = plt.subplots(2, 2, figsize=(14,14), subplot_kw={\"projection\": \"3d\"}) (ax1,ax2),(ax3,ax4) = axs  # plot the distances d = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-5.0, vmax=5.0, s=1)  plt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)  # plot the directions directions = m3c2.directions() dz = ax2.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=directions[:,2], cmap='cool', s=1)  plt.colorbar(dz, format=('%.2f'), label='Normal Z [0,1]', ax=ax2, shrink=.5, pad=.15)  # plot the level of detection values l = ax3.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=uncertainties['lodetection'], cmap='viridis', vmax=0.15, s=1)  plt.colorbar(l, format=('%.2f'), label='Level of detection [m]', ax=ax3, extend='max', shrink=.5, pad=.15)  # plot the significant change values (boolean) # ax4.scatter(corepoints[~change_sign][:,0], corepoints[~change_sign][:,1], corepoints[~change_sign][:,2], label='Not significant change', c='blue', s=1) # if added, visibility of significant change areas is poor ax4.scatter(corepoints[change_sign][:,0], corepoints[change_sign][:,1], corepoints[change_sign][:,2], label='Significant change', c='red', s=1)  ax4.legend()  # add plot elements for ax_set in axs:     for ax in ax_set:         ax.set_xlabel('Easting [m]')         ax.set_ylabel('Northing [m]')         ax.set_aspect('equal')         ax.view_init(elev=30., azim=120.)  plt.axis('equal') plt.tight_layout() plt.show() <p>For further analysis and visualization in external software, we store the result in one point cloud as laz file, adding all the attributes of the change analysis result:</p> In\u00a0[18]: Copied! <pre># define path for the las file\npc_m3c2 = f'{data_dir}/ahk_2020_2021_m3c2.laz'\n\n# create a dictionary of attributes to store with the point cloud\nattr = {'m3c2_distance':m3c2_distances,\n        'level_of_detection':uncertainties['lodetection'],\n        'significant_change':change_sign.astype(int),\n        'NormalX':directions[:,0], 'NormalY':directions[:,1], 'NormalZ':directions[:,2]\n        }\n\n# use las write function to write file\npcfuncs.write_las(corepoints, pc_m3c2, attribute_dict=attr)\n</pre> # define path for the las file pc_m3c2 = f'{data_dir}/ahk_2020_2021_m3c2.laz'  # create a dictionary of attributes to store with the point cloud attr = {'m3c2_distance':m3c2_distances,         'level_of_detection':uncertainties['lodetection'],         'significant_change':change_sign.astype(int),         'NormalX':directions[:,0], 'NormalY':directions[:,1], 'NormalZ':directions[:,2]         }  # use las write function to write file pcfuncs.write_las(corepoints, pc_m3c2, attribute_dict=attr)  <p>To create raster layers for further analysis and visualization in a GIS, we use PDAL (cf. Theme 2)</p> In\u00a0[19]: Copied! <pre>import pdal\n\n# replacing backslashes by forward slashes is required for some Windows paths\npc_m3c2 = pc_m3c2.replace(\"\\\\\", \"/\")\n\n# define the raster file names based on the input file name\nraster_distance = pc_m3c2.replace(\".laz\", \"_distances.tif\")\nraster_lodet = pc_m3c2.replace(\".laz\", \"_lodetection.tif\")\n\n# define the pdal pipeline as json, \"dimension\" allows us to define the attribute to be rasterized (default: \"Z\" value / elevation)\njson_rast = \"\"\"[\n    \"%s\",\n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":1.0,\n        \"dimension\":\"%s\",\n        \"window_size\":8\n    }\n]\"\"\"\n\n# execute the pipeline for the distance raster\njson_dist = json_rast % (pc_m3c2, raster_distance, \"m3c2_distance\")\npipeline = pdal.Pipeline(json_dist)\nexe = pipeline.execute()\n\n# execute the pipeline for the lodetection raster\njson_lodet = json_rast % (pc_m3c2, raster_lodet, \"level_of_detection\")\npipeline = pdal.Pipeline(json_lodet)\nexe = pipeline.execute()\n</pre> import pdal  # replacing backslashes by forward slashes is required for some Windows paths pc_m3c2 = pc_m3c2.replace(\"\\\\\", \"/\")  # define the raster file names based on the input file name raster_distance = pc_m3c2.replace(\".laz\", \"_distances.tif\") raster_lodet = pc_m3c2.replace(\".laz\", \"_lodetection.tif\")  # define the pdal pipeline as json, \"dimension\" allows us to define the attribute to be rasterized (default: \"Z\" value / elevation) json_rast = \"\"\"[     \"%s\",     {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":1.0,         \"dimension\":\"%s\",         \"window_size\":8     } ]\"\"\"  # execute the pipeline for the distance raster json_dist = json_rast % (pc_m3c2, raster_distance, \"m3c2_distance\") pipeline = pdal.Pipeline(json_dist) exe = pipeline.execute()  # execute the pipeline for the lodetection raster json_lodet = json_rast % (pc_m3c2, raster_lodet, \"level_of_detection\") pipeline = pdal.Pipeline(json_lodet) exe = pipeline.execute() <p>You can now create useful raster-based map visualizations as well, e.g. in QGIS. In the corresponding solution video you can see how this is done.</p> Raster-based map visualizations with Tyrol basemap: Level of detection (left) and M3C2 distances (right)(https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer).  <p>Compare the DEM of difference values to the M3C2 distances and their directions.</p> <ul> <li>Where do the derived changes differ mostly due to the vertical vs. 3D approach to change analysis?</li> <li>How does the analysis hence benefit from a full 3D method of change quantification?</li> </ul> In\u00a0[20]: Copied! <pre>import pdal\n\n# define the point cloud paths for both epochs\npc_file_2020 = f'{data_dir}/ahk_2020_uls.laz'\npc_file_2021 = f'{data_dir}/ahk_2021_uls.laz'\n\n# replacing backslashes by forward slashes is required for some Windows paths\npc_file_2020 = pc_file_2020.replace(\"\\\\\", \"/\")\npc_file_2021 = pc_file_2021.replace(\"\\\\\", \"/\")\n\n\n# define the DEM raster file name based on the input file name\nDEM_2020 = pc_file_2020.replace(\".laz\", \"_DEM.tif\")\nDEM_2021 = pc_file_2021.replace(\".laz\", \"_DEM.tif\")\n</pre> import pdal  # define the point cloud paths for both epochs pc_file_2020 = f'{data_dir}/ahk_2020_uls.laz' pc_file_2021 = f'{data_dir}/ahk_2021_uls.laz'  # replacing backslashes by forward slashes is required for some Windows paths pc_file_2020 = pc_file_2020.replace(\"\\\\\", \"/\") pc_file_2021 = pc_file_2021.replace(\"\\\\\", \"/\")   # define the DEM raster file name based on the input file name DEM_2020 = pc_file_2020.replace(\".laz\", \"_DEM.tif\") DEM_2021 = pc_file_2021.replace(\".laz\", \"_DEM.tif\") <p>We start calculating the DEM from the 2020 pointcloud with the following pipeline:</p> In\u00a0[21]: Copied! <pre>json_DEM = \"\"\"[\n    \"%s\",\n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":1.0,\n        \"window_size\":8\n    }\n]\"\"\"% (pc_file_2020, DEM_2020)  \n</pre> json_DEM = \"\"\"[     \"%s\",     {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":1.0,         \"window_size\":8     } ]\"\"\"% (pc_file_2020, DEM_2020)   In\u00a0[22]: Copied! <pre>pipeline = pdal.Pipeline(json_DEM)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_DEM) exe = pipeline.execute() <p>Next we calculate the dimensions of the 2020 DEM raster, so the 2021 DEM raster can be adapted accordingly.</p> In\u00a0[23]: Copied! <pre>import rasterio as rio\n\nwith rio.open(DEM_2020) as src:\n    DEM_2020_data = src.read(1, masked=True)\n    width = src.width\n    height = src.height\n    \nwith rio.open(DEM_2020) as src:\n    DEM_2020_data = src.read(1, masked=True)\n    DEM_2020_tf = src.transform\n    DEM_2020_bounds = src.bounds\n    DEM_meta = src.profile\n    width= src.width\n    height = src.height\n    \norigin_left, origin_bottom, origin_right, origin_top = DEM_2020_bounds\nDEM_width = DEM_meta['width']\nDEM_height = DEM_meta['height']\n    \n</pre> import rasterio as rio  with rio.open(DEM_2020) as src:     DEM_2020_data = src.read(1, masked=True)     width = src.width     height = src.height      with rio.open(DEM_2020) as src:     DEM_2020_data = src.read(1, masked=True)     DEM_2020_tf = src.transform     DEM_2020_bounds = src.bounds     DEM_meta = src.profile     width= src.width     height = src.height      origin_left, origin_bottom, origin_right, origin_top = DEM_2020_bounds DEM_width = DEM_meta['width'] DEM_height = DEM_meta['height']      <p>Next we create the DEM from the 2021 pointcloud, with the dimension parameters we just calculated.</p> In\u00a0[24]: Copied! <pre>json_DEM = \"\"\"[\n    \"%s\",\n    {\n        \"type\":\"writers.gdal\",\n        \"filename\": \"%s\",\n        \"output_type\":\"min\",\n        \"gdaldriver\":\"GTiff\",\n        \"resolution\":1.0,\n        \"window_size\":8,\n        \"origin_x\":\"%.3f\",\n        \"origin_y\":\"%.3f\",\n        \"width\":\"%i\",\n        \"height\":\"%i\"\n    }\n]\"\"\"% (pc_file_2021, DEM_2021, origin_left, origin_bottom, DEM_width, DEM_height)\n</pre> json_DEM = \"\"\"[     \"%s\",     {         \"type\":\"writers.gdal\",         \"filename\": \"%s\",         \"output_type\":\"min\",         \"gdaldriver\":\"GTiff\",         \"resolution\":1.0,         \"window_size\":8,         \"origin_x\":\"%.3f\",         \"origin_y\":\"%.3f\",         \"width\":\"%i\",         \"height\":\"%i\"     } ]\"\"\"% (pc_file_2021, DEM_2021, origin_left, origin_bottom, DEM_width, DEM_height) In\u00a0[25]: Copied! <pre>pipeline = pdal.Pipeline(json_DEM)\nexe = pipeline.execute()\n</pre> pipeline = pdal.Pipeline(json_DEM) exe = pipeline.execute() In\u00a0[26]: Copied! <pre>with rio.open(DEM_2021) as src:\n    DEM_2021_data = src.read(1, masked=True)\n</pre> with rio.open(DEM_2021) as src:     DEM_2021_data = src.read(1, masked=True) <p>Now we can calculate the DEM of difference (This can be done in QGIS as well):</p> In\u00a0[27]: Copied! <pre>DEM_diff = DEM_2021_data - DEM_2020_data\n</pre> DEM_diff = DEM_2021_data - DEM_2020_data <p>Save the created DEM of difference to a file so you can have a look at it in QGIS.</p> In\u00a0[28]: Copied! <pre>output_path = 'D:/data_etrainee/module3/ahk/DEM_diff.tif'\n\nwith rio.open(output_path, 'w', **DEM_meta) as ff:\n    ff.write(DEM_diff, 1)\n</pre> output_path = 'D:/data_etrainee/module3/ahk/DEM_diff.tif'  with rio.open(output_path, 'w', **DEM_meta) as ff:     ff.write(DEM_diff, 1) <p>The visualized raster should look something like this:</p> Change analysis via DEM differencing: Vertical change between 2020 and 2021. The basemap is provided by the Web Map Service of Tyrol (https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer)."},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#exercise-3d-change-analysis-at-an-active-rock-glacier-using-multitemporal-point-clouds","title":"Exercise: 3D Change Analysis at an Active Rock Glacier using Multitemporal Point Clouds\u00b6","text":"<p>In this exercise, you will perform a surface change analysis on TLS point clouds of the \u00c4u\u00dferes Hochebenkar rock glacier (46\u00b050\u201911''N, 11\u00b000\u201920\u2018\u2019E) for two consecutive years. See the introduction to the case study and dataset here.</p> <p>The objective is to perform a full workflow of 3D change analysis with</p> <ul> <li>Assessment of alignment uncertainty</li> <li>Change analysis using the M3C2 algorithm</li> <li>Change representation and assessment of results</li> <li>For the fast ones: Comparison to change analysis via DEM differencing</li> </ul> <p>Look into the article by Zahs et al., 2019 for comparison of possible surface dynamics at the site and help for deciding on suitable parameters, etc.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#software-and-data","title":"Software and data\u00b6","text":"<p>This exercise can be solved fully in CloudCompare or using Python with the <code>py4dgeo</code> library. If you have the Python skills, we recommend to work on the exercise with Python in a Jupyter notebook, hence obtaining a sharable and reproducible workflow.</p> <p>Use CloudCompare and GIS Software (e.g., QGIS) to check and visualize your results.</p> <p>In any case, make use of the software documentations!</p> <p>The dataset will be two epochs of point clouds acquired by UAV laser scanning in 2020 and 2021: <code>ahk/ahk_2020_uls.laz</code> and <code>ahk/ahk_2021_uls.laz</code>.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#solution","title":"Solution\u00b6","text":"<p>This solution uses Python with the <code>py4dgeo</code> library.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#assessment-of-alignment-uncertainty","title":"Assessment of alignment uncertainty\u00b6","text":"<p>The epochs are georeferenced, i.e., multitemporal point clouds are registered in a common coordinate reference frame. The point clouds have further been fine aligned using an ICP method on stable parts outside the rock glacier area. You may assume that the best possible alignment between the multitemporal data has been achieved.</p> <p>For change analysis, it is important to assess the uncertainty of the point cloud alignment for information on the minimum detectable change. Do this, by cutting out some stable rock surfaces outside the rock glacier and checking the cloud-to-cloud distances for these subsets. You may cut out point cloud subsets manually in CloudCompare, or define polygons (e.g., in QGIS) to extract the parts from the full point cloud in the Python workflow.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#3d-change-analysis-via-point-cloud-distance-computation","title":"3D change analysis via point cloud distance computation\u00b6","text":"<p>Calculate the distance between point clouds of the two epochs using the M3C2 algorithm (Lague et al., 2013).</p> <p>Think about a suitable parametrization:</p> <ul> <li>Normal scale D: diameter of point neighborhood to use for normal vector computation. Hint: Aim for overall, larger-scale surface change, e.g. due to rock glacier creep or heave/thaw (instead of individual boulders).</li> <li>Projection scale d: diameter of cylindrical point neighborhood to obtain average position.</li> <li>Maximum cylinder depth: Maximum distance along normal at which to obtain distances to the other point cloud.</li> <li>Preferred normal orientation.</li> <li>Registration error ~ alignment accuracy</li> </ul> <p>Consider using the py4dgeo documentation.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#change-representation-and-assessment-of-results","title":"Change representation and assessment of results\u00b6","text":"<p>Visualize the obtained point cloud distances and corresponding information layers, such as the level of detection and the normal vectors representing the direction of changes.</p> <p>Prepare the result assessment for interpretation by analysts. For example, you may rasterize the different layers and create a map, e.g., of the derived surface changes in GIS software. Tip: Use the Web Map Service (WMS) of Tyrol as a basemap: https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer.</p> <p>What are the different properties of visible changes and to which types of surface activity would you attribute them? Think of heave and subsidence processes, individual boulder movement, and rock glacier creep (cf. Zahs et al., 2019).</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#for-the-fast-ones-change-analysis-via-dem-differencing","title":"For the fast ones: Change analysis via DEM differencing\u00b6","text":"<p>Generate a DEM from the point cloud of each epoch and subsequently difference them. Think about a suitable parametrization, particularly the interpolation method and neighborhood definition. Visualize the raster of change information in GIS software. Check the solution video for further assistance.</p>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution1.html#references","title":"References\u00b6","text":"<ul> <li><p>Lague, D., Brodu, N., &amp; Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82, pp. 10-26. doi: 10.1016/j.isprsjprs.2013.04.009.</p> </li> <li><p>Zahs, V., H\u00e4mmerle, M., Anders, K., Hecht, S., Sailer, R., Rutzinger, M., Williams, J. G., &amp; H\u00f6fle, B. (2019). Multi-temporal 3D point cloud-based quantification and analysis of geomorphological activity at an alpine rock glacier using airborne and terrestrial LiDAR. Permafrost and Periglacial Processes, 30 (3), pp. 222-238. doi: 10.1002/ppp.2004.</p> </li> </ul>"},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution2.html","title":"Exercise: 3D Change Analysis at an Active Rock Glacier using Multitemporal Point Clouds","text":""},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution2.html#solution-using-cloudcompare","title":"Solution using CloudCompare","text":""},{"location":"module3/03_3d_change_analysis/exercise/m3_theme3_exercise1_solution2.html#change-analysis-via-dem-differencing","title":"Change analysis via DEM differencing","text":"<p>Tip: In the video we use <code>pdal</code> but in the new version of QGIS you can also rasterize point clouds directly.</p> <p>At the end you can visualize the DEM in OGIS, which should look something like this: </p> <p></p> <p>Change analysis via DEM differencing: Vertical change between 2020 and 2021. The basemap is provided by the Web Map Service of Tyrol (https://gis.tirol.gv.at/arcgis/services/Service_Public/orthofoto/MapServer/WMSServer). </p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html","title":"Lesson","text":"Metadata     title: \"E-TRAINEE Time series analysis of 3D point clouds\"     description: \"This is the fourth theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2022-01-12     authors: Katharina Anders and Bernhard H\u00f6fle     contributors:      estimatedTime: 1.5 hours  <p>An example for permanent TLS is the CoastScan project, which operates multiple permanent TLS stations for capturing dynamic surface processes in beach environments. Sandy beaches are highly dynamic areas undergoing continuous change. To observe different processes, like erosion during storms, formation and movement of sand bars, aeolian sand transport and effects of human interventions, topographic data at high spatial and temporal resolution is required. For this, a TLS was installed in a fixed position at three locations at the Dutch and Belgian coast. One of these locations is the beach at Kijkduin, The Netherlands, where hourly point clouds were acquired for six months, resulting in over 4,000 consecutive scans of the beach scene (Vos et al., 2022). The permanent TLS monitoring at Kijkduin is a case study in this course.</p> <p>Permanent laser scanning of a sandy beach at Kijkduin, The Netherlands, to capture dynamic sand transport, which can be assessed as bitemporal surface change maps or time series at each location from the hourly data. Figure by K. Anders, following Anders et al. (2019).</p> <p>An example for time-lapse photogrammetry is monitoring of rockfalls as performed by Blanch et al. (2021) at a rock face in north-east Spain. The observation of rockfall activity is highly relevant in the context of natural hazards, and spatiotemporal analysis of rockfall processes can provide new insights into, e.g. prefailure deformation, which is an important factor for the development of early warning systems (see also monitoring studies by Kromer et al., 2017 or Williams et al., 2018). In the observation system by Blanch et al. (2021), five cameras were installed in a fixed position with view of the rock face. Images were captured three times per day over a period of around two years. By recosntructing 3D point clouds for each epoch, individual rockfall events can then be detected throughout the observation period, e.g., using direct point cloud comparison.</p> <p>Time-lapse photogrammetry of rockfalls in Catalonia, north-east Spain, from five camera locations captures rockfall events at an interval of three times per day. Individual rockfall events can be derived from bitemporal point coud comparison. Figure modified from Blanch (2022) / CC BY-SA 4.0 and Blanch et al. (2021) / CC BY 4.0.</p> <p></p> <p>Coinciding rockfall events captured by hourly point clouds, which would be aggregated in change analysis using longer timespans. The spatial delineation of single events is colored by timing (see colorbar of observation period). Figure modified from Williams et al. (2019) / CC BY 4.0.</p> <p>The rockfall example shows the importance to resolve single events, here of erosion. In other settings, we may face the situation that even different types of processes act on the surface with overlap. For the example of an active rock glacier (which is a case study in this course), annual point clouds show enormous changes of the surface, but are not sufficient to attribute the observe change to the underlying processes. The rock glacier experiences overall creep (i.e. forward movement of the entire object), including deformation of the rock glacier body, heave and subsidence due to thawing, as well as individual boulder movement of the massive rocks composing the phenomenon.</p> <p>Changes at an active rock glacier derived for a one-year timespan (left) and schematic profile of possible rock glacier changes in a three-week and one-year timespan (right). Changes in the profile visualize how specific change types can only be separated at shorter timespans. For example, boulder movement can be identified in the three-week timespan, because change due to creep-related movement is lower than the associated level of detection. Figure modified from Ulrich et al. (2021) / CC BY 4.0.</p> <p>So one major advantage of dense temporal information is that it allows to disaggregate process types in space and time. When the time series information is near-continuous, it further allows as do describe the spatiotemporal properties of the observed process. Think of the example of the beach above: from bitemporal changes we can interpret that a sand bar formed within a timespan of several weeks, but the much denser (hourly) time series describes the change rate of formation, maybe even several stages of formation, the timing of highest magnitude, and when the sand bar disappeared again.</p> <p>Surface changes on the beach represented as bitemporal change map for a three-week timespan (left) and as time series of one location on a sandbar (right). The combination of spatial and temporal information allows identification and description of a specific process type from the 3D time series. Figure by K. Anders, following Anders et al. (2019).</p> <p>Beyond the increased information content for change representation, spatiotemporal information in 4D point clouds can even support change analysis itself. Why and how this can be done, will be introduced in a later section on time series-based methods of change analysis. Subsequently, we will first have a look at processing requirements of laser scanning time series, as opposed to few multitemporal 3D point clouds.</p> <p></p> <p>While effects in the measurement stability and uncertainties are more visible in near-continuous 3D data, and thereby more influencing for near-continuous change analysis, we can draw one advantage from these special data: The temporally dense acquisition can be used to leviate the effect of uncertainties through the dense spatiotemporal information at hand. Such a 4D filtering strategy has been developed by Kromer et al. (2015), with the concept of leveraging point redundancy in space and time to reduce noise and being able to detect smaller-scale changes confidently.</p> <p>Combination of spatial neighbors (left) and temporal neighbors (middle) for 4D filtering. Within the spatiotemporal neighborhood of each point in each epoch, the median values are computed, corresponding to a sliding temporal window (right). Figure modified from Kromer et al. (2015) / CC BY 4.0.</p> <p>The method is demonstrated to strongly reduce outliers and to increase the level of detection in change analysis by using the spatiotemporal average, if the sampling in space and time is dense enough. So if we have high point redundancy as in 4D point clouds, we can use this to reduce the overall uncertainty of our change analysis. Have a look at the paper by Kromer et al. (2015) to learn about this concept in more detail.</p> <p>The 4D filtering method by Kromer et al. (2015) performs averaging in space and time simultaneously. In other studies, these steps have sometimes been separated into first spatial and secondly temporal averaging or filtering. For example, spatial smoothing may already be introduced by deriving digital elevation models (DEMs), in the process of rasterizing point clouds. Thereafter, averaging of change values in DEMs of difference along the 1D time series in each raster cell yields temporal smoothing to achieve further reduction of noise. This approach is used by Eltner et al. (2017) for the analysis of soil surface changes acquired by time-lapse photogrammetry. The concept of additional temporal smoothing can also be applied to 3D time series of surface changes (rather than solely rasterized topographic changes). For example, time series of M3C2 distances Lague et al. (2013) at core point locations cf. introduction of the method in previous theme can be used to apply, e.g., median filtering to derived 3D changes (e.g., Anders et al., 2021). With the M3C2, spatial smoothing has already been introduced in the step of point cloud distance computation, where points are averaged in the 3D neighborhood around each core point. Beyond this, advanced time series filtering methods have been proposed to further reduce uncertainties through full incorporation of the temporal domain (e.g., Winiwarter et al., 2022). These will not be introduced in more detail here.</p> <p></p> <p>Pairwise observation of change types \u2018accumulation\u2019, \u2018erosion\u2019, and \u2018transport\u2019. The processes that underlie single values of local surface height change are ambiguous. Figure by K. Anders, following Anders et al. (2020).</p> <p>For example, the erosion and transport path of an avalanche would be aggregated into overall snow cover decrease, such as compaction or melting of the snow cover, if both occurred during the timespan of analyzed epochs. At the same time, the extent and magnitude of avalanche deposition may be underestimated if the process coincides with overall erosion of the surrounding surface. This example is visualized here with bitemporal change information of a three-day timespan:</p> <p>Bitemporal point cloud distances of snow-covered scene acquired by terrestrial laser scanning (TLS) within a three-day timespan, where avalanche erosion and deposition forms overlap with overall decrease of the snow cover. Figure by K. Anders, following Anders et al. (2022).</p> <p>To overcome limitations of bitemporal analysis in such cases, and to further leverage the temporal domain contained in 4D point clouds, time series-based methods are increasingly developed. In the following, we will look at two different methods of time series-based analysis of 4D point clouds with time series clustering and the extraction of 4D objects-by-change. For this, we will use a hands-on analysis example  to learn how time series-based analysis can be performed.</p> <p></p> <p>First, we start by setting up the Python environment and data:</p> In\u00a0[1]: Copied! <pre># import required modules\nimport os\nimport numpy as np\nimport py4dgeo\nfrom datetime import datetime\n\n# specify the data path\ndata_path = 'path-to-data'\n\n# check if the specified path exists\nif not os.path.isdir(data_path):\n    print(f'ERROR: {data_path} does not exist')\n    print('Please specify the correct path to the data directory by replacing &lt;path-to-data&gt; above.')\n\n# sub-directory containing the point clouds\npc_dir = os.path.join(data_path, 'pointclouds')\n\n# list of point clouds (time series)\npc_list = os.listdir(pc_dir)\npc_list[:5] # print the first elements\n</pre> # import required modules import os import numpy as np import py4dgeo from datetime import datetime  # specify the data path data_path = 'path-to-data'  # check if the specified path exists if not os.path.isdir(data_path):     print(f'ERROR: {data_path} does not exist')     print('Please specify the correct path to the data directory by replacing  above.')  # sub-directory containing the point clouds pc_dir = os.path.join(data_path, 'pointclouds')  # list of point clouds (time series) pc_list = os.listdir(pc_dir) pc_list[:5] # print the first elements Out[1]: <pre>['schneeferner_180418_120027.laz',\n 'schneeferner_180418_130027.laz',\n 'schneeferner_180418_140027.laz',\n 'schneeferner_180418_150027.laz',\n 'schneeferner_180418_160023.laz']</pre> <p>In the list of point cloud files you can see that we have one laz file per epoch available. The file name contains the timestamp of the epoch, respectively, in format <code>YYMMDD_hhmmss</code>. To use this information for our analysis, we read the timestamp information from the file names into <code>datetime</code> objects.</p> In\u00a0[2]: Copied! <pre># read the timestamps from file names\ntimestamps = []\nfor f in pc_list:\n    if not f.endswith('.laz'):\n        continue\n\n    # get the timestamp from the file name\n    timestamp_str = '_'.join(f.split('.')[0].split('_')[1:]) # yields YYMMDD_hhmmss\n\n    # convert string to datetime object\n    timestamp = datetime.strptime(timestamp_str, '%y%m%d_%H%M%S')\n    timestamps.append(timestamp)\n\ntimestamps[:5]\n</pre> # read the timestamps from file names timestamps = [] for f in pc_list:     if not f.endswith('.laz'):         continue      # get the timestamp from the file name     timestamp_str = '_'.join(f.split('.')[0].split('_')[1:]) # yields YYMMDD_hhmmss      # convert string to datetime object     timestamp = datetime.strptime(timestamp_str, '%y%m%d_%H%M%S')     timestamps.append(timestamp)  timestamps[:5] Out[2]: <pre>[datetime.datetime(2018, 4, 18, 12, 0, 27),\n datetime.datetime(2018, 4, 18, 13, 0, 27),\n datetime.datetime(2018, 4, 18, 14, 0, 27),\n datetime.datetime(2018, 4, 18, 15, 0, 27),\n datetime.datetime(2018, 4, 18, 16, 0, 23)]</pre> <p>Now we use the point cloud files and timestamp information to create a <code>SpatiotemporalAnalysis</code> object, which is the main data structure for 3D time series in <code>py4dgeo</code>. The data object is backed by an archive file (zip), which needs to be specified when instantiating the object:</p> In\u00a0[3]: Copied! <pre>analysis = py4dgeo.SpatiotemporalAnalysis(f'{data_path}/schneeferner.zip', force=True)\n</pre> analysis = py4dgeo.SpatiotemporalAnalysis(f'{data_path}/schneeferner.zip', force=True) <pre>[2023-03-29 15:15:19][INFO] Creating analysis file I:/etrainee_data/schneeferner/schneeferner.zip\n</pre> <p>The concept of the py4dgeo <code>SpatiotemporalAnalysis</code> object is to add a time series of 3D point clouds in terms of their change values to one global reference epoch. These change values are derived for a set of core points using the M3C2 algorithm, which was introduced in py4dgeo in the previous theme. With each epoch added as M3C2 distances compared to the reference epoch, we also add the timestamp to be usable in time series analysis.</p> <p>As reference epoch, we use the first epoch in our time series (i.e., list of point clouds):</p> In\u00a0[4]: Copied! <pre># specify the reference epoch\nreference_epoch_file = os.path.join(pc_dir, pc_list[0])\n\n# read the reference epoch and set the timestamp\nreference_epoch = py4dgeo.read_from_las(reference_epoch_file)\nreference_epoch.timestamp = timestamps[0]\n\n# set the reference epoch in the spatiotemporal analysis object\nanalysis.reference_epoch = reference_epoch\n</pre> # specify the reference epoch reference_epoch_file = os.path.join(pc_dir, pc_list[0])  # read the reference epoch and set the timestamp reference_epoch = py4dgeo.read_from_las(reference_epoch_file) reference_epoch.timestamp = timestamps[0]  # set the reference epoch in the spatiotemporal analysis object analysis.reference_epoch = reference_epoch <pre>[2023-03-29 15:15:19][INFO] Reading point cloud from file 'I:/etrainee_data/schneeferner\\pointclouds\\schneeferner_180418_120027.laz'\n[2023-03-29 15:15:19][INFO] Building KDTree structure with leaf parameter 10\n[2023-03-29 15:15:19][INFO] Saving epoch to file 'C:\\Users\\k53\\AppData\\Local\\Temp\\tmpohrn6dlg\\reference_epoch.zip'\n</pre> <p>For epochs to be added, we now configure the M3C2 algorithm to derive the change values. The registration error is used from Anders et al. (2022), where the average alignment accuracy of point clouds in the time series is assessed at 2.5 cm.</p> In\u00a0[5]: Copied! <pre># specify corepoints, here all points of the reference epoch\nanalysis.corepoints = reference_epoch.cloud[::]\n\n# specify M3C2 parameters\nanalysis.m3c2 = py4dgeo.M3C2(cyl_radii=(1.0,), normal_radii=(1.0,), max_distance=10.0, registration_error = 0.025)\n</pre> # specify corepoints, here all points of the reference epoch analysis.corepoints = reference_epoch.cloud[::]  # specify M3C2 parameters analysis.m3c2 = py4dgeo.M3C2(cyl_radii=(1.0,), normal_radii=(1.0,), max_distance=10.0, registration_error = 0.025) <pre>[2023-03-29 15:15:20][INFO] Initializing Epoch object from given point cloud\n[2023-03-29 15:15:20][INFO] Building KDTree structure with leaf parameter 10\n[2023-03-29 15:15:20][INFO] Saving epoch to file 'C:\\Users\\k53\\AppData\\Local\\Temp\\tmplexs_dnn\\corepoints.zip'\n</pre> <p>Now we add all the other epochs with their timestamps. Note that we do not add every single epoch using the <code>add_epochs()</code> method, but compile a list of all epochs (limited only by available RAM). Adding them as entire batches saves a lot of processing time, as the analysis object needs to be re-configured in memory for each adding operation.</p> In\u00a0[\u00a0]: hide-output Copied! <pre># create a list to collect epoch objects\nepochs = []\nfor e, pc_file in enumerate(pc_list[1:]):\n    epoch_file = os.path.join(pc_dir, pc_file)\n    epoch = py4dgeo.read_from_las(epoch_file)\n    epoch.timestamp = timestamps[e]\n    epochs.append(epoch)\n\n# add epoch objects to the spatiotemporal analysis object\nanalysis.add_epochs(*epochs)\n</pre> # create a list to collect epoch objects epochs = [] for e, pc_file in enumerate(pc_list[1:]):     epoch_file = os.path.join(pc_dir, pc_file)     epoch = py4dgeo.read_from_las(epoch_file)     epoch.timestamp = timestamps[e]     epochs.append(epoch)  # add epoch objects to the spatiotemporal analysis object analysis.add_epochs(*epochs) <p>Now we have a fully constructed spatiotemporal object, which contains the change values in the scene at each epoch, and the time series of surface changes at each core point location, along with all metadata.</p> In\u00a0[7]: Copied! <pre># print the spatiotemporal analysis data for 3 corepoints and 5 epochs, respectively\nprint(f\"Space-time distance array:\\n{analysis.distances[:3,:5]}\")\nprint(f\"Uncertainties of M3C2 distance calculation:\\n{analysis.uncertainties['lodetection'][:3, :5]}\")\nprint(f\"Timestamp deltas of analysis:\\n{analysis.timedeltas[:5]}\")\n</pre> # print the spatiotemporal analysis data for 3 corepoints and 5 epochs, respectively print(f\"Space-time distance array:\\n{analysis.distances[:3,:5]}\") print(f\"Uncertainties of M3C2 distance calculation:\\n{analysis.uncertainties['lodetection'][:3, :5]}\") print(f\"Timestamp deltas of analysis:\\n{analysis.timedeltas[:5]}\") <pre>Space-time distance array:\n[[-0.00090916 -0.0092291  -0.00523798  0.00357471 -0.01300597]\n [ 0.00472494 -0.00633247 -0.00151809  0.00078728 -0.01732864]\n [ 0.0026491  -0.01452403 -0.00742233 -0.00195866 -0.02317615]]\nUncertainties of M3C2 distance calculation:\n[[0.05939054 0.05760474 0.06653623 0.05911205 0.06040037]\n [0.06383101 0.06259084 0.06683246 0.06625019 0.06283139]\n [0.06452989 0.06335329 0.06382436 0.06584868 0.06144326]]\nTimestamp deltas of analysis:\n[datetime.timedelta(0), datetime.timedelta(seconds=3600), datetime.timedelta(seconds=7200), datetime.timedelta(seconds=10800), datetime.timedelta(seconds=14396)]\n</pre> <p>We use these elements to visualize the changes in the scene for a selected epoch, together with the time series of surface changes at a selected location. The location here was selected separately in CloudCompare (as the corepoint id). You may select your own location coordinates or, in general, use external measurements, e.g., from GNSS, to look into a location of interest.</p> In\u00a0[8]: Copied! <pre># import plotting module\nimport matplotlib.pyplot as plt\n\n# allow interactive rotation in notebook\n%matplotlib inline\n\n# create the figure\nfig=plt.figure(figsize=(12,5))\nax1=fig.add_subplot(1,2,1,projection='3d',computed_zorder=False)\nax2=fig.add_subplot(1,2,2)\n\n# get the corepoints\ncorepoints = analysis.corepoints.cloud\n\n# get change values of last epoch for all corepoints\ndistances = analysis.distances\ndistances_epoch = [d[100] for d in distances]\n\n# get the time series of changes at a specific core point locations\ncp_idx_sel = 62000\ncoord_sel = analysis.corepoints.cloud[cp_idx_sel]\ntimeseries_sel = distances[cp_idx_sel]\n\n# get the list of timestamps from the reference epoch timestamp and timedeltas\ntimestamps = [t + analysis.reference_epoch.timestamp for t in analysis.timedeltas]\n\n# plot the scene\nd = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=distances_epoch[:], cmap='seismic_r', vmin=-1.5, vmax=1.5, s=1, zorder=1) \nplt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)\n\n# add the location of the selected coordinate\nax1.scatter(coord_sel[0], coord_sel[1], coord_sel[2], c='black', s=3, zorder=2, label='Selected location')\nax1.legend()\n\nax1.set_xlabel('X [m]')\nax1.set_ylabel('Y [m]')\nax1.set_zlabel('Z [m]')\nax1.set_aspect('equal')\nax1.view_init(elev=30., azim=150.)\nax1.set_title('Changes at %s' % (analysis.reference_epoch.timestamp+analysis.timedeltas[100]))\n\n# plot the time series\nax2.plot(timestamps, timeseries_sel, color='blue')\nax2.set_xlabel('Date')\nax2.set_ylabel('Distance [m]')\nax2.grid()\nax2.set_ylim(-0.2,1.0)\nax2.set_title('Time series at selected location')\n\nplt.tight_layout()\nplt.show()\n</pre> # import plotting module import matplotlib.pyplot as plt  # allow interactive rotation in notebook %matplotlib inline  # create the figure fig=plt.figure(figsize=(12,5)) ax1=fig.add_subplot(1,2,1,projection='3d',computed_zorder=False) ax2=fig.add_subplot(1,2,2)  # get the corepoints corepoints = analysis.corepoints.cloud  # get change values of last epoch for all corepoints distances = analysis.distances distances_epoch = [d[100] for d in distances]  # get the time series of changes at a specific core point locations cp_idx_sel = 62000 coord_sel = analysis.corepoints.cloud[cp_idx_sel] timeseries_sel = distances[cp_idx_sel]  # get the list of timestamps from the reference epoch timestamp and timedeltas timestamps = [t + analysis.reference_epoch.timestamp for t in analysis.timedeltas]  # plot the scene d = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=distances_epoch[:], cmap='seismic_r', vmin=-1.5, vmax=1.5, s=1, zorder=1)  plt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)  # add the location of the selected coordinate ax1.scatter(coord_sel[0], coord_sel[1], coord_sel[2], c='black', s=3, zorder=2, label='Selected location') ax1.legend()  ax1.set_xlabel('X [m]') ax1.set_ylabel('Y [m]') ax1.set_zlabel('Z [m]') ax1.set_aspect('equal') ax1.view_init(elev=30., azim=150.) ax1.set_title('Changes at %s' % (analysis.reference_epoch.timestamp+analysis.timedeltas[100]))  # plot the time series ax2.plot(timestamps, timeseries_sel, color='blue') ax2.set_xlabel('Date') ax2.set_ylabel('Distance [m]') ax2.grid() ax2.set_ylim(-0.2,1.0) ax2.set_title('Time series at selected location')  plt.tight_layout() plt.show() <pre>[2023-03-29 15:16:10][INFO] Restoring epoch from file 'C:\\Users\\k53\\AppData\\Local\\Temp\\tmp603_y6md\\reference_epoch.zip'\n</pre> <p>In the changes on the snow-covered slope, we can see overall negative surface change, i.e. snow cover decrease. The linear structures of negative distances can be interpreted as avalanche erosion paths, with corresponding deposition areas at their bottom. The selected location of the plotted time series is located in such an avalanche deposition. Here, we can now derive the timing of the avalanche, which we expect as sudden increase in local snow cover. This happens on the afternoon of 2018-04-19. After the deposition event, the time series shows a continued trend of surface (snow cover) decrease, here related to warming temperatures in April.</p> <p>What you will have immediately noted are the strong jumps in distance increase and decrease in the night of 2018-04-20 to 2018-04-21. Due to published analysis of the data (cf. Anders et al., 2022), we know that this part of the time series are point clouds which are of poor quality and not well aligned, and therefore cause strong artifacts in the change analysis. The raw time series values we are currently using further show some fluctuation around the overall trend. These are likely related to noise, i.e. we cannot confidently assume that they are actual small changes. Therefore, we will now apply temporal smoothing to the data.</p> <p>As you learnt above, there are different approaches to spatiotemporal filtering. In our case, we already introduced spatial smoothing by applying the M3C2 to the point cloud epochs. We are therefore now using only time series averaging, to further filter the data in the temporal domain. We use a rolling median with a defined temporal window, here of six epochs. For this, we use the <code>temporal_averaging()</code> function in py4dgeo.</p> In\u00a0[9]: Copied! <pre>analysis.smoothed_distances = py4dgeo.temporal_averaging(\n    analysis.distances, smoothing_window=6\n)\n</pre> analysis.smoothed_distances = py4dgeo.temporal_averaging(     analysis.distances, smoothing_window=6 ) <pre>[2023-03-29 15:16:13][INFO] Starting: Smoothing temporal data\n</pre> <pre>h:\\conda_envs\\etrainee\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n</pre> <pre>[2023-03-29 15:16:19][INFO] Finished in 6.4172s: Smoothing temporal data\n</pre> <p>Now we can compare the raw and smoothed time series at our selected location:</p> In\u00a0[10]: Copied! <pre># create the figure\nfig, ax = plt.subplots(1,1,figsize=(7,5))\n\n# plot the raw time series\nax.scatter(timestamps, timeseries_sel, color='blue', label='raw', s=5)\n\n# plot the smoothed time series\ntimeseries_sel_smooth = analysis.smoothed_distances[cp_idx_sel]\nax.plot(timestamps, timeseries_sel_smooth, color='red', label='smooth')\n\nax.set_xlabel('Date')\nax.set_ylabel('Distance [m]')\nax.grid()\nax.set_ylim(-0.2,1.0)\n\nplt.tight_layout()\nplt.show()\n</pre> # create the figure fig, ax = plt.subplots(1,1,figsize=(7,5))  # plot the raw time series ax.scatter(timestamps, timeseries_sel, color='blue', label='raw', s=5)  # plot the smoothed time series timeseries_sel_smooth = analysis.smoothed_distances[cp_idx_sel] ax.plot(timestamps, timeseries_sel_smooth, color='red', label='smooth')  ax.set_xlabel('Date') ax.set_ylabel('Distance [m]') ax.grid() ax.set_ylim(-0.2,1.0)  plt.tight_layout() plt.show() <p>So far, we have explored the 4D point cloud dataset by looking into bitemporal surface changes (M3C2 distances) in the scene at selected epochs, and by visualizing the time series at a selected location. Accordingly, the data structure of the <code>SpatiotemporalAnalysis</code> object facilitates access to the change information in 3D time series and provides an efficient way to store change analyses for many epochs.</p> <p>With the subsequent section, we will now get started with analysis methods making use of the time series information, firstly time series clustering and secondly the extraction of 4D objects-by-change.</p> <p></p> <p>We hence perform time series clustering on our smoothed time series of snow cover changes, looking into potential change patterns that may become visible. Following Kuschnerus et al. (2021), we apply a k-means clustering with a defined number of clusters, here 5.</p> In\u00a0[11]: Copied! <pre># import kmeans clustering module from scikit-learn\nfrom sklearn.cluster import KMeans\n\n# use the smoothed distances for clustering\ndistances = analysis.smoothed_distances\n\n# define the number of clusters\nk=5\n\n# create an array to store the labels\nlabels = np.full((distances.shape[0]), np.nan)\n\nnan_indicator = np.logical_not(np.isnan(np.sum(distances, axis=1)))\nkmeans = KMeans(n_clusters=k, random_state=0).fit(distances[nan_indicator, :])\nlabels[nan_indicator] = kmeans.labels_\n</pre> # import kmeans clustering module from scikit-learn from sklearn.cluster import KMeans  # use the smoothed distances for clustering distances = analysis.smoothed_distances  # define the number of clusters k=5  # create an array to store the labels labels = np.full((distances.shape[0]), np.nan)  nan_indicator = np.logical_not(np.isnan(np.sum(distances, axis=1))) kmeans = KMeans(n_clusters=k, random_state=0).fit(distances[nan_indicator, :]) labels[nan_indicator] = kmeans.labels_  <pre>h:\\conda_envs\\etrainee\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> In\u00a0[12]: Copied! <pre># create the figure\nfig=plt.figure(figsize=(12,5))\nax1=fig.add_subplot(1,2,1,projection='3d',computed_zorder=False)\nax2=fig.add_subplot(1,2,2)\n\n# set colormap for clustering\ncmap_clustering = 'tab10'\n\n# get the corepoints\ncorepoints = analysis.corepoints.cloud\n\n# plot the scene colored by cluster labels\nd = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=labels, cmap=cmap_clustering, s=1)\nticks = np.arange(0, k, 1).astype(int)\ncb = plt.colorbar(d, ticks=ticks, label='Cluster ID', ax=ax1, shrink=.8, pad=.15)\n\nax1.set_xlabel('Easting [m]')\nax1.set_ylabel('Northing [m]')\nax1.set_aspect('equal')\nax1.view_init(elev=30., azim=150.)\n\n# plot the time series colored by cluster labels (with the same coloring as map)\nimport matplotlib as mpl\ncmap_labels = labels / np.nanmax(labels)\ncmap_clusters = mpl.cm.get_cmap(cmap_clustering)\nlabels_plotted = {}\n\n# use only every 100th time series for plotting\nfor c in range(0,distances.shape[0],100):\n    ts = distances[c]\n    label_curr = labels[c]\n    if not label_curr in labels_plotted.keys():\n        labels_plotted[label_curr] = [c]\n    p1 = ax2.plot(timestamps,ts,c=cmap_clusters(cmap_labels[c]), label=label_curr,linewidth=.5)\n    \n    labels_plotted[label_curr].append(c)\n\nax2.set_xlabel('Date')\nax2.set_ylabel('Distance [m]')\nax2.grid()\n\nplt.tight_layout()\nplt.show()\n</pre> # create the figure fig=plt.figure(figsize=(12,5)) ax1=fig.add_subplot(1,2,1,projection='3d',computed_zorder=False) ax2=fig.add_subplot(1,2,2)  # set colormap for clustering cmap_clustering = 'tab10'  # get the corepoints corepoints = analysis.corepoints.cloud  # plot the scene colored by cluster labels d = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=labels, cmap=cmap_clustering, s=1) ticks = np.arange(0, k, 1).astype(int) cb = plt.colorbar(d, ticks=ticks, label='Cluster ID', ax=ax1, shrink=.8, pad=.15)  ax1.set_xlabel('Easting [m]') ax1.set_ylabel('Northing [m]') ax1.set_aspect('equal') ax1.view_init(elev=30., azim=150.)  # plot the time series colored by cluster labels (with the same coloring as map) import matplotlib as mpl cmap_labels = labels / np.nanmax(labels) cmap_clusters = mpl.cm.get_cmap(cmap_clustering) labels_plotted = {}  # use only every 100th time series for plotting for c in range(0,distances.shape[0],100):     ts = distances[c]     label_curr = labels[c]     if not label_curr in labels_plotted.keys():         labels_plotted[label_curr] = [c]     p1 = ax2.plot(timestamps,ts,c=cmap_clusters(cmap_labels[c]), label=label_curr,linewidth=.5)          labels_plotted[label_curr].append(c)  ax2.set_xlabel('Date') ax2.set_ylabel('Distance [m]') ax2.grid()  plt.tight_layout() plt.show() <p>In the resulting plot, it becomes visible how different types of change are separated into clusters. Knowing the scene, we recognize the avalanche deposition area and erosion paths. The clustering further separates the upper and lower part of the slope, and the flat area at the bottom of the slope. In the time series (colored by the same labels), we can see that the overall trend of surface decrease behaves differently in these three parts (green to blue time series). These spatial clusters could now be used to further assess process characterstics in the scene. Of course, the visible change patterns are dependent on the selected number of clusters <code>k</code>. If you are working on this lesson in an interactive notebook, try adapting <code>k</code> to see how it influences your result.</p> <p>Another important note on clustering, which becomes apparent from our result map: the metrics of determining the similarity of time series cannot handle gaps in the data (i.e. NoData values in the time series). For our dataset, these are present in large part of the scene in point clouds acquired during the night, as the TLS was moved to a protected, more occluded, position (cf. Anders et al., 2022). These gaps over multiple hours are not interpolated by our temporal averaging with a smaller window. Removing NoData values from single time series is no solution here, as the metrics can neither be derived for time series of unequal length (Kuschnerus et al., 2021). This aspect therefore needs to be accounted for during data preparation, for example, by introducing stronger spatial and/or temporal interpolation where possible. An alternative could also be to adapt the analysis approach, for example, assessing change patterns for temporal subsets of the time series or at lower temporal sampling, e.g., by selecting only one measurement per day, week, and so on.</p> <p></p> <p>Another approach to time series-based change analysis is provided with the extraction of 4D objects-by-change (4D-OBCs). Whereas clustering is applied to the full time series at each location, the concept of 4D-OBCs is to identify individual surface activities in space and time, meaning with different timing and durations across a scene. Therein, each objects represents a surface activity which occurs during a specific timespan (identified in the time series at a a location) and with a certain spatial extent (given by time series similarity in the local neighborhood during their timespan of occurrence). Accordingly, the extraction of a 4D-OBC follows two main steps:</p> <p>Main steps for the extraction of a 4D object-by-change with detection of a surface activity in the time series and subsequent region growing regarding the similarity of neighboring time series. Figure by K. Anders, following Anders et al. (2020).</p> <p>First, a surface activity is detected in the time series of a location. This surface activity can be defined by the analyst and depends on the type of processes to be analyzed (in the schematic figure above: surface increase followed by decrease, i.e. local temporary accumulation). Using this location and timespan as seed, region growing is performed in the second step. For this, each neighboring location is checked for time series similarity within the timespan of the seed. If a certain similarity threshold is met, the location is added to the segment, until no more neighboring locations are being added and the final spatial extent is obtained. Look into Anders et al. (2021) to learn about the details of the method.</p> <p>To better understand how the approach works, we will now apply the extraction of a 4D object-by-change in our snow cover dataset, following Anders et al., 2022. This method is implemented in py4dgeo with the two main steps of seed detection in the time series and spatial delineation using region growing based on the similarity of neighboring time series. In this example, we will not run the method for the entire scene, but extract one object at the selected location.</p> <p>In our example of the snow-covered slope, we are looking for linear changes, such as the surface increase through avalanche deposition. As the original 4D-OBCs implemented in py4dgeo are targeting a different type of temporal process (see Anders et al., 2021; you will be using this in the exercise), we define our own seed detection here.</p> In\u00a0[13]: Copied! <pre>from py4dgeo.segmentation import RegionGrowingSeed\n\nclass LinearChangeSeeds(py4dgeo.RegionGrowingAlgorithm):\n    def find_seedpoints(self):\n\n        # The list of generated seeds\n        seeds = []\n\n        # General seed criteria minimum magnitude and minimum timespan\n        min_magn = self.height_threshold\n        minperiod = self.minperiod\n        maxperiod = 12\n\n        # The list of core point indices to check as seeds\n        if self.seed_candidates is None:\n            # Use all corepoints if no selection specified, considering subsampling\n            seed_candidates_curr = range(\n                0, self.analysis.distances_for_compute.shape[0], self.seed_subsampling\n                )\n        else:\n            # Use the specified corepoint indices, but consider subsampling\n            seed_candidates_curr = self.seed_candidates[::self.seed_subsampling]\n\n        # Interpolate nans in time series\n        def interp_nan(data):\n            bad_indexes = np.isnan(data)\n            num_nans = len(np.argwhere(bad_indexes))\n            num_not_nans = len(data) - num_nans\n            if num_not_nans &gt; 3:\n                if num_nans &gt; 0:\n                    good_indexes = np.logical_not(bad_indexes)\n                    good_data = data[good_indexes]\n                    interpolated = np.interp(bad_indexes.nonzero()[0], good_indexes.nonzero()[0], good_data)\n                    data[bad_indexes] = interpolated\n            return data, num_nans, num_not_nans\n\n        # Iterate over all time series to analyse their change points\n        for cp_idx in seed_candidates_curr:\n\n            timeseries = self.analysis.distances_for_compute[cp_idx, :]\n\n            ts1d_interp, num_nans, num_not_nans = interp_nan(timeseries)\n            if num_not_nans &lt;= 3:\n                continue\n\n            # Use segment-wise linear regression to find change timespans\n            from sklearn.tree import DecisionTreeRegressor\n            from sklearn.linear_model import LinearRegression\n            num_epochs = len(timeseries)\n            xs = np.arange(0, num_epochs, dtype=float)\n\n            dys = np.gradient(ts1d_interp, xs)\n\n            rgr = DecisionTreeRegressor(max_depth=4) # depth controls the number of segments (sensitivity)\n            rgr.fit(xs.reshape(-1, 1), dys.reshape(-1, 1))\n            dys_dt = rgr.predict(xs.reshape(-1, 1)).flatten()\n\n            ys_sl = np.ones(len(xs)) * np.nan            \n            for y in np.unique(dys_dt):\n\n                msk = dys_dt == y\n                lin_reg = LinearRegression()\n                lin_reg.fit(xs[msk].reshape(-1, 1), ts1d_interp[msk].reshape(-1, 1))\n                ys_sl[msk] = lin_reg.predict(xs[msk].reshape(-1, 1)).flatten()\n                x_vertices = [xs[msk][0], xs[msk][-1]]\n                startn_det = int(round(x_vertices[0], 0))\n                stopn_det = int(round(x_vertices[-1], 0))\n                startp = np.max([startn_det - 1, 0])\n                stopp = np.min([stopn_det + 1, len(timeseries) - 1])\n                if (startp == 0) and stopp &gt;= (len(timeseries) - 1):\n                    continue\n\n                # check minimum and maximum period criterion\n                per = stopp - startp\n                if (per &lt; minperiod) or (per &gt; maxperiod):\n                    continue\n                # check minimum magnitude criterion\n                elif abs(np.max(timeseries[startp:stopp + 1]) - (np.min(timeseries[startp:stopp + 1]))) &lt; min_magn:\n                    continue\n                # add seed\n                else:\n                    # construct the RegionGrowingSeed object consisting of index, start_epoch, end_epoch\n                    curr_seed = RegionGrowingSeed(cp_idx, startp, stopp)\n                    seeds.append(curr_seed)\n\n        return seeds\n</pre> from py4dgeo.segmentation import RegionGrowingSeed  class LinearChangeSeeds(py4dgeo.RegionGrowingAlgorithm):     def find_seedpoints(self):          # The list of generated seeds         seeds = []          # General seed criteria minimum magnitude and minimum timespan         min_magn = self.height_threshold         minperiod = self.minperiod         maxperiod = 12          # The list of core point indices to check as seeds         if self.seed_candidates is None:             # Use all corepoints if no selection specified, considering subsampling             seed_candidates_curr = range(                 0, self.analysis.distances_for_compute.shape[0], self.seed_subsampling                 )         else:             # Use the specified corepoint indices, but consider subsampling             seed_candidates_curr = self.seed_candidates[::self.seed_subsampling]          # Interpolate nans in time series         def interp_nan(data):             bad_indexes = np.isnan(data)             num_nans = len(np.argwhere(bad_indexes))             num_not_nans = len(data) - num_nans             if num_not_nans &gt; 3:                 if num_nans &gt; 0:                     good_indexes = np.logical_not(bad_indexes)                     good_data = data[good_indexes]                     interpolated = np.interp(bad_indexes.nonzero()[0], good_indexes.nonzero()[0], good_data)                     data[bad_indexes] = interpolated             return data, num_nans, num_not_nans          # Iterate over all time series to analyse their change points         for cp_idx in seed_candidates_curr:              timeseries = self.analysis.distances_for_compute[cp_idx, :]              ts1d_interp, num_nans, num_not_nans = interp_nan(timeseries)             if num_not_nans &lt;= 3:                 continue              # Use segment-wise linear regression to find change timespans             from sklearn.tree import DecisionTreeRegressor             from sklearn.linear_model import LinearRegression             num_epochs = len(timeseries)             xs = np.arange(0, num_epochs, dtype=float)              dys = np.gradient(ts1d_interp, xs)              rgr = DecisionTreeRegressor(max_depth=4) # depth controls the number of segments (sensitivity)             rgr.fit(xs.reshape(-1, 1), dys.reshape(-1, 1))             dys_dt = rgr.predict(xs.reshape(-1, 1)).flatten()              ys_sl = np.ones(len(xs)) * np.nan                         for y in np.unique(dys_dt):                  msk = dys_dt == y                 lin_reg = LinearRegression()                 lin_reg.fit(xs[msk].reshape(-1, 1), ts1d_interp[msk].reshape(-1, 1))                 ys_sl[msk] = lin_reg.predict(xs[msk].reshape(-1, 1)).flatten()                 x_vertices = [xs[msk][0], xs[msk][-1]]                 startn_det = int(round(x_vertices[0], 0))                 stopn_det = int(round(x_vertices[-1], 0))                 startp = np.max([startn_det - 1, 0])                 stopp = np.min([stopn_det + 1, len(timeseries) - 1])                 if (startp == 0) and stopp &gt;= (len(timeseries) - 1):                     continue                  # check minimum and maximum period criterion                 per = stopp - startp                 if (per &lt; minperiod) or (per &gt; maxperiod):                     continue                 # check minimum magnitude criterion                 elif abs(np.max(timeseries[startp:stopp + 1]) - (np.min(timeseries[startp:stopp + 1]))) &lt; min_magn:                     continue                 # add seed                 else:                     # construct the RegionGrowingSeed object consisting of index, start_epoch, end_epoch                     curr_seed = RegionGrowingSeed(cp_idx, startp, stopp)                     seeds.append(curr_seed)          return seeds <p>Next, we parametrize the 4D-OBC extraction by specifying a spatial neighborhood radius for searching locations during region growing, a minimum number of segments for an object to be valid (i.e. not discarded), and a minimum period and height threshold for seed timespans to be considered for region growing. The <code>thresholds</code> parameter defines the set of thresholds to be used for determining time series similarity (cf. Anders et al., 2021). In general, the lower the values that are included (starting from 0.1), the stricter the spatial delineation in the segmentation. The seed candidates can be specified to limit the object extraction to specific core point locations (optional), otherwise all locations will be searched for potential seeds and the object extraction conducted for the full dataset (long and computationally intensive processing). Here, we apply the object extraction to our one selected core point location in the avalanche deposition.</p> In\u00a0[14]: Copied! <pre># parametrize the 4D-OBC extraction\nalgo = LinearChangeSeeds(neighborhood_radius=1.0,\n                         min_segments=50,\n                         minperiod=2,\n                         height_threshold=0.1,\n                         thresholds=[0.5,0.6,0.7,0.8,0.9], seed_candidates=list([cp_idx_sel]))\n</pre> # parametrize the 4D-OBC extraction algo = LinearChangeSeeds(neighborhood_radius=1.0,                          min_segments=50,                          minperiod=2,                          height_threshold=0.1,                          thresholds=[0.5,0.6,0.7,0.8,0.9], seed_candidates=list([cp_idx_sel])) <p>Finally, we simply run the method and the steps of seed detection and region growing are run automatically:</p> In\u00a0[15]: Copied! <pre># run the algorithm\nanalysis.invalidate_results(seeds=True, objects=True, smoothed_distances=False) # only required if you want to re-run the algorithm\nobjects = algo.run(analysis)\n</pre> # run the algorithm analysis.invalidate_results(seeds=True, objects=True, smoothed_distances=False) # only required if you want to re-run the algorithm objects = algo.run(analysis) <pre>[2023-03-29 15:16:27][INFO] Removing intermediate results from the analysis file I:/etrainee_data/schneeferner/schneeferner.zip\n[2023-03-29 15:16:27][INFO] Starting: Find seed candidates in time series\n[2023-03-29 15:16:27][INFO] Finished in 0.0105s: Find seed candidates in time series\n[2023-03-29 15:16:27][INFO] Starting: Sort seed candidates by priority\n[2023-03-29 15:16:27][INFO] Finished in 0.0007s: Sort seed candidates by priority\n[2023-03-29 15:16:27][INFO] Starting: Performing region growing on seed candidate 1/6\n[2023-03-29 15:16:27][INFO] Finished in 0.0217s: Performing region growing on seed candidate 1/6\n[2023-03-29 15:16:27][INFO] Starting: Performing region growing on seed candidate 2/6\n[2023-03-29 15:16:28][INFO] Finished in 0.3274s: Performing region growing on seed candidate 2/6\n[2023-03-29 15:16:28][INFO] Starting: Performing region growing on seed candidate 3/6\n[2023-03-29 15:16:28][INFO] Finished in 0.0148s: Performing region growing on seed candidate 3/6\n[2023-03-29 15:16:28][INFO] Starting: Performing region growing on seed candidate 6/6\n[2023-03-29 15:16:28][INFO] Finished in 0.0074s: Performing region growing on seed candidate 6/6\n</pre> <p>Once finished, our <code>SpatiotemporalAnalysis</code> object holds all the information about the seeds and objects extracted for our analysis. Let's first have a look at the detected seeds (at the single time series of our selected location):</p> In\u00a0[16]: Copied! <pre>seed_timeseries = analysis.smoothed_distances[cp_idx_sel]\nplt.plot(timestamps,seed_timeseries, c='black', linestyle='--', linewidth=0.5, label='Seed timeseries')\n\nfor sid, example_seed in enumerate(analysis.seeds):\n    seed_end = example_seed.end_epoch\n    seed_start = example_seed.start_epoch\n    seed_cp_idx = example_seed.index\n\n    plt.plot(timestamps[seed_start:seed_end+1], seed_timeseries[seed_start:seed_end+1], label=f'Seed {sid}')\n\nplt.legend()\nplt.show()\n</pre> seed_timeseries = analysis.smoothed_distances[cp_idx_sel] plt.plot(timestamps,seed_timeseries, c='black', linestyle='--', linewidth=0.5, label='Seed timeseries')  for sid, example_seed in enumerate(analysis.seeds):     seed_end = example_seed.end_epoch     seed_start = example_seed.start_epoch     seed_cp_idx = example_seed.index      plt.plot(timestamps[seed_start:seed_end+1], seed_timeseries[seed_start:seed_end+1], label=f'Seed {sid}')  plt.legend() plt.show() <p>Several timespans of surface increase and decrease are detected, but we are only interested in the avalanche-related increase. Other seeds are found in the timespan of poor scan alignment (which we should leave out in our analysis, e.g., by not adding the epochs at all). We can now select the corresponding object to this seed and use a `plot()\u00b4 method to visualize the 4D-OBC by its time series (left) and in its spatial extent (right), colored by the time series similarity metric.</p> In\u00a0[17]: Copied! <pre>sel_seed_idx = 2\nsel_seed = analysis.seeds[sel_seed_idx]\nsel_object = analysis.objects[sel_seed_idx]\n\nsel_object.plot()\n</pre> sel_seed_idx = 2 sel_seed = analysis.seeds[sel_seed_idx] sel_object = analysis.objects[sel_seed_idx]  sel_object.plot() <p>To better understand the object properties (time series behavior and spatial extent in the scene), we use the object data to visualize the change information:</p> In\u00a0[18]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize=(15,5))\nax1,ax2 = axs\n\n# get indices of 4D-OBC\nidxs = objects[sel_seed_idx].indices\n\n# get change values at end of object for each location\nepoch_of_interest = int(objects[sel_seed_idx].end_epoch)\ndistances_of_interest = analysis.smoothed_distances[:, epoch_of_interest]\n\n# get the change magnitude between end and start of object for each location\nmagnitudes_of_interest = analysis.smoothed_distances[:, epoch_of_interest] - analysis.smoothed_distances[:, int(objects[sel_seed_idx].start_epoch)]\n\n# set the colormap according to magnitude at each location in the object\ncrange = 1.0\nimport matplotlib.colors as mcolors\ncmap = plt.get_cmap('seismic_r').copy()\nnorm = mcolors.CenteredNorm(halfrange=crange)\ncmapvals = norm(magnitudes_of_interest)\n\n# plot the timeseries of the segmented locations (colored by time series similarity)\nfor idx in idxs[::10]:\n    ax1.plot(timestamps, analysis.smoothed_distances[idx], c=cmap(cmapvals[idx]), linewidth=0.5)\n# plot the seed time series\nax1.plot(timestamps, analysis.smoothed_distances[cp_idx_sel], c='black', linewidth=1., label='Seed timeseries')\n\n# fill the area of the object\nax1.axvspan(timestamps[objects[sel_seed_idx].start_epoch], timestamps[objects[sel_seed_idx].end_epoch], alpha=0.3, color='grey', label='4D-OBC timespan')\n\n# add legend\nax1.legend()\n\n# get subset of core points incorporated in 4D-OBC\ncloud = analysis.corepoints.cloud\nsubset_cloud = cloud[idxs,:2]\n\n# plot coordinates colored by change values at end magnitude of object\nd = ax2.scatter(cloud[:,0], cloud[:,1], c = magnitudes_of_interest, cmap='seismic_r', vmin=-crange, vmax=crange, s=1)\nplt.colorbar(d, format=('%.2f'), label='Change magnitude [m]', ax=ax2)\n\n# plot convex hull of 4D-OBC\nfrom scipy.spatial import ConvexHull\nfrom matplotlib.patches import Polygon\nhull = ConvexHull(subset_cloud)\nax2.add_patch(Polygon(subset_cloud[hull.vertices,0:2], label = '4D-OBC hull', fill = False))\n\n# plot seed location of 4D-OBC\nax2.scatter(cloud[cp_idx_sel,0], cloud[cp_idx_sel,1], marker = '*', c = 'black', label = 'Seed')\n\n# add plot elements\nax1.set_title('Time series of segmented 4D-OBC locations')\nax1.set_xlabel('Date')\nax1.set_ylabel('Distance [m]')\nax2.set_title(f'Magnitudes of change in the 4D-OBC timespan\\n({timestamps[epoch_of_interest]-timestamps[analysis.objects[sel_seed_idx].start_epoch]} hours)')\nax2.set_xlabel('X [m]')\nax2.set_ylabel('Y [m]')\nax2.legend(loc='upper right')\n\nplt.show()\n</pre> fig, axs = plt.subplots(1,2, figsize=(15,5)) ax1,ax2 = axs  # get indices of 4D-OBC idxs = objects[sel_seed_idx].indices  # get change values at end of object for each location epoch_of_interest = int(objects[sel_seed_idx].end_epoch) distances_of_interest = analysis.smoothed_distances[:, epoch_of_interest]  # get the change magnitude between end and start of object for each location magnitudes_of_interest = analysis.smoothed_distances[:, epoch_of_interest] - analysis.smoothed_distances[:, int(objects[sel_seed_idx].start_epoch)]  # set the colormap according to magnitude at each location in the object crange = 1.0 import matplotlib.colors as mcolors cmap = plt.get_cmap('seismic_r').copy() norm = mcolors.CenteredNorm(halfrange=crange) cmapvals = norm(magnitudes_of_interest)  # plot the timeseries of the segmented locations (colored by time series similarity) for idx in idxs[::10]:     ax1.plot(timestamps, analysis.smoothed_distances[idx], c=cmap(cmapvals[idx]), linewidth=0.5) # plot the seed time series ax1.plot(timestamps, analysis.smoothed_distances[cp_idx_sel], c='black', linewidth=1., label='Seed timeseries')  # fill the area of the object ax1.axvspan(timestamps[objects[sel_seed_idx].start_epoch], timestamps[objects[sel_seed_idx].end_epoch], alpha=0.3, color='grey', label='4D-OBC timespan')  # add legend ax1.legend()  # get subset of core points incorporated in 4D-OBC cloud = analysis.corepoints.cloud subset_cloud = cloud[idxs,:2]  # plot coordinates colored by change values at end magnitude of object d = ax2.scatter(cloud[:,0], cloud[:,1], c = magnitudes_of_interest, cmap='seismic_r', vmin=-crange, vmax=crange, s=1) plt.colorbar(d, format=('%.2f'), label='Change magnitude [m]', ax=ax2)  # plot convex hull of 4D-OBC from scipy.spatial import ConvexHull from matplotlib.patches import Polygon hull = ConvexHull(subset_cloud) ax2.add_patch(Polygon(subset_cloud[hull.vertices,0:2], label = '4D-OBC hull', fill = False))  # plot seed location of 4D-OBC ax2.scatter(cloud[cp_idx_sel,0], cloud[cp_idx_sel,1], marker = '*', c = 'black', label = 'Seed')  # add plot elements ax1.set_title('Time series of segmented 4D-OBC locations') ax1.set_xlabel('Date') ax1.set_ylabel('Distance [m]') ax2.set_title(f'Magnitudes of change in the 4D-OBC timespan\\n({timestamps[epoch_of_interest]-timestamps[analysis.objects[sel_seed_idx].start_epoch]} hours)') ax2.set_xlabel('X [m]') ax2.set_ylabel('Y [m]') ax2.legend(loc='upper right')  plt.show() <p>As aimed, our extracted 4D-OBC covers the timespan of the avalanche-related increase. The time series and changes in the scene are colored by the magnitude of each segmented location in the selected timespan - derived as the difference of change values at the end epoch and the start epoch of the individual object. By this, even though we use a time series of surface change to a fixed reference (first epoch), we are independent from fixed intervals and purely bitemporal change quantification. For each extracted object, we are considering exactly the time series information of their occurrence, which can be highly variable (and even overlapping) within a scene. The map shows how the time series-based segmentation approach nicely delineates the avalanche deposition area. Apparently, what looks like two neighboring avalanches occurred during exactly the same timespan, and is therefore extracted as one space-time event. A spatial separation of the deposition areas may be achieved by parametrizing the similarity thresholds more strictly - try it out if you are working in the interactive notebook, to get a better feeling of the method's properties.</p> <p></p>  What does the term \u201cnear-continuous acquisition\u201d describe?  acquisitions at sub-daily intervals, e.g. hourly temporal resolution temporal resolution which represents the surface change behavior repetition of acquisition as frequently as the instrument allows <p>temporal resolution which represents the surface change behavior</p> <p></p>  What is an important aspect how high-frequency observation improves monitoring of events, such as rockfalls?  coinciding and coalescing rockfalls can be identified as individual events changing material properties are captured with each acquisition the measurement uncertainty is reduced if the rockfall volume is measured twice <p>coinciding and coalescing rockfalls can be identified as individual events</p> <p></p>  What is the main property of 4D point clouds used in spatiotemporal filtering approaches (e.g. averaging to reduce noise)?  acquisition from a fixed position high measurement redundancy through high spatial and temporal resolution sampling differences between repeated laser scanning acquisitions <p>high measurement redundancy through high spatial and temporal resolution</p> <p></p>  What is an important temporally variable influence on LiDAR measurements, which is especially pronounced in high-frequency (i.e., sub-daily) acquisitions?  <p>atmospheric conditions</p> <p></p>  What is a main limitation of bitemporal change analysis methods to spatially delineate individual surface processes from multitemporal point clouds?  <p>ambiguous change information due to spatial overlap of surface activities</p> <p></p>  Using py4dgeo for change analysis, what are the main components of a SpatiotemporalAnalysis object?  <p>reference epoch, core points, M3C2 algorithm configuration, and epochs with timestamps, distances, and uncertainties (from M3C2 calculation)</p> <p></p>  How does the user-defined number of clusters (e.g., with k-mean algorithm) influence the result of time series clustering?  <p>The higher the number of clusters, the more fine-grained the derived change patterns. The choice of cluster number is therefore dependent on the analysis objective and properties of observed scene dynamics.</p> <p></p>  In the 4D objects-by-change method, what is typically used as reference epoch for bitemporal change computation?  the previous epoch for each point cloud in the time series a fixed, user-defined interval between epochs for each point cloud in the time series the first epoch for each point cloud in the time series <p>the first epoch for each point cloud in the time series</p> <p></p>  In the 4D objects-by-change method, what is typically used as reference epoch for bitemporal change computation?  local area with similar surface behavior during a certain timespan groups of time series with similar surface behavior across the scene similar change values in a specific epoch derived from user-defined thresholds <p>local area with similar surface behavior during a certain timespan</p> <p></p> <p></p> <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#time-series-analysis-of-3d-point-clouds","title":"Time series analysis of 3D point clouds\u00b6","text":"<p>Surface dynamics within a local landscape occur on a large range of spatiotemporal scales. The analysis of surface activities and structural dynamics in 4D point cloud data has therefore become an integral part of Earth observation. These special datasets contain detailed 3D information of the topography with time as additional dimension.</p> <p>The objective of this theme is to learn about</p> <ul> <li>Acquisition of 3D time series</li> <li>Spatiotemporal information in 4D point clouds</li> <li>Processing time series of 3D point clouds</li> <li>Time series-based methods of change analysis<ul> <li>Hands-on 3D time series analysis</li> <li>Time series clustering</li> <li>4D objects-by-change</li> </ul> </li> </ul> <p>After finishing this theme you will be able to:</p> <ul> <li>understand the difference between multitemporal point clouds and 4D point clouds in their information content</li> <li>derive change information from the temporal domain of 4D point clouds</li> <li>analyze 4D point clouds with automatic workflows in Python using open-source tools</li> </ul>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#acquisition-of-3d-time-series","title":"Acquisition of 3D time series\u00b6","text":"<p>Essentially, 4D point clouds are time series of topographic data with a dense temporal dimension, beyond a \"handful\" of multitemporal acquisitions. This means, the time domain represents the behavior of the surface and process characteristics, opposed to multitemporal 3D point clouds which contain mostly pre- and post-states of changes with fewer (typically less than 100) epochs. There is no fixed definition about the temporal density or length of 3D time series. Capturing surface processes near-continuously rather depends on the relation of acquisition frequency to the change rates of observed phenomena in a scene.</p> <p>The main strategies of near-continuous 3D acquisition are permanent terrestrial laser scanning (TLS) and time-lapse photogrammetry. A fixed installation of sensors enables to repeat the acquisition at very high frequency, even seconds to minutes (Eitel et al., 2016). The temporal interval depends on the requirements of the application, and is therefore typically defined by an analyst, but also limited by the sensor capabilities. As the sensors are programmed for automatic acquisition, they can be left alone for continued observation and generate time series of several weeks, months, and years.</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#spatiotemporal-information-in-4d-point-clouds","title":"Spatiotemporal information in 4D point clouds\u00b6","text":"<p>The example of rockfall monitoring demonstrates the benefit of high-frequency and long-term acquisitions. If the acquisition frequency were lower, multiple rockfall events may occur in the same area of the rock cliff in the meantime (Williams et al., 2018). Such coinciding events cannot be identified individually, if no acquisitions in-between the events are available. Using 4D point clouds, i.e. near-continuous observation, we can resolve single events and thereby also describe the spatiotemporal rockfall characteristics. For example, it can be observed how rockfall events coalesce and result in larger rockfall areas on a cliff. Williams et al. (2019) have used 4D point clouds acquired by hourly TLS at a coastal cliff to investigate the magnitude-frequency distribution of rockfall events. Only from the high-frequency data, it was possible to identify a large number of small - but more frequent - rockfalls. In sum, these small events make a large contribution to overall mass loss at the cliff and are therefore important to capture for gaining insight on underlying processes and drivers.</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#processing-time-series-of-3d-point-clouds","title":"Processing time series of 3D point clouds\u00b6","text":"<p>As you already know for change analysis of point clouds in general, there are different factors influencing the accuracy of single epochs and the uncertainty associated to derived change (cf. contents of Theme 3). Of course these influences are also present in case of near-continuous point cloud acquisitions. We are even facing a special situation now, because some influences may even become more pronounced - or more visible - in the analysis.</p> <p>Regarding the influence of surface characteristics (e.g., moisture) and atmospheric conditions (i.e., temperature, air pressure, humidity), it has been noted especially in high-frequency LiDAR time series that their is a direct link to temporal variations in measurements. For hourly laser scans of a sandy beach, the following plot shows the variation in height measurements at a paved, assumingly stable location:</p> <p>Elevation differences at a stable location (paved area) in a sandy beach scene acquired by hourly terrestrial laser scanning compared to daily air temperature measured at a nearby regional station. Figure modified from Anders et al. (2019) / CC BY 4.0.</p> <p>In this PLS setting, elevation differences are in a range of 5 cm, with peaks at 10 cm. A direct link to atmospheric conditions can be established when comparing to air temperature records from a regional meteorological station. It needs to be noted, that this strong influence is not necessarily caused only by the influence of atmospheric conditions on the refraction of the LiDAR beam (cf. Friedli et al, 2019). Another aspect we need to consider is the stability of the 'fixed' instrument itself. Depending on the installation, there may be material deformations linked to changing temperatures, which influence the orientation of the sensor and would have similar effects on the range measurement (Anders al., 2019). By now, detailed experiments and investigations have been conducted with high-frequency, permanent TLS to further understand environmental influences and the stability of laser scanning time series (e.g., Kuschnerus et al., 2021; Voordendag et al., 2023). Thereby, a lot has also been learnt to improve their setup and efforts are being made to reduce external influences to the level known by standard, multitemporal surveys. Systematic effects that remain, e.g. misalignments, can be reduced through standard approaches, such as rigid transformation using fixed reference objects or surfaces (cf. section on alignment in previous theme).</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#time-series-based-methods-of-change-analysis","title":"Time series-based methods of change analysis\u00b6","text":"<p>As laid out above, the information content in 4D point clouds holds potential to gain deeper insight into observed surface processes. Incorporating the time series information already in the analysis process, can overcome some important drawbacks of pairwise change analysis (as introduced in Theme 3).</p> <p>In situations where surface processes occur during overlapping timespans and also with spatial overlap, surface change information from two epochs is ambiguous and cannot necessarily be distinguished into single change types. The figure below visualizes schematically how individual erosion, deposition, or transport processes cannot necessarily be separated. If deposition coincides with overall surface increase, the spatial extent of the deposition form cannot be identified by some boundary in the change values.</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#hands-on-3d-time-series-analysis","title":"Hands-on 3D time series analysis\u00b6","text":"<p>As example data, we are using a time series of TLS-based snow cover monitoring acquired at the Zugspitze, Germany (see visualization of example change scene above). The data are openly available on the PANGAEA data repository (Anders et al., 2022). Here, we provide a subset of the data to reduce the volume, which you can download with the exercise data from the central course repository (data directory <code>schneeferner</code>). The data covers a smaller area of interest in the scene, namely part of the snow-covered slope, and is subsampled to a point spacing of 50 cm. The hourly point clouds cover 107 epochs acquired in April 2019. For the analysis, we will use the open source Python library <code>py4dgeo</code>. The package is contained in the Python environment of this course, or can be installed via pip - see the documentation for further information on installation and basic usage: https://py4dgeo.readthedocs.io/en/latest/index.html.</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#time-series-clustering","title":"Time series clustering\u00b6","text":"<p>Clustering of 3D time series is an approach developed to derive groups of similar change patterns in a scene, presented by Kuschnerus et al. (2021). As a method of unsupervised learning, it does not require to specify specific patterns or expected processes beforehand. This is ideal for near-continuous topographic observation, as we generally cannot know a priori about all possible types of surface activities occurring in the observed scene. The concept of time series clustering following Kuschnerus et al. (2021) is to group time series of a scene which exhibit a similar evolution of the topography throughout time. The objective is to separate the observed scene into distinct spatial regions where each region (cluster) represents a time series associated to a specific change pattern. This change pattern, ultimately, can be linked to characteristic surface processes shaping this region. The following figure visualizes how two time series in a scene can be similar according to their Euclidian distance (the lower, the more similar) or according to their correlation (between 0 and 1, with 1 being fully correlated):</p> <p>Example aspects how pairs of time series can be similar to one another according to Euclidian distance $d_E$ or correlation Cor. Figure by Kuschnerus et al. (2021) / CC BY 4.0.</p> <p>Using such similarity metrics, different algorithms of clustering can be applied to perform time series grouping, with k-means, agglomerative clustering, and DBSCAN investigated by Kuschnerus et al. (2021) for an hourly PLS time series of a sandy beach. The principles and different methods of machine learning will be introduced in the next theme. Here, we will focus on k-means clustering regarding Euclidian distances as one possible approach.</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#4d-objects-by-change","title":"4D objects-by-change\u00b6","text":""},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#self-evaluation-quiz","title":"Self-evaluation quiz\u00b6","text":"<p>You are now at the end of this theme's lesson contents and should have achieved a broad overview and specific methods of time series analysis in 4D point clouds. Use the following quiz questions to assess your learning success:</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#exercise","title":"Exercise\u00b6","text":"<p>Made it through the quiz? Then you are ready for the exercise - proceed with the next part using the button at the bottom of this page.</p>"},{"location":"module3/04_3d_timeseries_analysis/04_3d_timeseries_analysis.html#references","title":"References\u00b6","text":"<ul> <li>Anders, K., Lindenbergh, R. C., Vos, S. E., Mara, H., de Vries, S., &amp; H\u00f6fle, B. (2019). High-Frequency 3D Geomorphic Observation Using Hourly Terrestrial Laser Scanning Data Of A Sandy Beach. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-2/W5, pp. 317-324. doi: 10.5194/isprs-annals-IV-2-W5-317-2019.</li> <li>Anders, K., Winiwarter, L., Lindenbergh, R., Williams, J. G., Vos, S. E., &amp; H\u00f6fle, B. (2020). 4D objects-by-change: Spatiotemporal segmentation of geomorphic surface change from LiDAR time series. ISPRS Journal of Photogrammetry and Remote Sensing, 159, pp. 352-363. doi: 10.1016/j.isprsjprs.2019.11.025.</li> <li>Anders, K., Winiwarter, L., Mara, H., Lindenbergh, R., Vos, S. E., &amp; H\u00f6fle, B. (2021). Fully automatic spatiotemporal segmentation of 3D LiDAR time series for the extraction of natural surface changes. ISPRS Journal of Photogrammetry and Remote Sensing, 173, pp. 297-308. doi: 10.1016/j.isprsjprs.2021.01.015.</li> <li>Anders, K., Eberlein, S., &amp; H\u00f6fle, B. (2022). Hourly Terrestrial Laser Scanning Point Clouds of Snow Cover in the Area of the Schneeferner, Zugspitze, Germany: PANGAEA. https://doi.org/10.1594/PANGAEA.941550.</li> <li>Anders, K., Winiwarter, L., &amp; H\u00f6fle, B. (2022). Improving Change Analysis From Near-Continuous 3D Time Series by Considering Full Temporal Information. IEEE Geoscience and Remote Sensing Letters, 19, pp. 1-5. doi: 10.1109/LGRS.2022.3148920.</li> <li>Blanch, X., (2022).  Developing Advanced Photogrammetric Methods for Automated Rockfall Monitoring. Doctoral dissertation. URL: http://hdl.handle.net/10803/675397.</li> <li>Blanch, X., Eltner, A., Guinau, M., &amp; Abellan, A. (2021). Multi-Epoch and Multi-Imagery (MEMI) Photogrammetric Workflow for Enhanced Change Detection Using Time-Lapse Cameras. Remote Sensing, 13 (8), pp. 1460. doi: 10.3390/rs13081460.</li> <li>Eitel, J. U. H., H\u00f6fle, B., Vierling, L. A., Abell\u00e1n, A., Asner, G. P., Deems, J. S., Glennie, C. L., Joerg, P. C., LeWinter, A. L., Magney, T. S., Mandlburger, G., Morton, D. C., M\u00fcller, J., &amp; Vierling, K. T. (2016). Beyond 3-D: The new spectrum of lidar applications for earth and ecological sciences. Remote Sensing of Environment, 186, pp. 372-392. doi: 10.1016/j.rse.2016.08.018.</li> <li>Eltner, A., Kaiser, A., Abellan, A., &amp; Schindewolf, M. (2017). Time lapse structure-from-motion photogrammetry for continuous geomorphic monitoring. Earth Surface Processes and Landforms, 42, pp. 2240-2253. doi: 10.1002/esp.4178.</li> <li>Friedli, E., Presl, R., &amp; Wieser, A. (2019). Influence of atmospheric refraction on terrestrial laser scanning at long range. 4th Joint International Symposium on Deformation Monitoring: JISDM, pp. 6.</li> <li>Kromer, R., Abell\u00e1n, A., Hutchinson, D., Lato, M., Edwards, T., &amp; Jaboyedoff, M. (2015). A 4D Filtering and Calibration Technique for Small-Scale Point Cloud Change Detection with a Terrestrial Laser Scanner. Remote Sensing, 7 (10), pp. 13029-13052. doi: 10.3390/rs71013029.</li> <li>Kromer, R. A., Abell\u00e1n, A., Hutchinson, D. J., Lato, M., Chanut, M.-A., Dubois, L., &amp; Jaboyedoff, M. (2017). Automated Terrestrial Laser Scanning with Near Real-Time Change Detection - Monitoring of the S\u00e9chillenne Landslide. Earth Surface Dynamics, 5, pp. 293-310. doi: 10.5194/esurf-5-293-2017.</li> <li>Kuschnerus, M., Lindenbergh, R., &amp; Vos, S. (2021). Coastal change patterns from time series clustering of permanent laser scan data. Earth Surface Dynamics, 9 (1), pp. 89-103. doi: 10.5194/esurf-9-89-2021.</li> <li>Kuschnerus, M., Schr\u00f6der, D., &amp; Lindenbergh, R. (2021). Environmental Influences on the Stability of a Permanently Installed Laser Scanner. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLIII-B2-2021, pp. 745-752. doi: 10.5194/isprs-archives-XLIII-B2-2021-745-2021.</li> <li>Lague, D., Brodu, N., &amp; Leroux, J. (2013). Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82, pp. 10-26. doi: 10.1016/j.isprsjprs.2013.04.009.</li> <li>Ulrich, V., Williams, J. G., Zahs, V., Anders, K., Hecht, S. &amp; H\u00f6fle, B. (2021): Measurement of rock glacier surface change over different timescales using terrestrial laser scanning point clouds. Earth Surface Dynamics, 9, pp. 19-28. doi: 10.5194/esurf-9-19-2021.</li> <li>Voordendag, A., Goger, B., Klug, C., Prinz, R., Rutzinger, M., Sauter, T., &amp; Kaser, G. (2023). Uncertainty assessment of a permanent long-range terrestrial laser scanning system for the quantification of snow dynamics on Hintereisferner (Austria). Frontiers in Earth Science, 11,  doi: 10.3389/feart.2023.1085416.</li> <li>Vos, S., Anders, K., Kuschnerus, M., Lindenberg, R., H\u00f6fle, B., Aarnikhof, S. &amp; Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands.  Scientific Data, 9:191. doi: 10.1038/s41597-022-01291-9.</li> <li>Williams, J. G., Rosser, N. J., Hardy, R. J., Brain, M. J., &amp; Afana, A. A. (2018). Optimising 4-D surface change detection: an approach for capturing rockfall magnitude\u2013frequency. Earth Surface Dynamics, 6, pp. 101-119. doi: 10.5194/esurf-6-101-2018.</li> <li>Williams, J. G., Rosser, N. J., Hardy, R. J., &amp; Brain, M. J. (2019). The Importance of Monitoring Interval for Rockfall Magnitude-Frequency Estimation. Journal of Geophysical Research: Earth Surface, 124 (12),  doi: 10.1029/2019jf005225.</li> <li>Winiwarter, L., Anders, K., Schr\u00f6der, D., &amp; H\u00f6fle, B. (2022). Full 4D Change Analysis of Topographic Point Cloud Time Series using Kalman Filtering. Earth Surface Dynamics Discussions, 2022, pp. 1-25. doi: 10.5194/esurf-2021-103.</li> </ul>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html","title":"Exercise: Time series analysis of 3D point clouds","text":""},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#time-series-based-change-analysis-of-surface-dynamics-at-a-sandy-beach","title":"Time series-based change analysis of surface dynamics at a sandy beach","text":"<p>In this exercise, you will perform time series-based surface change analysis on a time series of permanent TLS point clouds of the sandy beach at Kijkduin for a timespan of around 6 months (Vos et al., 2022). See the introduction to the case study and dataset here.</p> <p>The objective is to assess surface dynamics with two methods: time series clustering (following Kuschnerus et al., 2021) and 4D objects-by-change (following Anders et al., 2021). Look into the related articles for comparison of possible surface dynamics at the site and help for deciding on suitable parameters, etc.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#software-and-data","title":"Software and data","text":"<p>This exercise should be solved using Python with the <code>py4dgeo</code> library. You can follow the workflow introduced in the lesson of this theme. Also, make use of the software documentations!</p> <p>Use CloudCompare or GIS Software (e.g., QGIS) to check and visualize your results.</p> <p>The dataset will be a subsampled version of the original time series, using 12-hourly epochs of point clouds and spatial subsampling to 50 cm. In the data directory <code>kijkduin</code>, you find the prepared input point clouds and a core points point cloud, which is manually cleaned from noise.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#loading-data-and-calculation-of-surface-changes","title":"Loading data and calculation of surface changes","text":"<p>Prepare the analysis by compiling the list of files (epochs) and read the timestamps from the file names (format <code>YYMMDD_hhmmss</code>) into <code>datetime</code> objects. Use the point cloud files and timestamps to create a py4dgeo <code>SpatiotemporalAnalysis</code> object. For this you need to instantiate the M3C2 algorithm. You can use the point cloud file <code>170115_150816_aoi_50cm.laz</code> as core points. Explore the point cloud properties in CloudCompare: </p> <ul> <li>Considering the available point density and surface characteristics, what would be a suitable cylinder radius for the distance calculation?</li> <li>What would be a suitable approach to derive the surface normals in this topography and expected types of surface changes?</li> </ul> <p>Hint: In this flat topography and predominant provess of sand deposition and erosion, it can be suitable to orient the normals purely vertically. In this case, they do not need to be computed, and you can customize the py4dgeo algorithm accordingly.</p> <p>Use the first point cloud in the time series (list of files) as reference epoch. You can assume a registration error of 1.9 cm for the M3C2 distance calculation (cf. Vos et al., 2022).</p> <p>Explore the spatiotemporal change information by visualizing the changes at a selected epoch and visualizing the time series at a selected location.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#temporal-smoothing","title":"Temporal smoothing","text":"<p>You are dealing with a temporal subset of the original hourly time series. The effect of temporal measurement variability may therefore be less pronounced (compared to the assessment in, e.g., Anders et al., 2019). Nonetheless, you may apply temporal smoothing to reduce the influence of noise on your change analysis using a rolling median averaging of one week. This will also fill possible gaps in your data, e.g., lower ranges during poor atmospheric conditions or no data due to water coverage during tides on the lower beach part. </p> <p>Visualize the raw and smoothed change values in the time series of your selected location.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#time-series-clustering","title":"Time series clustering","text":"<p>To derive characteristic change patterns on the sandy beach, perform k-means clustering of the time series following Kuschnerus et al. (2021). Assess the clustering for different selections of <code>k</code> numbers of clusters.</p> <ul> <li>Can you interpret the characteristics of different parts on the beach? Visualize example time series for different clusters.</li> <li>From which number of clusters do you see a clear separation in overall units of the beach area?</li> <li>What are some detail change patterns that become visible for a higher number of clusters?</li> </ul>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#extraction-of-4d-objects-by-change","title":"Extraction of 4D objects-by-change","text":"<p>Now use the 4D objects-by-change (4D-OBC) method to identify individual surface activities in the beach scene. The objective is to extract temporary occurrences of accumulation or erosion, as occurs when a sand bar is formed during some timespan, or when sand is deposited by anthropogenic works. This type of surface activity is implemented with the original seed detection in py4dgeo, so you do not need to customize the algorithm. Decide for suitable parameters following Anders et al. (2021) - but bear in mind that we are using a different temporal resolution, so you may need to adjust the temporal change detection.</p> <p>Perform the extraction for selected seed locations, e.g. considering interesting clusters of change patterns identified in the previous step. In principle, the spatiotemporal segmentation can also be applied to the full dataset (all time series at all core point locations are used as potential seed candidates), but long processing time needs to be expected.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#solution","title":"Solution","text":"<p>Good job on finishing this exercise! You may take a look at the solution provided here.</p> <p>We will not further go into the analysis here, but note that the 4D point clouds at this sandy beach are featured as a research-oriented case study!</p> <p></p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1.html#references","title":"References","text":"<ul> <li>Anders, K., Lindenbergh, R. C., Vos, S. E., Mara, H., de Vries, S., &amp; H\u00f6fle, B. (2019). High-Frequency 3D Geomorphic Observation Using Hourly Terrestrial Laser Scanning Data Of A Sandy Beach. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-2/W5, pp. 317-324. doi: 10.5194/isprs-annals-IV-2-W5-317-2019.</li> <li>Anders, K., Winiwarter, L., Mara, H., Lindenbergh, R., Vos, S. E., &amp; H\u00f6fle, B. (2021). Fully automatic spatiotemporal segmentation of 3D LiDAR time series for the extraction of natural surface changes. ISPRS Journal of Photogrammetry and Remote Sensing, 173, pp. 297-308. doi: 10.1016/j.isprsjprs.2021.01.015.</li> <li>Kuschnerus, M., Lindenbergh, R., &amp; Vos, S. (2021). Coastal change patterns from time series clustering of permanent laser scan data. Earth Surface Dynamics, 9 (1), pp. 89-103. doi: 10.5194/esurf-9-89-2021.</li> <li>Vos, S., Anders, K., Kuschnerus, M., Lindenberg, R., H\u00f6fle, B., Aarnikhof, S. &amp; Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands.  Scientific Data, 9:191. doi: 10.1038/s41597-022-01291-9.</li> </ul>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html","title":"M3 theme4 exercise1 solution1","text":"<p>First, we start by setting up the Python environment and data:</p> In\u00a0[1]: Copied! <pre># import required modules\nimport os\nimport numpy as np\nimport py4dgeo\nfrom datetime import datetime\n\n# specify the data path\ndata_path = 'path-to-data'\n\n# check if the specified path exists\nif not os.path.isdir(data_path):\n    print(f'ERROR: {data_path} does not exist')\n    print('Please specify the correct path to the data directory by replacing &lt;path-to-data&gt; above.')\n\n# sub-directory containing the point clouds\npc_dir = os.path.join(data_path, 'pointclouds')\n\n# list of point clouds (time series)\npc_list = os.listdir(pc_dir)\npc_list[:5] # print the first elements\n</pre> # import required modules import os import numpy as np import py4dgeo from datetime import datetime  # specify the data path data_path = 'path-to-data'  # check if the specified path exists if not os.path.isdir(data_path):     print(f'ERROR: {data_path} does not exist')     print('Please specify the correct path to the data directory by replacing  above.')  # sub-directory containing the point clouds pc_dir = os.path.join(data_path, 'pointclouds')  # list of point clouds (time series) pc_list = os.listdir(pc_dir) pc_list[:5] # print the first elements Out[1]: <pre>['kijkduin_170117_120041.laz',\n 'kijkduin_170118_000050.laz',\n 'kijkduin_170120_120036.laz',\n 'kijkduin_170121_000046.laz',\n 'kijkduin_170121_120055.laz']</pre> <p>In the list of point cloud files you can see that we have one laz file per epoch available. The file name contains the timestamp of the epoch, respectively, in format <code>YYMMDD_hhmmss</code>. To use this information for our analysis, we read the timestamp information from the file names into <code>datetime</code> objects.</p> In\u00a0[2]: Copied! <pre># read the timestamps from file names\ntimestamps = []\nfor f in pc_list:\n    if not f.endswith('.laz'):\n        continue\n\n    # get the timestamp from the file name\n    timestamp_str = '_'.join(f.split('.')[0].split('_')[1:]) # yields YYMMDD_hhmmss\n\n    # convert string to datetime object\n    timestamp = datetime.strptime(timestamp_str, '%y%m%d_%H%M%S')\n    timestamps.append(timestamp)\n\ntimestamps[:5]\n</pre> # read the timestamps from file names timestamps = [] for f in pc_list:     if not f.endswith('.laz'):         continue      # get the timestamp from the file name     timestamp_str = '_'.join(f.split('.')[0].split('_')[1:]) # yields YYMMDD_hhmmss      # convert string to datetime object     timestamp = datetime.strptime(timestamp_str, '%y%m%d_%H%M%S')     timestamps.append(timestamp)  timestamps[:5] Out[2]: <pre>[datetime.datetime(2017, 1, 17, 12, 0, 41),\n datetime.datetime(2017, 1, 18, 0, 0, 50),\n datetime.datetime(2017, 1, 20, 12, 0, 36),\n datetime.datetime(2017, 1, 21, 0, 0, 46),\n datetime.datetime(2017, 1, 21, 12, 0, 55)]</pre> <p>Now we use the point cloud files and timestamp information to create a <code>SpatiotemporalAnalysis</code> object</p> In\u00a0[3]: Copied! <pre>analysis = py4dgeo.SpatiotemporalAnalysis(f'{data_path}/kijkduin.zip', force=True)\n</pre> analysis = py4dgeo.SpatiotemporalAnalysis(f'{data_path}/kijkduin.zip', force=True) <pre>[2023-03-29 15:54:53][INFO] Creating analysis file I:\\etrainee_data\\kijkduin/kijkduin.zip\n</pre> <p>As reference epoch, we use the first epoch in our time series:</p> In\u00a0[4]: Copied! <pre># specify the reference epoch\nreference_epoch_file = os.path.join(pc_dir, pc_list[0])\n\n# read the reference epoch and set the timestamp\nreference_epoch = py4dgeo.read_from_las(reference_epoch_file)\nreference_epoch.timestamp = timestamps[0]\n\n# set the reference epoch in the spatiotemporal analysis object\nanalysis.reference_epoch = reference_epoch\n</pre> # specify the reference epoch reference_epoch_file = os.path.join(pc_dir, pc_list[0])  # read the reference epoch and set the timestamp reference_epoch = py4dgeo.read_from_las(reference_epoch_file) reference_epoch.timestamp = timestamps[0]  # set the reference epoch in the spatiotemporal analysis object analysis.reference_epoch = reference_epoch <pre>[2023-03-29 15:54:53][INFO] Reading point cloud from file 'I:\\etrainee_data\\kijkduin\\pointclouds\\kijkduin_170117_120041.laz'\n[2023-03-29 15:54:53][INFO] Building KDTree structure with leaf parameter 10\n[2023-03-29 15:54:53][INFO] Saving epoch to file 'C:\\Users\\k53\\AppData\\Local\\Temp\\tmp1p2qu55j\\reference_epoch.zip'\n</pre> <p>For epochs to be added, we now configure the M3C2 algorithm to derive the change values. We would like to set the normals purely vertically, so we define a customized computation of cylinder <code>directions</code>:</p> In\u00a0[5]: Copied! <pre># Inherit from the M3C2 algorithm class to define a custom direction algorithm\nclass M3C2_Vertical(py4dgeo.M3C2):\n    def directions(self):\n        return np.array([0, 0, 1]) # vertical vector orientation\n\n# specify corepoints, here all points of the reference epoch\nanalysis.corepoints = reference_epoch.cloud[::]\n\n# specify M3C2 parameters for our custom algorithm class\nanalysis.m3c2 = M3C2_Vertical(cyl_radii=(1.0,), max_distance=10.0, registration_error = 0.019)\n</pre> # Inherit from the M3C2 algorithm class to define a custom direction algorithm class M3C2_Vertical(py4dgeo.M3C2):     def directions(self):         return np.array([0, 0, 1]) # vertical vector orientation  # specify corepoints, here all points of the reference epoch analysis.corepoints = reference_epoch.cloud[::]  # specify M3C2 parameters for our custom algorithm class analysis.m3c2 = M3C2_Vertical(cyl_radii=(1.0,), max_distance=10.0, registration_error = 0.019) <pre>[2023-03-29 15:54:54][INFO] Initializing Epoch object from given point cloud\n[2023-03-29 15:54:54][INFO] Building KDTree structure with leaf parameter 10\n[2023-03-29 15:54:54][INFO] Saving epoch to file 'C:\\Users\\k53\\AppData\\Local\\Temp\\tmpfv704kty\\corepoints.zip'\n</pre> <p>Now we add all the other epochs with their timestamps:</p> In\u00a0[\u00a0]: Copied! <pre># create a list to collect epoch objects\nepochs = []\nfor e, pc_file in enumerate(pc_list[1:]):\n    epoch_file = os.path.join(pc_dir, pc_file)\n    epoch = py4dgeo.read_from_las(epoch_file)\n    epoch.timestamp = timestamps[e]\n    epochs.append(epoch)\n\n# add epoch objects to the spatiotemporal analysis object\nanalysis.add_epochs(*epochs)\n\n# print the spatiotemporal analysis data for 3 corepoints and 5 epochs, respectively\nprint(f\"Space-time distance array:\\n{analysis.distances[:3,:5]}\")\nprint(f\"Uncertainties of M3C2 distance calculation:\\n{analysis.uncertainties['lodetection'][:3, :5]}\")\nprint(f\"Timestamp deltas of analysis:\\n{analysis.timedeltas[:5]}\")\n</pre> # create a list to collect epoch objects epochs = [] for e, pc_file in enumerate(pc_list[1:]):     epoch_file = os.path.join(pc_dir, pc_file)     epoch = py4dgeo.read_from_las(epoch_file)     epoch.timestamp = timestamps[e]     epochs.append(epoch)  # add epoch objects to the spatiotemporal analysis object analysis.add_epochs(*epochs)  # print the spatiotemporal analysis data for 3 corepoints and 5 epochs, respectively print(f\"Space-time distance array:\\n{analysis.distances[:3,:5]}\") print(f\"Uncertainties of M3C2 distance calculation:\\n{analysis.uncertainties['lodetection'][:3, :5]}\") print(f\"Timestamp deltas of analysis:\\n{analysis.timedeltas[:5]}\") <p>We visualize the changes in the scene for a selected epoch, together with the time series of surface changes at a selected location. The location here was selected separately in CloudCompare (as the corepoint id).</p> In\u00a0[7]: Copied! <pre>cp_idx_sel = 15162 # selected core point index\nepoch_idx_sel = 28 # selected epoch index\n\n# import plotting module\nimport matplotlib.pyplot as plt\n\n# allow interactive rotation in notebook\n%matplotlib inline\n\n# create the figure\nfig=plt.figure(figsize=(15,5))\nax1=fig.add_subplot(1,2,1)\nax2=fig.add_subplot(1,2,2)\n\n# get the corepoints\ncorepoints = analysis.corepoints.cloud\n\n# get change values of last epoch for all corepoints\ndistances = analysis.distances\ndistances_epoch = [d[epoch_idx_sel] for d in distances] \n\n# get the time series of changes at a specific core point locations\ncoord_sel = analysis.corepoints.cloud[cp_idx_sel]\ntimeseries_sel = distances[cp_idx_sel]\n\n# get the list of timestamps from the reference epoch timestamp and timedeltas\ntimestamps = [t + analysis.reference_epoch.timestamp for t in analysis.timedeltas]\n\n# plot the scene\nd = ax1.scatter(corepoints[:,0], corepoints[:,1], c=distances_epoch[:], cmap='seismic_r', vmin=-1.5, vmax=1.5, s=1, zorder=1) \nplt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, pad=.15)\n\n# add the location of the selected coordinate\nax1.scatter(coord_sel[0], coord_sel[1], facecolor='yellow', edgecolor='black', s=100, zorder=2, label='Selected location', marker='*')\nax1.legend()\n\n# configure the plot layout\nax1.set_xlabel('X [m]')\nax1.set_ylabel('Y [m]')\nax1.set_aspect('equal')\nax1.set_title('Changes at %s' % (analysis.reference_epoch.timestamp+analysis.timedeltas[epoch_idx_sel]))\n\n# plot the time series\nax2.scatter(timestamps, timeseries_sel, s=5, color='black', label='Raw')\nax2.plot(timestamps, timeseries_sel, color='blue', label='Smoothed')\nax2.set_xlabel('Date')\n\n# add the epoch of the plotted scene\nax2.scatter(timestamps[epoch_idx_sel], timeseries_sel[epoch_idx_sel], facecolor='yellow', marker='*', edgecolor='black', s=100, color='red', label='Selected epoch')\nax2.legend()\n\n# format the date labels\nimport matplotlib.dates as mdates\ndtFmt = mdates.DateFormatter('%b-%d') \nplt.gca().xaxis.set_major_formatter(dtFmt) \n\n# configure the plot layout\nax2.set_ylabel('Distance [m]')\nax2.grid()\nax2.set_ylim(-0.3,1.6)\nax2.set_title('Time series at selected location')\n\nplt.tight_layout()\nplt.show()\n</pre> cp_idx_sel = 15162 # selected core point index epoch_idx_sel = 28 # selected epoch index  # import plotting module import matplotlib.pyplot as plt  # allow interactive rotation in notebook %matplotlib inline  # create the figure fig=plt.figure(figsize=(15,5)) ax1=fig.add_subplot(1,2,1) ax2=fig.add_subplot(1,2,2)  # get the corepoints corepoints = analysis.corepoints.cloud  # get change values of last epoch for all corepoints distances = analysis.distances distances_epoch = [d[epoch_idx_sel] for d in distances]   # get the time series of changes at a specific core point locations coord_sel = analysis.corepoints.cloud[cp_idx_sel] timeseries_sel = distances[cp_idx_sel]  # get the list of timestamps from the reference epoch timestamp and timedeltas timestamps = [t + analysis.reference_epoch.timestamp for t in analysis.timedeltas]  # plot the scene d = ax1.scatter(corepoints[:,0], corepoints[:,1], c=distances_epoch[:], cmap='seismic_r', vmin=-1.5, vmax=1.5, s=1, zorder=1)  plt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, pad=.15)  # add the location of the selected coordinate ax1.scatter(coord_sel[0], coord_sel[1], facecolor='yellow', edgecolor='black', s=100, zorder=2, label='Selected location', marker='*') ax1.legend()  # configure the plot layout ax1.set_xlabel('X [m]') ax1.set_ylabel('Y [m]') ax1.set_aspect('equal') ax1.set_title('Changes at %s' % (analysis.reference_epoch.timestamp+analysis.timedeltas[epoch_idx_sel]))  # plot the time series ax2.scatter(timestamps, timeseries_sel, s=5, color='black', label='Raw') ax2.plot(timestamps, timeseries_sel, color='blue', label='Smoothed') ax2.set_xlabel('Date')  # add the epoch of the plotted scene ax2.scatter(timestamps[epoch_idx_sel], timeseries_sel[epoch_idx_sel], facecolor='yellow', marker='*', edgecolor='black', s=100, color='red', label='Selected epoch') ax2.legend()  # format the date labels import matplotlib.dates as mdates dtFmt = mdates.DateFormatter('%b-%d')  plt.gca().xaxis.set_major_formatter(dtFmt)   # configure the plot layout ax2.set_ylabel('Distance [m]') ax2.grid() ax2.set_ylim(-0.3,1.6) ax2.set_title('Time series at selected location')  plt.tight_layout() plt.show() <pre>[2023-03-29 15:56:48][INFO] Restoring epoch from file 'C:\\Users\\k53\\AppData\\Local\\Temp\\tmpr9igr1u0\\reference_epoch.zip'\n</pre> <p>The map of changes in the scene shows us linear structures of sand deposition near the coast which can be interpreted as sand bars (with knowledge about coastal processes). This is confirmed by the surface behavior over time, expressed in the time series plot. However, the time series is quite noisy especially in this part of the beach, which is regularly covered by water during high tides (leading to missing data) and also varies strongly in surface moisture (influencing the LiDAR range measurement and causing noise). We therefore continue with temporal smoothing of the time series.</p> <p>We apply a rolling median with a defined temporal window of 14 epochs (corresponding to one week of 12-hourly point clouds) using the <code>temporal_averaging()</code> function in py4dgeo.</p> In\u00a0[8]: Copied! <pre>analysis.smoothed_distances = py4dgeo.temporal_averaging(analysis.distances, smoothing_window=14)\n</pre> analysis.smoothed_distances = py4dgeo.temporal_averaging(analysis.distances, smoothing_window=14) <pre>[2023-03-29 15:56:57][INFO] Starting: Smoothing temporal data\n</pre> <pre>h:\\conda_envs\\etrainee\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n</pre> <pre>[2023-03-29 15:57:20][INFO] Finished in 23.2358s: Smoothing temporal data\n</pre> <p>Now we can compare the raw and smoothed time series at our selected location:</p> In\u00a0[9]: Copied! <pre># create the figure\nfig, ax = plt.subplots(1,1,figsize=(7,5))\n\n# plot the raw time series\nax.scatter(timestamps, timeseries_sel, color='blue', label='raw', s=5)\n\n# plot the smoothed time series\ntimeseries_sel_smooth = analysis.smoothed_distances[cp_idx_sel]\nax.plot(timestamps, timeseries_sel_smooth, color='red', label='smooth')\n\n# format the date labels\nimport matplotlib.dates as mdates\ndtFmt = mdates.DateFormatter('%b-%d') \nplt.gca().xaxis.set_major_formatter(dtFmt) \n\n# add plot elements\nax.set_xlabel('Date')\nax.set_ylabel('Distance [m]')\nax.grid()\nax.set_ylim(-.25,1.25)\n\nplt.tight_layout()\nplt.show()\n</pre> # create the figure fig, ax = plt.subplots(1,1,figsize=(7,5))  # plot the raw time series ax.scatter(timestamps, timeseries_sel, color='blue', label='raw', s=5)  # plot the smoothed time series timeseries_sel_smooth = analysis.smoothed_distances[cp_idx_sel] ax.plot(timestamps, timeseries_sel_smooth, color='red', label='smooth')  # format the date labels import matplotlib.dates as mdates dtFmt = mdates.DateFormatter('%b-%d')  plt.gca().xaxis.set_major_formatter(dtFmt)   # add plot elements ax.set_xlabel('Date') ax.set_ylabel('Distance [m]') ax.grid() ax.set_ylim(-.25,1.25)  plt.tight_layout() plt.show() <p>From the smoothed time series at the selected location, we can now much better interpret the surface behavior. In fact, we can distinctly observe that there were two consecutive occurrences of temporary deposition of several weeks. These represent two phases where sand bars are present. They can be extracted as individual objects by the 4D objects-by-change method. Before, we continue with time series clustering and the assessment of overall change patterns in the following.</p> <p>We perform k-means clustering for a set of <code>k</code>s at once and collect the resulting labels for subsequent assessment:</p> In\u00a0[\u00a0]: Copied! <pre># import kmeans clustering module from scikit-learn\nfrom sklearn.cluster import KMeans\n\n# use the smoothed distances for clustering\ndistances = analysis.smoothed_distances\n\n# define the number of clusters\nks = [5,10,20,50]\n\n# create an array to store the labels\nlabels = np.full((distances.shape[0], len(ks)), np.nan)\n\n# perform clustering for each number of clusters\nfor kidx, k in enumerate(ks):\n    print(f'Performing clustering with k={k}...')\n    nan_indicator = np.logical_not(np.isnan(np.sum(distances, axis=1)))\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(distances[nan_indicator, :])\n    labels[nan_indicator, kidx] = kmeans.labels_\n</pre> # import kmeans clustering module from scikit-learn from sklearn.cluster import KMeans  # use the smoothed distances for clustering distances = analysis.smoothed_distances  # define the number of clusters ks = [5,10,20,50]  # create an array to store the labels labels = np.full((distances.shape[0], len(ks)), np.nan)  # perform clustering for each number of clusters for kidx, k in enumerate(ks):     print(f'Performing clustering with k={k}...')     nan_indicator = np.logical_not(np.isnan(np.sum(distances, axis=1)))     kmeans = KMeans(n_clusters=k, random_state=0).fit(distances[nan_indicator, :])     labels[nan_indicator, kidx] = kmeans.labels_ <p>Now we can visualize the resulting change patterns for different numbers of clusters:</p> In\u00a0[11]: Copied! <pre>fig, axs = plt.subplots(1,4, figsize=(12,7))\n(ax1,ax2,ax3,ax4) = axs\n\ncmap_clustering = 'tab20'\nsc1 = ax1.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,0],cmap=cmap_clustering,s=1, label=ks[0])\n\nsc2 = ax2.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,1],cmap=cmap_clustering,s=1, label=ks[1])\n\nsc3 = ax3.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,2],cmap=cmap_clustering,s=1, label=ks[2])\n\nsc4 = ax4.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,3],cmap=cmap_clustering,s=1, label=ks[3])\n\nax_c = 0\nfor ax in axs:\n    ax.set_aspect('equal')\n    ax.set_title(f'# clusters = {ks[ax_c]}')\n    ax_c+=1\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axs = plt.subplots(1,4, figsize=(12,7)) (ax1,ax2,ax3,ax4) = axs  cmap_clustering = 'tab20' sc1 = ax1.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,0],cmap=cmap_clustering,s=1, label=ks[0])  sc2 = ax2.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,1],cmap=cmap_clustering,s=1, label=ks[1])  sc3 = ax3.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,2],cmap=cmap_clustering,s=1, label=ks[2])  sc4 = ax4.scatter(corepoints[:,0],corepoints[:,1],c=labels[:,3],cmap=cmap_clustering,s=1, label=ks[3])  ax_c = 0 for ax in axs:     ax.set_aspect('equal')     ax.set_title(f'# clusters = {ks[ax_c]}')     ax_c+=1  plt.tight_layout() plt.show() <p>When using a small number of clusters (k=5), a large part of the beach is assigned to one cluster of assumingly little activity (gray area). From our exploration of changes in the scene at a selected epoch above, we further obtain two clusters with mainly deposition (blue clusters) and one cluster in the erosion areas around the pathway through the dunes. With a higher number of clusters (k=10 to k=50), the coarser clusters are further split up into (assumingly) finer patterns.</p> <p>We can look into the time series properties within selected clusters, to interpret the change pattern they are representing. Here, we will check the clusters derived with k=10 by plotting the median values of all time series per cluster:</p> In\u00a0[12]: Copied! <pre># create the figure\nfig, axs = plt.subplots(1,2, figsize=(12,7))\nax1,ax2 = axs\n\n# get the labels for the selected number of clusters\nlabels_k = labels[:,1]\n\n# plot the map of clusters\nsc = ax1.scatter(corepoints[:,0],corepoints[:,1],c=labels_k,cmap=cmap_clustering,s=1)\n\n# plot the time series representing the clusters (median of each cluster)\nfor l in np.unique(labels_k):\n    ax2.plot(timestamps, np.nanmedian(distances[labels_k==l,:], axis=0), label=f'cluster {l}')\n\n# format the date labels\nimport matplotlib.dates as mdates\ndtFmt = mdates.DateFormatter('%b-%d')\nplt.gca().xaxis.set_major_formatter(dtFmt)\n\n# add plot elements\nax1.set_aspect('equal')\nax1.set_title(f'# clusters = {ks[1]}')\nax2.set_xlabel('Date')\nax2.set_ylabel('Distance [m]')\nax2.grid()\n\nplt.tight_layout()\nplt.show()\n</pre> # create the figure fig, axs = plt.subplots(1,2, figsize=(12,7)) ax1,ax2 = axs  # get the labels for the selected number of clusters labels_k = labels[:,1]  # plot the map of clusters sc = ax1.scatter(corepoints[:,0],corepoints[:,1],c=labels_k,cmap=cmap_clustering,s=1)  # plot the time series representing the clusters (median of each cluster) for l in np.unique(labels_k):     ax2.plot(timestamps, np.nanmedian(distances[labels_k==l,:], axis=0), label=f'cluster {l}')  # format the date labels import matplotlib.dates as mdates dtFmt = mdates.DateFormatter('%b-%d') plt.gca().xaxis.set_major_formatter(dtFmt)  # add plot elements ax1.set_aspect('equal') ax1.set_title(f'# clusters = {ks[1]}') ax2.set_xlabel('Date') ax2.set_ylabel('Distance [m]') ax2.grid()  plt.tight_layout() plt.show() <pre>h:\\conda_envs\\etrainee\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n</pre> <p>Now we can relate the temporal behavior to the spatial clusters. For example, we see strong and sudden surface increases for the orange and red clusters (with different timing and magnitude). The gray cluster (occuring in two spatial extents) represents sand bars, which we know from our time series example above. The large coverage of the brown cluster shows a time series with little activity - except for fluctuations, especially towards the end of the observation period, which is contained in all clusters. We can assume that this represents some measurement effects in the data that could not be corrected by the preprocessing and alignment procedure ([Vos et al., 2022]).</p> <p>In this solution, we will use the selected location from above to extract the sand bars as 4D object-by-change. The strength of the method is that the occurrences are identified individually, even though they have spatial overlap, as they can be separated in the time series information.</p> <p>We instantiate the <code>RegionGrowingAlgorithm</code> class using the parameters of Anders et al, 2021, and run the object extraction:</p> In\u00a0[13]: Copied! <pre># parametrize the 4D-OBC extraction\nalgo = py4dgeo.RegionGrowingAlgorithm(window_width=14, \n                                      minperiod=2, \n                                      height_threshold=0.05, \n                                      neighborhood_radius=1.0,\n                                      min_segments=10, \n                                      thresholds=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n                                      seed_candidates=[cp_idx_sel])\n\n# run the algorithm\nanalysis.invalidate_results(seeds=True, objects=True, smoothed_distances=False) # only required if you want to re-run the algorithm\nobjects = algo.run(analysis)\n</pre> # parametrize the 4D-OBC extraction algo = py4dgeo.RegionGrowingAlgorithm(window_width=14,                                        minperiod=2,                                        height_threshold=0.05,                                        neighborhood_radius=1.0,                                       min_segments=10,                                        thresholds=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],                                        seed_candidates=[cp_idx_sel])  # run the algorithm analysis.invalidate_results(seeds=True, objects=True, smoothed_distances=False) # only required if you want to re-run the algorithm objects = algo.run(analysis) <pre>[2023-03-29 15:58:32][INFO] Removing intermediate results from the analysis file I:\\etrainee_data\\kijkduin/kijkduin.zip\n[2023-03-29 15:58:32][INFO] Starting: Find seed candidates in time series\n[2023-03-29 15:58:32][INFO] Finished in 0.0017s: Find seed candidates in time series\n[2023-03-29 15:58:32][INFO] Starting: Sort seed candidates by priority\n[2023-03-29 15:58:32][INFO] Finished in 0.0016s: Sort seed candidates by priority\n[2023-03-29 15:58:32][INFO] Starting: Performing region growing on seed candidate 1/3\n[2023-03-29 15:58:32][INFO] Finished in 0.0565s: Performing region growing on seed candidate 1/3\n[2023-03-29 15:58:32][INFO] Starting: Performing region growing on seed candidate 2/3\n[2023-03-29 15:58:32][INFO] Finished in 0.0879s: Performing region growing on seed candidate 2/3\n[2023-03-29 15:58:32][INFO] Starting: Performing region growing on seed candidate 3/3\n[2023-03-29 15:58:33][INFO] Finished in 0.5324s: Performing region growing on seed candidate 3/3\n</pre> <p></p> In\u00a0[14]: Copied! <pre>seed_timeseries = analysis.smoothed_distances[cp_idx_sel]\nplt.plot(timestamps,seed_timeseries, c='black', linestyle='--', linewidth=0.5, label='Seed timeseries')\n\nfor sid, example_seed in enumerate(analysis.seeds):\n    seed_end = example_seed.end_epoch\n    seed_start = example_seed.start_epoch\n    seed_cp_idx = example_seed.index\n\n    plt.plot(timestamps[seed_start:seed_end+1], seed_timeseries[seed_start:seed_end+1], label=f'Seed {sid}')\n\n# format the date labels\ndtFmt = mdates.DateFormatter('%b-%d')\nplt.gca().xaxis.set_major_formatter(dtFmt)\n\n# add plot elements\nplt.xlabel('Date')\nplt.ylabel('Distance [m]')\n\nplt.legend()\nplt.show()\n</pre> seed_timeseries = analysis.smoothed_distances[cp_idx_sel] plt.plot(timestamps,seed_timeseries, c='black', linestyle='--', linewidth=0.5, label='Seed timeseries')  for sid, example_seed in enumerate(analysis.seeds):     seed_end = example_seed.end_epoch     seed_start = example_seed.start_epoch     seed_cp_idx = example_seed.index      plt.plot(timestamps[seed_start:seed_end+1], seed_timeseries[seed_start:seed_end+1], label=f'Seed {sid}')  # format the date labels dtFmt = mdates.DateFormatter('%b-%d') plt.gca().xaxis.set_major_formatter(dtFmt)  # add plot elements plt.xlabel('Date') plt.ylabel('Distance [m]')  plt.legend() plt.show() <p>At the selected location, three separate surface activities are detected as seed for 4D-OBC extraction. We may not be satisfied with the determination of the timing. In a real analysis, we would now aim to improve the temporal detection - either by using a different approach of seed detection (cf. algorithm customization), or by postprocessing the temporal segments from the current seed detection.</p> <p>Here, we use the result and look into the spatial object properties:</p> In\u00a0[15]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize=(15,7))\nax1,ax2 = axs\n\n# get indices of 4D-OBC\nsel_seed_idx = 1\nidxs = objects[sel_seed_idx].indices\n\n# get change values at end of object for each location\nepoch_of_interest = int((objects[sel_seed_idx].end_epoch - objects[sel_seed_idx].start_epoch)/2 + objects[sel_seed_idx].start_epoch)\ndistances_of_interest = analysis.smoothed_distances[:, epoch_of_interest]\n\n# get the change magnitude between end and start of object for each location\nmagnitudes_of_interest = analysis.smoothed_distances[:, epoch_of_interest] - analysis.smoothed_distances[:, int(objects[sel_seed_idx].start_epoch)]\n\n# set the colormap according to magnitude at each location in the object\ncrange = 1.0\nimport matplotlib.colors as mcolors\ncmap = plt.get_cmap('seismic_r').copy()\nnorm = mcolors.CenteredNorm(halfrange=crange)\ncmapvals = norm(magnitudes_of_interest)\n\n# plot the timeseries of the segmented locations (colored by time series similarity)\nfor idx in idxs[::10]:\n    ax1.plot(timestamps, analysis.smoothed_distances[idx], c=cmap(cmapvals[idx]), linewidth=0.5)\n\n# plot the seed time series\nax1.plot(timestamps, analysis.smoothed_distances[cp_idx_sel], c='black', linewidth=1., label='Seed timeseries')\n\n# fill the area of the object\nax1.axvspan(timestamps[objects[sel_seed_idx].start_epoch], timestamps[objects[sel_seed_idx].end_epoch], alpha=0.3, color='grey', label='4D-OBC timespan')\n\n# add legend and format the date labels\ndtFmt = mdates.DateFormatter('%b-%d')\nplt.gca().xaxis.set_major_formatter(dtFmt)\nax1.legend()\n\n# get subset of core points incorporated in 4D-OBC\ncloud = analysis.corepoints.cloud\nsubset_cloud = cloud[idxs,:2]\n\n# plot coordinates colored by change values at end magnitude of object\nd = ax2.scatter(cloud[:,0], cloud[:,1], c = magnitudes_of_interest, cmap='seismic_r', vmin=-crange, vmax=crange, s=1)\nplt.colorbar(d, format=('%.2f'), label='Change magnitude [m]', ax=ax2)\nax2.set_aspect('equal')\n\n# plot convex hull of 4D-OBC\nfrom scipy.spatial import ConvexHull\nfrom matplotlib.patches import Polygon\nhull = ConvexHull(subset_cloud)\nax2.add_patch(Polygon(subset_cloud[hull.vertices,0:2], label = '4D-OBC hull', fill = False))\n\n# plot seed location of 4D-OBC\nax2.scatter(cloud[cp_idx_sel,0], cloud[cp_idx_sel,1], marker = '*', c = 'black', label = 'Seed')\n\n# add plot elements\nax1.set_title('Time series of segmented 4D-OBC locations')\nax1.set_xlabel('Date')\nax1.set_ylabel('Distance [m]')\nax2.set_title(f'Magnitudes of change in the 4D-OBC timespan\\n({timestamps[epoch_of_interest]-timestamps[analysis.objects[sel_seed_idx].start_epoch]} hours)')\nax2.set_xlabel('X [m]')\nax2.set_ylabel('Y [m]')\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axs = plt.subplots(1,2, figsize=(15,7)) ax1,ax2 = axs  # get indices of 4D-OBC sel_seed_idx = 1 idxs = objects[sel_seed_idx].indices  # get change values at end of object for each location epoch_of_interest = int((objects[sel_seed_idx].end_epoch - objects[sel_seed_idx].start_epoch)/2 + objects[sel_seed_idx].start_epoch) distances_of_interest = analysis.smoothed_distances[:, epoch_of_interest]  # get the change magnitude between end and start of object for each location magnitudes_of_interest = analysis.smoothed_distances[:, epoch_of_interest] - analysis.smoothed_distances[:, int(objects[sel_seed_idx].start_epoch)]  # set the colormap according to magnitude at each location in the object crange = 1.0 import matplotlib.colors as mcolors cmap = plt.get_cmap('seismic_r').copy() norm = mcolors.CenteredNorm(halfrange=crange) cmapvals = norm(magnitudes_of_interest)  # plot the timeseries of the segmented locations (colored by time series similarity) for idx in idxs[::10]:     ax1.plot(timestamps, analysis.smoothed_distances[idx], c=cmap(cmapvals[idx]), linewidth=0.5)  # plot the seed time series ax1.plot(timestamps, analysis.smoothed_distances[cp_idx_sel], c='black', linewidth=1., label='Seed timeseries')  # fill the area of the object ax1.axvspan(timestamps[objects[sel_seed_idx].start_epoch], timestamps[objects[sel_seed_idx].end_epoch], alpha=0.3, color='grey', label='4D-OBC timespan')  # add legend and format the date labels dtFmt = mdates.DateFormatter('%b-%d') plt.gca().xaxis.set_major_formatter(dtFmt) ax1.legend()  # get subset of core points incorporated in 4D-OBC cloud = analysis.corepoints.cloud subset_cloud = cloud[idxs,:2]  # plot coordinates colored by change values at end magnitude of object d = ax2.scatter(cloud[:,0], cloud[:,1], c = magnitudes_of_interest, cmap='seismic_r', vmin=-crange, vmax=crange, s=1) plt.colorbar(d, format=('%.2f'), label='Change magnitude [m]', ax=ax2) ax2.set_aspect('equal')  # plot convex hull of 4D-OBC from scipy.spatial import ConvexHull from matplotlib.patches import Polygon hull = ConvexHull(subset_cloud) ax2.add_patch(Polygon(subset_cloud[hull.vertices,0:2], label = '4D-OBC hull', fill = False))  # plot seed location of 4D-OBC ax2.scatter(cloud[cp_idx_sel,0], cloud[cp_idx_sel,1], marker = '*', c = 'black', label = 'Seed')  # add plot elements ax1.set_title('Time series of segmented 4D-OBC locations') ax1.set_xlabel('Date') ax1.set_ylabel('Distance [m]') ax2.set_title(f'Magnitudes of change in the 4D-OBC timespan\\n({timestamps[epoch_of_interest]-timestamps[analysis.objects[sel_seed_idx].start_epoch]} hours)') ax2.set_xlabel('X [m]') ax2.set_ylabel('Y [m]') ax2.legend(loc='upper right')  plt.tight_layout() plt.show() <p>You may now check the different spatial extents for the three objects extracted from this seed location. In subsequent analysis, the spatial-temporal extent of each object can be used to describe individual activities (e.g., their change rates, magnitudes, ...) and relating them to one another in their timing and spatial distribution.</p> <p>We will not further go into the analysis here, but note that the 4D point clouds at this sandy beach are features as a research-oriented case study!</p> <p></p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#exercise-time-series-analysis-of-3d-point-clouds","title":"Exercise: Time series analysis of 3D point clouds\u00b6","text":""},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#time-series-based-change-analysis-of-surface-dynamics-at-a-sandy-beach","title":"Time series-based change analysis of surface dynamics at a sandy beach\u00b6","text":"<p>In this exercise, you will perform time series-based surface change analysis on a time series of permanent TLS point clouds of the sandy beach at Kijkduin for a timespan of around 6 months (Vos et al., 2022). See the introduction to the case study and dataset here.</p> <p>The objective is to assess surface dynamics with two methods: time series clustering (following Kuschnerus et al., 2021) and 4D objects-by-change (following Anders et al., 2021). Look into the related articles for comparison of possible surface dynamics at the site and help for deciding on suitable parameters, etc.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#software-and-data","title":"Software and data\u00b6","text":"<p>This exercise should be solved using Python with the <code>py4dgeo</code> library. You can follow the workflow introduced in the lesson of this theme. Also, make use of the software documentations!</p> <p>Use CloudCompare or GIS Software (e.g., QGIS) to check and visualize your results.</p> <p>The dataset will be a subsampled version of the original time series, using 12-hourly epochs of point clouds and spatial subsampling to 50 cm. In the data directory <code>kijkduin</code>, you find the prepared input point clouds and a core points point cloud, which is manually cleaned from noise.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#loading-data-and-calculation-of-surface-changes","title":"Loading data and calculation of surface changes\u00b6","text":"<p>Prepare the analysis by compiling the list of files (epochs) and read the timestamps from the file names (format <code>YYMMDD_hhmmss</code>) into <code>datetime</code> objects. Use the point cloud files and timestamps to create a py4dgeo <code>SpatiotemporalAnalysis</code> object. For this you need to instantiate the M3C2 algorithm. You can use the point cloud file <code>170115_150816_aoi_50cm.laz</code> as core points. Explore the point cloud properties in CloudCompare:</p> <ul> <li>Considering the available point density and surface characteristics, what would be a suitable cylinder radius for the distance calculation?</li> <li>What would be a suitable approach to derive the surface normals in this topography and expected types of surface changes?</li> </ul> <p>Hint: In this flat topography and predominant provess of sand deposition and erosion, it can be suitable to orient the normals purely vertically. In this case, they do not need to be computed, and you can customize the py4dgeo algorithm accordingly.</p> <p>Use the first point cloud in the time series (list of files) as reference epoch. You can assume a registration error of 1.9 cm for the M3C2 distance calculation (cf. Vos et al., 2022).</p> <p>Explore the spatiotemporal change information by visualizing the changes at a selected epoch and visualizing the time series at a selected location.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#temporal-smoothing","title":"Temporal smoothing\u00b6","text":"<p>You are dealing with a temporal subset of the original hourly time series. The effect of temporal measurement variability may therefore be less pronounced (compared to the assessment in, e.g., Anders et al., 2019). Nonetheless, you may apply temporal smoothing to reduce the influence of noise on your change analysis using a rolling median averaging of one week. This will also fill possible gaps in your data, e.g., lower ranges during poor atmospheric conditions or no data due to water coverage during tides on the lower beach part.</p> <p>Visualize the raw and smoothed change values in the time series of your selected location.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#time-series-clustering","title":"Time series clustering\u00b6","text":"<p>To derive characteristic change patterns on the sandy beach, perform k-means clustering of the time series following Kuschnerus et al. (2021). Assess the clustering for different selections of <code>k</code> numbers of clusters.</p> <ul> <li>Can you interpret the characteristics of different parts on the beach? Visualize example time series for different clusters.</li> <li>From which number of clusters do you see a clear separation in overall units of the beach area?</li> <li>What are some detail change patterns that become visible for a higher number of clusters?</li> </ul>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#extraction-of-4d-objects-by-change","title":"Extraction of 4D objects-by-change\u00b6","text":"<p>Now use the 4D objects-by-change (4D-OBC) method to identify individual surface activities in the beach scene. The objective is to extract temporary occurrences of accumulation or erosion, as occurs when a sand bar is formed during some timespan, or when sand is deposited by anthropogenic works. This type of surface activity is implemented with the original seed detection in py4dgeo, so you do not need to customize the algorithm. Decide for suitable parameters following Anders et al. (2021) - but bear in mind that we are using a different temporal resolution, so you may need to adjust the temporal change detection.</p> <p>Perform the extraction for selected seed locations, e.g. considering interesting clusters of change patterns identified in the previous step. In principle, the spatiotemporal segmentation can also be applied to the full dataset (all time series at all core point locations are used as potential seed candidates), but long processing time needs to be expected.</p>"},{"location":"module3/04_3d_timeseries_analysis/exercise/m3_theme4_exercise1_solution1.html#references","title":"References\u00b6","text":"<ul> <li>Anders, K., Lindenbergh, R. C., Vos, S. E., Mara, H., de Vries, S., &amp; H\u00f6fle, B. (2019). High-Frequency 3D Geomorphic Observation Using Hourly Terrestrial Laser Scanning Data Of A Sandy Beach. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-2/W5, pp. 317-324. doi: 10.5194/isprs-annals-IV-2-W5-317-2019.</li> <li>Anders, K., Winiwarter, L., Mara, H., Lindenbergh, R., Vos, S. E., &amp; H\u00f6fle, B. (2021). Fully automatic spatiotemporal segmentation of 3D LiDAR time series for the extraction of natural surface changes. ISPRS Journal of Photogrammetry and Remote Sensing, 173, pp. 297-308. doi: 10.1016/j.isprsjprs.2021.01.015.</li> <li>Kuschnerus, M., Lindenbergh, R., &amp; Vos, S. (2021). Coastal change patterns from time series clustering of permanent laser scan data. Earth Surface Dynamics, 9 (1), pp. 89-103. doi: 10.5194/esurf-9-89-2021.</li> <li>Vos, S., Anders, K., Kuschnerus, M., Lindenberg, R., H\u00f6fle, B., Aarnikhof, S. &amp; Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands.  Scientific Data, 9:191. doi: 10.1038/s41597-022-01291-9.</li> </ul>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html","title":"Lesson","text":"Metadata      title: \"E-TRAINEE Machine learning on point clouds with Python\"     description: \"This is the fifth theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2022-06-15      author: Lukas Winiwarter      contributors: Katharina Anders, Bernhard H\u00f6fle     estimatedTime: 1.5 hrs  <p>The objective in this theme is to:</p> <ol> <li>Use classification on point cloud data to separate the dataset into semantically meaningful segments</li> <li>Derive point cloud distances (cf. Theme 3: 3D change detection and analysis)</li> <li>Train a regression method to predict distances from one single epoch for us (i.e., predicting where how much change will happen)</li> </ol> <p>In this theme, you will learn about:</p> <ul> <li>Creating feature spaces based on local neighborhoods</li> <li>Training a supervised classifier</li> <li>Support Vector Machines</li> <li>Scaling</li> <li>Evaluation of supervised classifiers</li> <li>Random forests</li> <li>Unsupervised classification</li> <li>Regression models</li> <li>A few words on Deep Learning</li> </ul> <p>After finishing this theme you will be able to:</p> <ul> <li>derive a feature vector from the local neighbourhood of a point cloud</li> <li>apply supervised (SVM, Random Forest) and unsupervised (kMeans) classification methods</li> <li>apply supervised regression methods (Random Forest Regression)</li> </ul> In\u00a0[1]: Copied! <pre>import polyscope as ps\nimport numpy as np\n\n# add the script assets folder to the path, so that we can import the functions created there\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))\n\n# import point cloud functions\nimport pointcloud_functions as pcfuncs\n\nDATA_PATH = 'path-to-data'\n\n# check if the specified data path exists\nimport os\nif not os.path.isdir(DATA_PATH):\n    print(f'ERROR: {DATA_PATH} does not exist')\n    print('Please specify the correct path to the data directory by replacing &lt;path-to-data&gt; above.')\n\n# read data, but only every 100th point (for quick visualisation)\ndata_2009 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2009.las\", use_every=100, get_attributes=True)\ndata_2017 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2017.las\", use_every=100, get_attributes=True)\n</pre> import polyscope as ps import numpy as np  # add the script assets folder to the path, so that we can import the functions created there import sys from pathlib import Path sys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))  # import point cloud functions import pointcloud_functions as pcfuncs  DATA_PATH = 'path-to-data'  # check if the specified data path exists import os if not os.path.isdir(DATA_PATH):     print(f'ERROR: {DATA_PATH} does not exist')     print('Please specify the correct path to the data directory by replacing  above.')  # read data, but only every 100th point (for quick visualisation) data_2009 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2009.las\", use_every=100, get_attributes=True) data_2017 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2017.las\", use_every=100, get_attributes=True) In\u00a0[2]: Copied! <pre># start up polyscope\nps.init()\npc_2009 = ps.register_point_cloud(\"point cloud 2009\", data_2009[0])\n# for the ALS point cloud, add the Intensity as a scalar value\npc_2009.add_scalar_quantity('Intensity', data_2009[1][\"intensity\"])\npc_2017 = ps.register_point_cloud(\"point cloud 2017\", data_2017[0])\n# for the DIM point cloud, add RGB attributes. We need to provide them as a Nx3 array, so we stack red, green and blue together.\n# furthermore, they need to be values between 0 and 1, but are \"unsigned integer with 16 bit\", i.e. values between 0 and 65535.\npc_2017.add_color_quantity('RGB', np.array([data_2017[1][\"red\"], data_2017[1][\"green\"], data_2017[1][\"blue\"]]).T / 65535)\nps.set_up_dir(\"z_up\")\nps.show()\n</pre> # start up polyscope ps.init() pc_2009 = ps.register_point_cloud(\"point cloud 2009\", data_2009[0]) # for the ALS point cloud, add the Intensity as a scalar value pc_2009.add_scalar_quantity('Intensity', data_2009[1][\"intensity\"]) pc_2017 = ps.register_point_cloud(\"point cloud 2017\", data_2017[0]) # for the DIM point cloud, add RGB attributes. We need to provide them as a Nx3 array, so we stack red, green and blue together. # furthermore, they need to be values between 0 and 1, but are \"unsigned integer with 16 bit\", i.e. values between 0 and 65535. pc_2017.add_color_quantity('RGB', np.array([data_2017[1][\"red\"], data_2017[1][\"green\"], data_2017[1][\"blue\"]]).T / 65535) ps.set_up_dir(\"z_up\") ps.show() <p>You can see that the 2009 point cloud is higher than the 2017 point cloud in two areas. These can be attributed to significant mass loss of the glacier over the course of these 8 years (remember also your change detection results with the same data in Theme 2). If you display either the intensity values of the 2009 point cloud or the RGB values of the 2017 point cloud (use the menu on the left side), you will also see the glacier ice in contrast to the bare rock surrounding it.</p> <p></p> In\u00a0[3]: Copied! <pre>def create_feature_vector(neighbour_points):\n    # structure tensor\n    struct_tensor = np.cov(neighbour_points.T)\n    # eigenvalue decomposition\n    eigvals, eigvec = np.linalg.eigh(struct_tensor)\n    l3, l2, l1 = eigvals\n\n    # find eigenvector to smallest eigenvalue = normal vector to best fitting plane\n    normalvector = eigvec[:, 0]\n    # flip so that it always points \"upwards\"\n    if normalvector[2] &lt; 0:\n        normalvector *= -1\n\n    # feature calculation\n    planarity = (l2-l3)/l1\n    linearity = (l1-l2)/l1\n    omnivariance = (l1*l2*l3)**(1./3)\n    roughness = l3\n    slope = np.arctan2(np.linalg.norm(normalvector[:2]), normalvector[2])\n\n    return np.array([planarity,\n                     linearity,\n                     omnivariance,\n                     roughness,\n                     slope])\n</pre> def create_feature_vector(neighbour_points):     # structure tensor     struct_tensor = np.cov(neighbour_points.T)     # eigenvalue decomposition     eigvals, eigvec = np.linalg.eigh(struct_tensor)     l3, l2, l1 = eigvals      # find eigenvector to smallest eigenvalue = normal vector to best fitting plane     normalvector = eigvec[:, 0]     # flip so that it always points \"upwards\"     if normalvector[2] &lt; 0:         normalvector *= -1      # feature calculation     planarity = (l2-l3)/l1     linearity = (l1-l2)/l1     omnivariance = (l1*l2*l3)**(1./3)     roughness = l3     slope = np.arctan2(np.linalg.norm(normalvector[:2]), normalvector[2])      return np.array([planarity,                      linearity,                      omnivariance,                      roughness,                      slope]) <p>Now we will query the neighbours of every point and subsequently call the function to create the feature vector. As we have a lot of points and neighbours, this will take some time, even if we use a spatial index for the neighbourhood query.</p> In\u00a0[4]: Copied! <pre>points_2009 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2009.las\")\nfrom scipy import spatial\nprint(\"Building KD tree...\")\ntree_2009 = spatial.KDTree(points_2009)\nprint(\"Querying neighbours...\")\nneighbours = tree_2009.query_ball_tree(tree_2009, r=5)\n\nfeature_array = np.empty((points_2009.shape[0], 5), dtype=np.float32)\nprint(\"Calculating features...\")\nfrom tqdm import tqdm\nfor pix, point_neighbours in tqdm(enumerate(neighbours), unit='pts', total=points_2009.shape[0]):\n    curr_neighbours = points_2009[point_neighbours]\n    curr_features = create_feature_vector(curr_neighbours)\n    feature_array[pix, :] = curr_features\n</pre> points_2009 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2009.las\") from scipy import spatial print(\"Building KD tree...\") tree_2009 = spatial.KDTree(points_2009) print(\"Querying neighbours...\") neighbours = tree_2009.query_ball_tree(tree_2009, r=5)  feature_array = np.empty((points_2009.shape[0], 5), dtype=np.float32) print(\"Calculating features...\") from tqdm import tqdm for pix, point_neighbours in tqdm(enumerate(neighbours), unit='pts', total=points_2009.shape[0]):     curr_neighbours = points_2009[point_neighbours]     curr_features = create_feature_vector(curr_neighbours)     feature_array[pix, :] = curr_features <pre>Building KD tree...\nQuerying neighbours...\nCalculating features...\n</pre> <pre>  1%|          | 8621/1694840 [00:06&lt;07:08, 3938.63pts/s]C:\\Users\\Katharina\\AppData\\Local\\Temp\\ipykernel_7548\\3722135646.py:15: RuntimeWarning: invalid value encountered in scalar divide\n  planarity = (l2-l3)/l1\nC:\\Users\\Katharina\\AppData\\Local\\Temp\\ipykernel_7548\\3722135646.py:16: RuntimeWarning: invalid value encountered in scalar divide\n  linearity = (l1-l2)/l1\n  1%|          | 11679/1694840 [00:07&lt;08:43, 3214.33pts/s]C:\\Users\\Katharina\\AppData\\Local\\Temp\\ipykernel_7548\\3722135646.py:17: RuntimeWarning: invalid value encountered in scalar power\n  omnivariance = (l1*l2*l3)**(1./3)\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1694840/1694840 [07:09&lt;00:00, 3945.06pts/s]\n</pre> In\u00a0[5]: Copied! <pre>ps.init()\npc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009)\n\npc_2009.add_scalar_quantity('Planarity', feature_array[:, 0])\npc_2009.add_scalar_quantity('Linearity', feature_array[:, 1])\npc_2009.add_scalar_quantity('Omnivariance', feature_array[:, 2])\npc_2009.add_scalar_quantity('Roughness', feature_array[:, 3])\npc_2009.add_scalar_quantity('Slope', feature_array[:, 4])\nps.set_up_dir(\"z_up\")\nps.show()\n</pre> ps.init() pc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009)  pc_2009.add_scalar_quantity('Planarity', feature_array[:, 0]) pc_2009.add_scalar_quantity('Linearity', feature_array[:, 1]) pc_2009.add_scalar_quantity('Omnivariance', feature_array[:, 2]) pc_2009.add_scalar_quantity('Roughness', feature_array[:, 3]) pc_2009.add_scalar_quantity('Slope', feature_array[:, 4]) ps.set_up_dir(\"z_up\") ps.show() <p>In the visualization, we can see that some features (Omnivariance, Roughness) outline the glacier well, but a simple thresholding would not suffice. We can also plot the points in a 2D slice (as a histogram) through the feature space using these variables as dimensions:</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.hist2d(feature_array[:, 2], feature_array[:, 4], bins=100, range=[[0, 1], [0, 3.14/2]])\nplt.xlabel(\"Omnivariance\")\nplt.ylabel(\"Slope [rad]\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.hist2d(feature_array[:, 2], feature_array[:, 4], bins=100, range=[[0, 1], [0, 3.14/2]]) plt.xlabel(\"Omnivariance\") plt.ylabel(\"Slope [rad]\") plt.show() <p></p> In\u00a0[7]: Copied! <pre>from ipyleaflet import Map, Rectangle, DrawControl\n\nfrom pyproj import Transformer\nt_utm_to_wgs = Transformer.from_crs(25832, 4326)\nt_wgs_to_utm = Transformer.from_crs(4326, 25832)\n\ndraw_control1 = DrawControl()\ndraw_control1.polyline = {}\ndraw_control1.circlemarker = {}\ndraw_control1.polygon = {\n    \"shapeOptions\": {\n        \"fillColor\": \"#6be5c3\",\n        \"color\": \"#6be5c3\",\n        \"fillOpacity\": 1.0\n    },\n    \"drawError\": {\n        \"color\": \"#dd253b\",\n        \"message\": \"Oups!\"\n    },\n    \"allowIntersection\": False\n}\n\ndraw_control1.data = [{'type': 'Feature',\n  'properties': {'style': {'pane': 'overlayPane',\n    'attribution': None,\n    'bubblingMouseEvents': True,\n    'fill': True,\n    'smoothFactor': 1,\n    'noClip': False,\n    'stroke': True,\n    'color': '#6be5c3',\n    'weight': 4,\n    'opacity': 0.5,\n    'lineCap': 'round',\n    'lineJoin': 'round',\n    'dashArray': None,\n    'dashOffset': None,\n    'fillColor': '#6be5c3',\n    'fillOpacity': 1,\n    'fillRule': 'evenodd',\n    'interactive': True,\n    'clickable': True}},\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[\n       [ 8.443458056811993, 61.573158353361549 ], \n       [ 8.444874560126243, 61.574126167025533 ], \n       [ 8.447390936602142, 61.573880250022164 ], \n       [ 8.447774225734232, 61.572595103638051 ], \n       [ 8.44485789538137, 61.572301575079962 ], \n       [ 8.443458056811993, 61.573158353361549 ]\n   ]]}},\n        {'type': 'Feature',\n  'properties': {'style': {'pane': 'overlayPane',\n    'attribution': None,\n    'bubblingMouseEvents': True,\n    'fill': True,\n    'smoothFactor': 1,\n    'noClip': False,\n    'stroke': True,\n    'color': '#1515f1',\n    'weight': 4,\n    'opacity': 0.5,\n    'lineCap': 'round',\n    'lineJoin': 'round',\n    'dashArray': None,\n    'dashOffset': None,\n    'fillColor': '#5115f1',\n    'fillOpacity': 1,\n    'fillRule': 'evenodd',\n    'interactive': True,\n    'clickable': True}},\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[\n       [ 8.450823874046085, 61.576712143890624 ], \n       [ 8.453556892205341, 61.577663982918892 ], \n       [ 8.456356569344091, 61.576426586484779 ], \n       [ 8.453323585777111, 61.57536365525263 ], \n       [ 8.451390475371786, 61.575062220361872 ], \n       [ 8.450823874046085, 61.576712143890624 ]\n   ]]}}]\n\ndraw_control2 = DrawControl()\ndraw_control2.polyline = {}\ndraw_control2.circlemarker = {}\ndraw_control2.polygon = {\n    \"shapeOptions\": {\n        \"fillColor\": \"#5115f1\",\n        \"color\": \"#1515f1\",\n        \"fillOpacity\": 1.0\n    },\n    \"drawError\": {\n        \"color\": \"#dd253b\",\n        \"message\": \"Oups!\"\n    },\n    \"allowIntersection\": False\n}\n\nrectangle = Rectangle(bounds=(\nt_utm_to_wgs.transform(np.min(points_2009[:, 0]), np.min(points_2009[:, 1])), t_utm_to_wgs.transform(np.max(points_2009[:, 0]), np.max(points_2009[:, 1]))\n), fill_color='#3eab47', color=\"#225e27\")\n\nm = Map(center=(61.5759, 8.4459), zoom=13)\nm.add_layer(rectangle)\nm.add_control(draw_control1)\nm.add_control(draw_control2)\nm\n</pre> from ipyleaflet import Map, Rectangle, DrawControl  from pyproj import Transformer t_utm_to_wgs = Transformer.from_crs(25832, 4326) t_wgs_to_utm = Transformer.from_crs(4326, 25832)  draw_control1 = DrawControl() draw_control1.polyline = {} draw_control1.circlemarker = {} draw_control1.polygon = {     \"shapeOptions\": {         \"fillColor\": \"#6be5c3\",         \"color\": \"#6be5c3\",         \"fillOpacity\": 1.0     },     \"drawError\": {         \"color\": \"#dd253b\",         \"message\": \"Oups!\"     },     \"allowIntersection\": False }  draw_control1.data = [{'type': 'Feature',   'properties': {'style': {'pane': 'overlayPane',     'attribution': None,     'bubblingMouseEvents': True,     'fill': True,     'smoothFactor': 1,     'noClip': False,     'stroke': True,     'color': '#6be5c3',     'weight': 4,     'opacity': 0.5,     'lineCap': 'round',     'lineJoin': 'round',     'dashArray': None,     'dashOffset': None,     'fillColor': '#6be5c3',     'fillOpacity': 1,     'fillRule': 'evenodd',     'interactive': True,     'clickable': True}},   'geometry': {'type': 'Polygon',    'coordinates': [[        [ 8.443458056811993, 61.573158353361549 ],         [ 8.444874560126243, 61.574126167025533 ],         [ 8.447390936602142, 61.573880250022164 ],         [ 8.447774225734232, 61.572595103638051 ],         [ 8.44485789538137, 61.572301575079962 ],         [ 8.443458056811993, 61.573158353361549 ]    ]]}},         {'type': 'Feature',   'properties': {'style': {'pane': 'overlayPane',     'attribution': None,     'bubblingMouseEvents': True,     'fill': True,     'smoothFactor': 1,     'noClip': False,     'stroke': True,     'color': '#1515f1',     'weight': 4,     'opacity': 0.5,     'lineCap': 'round',     'lineJoin': 'round',     'dashArray': None,     'dashOffset': None,     'fillColor': '#5115f1',     'fillOpacity': 1,     'fillRule': 'evenodd',     'interactive': True,     'clickable': True}},   'geometry': {'type': 'Polygon',    'coordinates': [[        [ 8.450823874046085, 61.576712143890624 ],         [ 8.453556892205341, 61.577663982918892 ],         [ 8.456356569344091, 61.576426586484779 ],         [ 8.453323585777111, 61.57536365525263 ],         [ 8.451390475371786, 61.575062220361872 ],         [ 8.450823874046085, 61.576712143890624 ]    ]]}}]  draw_control2 = DrawControl() draw_control2.polyline = {} draw_control2.circlemarker = {} draw_control2.polygon = {     \"shapeOptions\": {         \"fillColor\": \"#5115f1\",         \"color\": \"#1515f1\",         \"fillOpacity\": 1.0     },     \"drawError\": {         \"color\": \"#dd253b\",         \"message\": \"Oups!\"     },     \"allowIntersection\": False }  rectangle = Rectangle(bounds=( t_utm_to_wgs.transform(np.min(points_2009[:, 0]), np.min(points_2009[:, 1])), t_utm_to_wgs.transform(np.max(points_2009[:, 0]), np.max(points_2009[:, 1])) ), fill_color='#3eab47', color=\"#225e27\")  m = Map(center=(61.5759, 8.4459), zoom=13) m.add_layer(rectangle) m.add_control(draw_control1) m.add_control(draw_control2) m Out[7]: <pre>Map(center=[61.5759, 8.4459], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoo\u2026</pre> In\u00a0[8]: Copied! <pre>glacier_parts = []\nrock_parts = []\nfor feature in draw_control1.data:\n    if feature['properties']['style']['color'] == '#1515f1':\n        rock_parts.append(feature['geometry']['coordinates'][0])\n    else:\n        glacier_parts.append(feature['geometry']['coordinates'][0])\nprint(\"Glacier:\", glacier_parts)\nprint(\"Rock:\", rock_parts)\n</pre> glacier_parts = [] rock_parts = [] for feature in draw_control1.data:     if feature['properties']['style']['color'] == '#1515f1':         rock_parts.append(feature['geometry']['coordinates'][0])     else:         glacier_parts.append(feature['geometry']['coordinates'][0]) print(\"Glacier:\", glacier_parts) print(\"Rock:\", rock_parts) <pre>Glacier: [[[8.443458056811993, 61.57315835336155], [8.444874560126243, 61.57412616702553], [8.447390936602142, 61.573880250022164], [8.447774225734232, 61.57259510363805], [8.44485789538137, 61.57230157507996], [8.443458056811993, 61.57315835336155]]]\nRock: [[[8.450823874046085, 61.576712143890624], [8.453556892205341, 61.57766398291889], [8.456356569344091, 61.57642658648478], [8.453323585777111, 61.57536365525263], [8.451390475371786, 61.57506222036187], [8.450823874046085, 61.576712143890624]]]\n</pre> In\u00a0[9]: Copied! <pre>glacier_parts_utm = [[t_wgs_to_utm.transform(*part[::-1]) for part in poly] for poly in glacier_parts]\n</pre> glacier_parts_utm = [[t_wgs_to_utm.transform(*part[::-1]) for part in poly] for poly in glacier_parts] In\u00a0[10]: Copied! <pre>rock_parts_utm = [[t_wgs_to_utm.transform(*part[::-1]) for part in poly] for poly in rock_parts]\n</pre> rock_parts_utm = [[t_wgs_to_utm.transform(*part[::-1]) for part in poly] for poly in rock_parts] <p>Now we have defined training areas and can visualize the feature distribution again - but this time, stratified for each class. First, we select all the points within the respective polygons, then we create the 2D histograms on the subsets.</p> In\u00a0[11]: Copied! <pre>from shapely import geometry\n\nglacier_polys = []\nfor glacier_part in glacier_parts_utm:\n    line = geometry.LineString(glacier_part)\n    polygon = geometry.Polygon(line)\n    glacier_polys.append(polygon)\nglacier = geometry.MultiPolygon(glacier_polys)\n    \nrock_polys = []\nfor rock_part in rock_parts_utm:\n    line = geometry.LineString(rock_part)\n    polygon = geometry.Polygon(line)\n    rock_polys.append(polygon)\nrock = geometry.MultiPolygon(rock_polys)\n</pre> from shapely import geometry  glacier_polys = [] for glacier_part in glacier_parts_utm:     line = geometry.LineString(glacier_part)     polygon = geometry.Polygon(line)     glacier_polys.append(polygon) glacier = geometry.MultiPolygon(glacier_polys)      rock_polys = [] for rock_part in rock_parts_utm:     line = geometry.LineString(rock_part)     polygon = geometry.Polygon(line)     rock_polys.append(polygon) rock = geometry.MultiPolygon(rock_polys)   In\u00a0[12]: Copied! <pre>rock_idx = []\nglacier_idx = []\nfor ptidx in tqdm(range(points_2009.shape[0])):\n    point = geometry.Point(points_2009[ptidx, 0], points_2009[ptidx, 1])\n    if glacier.contains(point):\n        glacier_idx.append(ptidx)\n    if rock.contains(point):\n        rock_idx.append(ptidx)\n</pre> rock_idx = [] glacier_idx = [] for ptidx in tqdm(range(points_2009.shape[0])):     point = geometry.Point(points_2009[ptidx, 0], points_2009[ptidx, 1])     if glacier.contains(point):         glacier_idx.append(ptidx)     if rock.contains(point):         rock_idx.append(ptidx)  <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1694840/1694840 [01:22&lt;00:00, 20633.51it/s]\n</pre> In\u00a0[13]: Copied! <pre>print(f\"Dataset size:\\n\\tGlacier: {len(glacier_idx)} points\\n\\tRock: {len(rock_idx)} points\")\n</pre> print(f\"Dataset size:\\n\\tGlacier: {len(glacier_idx)} points\\n\\tRock: {len(rock_idx)} points\") <pre>Dataset size:\n\tGlacier: 40444 points\n\tRock: 50576 points\n</pre> In\u00a0[14]: Copied! <pre>import matplotlib.pyplot as plt\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Comparsion in feature space')\nax1.hist2d(feature_array[glacier_idx, 2], feature_array[glacier_idx, 4], bins=100, range=[[0, 1], [0, 3.14/2]])\nax1.set_xlabel('Omnivariance')\nax1.set_ylabel('Slope [rad]')\nax1.set_title('Glacier area')\nax2.hist2d(feature_array[rock_idx, 2], feature_array[rock_idx, 4], bins=100, range=[[0, 1], [0, 3.14/2]])\nax2.set_xlabel('Omnivariance')\nax2.set_ylabel('Slope [rad]')\nax2.set_title('Rock area')\nplt.gcf().tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt fig, (ax1, ax2) = plt.subplots(1, 2) fig.suptitle('Comparsion in feature space') ax1.hist2d(feature_array[glacier_idx, 2], feature_array[glacier_idx, 4], bins=100, range=[[0, 1], [0, 3.14/2]]) ax1.set_xlabel('Omnivariance') ax1.set_ylabel('Slope [rad]') ax1.set_title('Glacier area') ax2.hist2d(feature_array[rock_idx, 2], feature_array[rock_idx, 4], bins=100, range=[[0, 1], [0, 3.14/2]]) ax2.set_xlabel('Omnivariance') ax2.set_ylabel('Slope [rad]') ax2.set_title('Rock area') plt.gcf().tight_layout() plt.show() <p>Now we can get to training!</p> <p></p> In\u00a0[15]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\n# Create a \"label\" vector that contains 0 for all rock training points and 1 for all glacier training points\nlabelled_features = np.concatenate([feature_array[rock_idx, :], feature_array[glacier_idx, :]])\nlabel = np.zeros(shape=(labelled_features.shape[0]))\nlabel[len(rock_idx):] = 1 # everything after the rock entries is glacier\n\n# Split the dataset for train/test\nX_train, X_test, y_train, y_test = train_test_split(labelled_features, label, test_size=0.8, random_state=42)\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp = imp.fit(X_train)\n\nX_train = imp.transform(X_train)\nX_test = imp.transform(X_test)\n\n# Try the linear SVM Classifier first:\nlinearSVM = SVC(kernel='linear')\nlinearSVM.fit(X_train, y_train)\nscores = linearSVM.score(X_test, y_test)\nprint(f\"Linear SVM: {scores}\")\n\n# Then try the rbf SVM Classifier. Uses a 3rd degree polynomial by default:\nrbfSVM = SVC(kernel='rbf')\nrbfSVM.fit(X_train, y_train)\nscores = rbfSVM.score(X_test, y_test)\nprint(f\"RBF SVM   : {scores}\")\n</pre> from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer  # Create a \"label\" vector that contains 0 for all rock training points and 1 for all glacier training points labelled_features = np.concatenate([feature_array[rock_idx, :], feature_array[glacier_idx, :]]) label = np.zeros(shape=(labelled_features.shape[0])) label[len(rock_idx):] = 1 # everything after the rock entries is glacier  # Split the dataset for train/test X_train, X_test, y_train, y_test = train_test_split(labelled_features, label, test_size=0.8, random_state=42)  imp = SimpleImputer(missing_values=np.nan, strategy='mean') imp = imp.fit(X_train)  X_train = imp.transform(X_train) X_test = imp.transform(X_test)  # Try the linear SVM Classifier first: linearSVM = SVC(kernel='linear') linearSVM.fit(X_train, y_train) scores = linearSVM.score(X_test, y_test) print(f\"Linear SVM: {scores}\")  # Then try the rbf SVM Classifier. Uses a 3rd degree polynomial by default: rbfSVM = SVC(kernel='rbf') rbfSVM.fit(X_train, y_train) scores = rbfSVM.score(X_test, y_test) print(f\"RBF SVM   : {scores}\")   <pre>Linear SVM: 0.9996704021094265\nRBF SVM   : 0.999780268072951\n</pre> <p></p> In\u00a0[16]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\n# Train the scaler (i.e., estimate the transformation parameters from the training data)\nscaler = StandardScaler()\nscaler.fit(X_train)\n\n# Then try the rbf SVM Classifier. Uses a 3rd degree polynomial by default:\nrbfSVM = SVC(kernel='rbf')\nrbfSVM.fit(scaler.transform(X_train), y_train)\nscores = rbfSVM.score(scaler.transform(X_test), y_test)\nprint(f\"RBF SVM (scaled): {scores}\")\n</pre> from sklearn.preprocessing import StandardScaler  # Train the scaler (i.e., estimate the transformation parameters from the training data) scaler = StandardScaler() scaler.fit(X_train)  # Then try the rbf SVM Classifier. Uses a 3rd degree polynomial by default: rbfSVM = SVC(kernel='rbf') rbfSVM.fit(scaler.transform(X_train), y_train) scores = rbfSVM.score(scaler.transform(X_test), y_test) print(f\"RBF SVM (scaled): {scores}\")  <pre>RBF SVM (scaled): 0.9999176005273567\n</pre> <p>Now the improvement might not seem like a lot, but keep in mind that an increase in accuracy from 0.9998 to 0.9999 represents a 50% reduction in incorrect estimations!</p> <p>Note: your numbers might differ because of differently drawn training area polygons.</p> <p></p> In\u00a0[17]: Copied! <pre>full_result = rbfSVM.predict(scaler.transform(imp.transform(feature_array[::100])))\nprint(f\"{np.count_nonzero(full_result==1)/len(full_result)*100.:.2f}% of the points are predicted as glacier.\")\nps.init()\npc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009[::100])\npc_2009.add_scalar_quantity('Predicted class', full_result)\nps.set_up_dir(\"z_up\")\nps.show()\n</pre> full_result = rbfSVM.predict(scaler.transform(imp.transform(feature_array[::100]))) print(f\"{np.count_nonzero(full_result==1)/len(full_result)*100.:.2f}% of the points are predicted as glacier.\") ps.init() pc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009[::100]) pc_2009.add_scalar_quantity('Predicted class', full_result) ps.set_up_dir(\"z_up\") ps.show() <pre>20.43% of the points are predicted as glacier.\n</pre> <p></p> In\u00a0[18]: Copied! <pre>from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(scaler.transform(X_train), y_train)\nscores = rfc.score(scaler.transform(X_test), y_test)\nprint(f\"Random Forest (scaled): {scores}\")\n</pre> from sklearn.ensemble import RandomForestClassifier  rfc = RandomForestClassifier(n_estimators=100) rfc.fit(scaler.transform(X_train), y_train) scores = rfc.score(scaler.transform(X_test), y_test) print(f\"Random Forest (scaled): {scores}\")  <pre>Random Forest (scaled): 1.0\n</pre> In\u00a0[19]: Copied! <pre>full_result = rfc.predict(scaler.transform(imp.transform(feature_array[::100])))\nprint(f\"{np.count_nonzero(full_result==1)/len(full_result)*100.:.2f}% of the points are predicted as glacier.\")\nplt.figure(figsize=(10,5))\nplt.scatter(points_2009[::100][:, 0], points_2009[::100][:, 1], c=full_result, s=0.3)\nplt.axis('equal')\nplt.show()\n#ps.init()\n#pc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009[::100])\n#pc_2009.add_scalar_quantity('Predicted class', full_result)\n#ps.set_up_dir(\"z_up\")\n#ps.show()\n</pre> full_result = rfc.predict(scaler.transform(imp.transform(feature_array[::100]))) print(f\"{np.count_nonzero(full_result==1)/len(full_result)*100.:.2f}% of the points are predicted as glacier.\") plt.figure(figsize=(10,5)) plt.scatter(points_2009[::100][:, 0], points_2009[::100][:, 1], c=full_result, s=0.3) plt.axis('equal') plt.show() #ps.init() #pc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009[::100]) #pc_2009.add_scalar_quantity('Predicted class', full_result) #ps.set_up_dir(\"z_up\") #ps.show() <pre>18.23% of the points are predicted as glacier.\n</pre> <p>You can also adapt the parameters of these classifiers - e.g. the number of trees in the Random Forest. Check out the documentation for all options by writing <code>?RandomForestClassifier</code> in a code cell!</p> In\u00a0[20]: Copied! <pre>?RandomForestClassifier\n</pre> ?RandomForestClassifier <pre>Init signature:\nRandomForestClassifier(\n    n_estimators=100,\n    *,\n    criterion='gini',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='sqrt',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n    ccp_alpha=0.0,\n    max_samples=None,\n)\nDocstring:     \nA random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\n\nRead more in the :ref:`User Guide &lt;forest&gt;`.\n\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\n\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n    Note: This parameter is tree-specific.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"auto\", then `max_features=sqrt(n_features)`.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n    .. deprecated:: 1.1\n        The `\"auto\"` option was deprecated in 1.1 and will be removed\n        in 1.3.\n\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\n\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\n\noob_score : bool, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    Only available if bootstrap=True.\n\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    &lt;n_jobs&gt;` for more details.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features &lt; n_features``).\n    See :term:`Glossary &lt;random_state&gt;` for details.\n\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\n\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary &lt;warm_start&gt;` and\n    :ref:`gradient_boosting_warm_start` for details.\n\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n\n    .. versionadded:: 0.22\n\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n\n    .. versionadded:: 0.22\n\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\n\nbase_estimator_ : DecisionTreeClassifier\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n\n    .. deprecated:: 1.2\n        `base_estimator_` is deprecated and will be removed in 1.4.\n        Use `estimator_` instead.\n\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\n\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\n\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\n\noob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\n\nSee Also\n--------\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n    tree classifiers.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\n\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n&gt;&gt;&gt; clf = RandomForestClassifier(max_depth=2, random_state=0)\n&gt;&gt;&gt; clf.fit(X, y)\nRandomForestClassifier(...)\n&gt;&gt;&gt; print(clf.predict([[0, 0, 0, 0]]))\n[1]\nFile:           c:\\users\\katharina\\.conda\\envs\\etrainee\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\nType:           ABCMeta\nSubclasses:     \n</pre> <p></p> In\u00a0[21]: Copied! <pre>from sklearn.cluster import KMeans\n\nfeature_subset = feature_array[::100][:, [2,4]]\n\nimp_sub = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_sub = imp.fit(feature_subset)\nscaler_sub = StandardScaler()\nscaler_sub.fit(feature_subset)\n\nkm = KMeans(n_clusters=2)\n\nkm.fit(scaler_sub.transform(imp_sub.transform(feature_subset)))\npredicted_clusters = km.labels_\n\nprint(f\"{np.count_nonzero(full_result==1)/len(full_result)*100.:.2f}% of the points are predicted as 'Cluster A'.\")\nplt.figure(figsize=(10,5))\nplt.scatter(points_2009[::100][:, 0], points_2009[::100][:, 1], c=predicted_clusters, s=0.3)\nplt.axis('equal')\nplt.show()\n</pre> from sklearn.cluster import KMeans  feature_subset = feature_array[::100][:, [2,4]]  imp_sub = SimpleImputer(missing_values=np.nan, strategy='mean') imp_sub = imp.fit(feature_subset) scaler_sub = StandardScaler() scaler_sub.fit(feature_subset)  km = KMeans(n_clusters=2)  km.fit(scaler_sub.transform(imp_sub.transform(feature_subset))) predicted_clusters = km.labels_  print(f\"{np.count_nonzero(full_result==1)/len(full_result)*100.:.2f}% of the points are predicted as 'Cluster A'.\") plt.figure(figsize=(10,5)) plt.scatter(points_2009[::100][:, 0], points_2009[::100][:, 1], c=predicted_clusters, s=0.3) plt.axis('equal') plt.show()   <pre>c:\\Users\\Katharina\\.conda\\envs\\etrainee\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n</pre> <pre>18.23% of the points are predicted as 'Cluster A'.\n</pre> <p>What happens if you set $k=3$? Can you discover a third cluster/class present in the dataset, or do the results just get noisy?</p> <p>In the end, with unsupervised methods, it is down to the user (you!) to make sense of the clusters the algorithm outputs.</p> <p></p> In\u00a0[22]: Copied! <pre>import py4dgeo\n\n# let's read the full datasets\ndata_2009 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2009.las\", get_attributes=True)\ndata_2017 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2017.las\", get_attributes=True)\npoints_2009 = data_2009[0]\npoints_2017 = data_2017[0]\n\nepoch1 = py4dgeo.Epoch(points_2009)\nepoch2 = py4dgeo.Epoch(points_2017)\ncorepoints = points_2009[::100, :]\n\nm3c2 = py4dgeo.M3C2(\n    epochs=(epoch1, epoch2),\n    corepoints=corepoints,\n    cyl_radii=(30.0,),\n    normal_radii=(15.0,),\n    max_distance=50.0\n)\ndistances, uncertainties = m3c2.run()\n</pre> import py4dgeo  # let's read the full datasets data_2009 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2009.las\", get_attributes=True) data_2017 = pcfuncs.read_las(DATA_PATH + \"/hellstugubrean_2017.las\", get_attributes=True) points_2009 = data_2009[0] points_2017 = data_2017[0]  epoch1 = py4dgeo.Epoch(points_2009) epoch2 = py4dgeo.Epoch(points_2017) corepoints = points_2009[::100, :]  m3c2 = py4dgeo.M3C2(     epochs=(epoch1, epoch2),     corepoints=corepoints,     cyl_radii=(30.0,),     normal_radii=(15.0,),     max_distance=50.0 ) distances, uncertainties = m3c2.run() <pre>[2023-03-29 20:49:16][INFO] Building KDTree structure with leaf parameter 10\n[2023-03-29 20:49:17][INFO] Building KDTree structure with leaf parameter 10\n</pre> In\u00a0[23]: Copied! <pre>import matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.scatter(corepoints[:, 0], corepoints[:, 1], c=distances, s=0.3)\nplt.axis('equal')\nplt.colorbar()\nplt.show()\n</pre> import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) plt.scatter(corepoints[:, 0], corepoints[:, 1], c=distances, s=0.3) plt.axis('equal') plt.colorbar() plt.show() <p>You can see changes up to 25m where the glacier has retreated. We want to train a machine learning model to predict these changes from a single epoch, i.e., predict how much change occurs over the period of 8 years. For this, we will again define a feature space. As the surface change is dependant on the altitude, we will consider the Z coordinate of the points as one input feature. Additionaly, we will calculate roughness, slope and aspect angles and planarity. For this, we define a function as before:</p> In\u00a0[24]: Copied! <pre>def create_feature_vector(neighbour_points):\n    # structure tensor\n    struct_tensor = np.cov(neighbour_points.T)\n    # eigenvalue decomposition\n    eigvals, eigvec = np.linalg.eigh(struct_tensor)\n    l3, l2, l1 = eigvals\n\n    # find eigenvector to smallest eigenvalue = normal vector to best fitting plane\n    normalvector = eigvec[:, 0]\n    # flip so that it always points \"upwards\"\n    if normalvector[2] &lt; 0:\n        normalvector *= -1\n\n    # feature calculation\n    planarity = (l2-l3)/l1\n    roughness = l3\n    slope = np.arctan2(np.linalg.norm(normalvector[:2]), normalvector[2])\n    aspect = np.arctan2(normalvector[0], normalvector[1])\n\n    return np.array([planarity,\n                     roughness,\n                     slope,\n                     aspect]), normalvector\n</pre> def create_feature_vector(neighbour_points):     # structure tensor     struct_tensor = np.cov(neighbour_points.T)     # eigenvalue decomposition     eigvals, eigvec = np.linalg.eigh(struct_tensor)     l3, l2, l1 = eigvals      # find eigenvector to smallest eigenvalue = normal vector to best fitting plane     normalvector = eigvec[:, 0]     # flip so that it always points \"upwards\"     if normalvector[2] &lt; 0:         normalvector *= -1      # feature calculation     planarity = (l2-l3)/l1     roughness = l3     slope = np.arctan2(np.linalg.norm(normalvector[:2]), normalvector[2])     aspect = np.arctan2(normalvector[0], normalvector[1])      return np.array([planarity,                      roughness,                      slope,                      aspect]), normalvector <p>In regression tasks, we want to predict a scalar (or vector) quantity for each data point instead of a class label. We will explore the Random Forest Regression, which works similar to the Random forest we used for classification. Instead of predicting a class label, each tree in the random forest will give a scalar value. The result of the random forest is then the average of these scalar values.</p> <p>As in the classification, we need some way of describing the data that we can use as input to the algorithm, i.e., the feature vector. Since we know the M3C2 distance (our regression target) only for a subset of the full point cloud (the corepoints), we create a feature vector for these points only. As we want to use the full information of the 3D point cloud, we query the neighbours from the full point cloud, though.</p> <p>To give the algorithm even more information, we also add the z-Coordinate of the points as a feature. If you want to see how it performs without the z-coordinate, try changing the last line to set the final feature to a constant value (e.g., 0) - then, the information coming from this feature is none.</p> In\u00a0[25]: Copied! <pre># first calculate the features for all core points (where we calculated an M3C2 distance), based on the neighbourhood \n# of the full point cloud \nfrom scipy import spatial\nprint(\"Building KD tree...\")\ntree_2009 = spatial.KDTree(points_2009)\ncorepoints_tree = spatial.KDTree(corepoints)\nprint(\"Querying neighbours...\")\nneighbours = corepoints_tree.query_ball_tree(tree_2009, r=5)\n\nfeature_array = np.empty((corepoints.shape[0], 5), dtype=np.float32)\nprint(\"Calculating features...\")\nfrom tqdm import tqdm\nfor pix, point_neighbours in tqdm(enumerate(neighbours), unit='pts', total=corepoints.shape[0]):\n    curr_neighbours = points_2009[point_neighbours]\n    curr_features, _ = create_feature_vector(curr_neighbours)\n    feature_array[pix, :4] = curr_features\n    feature_array[pix, -1] = corepoints[pix, 2]  # add z-coordinate as feature\n</pre> # first calculate the features for all core points (where we calculated an M3C2 distance), based on the neighbourhood  # of the full point cloud  from scipy import spatial print(\"Building KD tree...\") tree_2009 = spatial.KDTree(points_2009) corepoints_tree = spatial.KDTree(corepoints) print(\"Querying neighbours...\") neighbours = corepoints_tree.query_ball_tree(tree_2009, r=5)  feature_array = np.empty((corepoints.shape[0], 5), dtype=np.float32) print(\"Calculating features...\") from tqdm import tqdm for pix, point_neighbours in tqdm(enumerate(neighbours), unit='pts', total=corepoints.shape[0]):     curr_neighbours = points_2009[point_neighbours]     curr_features, _ = create_feature_vector(curr_neighbours)     feature_array[pix, :4] = curr_features     feature_array[pix, -1] = corepoints[pix, 2]  # add z-coordinate as feature   <pre>Building KD tree...\nQuerying neighbours...\nCalculating features...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16949/16949 [00:03&lt;00:00, 4814.14pts/s]\n</pre> <p>We now load the random forest regressor class from the <code>sklearn</code> package, initialize it with a maximum tree depth of 15 and a random state (to ensure reproducible results). With the <code>.fit</code>-method, we can pass the feature array and the corresponding regression target to train the regressor.</p> <p>As an example, we can print the prediction of the M3C2 distance for a feauture vector of $\\vec{f} = \\left( 0, 0, 0, 0\\right)^T$</p> In\u00a0[26]: Copied! <pre>from sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(max_depth=15, random_state=0)\nregr.fit(feature_array, distances)\nprint(regr)\nprint(regr.predict([[0, 0, 0, 0, 0]]))\n</pre>  from sklearn.ensemble import RandomForestRegressor regr = RandomForestRegressor(max_depth=15, random_state=0) regr.fit(feature_array, distances) print(regr) print(regr.predict([[0, 0, 0, 0, 0]])) <pre>RandomForestRegressor(max_depth=15, random_state=0)\n[-0.6293358]\n</pre> <p>Now let us predict the actual M3C2 distances using the point cloud features and plot them. We can also plot the differences to the input data.</p> In\u00a0[27]: Copied! <pre>pred_dists = regr.predict(feature_array)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.scatter(corepoints[:, 0], corepoints[:, 1], c=pred_dists, s=0.3)\nplt.axis('equal')\nplt.colorbar()\nplt.show()\n\nimport matplotlib.cm as cm\nplt.figure(figsize=(10,5))\nplt.scatter(corepoints[:, 0], corepoints[:, 1], c=pred_dists - distances, s=0.3, cmap=cm.seismic)\nplt.axis('equal')\nplt.colorbar()\nplt.show()\n</pre> pred_dists = regr.predict(feature_array)  import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) plt.scatter(corepoints[:, 0], corepoints[:, 1], c=pred_dists, s=0.3) plt.axis('equal') plt.colorbar() plt.show()  import matplotlib.cm as cm plt.figure(figsize=(10,5)) plt.scatter(corepoints[:, 0], corepoints[:, 1], c=pred_dists - distances, s=0.3, cmap=cm.seismic) plt.axis('equal') plt.colorbar() plt.show() <p>As we can see, the prediction works quite well on the main glacier body and on the rocks around it. The largest errors &gt;5 m appear at the edge of the glacier, where change on the inside of the glacier is overestimated (red), and it is underestimated on the outside (blue).</p> <p>Now, we can use this information to evaluate the quality of the estimation and compare it, e.g., to a regressor that does not have the z-coordinate information in its feature array. However, note that we trained the regressor on the full dataset, so here we are actually evaluating the training error. A more realistic and correct estimate of how the method would perform on previously unseen data would be to - as in the classification example - separate training and test data. As the M3C2 distance is subject to spatial autocorrelation (owing to - at least - the search radius for M3C2), a spatial split of the data would be the cleanest solution in this case.</p> <p>Finally, let us predict the state of the glacier from the newer dataset (2017). As we have trained on the difference from 2009 to 2017, this can be seen as a very simple prediction on how the glacier looks like in 2025. First, we have to calculate the feature vector for points in the 2017 dataset.</p> In\u00a0[28]: Copied! <pre>from scipy import spatial\nprint(\"Building KD tree...\")\ntree_2017 = spatial.KDTree(points_2017)\ncorepoints_2017 = points_2017[::300,:]\ncorepoints_2017_tree = spatial.KDTree(corepoints_2017)  # only use every 100th point\nprint(\"Querying neighbours...\")\nneighbours = corepoints_2017_tree.query_ball_tree(tree_2017, r=5)\n\nfeature_array = np.empty((corepoints_2017.shape[0], 5), dtype=np.float32)\nnormalvectors  = np.empty((corepoints_2017.shape[0], 3), dtype=np.float32)\nprint(\"Calculating features...\")\nfrom tqdm import tqdm\nfor pix, point_neighbours in tqdm(enumerate(neighbours), unit='pts', total=corepoints_2017.shape[0]):\n    curr_neighbours = points_2017[point_neighbours]\n    curr_features, normalvector = create_feature_vector(curr_neighbours)\n    feature_array[pix, :4] = curr_features\n    feature_array[pix, -1] = corepoints_2017[pix, 2]  # add z-coordinate as feature\n    normalvectors[pix, :] = normalvector  # we will need this later for the prediction\n</pre> from scipy import spatial print(\"Building KD tree...\") tree_2017 = spatial.KDTree(points_2017) corepoints_2017 = points_2017[::300,:] corepoints_2017_tree = spatial.KDTree(corepoints_2017)  # only use every 100th point print(\"Querying neighbours...\") neighbours = corepoints_2017_tree.query_ball_tree(tree_2017, r=5)  feature_array = np.empty((corepoints_2017.shape[0], 5), dtype=np.float32) normalvectors  = np.empty((corepoints_2017.shape[0], 3), dtype=np.float32) print(\"Calculating features...\") from tqdm import tqdm for pix, point_neighbours in tqdm(enumerate(neighbours), unit='pts', total=corepoints_2017.shape[0]):     curr_neighbours = points_2017[point_neighbours]     curr_features, normalvector = create_feature_vector(curr_neighbours)     feature_array[pix, :4] = curr_features     feature_array[pix, -1] = corepoints_2017[pix, 2]  # add z-coordinate as feature     normalvectors[pix, :] = normalvector  # we will need this later for the prediction <pre>Building KD tree...\nQuerying neighbours...\nCalculating features...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20191/20191 [00:04&lt;00:00, 4688.71pts/s]\n</pre> <p>Now we can run this on the already trained regressor and visualize the results:</p> In\u00a0[29]: Copied! <pre>pred_dists = regr.predict(feature_array)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nplt.scatter(corepoints_2017[:, 0], corepoints_2017[:, 1], c=pred_dists, s=0.3)\nplt.axis('equal')\nplt.colorbar()\nplt.show()\n</pre> pred_dists = regr.predict(feature_array)  import matplotlib.pyplot as plt plt.figure(figsize=(10,5)) plt.scatter(corepoints_2017[:, 0], corepoints_2017[:, 1], c=pred_dists, s=0.3) plt.axis('equal') plt.colorbar() plt.show() <p>We do not have data to validate this plot, but we can check for plausibility. What are the main observations you can make here?</p> <p>Can you find out how much more the glacier is predicted to retreat in the years 2017-2025? Does it fit with the real observations found here: https://glacier.nve.no/glacier/viewer/ci/en/cc/ClimateIndicatorInfo/2768?name=Hellstugubreen ?</p> <p>Finally, let us convert this prediction back into a 3D point cloud. We use the core point's normal vectors to displace them by the predicted distance and load the result into a 3D viewer.</p> In\u00a0[30]: Copied! <pre>points_2025 = corepoints_2017 + normalvectors * pred_dists[:, np.newaxis]  # newaxis needed to make dimensions fit\n\nps.init()\npc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009[::100])\npc_2017 = ps.register_point_cloud(\"point cloud 2017\", corepoints_2017)\npc_2025 = ps.register_point_cloud(\"point cloud 2025\", points_2025)\nps.set_up_dir(\"z_up\")\nps.show()\n</pre> points_2025 = corepoints_2017 + normalvectors * pred_dists[:, np.newaxis]  # newaxis needed to make dimensions fit  ps.init() pc_2009 = ps.register_point_cloud(\"point cloud 2009\", points_2009[::100]) pc_2017 = ps.register_point_cloud(\"point cloud 2017\", corepoints_2017) pc_2025 = ps.register_point_cloud(\"point cloud 2025\", points_2025) ps.set_up_dir(\"z_up\") ps.show() <p>You can see how the 2025 prediction shows further glacier retreat. Do note, however, that this is not a very elaborate prediction, but rather a simple machine learning example. You can e.g. see some artifacts due to the rectangular data format of the 2017 point cloud in the predictions.</p> <p></p> <p></p>  What type of features are most suitable to derive feature vectors for different point cloud sources (e.g. laser scanning and photogrammetry)?  radiometric features geometric features true color (i.e. RGB) <p>geometric features</p> <p></p>  What is the dimensionality of a vector describing the feature space?  1 3 n <p>n</p> <p></p>  In which step of supervised classification is input of human operators typically leveraged?  determination of thresholds for class assignment generation of training data supervising the application of the machine learning model <p>generation of training data</p> <p></p>  How is the class split represented in Support Vector Machines (acting as linear separator)?  <p>(hyper)plane in feature space</p> <p></p>  What is the caveat of using Euclidian distances in feature space?  <p>The feature dimensionality influences the result if features are not scaled.</p> <p></p>  Which of these random subsets is *not* used by the Random Forest classification method?  random subsets of training data random subsets of the feature space dimensions random subsets of target classes <p>random subsets of target classes</p> <p></p>  What is the main difference of a regression task compared to classification?  <p>a scalar quantity is derived instead of class labels</p> <p></p>  What is an important strength of deep learning compared to other supervised classification methods, such as a random forest?  <p>the features are learned by the DL model from the data itself, no user definitions required (representation learning)</p> <p></p> <p></p> <p>You made it through the theme - no exercise after this highly interactive session. Now that you learnt about many different methods, proceed with the research-oriented case studies!</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#machine-learning-on-point-clouds-with-python","title":"Machine learning on point clouds with Python\u00b6","text":"<p>In this course, machine learning is introduced and applied to point cloud data. We use Python to experiment with different machine learning methods and see how regression and classification methods can be used with point clouds to identify changes. The main concepts of supervised and unsupervised classification are discussed.</p> <p>For data handling (reading and loading of point clouds), we will be reusing code from the second theme: Programming for point cloud analysis with Python. If you struggle with reading files, please revisit this theme.</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#requirements-for-this-lesson","title":"Requirements for this lesson\u00b6","text":"<p>To perform the exercises in this theme, some specific (and common) Python libraries are required. Set up the Python environment following the course instructions in the software section.</p> <p>Alternatively, to install only packages needed in this exercise, use:</p> <p><code>mamba install -c conda-forge polyscope numpy matplotlib ipyleaflet laspy pyproj scikit-learn tqdm shapely</code></p> <p><code>python -m pip install py4dgeo</code></p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#data","title":"Data\u00b6","text":"<p>The data used in this exercise are two point clouds of the Hellstugubrean glacier in the Jotunheimen nasjonalpark, Norway 61.5759\u00b0 N, 8.4459\u00b0 E. Data is provided by the Norwegian Mapping Authority under the CC BY 4.0 License: \u00a9Kartverket. For more details, see here. The data was downloaded from H\u00f8ydedata and cropped to the area of interest.</p> <p>Find the data files in the directory <code>hellstugubrean</code> of the course data repository.</p> <p>We use two acquisitions, an airborne laser scanning point cloud from 2009 with an average point density of 0.3 $pts/m^2$ and a dense image matching (DIM, i.e. photogrammetry-based) point cloud from 2017 with an average point density of 4 $pts/m^2$. Let us take a look at the data using polyscope:</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#creating-feature-spaces-based-on-local-neighbourhoods","title":"Creating feature spaces based on local neighbourhoods\u00b6","text":"<p>Before we can start separating the glacier surfaces from the bare rock around them, we need to create a so-called feature space for the classification. As we want to assign each ALS/DIM point to either glacier or rock, we also calculate the features for each point.</p> <p>The feature space is given as an $n$-dimensional vector. This is the input data that will then be used to infer the classification. Mathematically, the machine learning method is a function $f(x)$ that operates on the feature vector ($x$) and outputs a class label $y$. In our case, the class label $y=0$ will refer to rock surfaces, while $y=1$ will refer to glacier surfaces:</p> <p>$y = f(x)$,</p> <p>$x \\in \\mathbb{R}^n$,</p> <p>$y \\in \\{0... rock, 1... glacier\\}$</p> <p>The features we want to derive for the feature vector should therefore describe the point and its local properties. As we don't have laser intensity data for the DIM dataset, and no RGB information for the ALS dataset, we cannot simply use these values, if we want to create a classifier that can be applied to both datasets. Instead, we will rely on geometric information that is contained in the point cloud by the local neighbours for each point.</p> <p>The structure of the local neighbourhood can be described by the structure tensor, a 3x3-Matrix derived from the XYZ-coordinates of the points in the neighbourhood of a query point. From this tensor, we can derive normalized values like the planarity, the linearity and the omnivariance, and metric values like the roughness and the slope. We will derive these values for each point in the ALS point cloud first and visualize them - both as point cloud attributes and in the feature space. The formulas for planarity, linearity and omnivariance are taken from Weinmann et al. (2013). First, let us create a function to calculate these features for a single point and its neighbours:</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#training-a-supervised-classifier","title":"Training a supervised classifier\u00b6","text":"<p>We have seen that the separation of the glacial areas from the rock surface is non-trivial, if we don't include intensity or RGB information. However, we (as human operators) can define areas where we know the surface type, and use that to teach the computer how to interpret the feature space. For this, we need so-called training data. We will now create training data by drawing polygons on a map that shows the glacier outline (OpenStreetMap).</p> <p>We use <code>iypyleaflet</code> to display a map directly in the notebook. Use the upper \"Create Polygon\" button to create polygons for the glacier training area (light blue) and the lower \"Create Polygon\" button for the rock training area (dark blue). Two examples have been added for you, which you can adapt freely. We use the color of the polygons to separate them in the next code block.</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#support-vector-machines","title":"Support Vector Machines\u00b6","text":"<p>Support Vector Machines are a type of supervised machine learning method. Based on a set of training data, an optimal split between the different target classes is found. This split can be represented by a (hyper)plane in feature space. With SVM, the plane is placed such that the orthogonal distance from the plane to the nearest training data point is maximised.</p> <p>You can also imagine the plane to expand into the feature space. The optimal split is then found where this expansion is the largest.</p> <p>In this primitive classifier, the hyperplane acts a a linear separator. However, many problems in real live require a non-linear split. To overcome this, SVMs can use kernels that transform the feature space before creating a separation hyperplane. Common kernel functions are radial base functions: polynomials depending on the radial distance from a point.</p> <p>In the following, we will train &amp; evaluate a linear SVM and an SVM employing such a radial base function, and see, how the feature space separation differs between the two.</p> <p>Sometimes, some features are not available or cannot be computed (e.g., when a division by zero occurs). The classifier does not know how to deal with these values, and it is up to the user to clean the dataset beforehand. One option would be to remove all the data points where at least one of the features is invalid. Another option is to remove all the feature dimensions where this is the case. What we are going to do in our case is impute the invalid/missing values by calculating the mean value of all valid data points for each feature separately. Then, we set all the missing values to this mean value. To do so, we use a tool from <code>sklearn</code>, the <code>sklearn.impute.SimpleImputer</code>, which we \"fit\" on the training set (i.e., calculate the mean values based on the training set) and then apply it on both training and testing datasets. Generally, one could assume that the test dataset is not available at the time when the model is trained, so it is good practice to only use the training dataset up until the fit of the model has been done.</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#scaling","title":"Scaling\u00b6","text":"<p>You might have noticed that when the SVM tries to maximize the band around the hyperplane before it \"touches\" the first data point of the training set, a euclidean distance is used. This euclidean distance across the feature space is a bit problematic. Why? We include the value \"slope\" in the feature space. Above, we have calculated it to be in radians, but nothing is keeping us from converting it to degrees, or even percentages. However, if the distance to maximize is calculated based on that value, and it is mixed with other values of a different dimensionality (like planarity or linearity, which are unitless values between 0 and 1), we might get different results depending on the unit we choose. This is not ideal.</p> <p>Therefore, for most machine learning applications, data needs to be scaled or normalized before training and testing. Typical options include:</p> <ul> <li>Transform the data so it fits in the $[0, 1[$ interval</li> <li>Transform the data so that it has mean $\\mu=0$ and unit standard deviation $\\sigma=1$.</li> </ul> <p><code>sklearn</code> provides us with tools to do exactly that: the <code>sklearn.preprocessing.MinMaxScaler</code> and the <code>sklearn.preprocessing.StandardScaler</code>. Depending on the data type, additional scalers are provided: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing</p> <p>Now, let's apply the <code>StandardScaler</code> using the RBF SVM classifier and compare results. Again, we fit the scaler on the training dataset only and apply it on both:</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#evaluation-of-supervised-classifiers","title":"Evaluation of supervised classifiers\u00b6","text":"<p>So far, we have looked at a single score representing the quality of the estimation. However, we might also want to take a look at the final result. Let us run the full point cloud through the classifier and visualize it.</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#random-forests","title":"Random forests\u00b6","text":"<p>Random forests are a different supervised classification method. They rely on a set of binary trees (hence the term forest), which are trained using (a) random subsets of the training data points and (b) random subsets of the feature space dimensions.</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#unsupervised-classification","title":"Unsupervised classification\u00b6","text":"<p>As you might have noticed in the plot of feature space above, the two classes form two clear clusters. In unsupervised classification, we make use of this and try to assign each data point to a cluster. Depending on the algorithm used, we may need to supply a number of clusters we want to extract (e.g. kMeans), or define how much two data points have to be separated by to form a new cluster (e.g. DBSCAN). As we know that we want to find two clusters in this dataset, we can use kMeans with $k=2$. Let's give it a try and see what happens!</p> <p>Note that this works completely without the training labels. The algorithm therefore also does not know which result id is which - the glacier, or the rocks!</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#regression-models","title":"Regression models\u00b6","text":"<p>By now you are probably wondering what we loaded the second dataset in for. As the datasets are from different years, the change in surface topography (i.e., mainly retreat of the Hellstugubrean glacier) can be quantified. Let us first do that using the M3C2 point cloud distance metric you have learned about in Theme 3:</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#a-few-words-on-deep-learning","title":"A few words on Deep Learning\u00b6","text":"<p>You probably have heard about Deep Learning in a machine learning context. Deep Learning is a subset of machine learning that uses mathematical models of nervous cells (called neurons) to imitate brain function. In the last decade, Deep Learning has become the state-of-the-art in most machine learning disciplines.</p> <p>The main advantage of Deep Learning over other machine learning algorithms is the so-called representation learning. Here, the user (you!) doesn't have to come up with their own features describing the data, but it is learned from the data itself. The algorithm therefore learns how to represent the data in a way that then allows easy classification or regression.</p> <p>For point clouds, the neighbourhood information can be included as well. For further reading, see this review paper from 2019:</p> <p>Griffiths, D. &amp; Boehm, J. 2019: A Review on Deep Learning Techniques for 3D Sensed Data Classification. Remote Sensing, 11(12), 1499. DOI: 10.3390/rs11121499.</p>"},{"location":"module3/05_pointcloud_machine_learning/05_pointcloud_machine_learning.html#self-evaluation-quiz","title":"Self-evaluation quiz\u00b6","text":"<p>You are now at the end of this theme's lesson contents. Use the following quiz questions to assess your learning success:</p>"},{"location":"module3/05_pointcloud_machine_learning/exercise/m3_theme5_exercise1.html","title":"Exercise: Correspondence-driven M3C2 for change analysis at an active rock glacier","text":"<p>may be added in the future</p> <p>Proceed with the next part using the button at the bottom of this page.</p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html","title":"Multitemporal 3D change analysis at an active rock glacier","text":"Metadata      title: \"E-TRAINEE Research-oriented case study: Multitemporal 3D change analysis at an active rock glacier\"     description: \"This is the sixth theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2023-03     authors: Vivien Zahs, Katharina Anders     contributors: Bernhard H\u00f6fle, Sina Zumstein     estimatedTime: 1.5 hrs In\u00a0[4]: Copied! <pre># import required modules\nimport os\nimport py4dgeo\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\n\n# add the script assets folder to the path, so that we can import the functions created there\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))\n\n# import point cloud functions\nimport pointcloud_functions as pcfuncs\n\n# specify the data path\ndata_dir = 'path-to-data'\n\n# check if the data path exists\nif not os.path.exists(data_dir):\n    raise Exception('Data path does not exist. Please make sure to specify the correct path.')\n\n# derive the file paths and read in the data\npc_file_2020 = f'{data_dir}/ahk_2020_uls.laz'\npc_file_2021 = f'{data_dir}/ahk_2021_uls.laz'\n\n# read the point clouds into numpy arrays\n#pc_2020 = pcfuncs.read_las(pc_file_2020)\n#pc_2021 = pcfuncs.read_las(pc_file_2021)\n</pre> # import required modules import os import py4dgeo import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates from datetime import datetime, timedelta  # add the script assets folder to the path, so that we can import the functions created there import sys from pathlib import Path sys.path.insert(0, str((Path.cwd() / \"..\" / \"..\" / \"assets\" / \"python_functions\").resolve()))  # import point cloud functions import pointcloud_functions as pcfuncs  # specify the data path data_dir = 'path-to-data'  # check if the data path exists if not os.path.exists(data_dir):     raise Exception('Data path does not exist. Please make sure to specify the correct path.')  # derive the file paths and read in the data pc_file_2020 = f'{data_dir}/ahk_2020_uls.laz' pc_file_2021 = f'{data_dir}/ahk_2021_uls.laz'  # read the point clouds into numpy arrays #pc_2020 = pcfuncs.read_las(pc_file_2020) #pc_2021 = pcfuncs.read_las(pc_file_2021) <p>You know the surface change analysis of the rock glacier from the exercise in theme 3 of this module. So we will just repeat this analysis here to obtain the distance and uncertainty information from the M3C2 algorithm. As registration error, we use the value of 3.7 cm as assessed in the exercise.</p> In\u00a0[\u00a0]: Copied! <pre># load epochs into py4dgeo objects\nepoch2020 = py4dgeo.read_from_las(pc_file_2020)\nepoch2021 = py4dgeo.read_from_las(pc_file_2021)\n\n# use every nth point as core point for distance calculation\ncorepoints = epoch2020.cloud[::100]\n\n# instantiate and parametrize the M3C2 algorithm object\nm3c2 = py4dgeo.M3C2(\n    epochs=(epoch2020, epoch2021),\n    corepoints=corepoints,\n    normal_radii=(4.0, 0.5, 6.0),\n    cyl_radii=(0.5,),\n    max_distance=(15.0),\n    registration_error=0.037\n)\n\n# run the algorithm\nm3c2_distances, uncertainties = m3c2.run()\n</pre> # load epochs into py4dgeo objects epoch2020 = py4dgeo.read_from_las(pc_file_2020) epoch2021 = py4dgeo.read_from_las(pc_file_2021)  # use every nth point as core point for distance calculation corepoints = epoch2020.cloud[::100]  # instantiate and parametrize the M3C2 algorithm object m3c2 = py4dgeo.M3C2(     epochs=(epoch2020, epoch2021),     corepoints=corepoints,     normal_radii=(4.0, 0.5, 6.0),     cyl_radii=(0.5,),     max_distance=(15.0),     registration_error=0.037 )  # run the algorithm m3c2_distances, uncertainties = m3c2.run() <p>We visualize the changes, and based on these results, continue with the case study by first looking into the importance of timescales for the observation of rock glacier deformation.</p> In\u00a0[7]: Copied! <pre># create the figure\nfig, axs = plt.subplots(1, 2, figsize=(14,7), subplot_kw={\"projection\": \"3d\"})\n(ax1,ax2) = axs\n\n# plot the distances\nd = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-5.0, vmax=5.0, s=1) \nplt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)\n\n# plot the level of detection values\nl = ax2.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=uncertainties['lodetection'], cmap='viridis', vmax=0.15, s=1) \nplt.colorbar(l, format=('%.2f'), label='Level of detection [m]', ax=ax2, extend='max', shrink=.5, pad=.15)\n\n# add plot elements\nfor ax in axs:\n    ax.set_xlabel('Easting [m]')\n    ax.set_ylabel('Northing [m]')\n    ax.set_aspect('equal')\n    ax.view_init(elev=30., azim=120.)\n\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n</pre> # create the figure fig, axs = plt.subplots(1, 2, figsize=(14,7), subplot_kw={\"projection\": \"3d\"}) (ax1,ax2) = axs  # plot the distances d = ax1.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=m3c2_distances, cmap='seismic_r', vmin=-5.0, vmax=5.0, s=1)  plt.colorbar(d, format=('%.2f'), label='Distance [m]', ax=ax1, shrink=.5, pad=.15)  # plot the level of detection values l = ax2.scatter(corepoints[:,0], corepoints[:,1], corepoints[:,2], c=uncertainties['lodetection'], cmap='viridis', vmax=0.15, s=1)  plt.colorbar(l, format=('%.2f'), label='Level of detection [m]', ax=ax2, extend='max', shrink=.5, pad=.15)  # add plot elements for ax in axs:     ax.set_xlabel('Easting [m]')     ax.set_ylabel('Northing [m]')     ax.set_aspect('equal')     ax.view_init(elev=30., azim=120.)  plt.axis('equal') plt.tight_layout() plt.show() <p></p> <p>Comparison of surface changes perpendicular to local surface orientation in the lower tongue area of the \u00c4u\u00dferes Hochebenkar rock glacier computed with the M3C2 algorithm for (a) the three-week timespan and (b) the one-year timespan. Figure by Ulrich et al. (2021) / CC BY 4.0.</p> <p>Observing surface activities at smaller timescales implies that derived surface changes are of much lower magnitude - depending on the respective change rates. While these smaller change magnitudes help us to identify corresponding objects (e.g., boulders) or processes (e.g. surface lowering separated from creep), they also require us to be able to confidently detect small-scale changes. The following section presents methods accounting for exactly this in the context of research of our case study.</p> <p></p> <p>Another method to increase the detectability of changes in complex topography is provided with the correspondence-driven plane-based M3C2 developed by Zahs et al. (2022b). This approach targets the planarity assumption in M3C2 change analysis by making full use of it: change is calculated between planes, that are explicitly extracted and determined as corresponding between two epochs. This ensures that surfaces are excluded if they exhibit high roughness (scale- and application-dependent!), and that only corresponding surfaces are compared. In this context, think back of the importance of timescales above. If the surface changed a lot between acquisitions, the two epochs of local surfaces compared by the M3C2 may not actually represent the same objects or features. Alternatively, we may of course identify objects - for example individual boulders on the rock glacier surface - for object-based change analysis. However, identifying objects in natural scenes is not trivial as they occur in many different variations and are also often not fully or well represented in the point cloud. For the rock glacier, you experienced yourselves in the data how boulders occur at variable sizes and shapes, and how they are narrowly placed and only covered by the point cloud at the non-occluded surfaces.</p> <p>Therefore, the correspondence-driven plane-based M3C2 does not require recognition or reconstruction of geometrically complex objects but instead quantifies change between less complex, homologous planar surfaces. The method consists of the following main steps:</p> <ol> <li>Extraction of planar surfaces in point clouds of successive epochs</li> <li>Identification of corresponding planar surfaces between two point clouds using a binary random forest classification</li> <li>Calculation of M3C2 distances and the associated uncertainty between the corresponding planar surfaces</li> </ol> <p>Overview of the correspondence-driven plane-based M3C2 approach to quantify change between homologous planar areas and the associated uncertainty based on point clouds of two epochs. Figure by V. Zahs, following Zahs et al. (2022b). </p> <p>Thereby, the method is tailored to quantifying 3D topographic change of rough surfaces at smaller magnitudes by reducing the uncertainty of quantified change. The approach further allows to relate change directly to a moving object, even if the change is not oriented in the direction of the surface.</p> <p>In their study, Zahs et al. (2022b) apply the correspondence-driven M3C2 to a bi-weekly time series of terrestrial laser scanning point clouds acquired at the \u00c4u\u00dferes Hochebenkar rock glacier. The method enables a sevenfold reduction in the uncertainty associated with topographic change compared to standard M3C2. Significant change was therefore detected in around 75% of the area of change analysis, whereas the standard M3C2 detects significant change in only 16% (2-week timespan) to 60% (10-week timespan) of the same area.</p> <p>Note: The correspondence-driven plane-based M3C2 method is available in development stage in py4dgeo. You may test it yourself by setting up the py4dgeo in development mode (see installation instructions on GitHub), using the data of the rock glacier from this course. Stay tuned for the next release, the method will soon be fully available and integrated as interactive analysis in this case study!</p> <p>If you are interested in the full datasets, they are available here as published with Zahs et al. (2022b).</p> <p></p> <p>These results demonstrate the potential of UAV-borne point clouds from different sensing strategies for accurate 3D topographic change analysis. Though the accuracy is lower than for terrestrial laser scanning, there are clear advantages of these survey strategies: In the complex surface geometry of the rock glacier, a higher coverage can be achieved by the airborne perspective of the UAVs. Moreover, a larger area (in the future maybe even the entire rock glacier) can be covered by the acquisition. This strongly reduces the effort (i.e. cost) of data acquisition. In continuous, long-term observation of the rock glacier, the optimal solution will most likely be a combination of different sensing strategies (terrestrial, UAV-borne, laser scanning, photogrammetry, ...) at different timescales.</p> <p>Your turn: As you have a ULS and a photogrammetric point cloud available with the data of this course, go ahead and look into the M3C2 change analysis with different point cloud sources: How do the results differ when analyzing change between ULS to ULS compared to ULS to photogrammetry? Consider different aspects of the py4dgeo M3C2 result, such as the change (<code>distances</code>) and level of detection (<code>lodetection</code>) values, but also the point density and surface roughness (<code>num_samples2</code> and <code>spread2</code> of the compared ULS and photogrammetry point clouds in the returned <code>uncertainties</code>).</p> <p>You have now learnt a lot about survey strategies and methods evolving around the rock glacier case study, enabling enhanced monitoring and new insights about surface processes. In the following section, we will look into the actual interpretation of rock glacier processes at the \u00c4u\u00dferes Hochebenkar with insights from recent research.</p> <p></p> <p>In Zahs et al. (2019), the interpretation of 3D topographic surface change at the rock glacier \u00c4u\u00dferes Hochebenkar has been supported by electrical resistivity tomography (ERT) data of the subsurface. The combination of topographic point clouds and ERT data allowed to assess the influences of thaw-related deformation processes. It becomes visible that areas of negative surface changes are related to the presence of subsurface ice. These measurements at two profiles fit the expectation that the degree of thawing processes depend strongly upon the rock glacier's internal composition.</p> <p>Overview map and ERT profiles of the investigated study site in the lower tongue area of the \u00c4u\u00dferes Hochebenkar rock glacier. (a) Locations of terrestrial laser scanning positions, stable rock walls and ERT profile lines. (b) Model of the near subsurface electrical resistivity distribution along ERT profile 1 and (c) deduced interpretation sketch. Figure by V. Zahs, following Zahs et al. (2019). </p> <p>Due to the constraint of the ERT technique that stakes must have full contact to the ground material, more extensive measurements on the rock glacier itself are hardly possible. As you can see in the map above, the ERT profiles are therefore located at the edge of the rock glacier, where fine material can be reached near the surface. It is hence especially valuable being able to relate surface deformation to subsurface conditions. If we are able to derive a good explanation, i.e. model, of the relation between 3D topographic observations and other survey methods, comprehensive observation of rock glacier processes is enabled with targeted surveying.</p> <p>Continued high-resolution monitoring of the rock glacier is also important to assess the hazard potential of the rock glacier in the destabilized lower sections. Single rock falls from the rock glacier front are already occurring and have affected the road below. It might even be possible that the rock glacier front collapses, with rapid mass movement as a consequence (Hartl et al., 2023). The likelihood of a collapse, following Hartl et al. (2023) can however not be predicted with the current lack of information on the composition of the rock glacier, i.e. subsurface data.</p> <p></p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#case-study-multitemporal-3d-change-analysis-at-an-active-rock-glacier","title":"Case study: Multitemporal 3D change analysis at an active rock glacier\u00b6","text":"<p>In this case study, multitemporal point clouds acquired at different timescales (bi-weekly to annually) and from different sensors and platforms (UAV-borne laser scanning and photogrammetry, terrestrial laser scanning) are used to assess surface changes on an active rock glacier. Throughout the case study, you will learn about:</p> <ul> <li>Detection of surface change over different timespans</li> <li>Change detection at small magnitudes under consideration of uncertainties</li> <li>Observation of rock glacier processes using multi-source point clouds</li> <li>Interpretation of geomorphic change using multi-sensor surface and subsurface information</li> </ul> <p>Most parts are theoretical based on published research that relates to the case study, parts of the methods will be performed interactively (in the Jupyter notebook by you).</p> <p>After finishing this theme you will be able to:</p> <ul> <li>understand the relevance and challenges of rock glacier observation as a setting of complex topographic structure and dynamics</li> <li>assess surface changes with different approaches to increase the confidence of change detection</li> <li>differentiate benefits of different data sources and point cloud properties for topographic monitoring.</li> </ul>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#study-area-and-data","title":"Study area and data\u00b6","text":"<p>Before you continue, get familiar with the use case: Multitemporal and multi-sensor observation of the active rock glacier \u00c4u\u00dferes Hochebenkar (Austria).</p> <p>... and read the key paper accompanying the use case and case study:</p> <p>Hartl, L., Zieher, T., Bremer, M., Stocker-Waldhuber, M., Zahs, V., H\u00f6fle, B., Klug, C. &amp; Cicoira, A. (2023): Multisensor monitoring and data integration reveal cyclical destabilization of \u00c4u\u00dferes Hochebenkar Rock Glacier. Earth Surface Dynamics. Vol. 11, pp. 117-147. DOI: 10.5194/esurf-11-117-2023.</p> <p>For interactive parts in the case study, we will use two epochs of annual point clouds subsampled to 10 cm point spacing (to reduce the required computing capacity). For this, use the dataset <code>ahk</code> from the course data repository. As the rock glacier is being observed with long-term and extensive 3D acquisitions, the major part of the case study and research will be presented in a theoretical manner. Nonetheless, data and methods are mostly openly available and linked in the respective sections if you would like to try them out yourselves!</p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#getting-started-with-the-data-interactive-session","title":"Getting started with the data (interactive session)\u00b6","text":"<p>In the first step, we will set up the Python environment and load the data for the analysis steps shown interactively throughout this case study. If you are following the session hands-on (highly recommended), you need to adapt the path to the data and continue using the <code>etrainee</code> Python environment of the course. For explanations on the procedure to read in the py4dgeo <code>Epoch</code> object and running the M3C2 algorithm, look into theme 3 of this module.</p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#detection-of-surface-change-over-different-timespans","title":"Detection of surface change over different timespans\u00b6","text":"<p>In this case study, we have two annual point clouds available, acquired in summer of each year, respectively (see visualization of change above). However, rock glacier deformation is a result of multiple processes that occur over different timespans. Even if we are aware of the different possible processes, we are not able to attribute how much of the observed surface change was caused, for example, by seasonal thaw, by rock glacier creep, or by movement of individual boulders. The study of Ulrich et al. (2021) demonstrates the benefit of more frequent 3D monitoring to separate surface change types that occur at different timescales related to active rock glacier deformation.</p> <p>The study uses data acquired of the rock glacier \u00c4u\u00dferes Hochebenkar with terrestrial laser scanning at sub-monthly intervals, i.e. acquired over a timespan of three weeks in the summer season of 2018. 3D surface changes across the rock glacier are derived as relative contribution of the three-week timespan to annual surface change since the previous year (2017 to 2018). Changes are further separated according to their directions. Change directions on the rock glacier are spatially variable, depending on the underlying process, and mainly occur either perpendicular to the local rock glacier surface or along the main direction of rock glacier flow.</p> <p>This analysis revealed that the contribution of the three-week timespan to the annual change perpendicular to the surface is 20%, compared with 6% in the direction of rock glacier flow. The figure below visualizes the surface changes derived for each timespan. We observe a certain degree of surface increase which can be attributed to mass accumulation and thickening affecting both periods. There is, however, a higher portion of negative change in the three-week period, indicating that surface lowering is more active in this timespan during summer.</p> <p>Envision the contribution of the shorter timespan like this: If the rate of surface lowering derived in the three-week period would apply throughout the entire year, surface lowering would be four times higher than actually observed. The other way around, if we only observe change at annual timespans, the activity and contribution of this process in the summer season is strongly underestimated - as the change rate would be spread as average across a full year. Using the approach of Ulrich et al. (2021), estimates of surface change (in different directions) are obtained that are dominant at different times of the year - read the paper to learn more about the disaggregation of surface changes at the rock glacier!</p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#change-detection-at-small-magnitudes-under-consideration-of-uncertainties","title":"Change detection at small magnitudes under consideration of uncertainties\u00b6","text":"<p>The ability to quantify small-magnitude change with low uncertainties becomes particularly important in settings where relevant surface changes are small and the frequency of monitoring is high to be adapted to the change rates of the monitored surface. Appropriate frequencies of acquisition and relevant magnitudes of change thereby depend on the dynamics of the observed phenomenon.</p> <p>At the rock glacier, surface changes occur at different change rates and may be of very small magnitudes (centimeter scale) especially if acquisition intervals are small (sub-monthly). To confidently detect changes, i.e. deriving them as significant, we require methods that reduce associated uncertainties, on the one hand, and account for uncertainties as good as possible on the other hand. If you would like to revisit uncertainties in change analysis and the assessment of the level of detection in the M3C2, look into theme 3 of this module.</p> <p>The methods and analysis of rock glacier changes in this section center around the assumption of the M3C2 that surfaces are locally planar. Let's first have a look at the level of detection, which is derived as a combination of surface roughness, point sampling, and the registration error. A non-planar or even curved surface will yield a high roughness value which increases the level of detection value even though the change between surfaces may be real and even distinct:</p> <p>Influence of surface geometry on the level of detection derived with the M3C2 algorithm, leading to a lower detectability of change where the planarity assumption does not hold. Figure by L. Winiwarter, following Winiwarter et al. (2021)/ CC BY 4.0.</p> <p>The limitation of the level of detection calculation here is that it is derived in a purely data-driven manner (and thereby quite powerful). But by including knowledge about the local geometry, survey geometry, and sensors, we are actually able to more rigorously account for the uncertainty in change analysis at each 3D point location. Therefore, a method was developed by Winiwarter et al. (2021), who extend the M3C2 algorithm with error propapagation (M3C2-EP). M3C2-EP quantifies the uncertainty for each point in a point cloud by propagating the uncertainty of the measurements themselves (e.g., sensor accuracy) and of the alignment uncertainty to the 3D points. This allows the 3D covariance information to be calculated for the point cloud, which is used in an extended statistical test for equality of multivariate means. By this, the M3C2-EP gives a less biased estimate of the level of detection, allowing a more appropriate significance threshold in typical cases.</p> <p>The method was applied to terrestrial laser scans of the rock glacier \u00c4u\u00dferes Hochebenkar at two different timespans (three weeks and one year). Over the three-week period, significant change was detected at 12.5% fewer 3D locations, while quantifying additional 25.2% of change volume, when compared to the standard M3C2 method Lague et al. (2013). M3C2-EP further allows the combination of data from multiple scan positions or data sources with different levels of uncertainty.</p> <p>3D view of the LoDetection for the \u00c4u\u00dferes Hochebenkar rock glacier. Left: M3C2. Center: M3C2-EP. Right: Differences between the two. The top row shows the one-year timespan from 2017 to 2018, the bottom row the three-week timespan in 2018. The difference is taken as baseline LoDetection minus LoDetection-EP. Figure by Winiwarter et al. (2021) / CC BY 4.0.</p> <p>Note: The M3C2-EP method is available in development stage in py4dgeo. You may test it yourself by setting up the py4dgeo in development mode (see installation instructions on GitHub), and using the test data (terrestrial laser scans of the rock glacier) provided in the corresponding py4dgeo test data repository. Stay tuned for the next release, the method will soon be fully available and integrated as interactive analysis in this case study!</p> <p>If you are interested in the full datasets, they are available here as published with Winiwarter et al. (2021).</p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#observation-of-rock-glacier-changes-using-multi-source-point-clouds","title":"Observation of rock glacier changes using multi-source point clouds\u00b6","text":"<p>The research on the rock glacier presented so far has made use of terrestrial laser scanning point clouds. But as you already know from the course, UAV-borne acquisitions are available. These acquisitions have been conducted at the \u00c4u\u00dferes Hochebenkar since the summer of 2018, to extend the observation at the rock glacier by these more recent survey strategies. Point clouds derived from UAV-borne laser scanning and UAV-borne photogrammetry provide new opportunities for 3D topographic monitoring in geographic research. The airborne acquisition strategy overcomes common challenges of ground-based techniques, such as limited spatial coverage or heterogeneous measurement distribution, and allows flexible repeated acquisitions at high temporal and spatial resolution. While UAV-borne 3D sensing techniques are expected to thereby enhance geographic observation, an important question is how well they are suited for 3D change analysis of complex natural surfaces, such as the rock glacier.</p> <p>Zahs et al. (2022a) have assessed point clouds originating from UAV-borne laser scanning (ULS) and photogrammetry as input for 3D topographic change analysis at the rock glacier \u00c4u\u00dferes Hochebenkar. Surface change was analyzed by using ULS and photogrammetric point clouds of two years (2019 and 2021) as input for the M3C2 algorithm (Lague et al., 2013) and the correspondence-driven plane-based M3C2 (Zahs et al., 2022b). ULS-based and photogrammetry-based change analysis are evaluated regarding their performance in (1) achieving high spatial coverage of derived changes, (2) accurately quantifying magnitude and uncertainty of change, and (3) detecting significant change (change magnitudes &gt; associated uncertainty). Terrestrial laser scanning (TLS) surveys undertaken simultaneously with the UAV-borne acquisitions are used as reference for change analysis.</p> <p>The study shows the improved spatial coverage of M3C2 achieved with point clouds acquired with UAVs (60% increase of core points used for change analysis). For the correspondence-driven plane-based M3C2, UAV-borne point clouds enabled a spatially more uniform distribution of plane pairs used for change quantification and a slightly higher spatial coverage (7% increase of core points used for change analysis) compared to the TLS reference. Magnitudes of M3C2 change were closer to the TLS reference for change analysis between two ULS acquisitions (mean difference: 0.04 m; std. dev.: 0.05 m) compared to analysis between ULS and photogrammetric acquisitions (mean difference: 0.12 m; std. dev.: 0.08 m). Magnitudes of change were above the associated uncertainty in 82% to 89% of the area of change analysis.</p> <p>Results of surface change analysis using point clouds derived from terrestrial laser scanning (TLS), UAV-borne laser scanning (ULS), and UAV-borne photogrammetry using dense image matching (DIM) as input. Spatial coverage (a) - (c), LoDetection (d) - (f), magnitudes of surface change (g) - (i), and significance of surface change (j) - (l) are shown on top of a hillshade derived from airborne laser scanning data. Figure by Zahs et al. (2022a) / CC BY 4.0. </p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#interpretation-of-geomorphic-processes-using-multi-sensor-surface-and-subsurface-information","title":"Interpretation of geomorphic processes using multi-sensor surface and subsurface information\u00b6","text":"<p>The interpretation of geomorphic change and underlying processes at the rock glacier benefits from the integration of multi-sensor surface and subsurface information, such as geophysical measurements using electrical resistivity tomography (ERT).</p> <p>Hartl et al. (2023) realize this by using the interdisciplinary, long-term, continuous, and high-resolution monitoring strategies available at the rock glacier \u00c4u\u00dferes Hochebenkar. This enables to improve process understanding and model development related to rock glacier rheology and deformation.</p> <p>A main focus of observation at the rock glacier is on its kinematics and destabilization, investigated by Hartl et al. (2023).</p> <p>The unique in-situ and remote-sensing-based monitoring network at the \u00c4u\u00dferes Hochebenkar is used by consolidating, homogenizing, and extending the available time series of multi-sensor surface observations, thereby generating a comprehensive dataset including:</p> <ul> <li>14 digital surface models (from historical aerial imagery and airborne and UAV-borne laser scanning) covering a 68-year time period</li> <li>in-situ measurements of block displacement since the early 1950s,</li> <li>high-resolution airborne and UAV laser scanning point clouds at annual temporal resolution from 2017 to 2021</li> <li>terrestrial laser scanning point clouds collected in bi-weekly intervals during the summer of 2019 at the rock glacier front.</li> </ul> <p>Movement and change information derived from the combination of these datasets reveal two cycles of destabilization in the lower section of the rock glacier (find an animation here. The first cycle lasted from the early 1950s until the mid-1970s. The second cycle began around 2017 after approximately two decades of more gradual acceleration and is currently ongoing. Both destabilization periods are characterized by high velocities and the development of morphological destabilization features on the rock glacier surface. Acceleration in the most recent years has been very pronounced, with velocities reaching 20 to 30 m per year in 2020 to 2021. These rates are unprecedented in the time series and suggest highly destabilized conditions in the lower section of the rock glacier, which shows signs of translational and rotational landslide-like movement.</p> <p>Velocity vectors (m/a) for the time series of digital surface models, plotted over hillshades of the later digital surface model of each consecutive pair. Reference lines defining block profiles P0\u2013P3 have been added for orientation purposes. Figure by Hartl et al. (2023) / CC BY 4.0. </p>"},{"location":"module3/06_casestudy_rockglacier/06_casestudy_rockglacier.html#references","title":"References\u00b6","text":"<ul> <li>Hartl, L., Zieher, T., Bremer, M., Stocker-Waldhuber, M., Zahs, V., H\u00f6fle, B., Klug, C. &amp; Cicoira, A. (2023): Multisensor monitoring and data integration reveal cyclical destabilization of \u00c4u\u00dferes Hochebenkar Rock Glacier. Earth Surface Dynamics. Vol. 11, pp. 117-147. doi: 10.5194/esurf-11-117-2023.</li> <li>Lague, D., Brodu, N., &amp; Jeroux, J. (2013): Accurate 3D comparison of complex topography with terrestrial laser scanner: Application to the Rangitikei canyon (N-Z). ISPRS Journal of Photogrammetry and Remote Sensing, 82 (2013), pp. 10-26. doi: 10.1016/j.isprsjprs.2013.04.009.</li> <li>Ulrich, V., Williams, J.G., Zahs, V., Anders, K., Hecht, S. &amp; H\u00f6fle, B. (2021): Measurement of rock glacier surface change over different timescales using terrestrial laser scanning point clouds. Earth Surface Dynamics. Vol. 9, pp. 19-28. doi: 10.5194/esurf-9-19-2021.</li> <li>Williams, J.G., Anders, K., Winiwarter, L., Zahs, V. &amp; H\u00f6fle, B. (2021): Multi-directional change detection between point clouds. ISPRS Journal of Photogrammetry and Remote Sensing. Vol. 172, pp. 95-113. doi: 10.1016/j.isprsjprs.2020.12.002.</li> <li>Winiwarter, L., Anders, K. &amp; H\u00f6fle, B. (2021): M3C2-EP: Pushing the limits of 3D topographic point cloud change detection by error propagation. ISPRS Journal of Photogrammetry and Remote Sensing, 178, pp. 240\u2013258. doi: 10.1016/j.isprsjprs.2021.06.011.</li> <li>Zahs, V., H\u00e4mmerle, M., Anders, K., Hecht, S., Rutzinger, M., Sailer, R., Williams, J.G. &amp; H\u00f6fle, B. (2019): Multi-temporal 3D point cloud-based quantification and analysis of geomorphological activity at an alpine rock glacier using airborne and terrestrial LiDAR. Permafrost and Periglacial Processes. Vol. 30 (3), pp. 222-238. doi: 10.1002/ppp.2004.</li> <li>Zahs, V., Winiwarter, L., Anders, K., Bremer, M. Rutzinger, M. Pot\u016f\u010dkov\u00e1, M. &amp; H\u00f6fle, B. (2022a): Evaluation of UAV-borne photogrammetry and UAV-borne laser scanning for 3D topographic change analysis of an active rock glacier. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences. Vol. XLIII-B2-2022, pp. 1109-1116. doi: 10.5194/isprs-archives-XLIII-B2-2022-1109-2022.</li> <li>Zahs, V., Winiwarter, L., Anders, K., Williams, J.G., Rutzinger, M. &amp; H\u00f6fle, B. (2022b): Correspondence-driven plane-based M3C2 for lower uncertainty in 3D topographic change quantification. ISPRS Journal of Photogrammetry and Remote Sensing. Vol. 183, pp. 541-559. doi: 10.1016/j.isprsjprs.2021.11.018.</li> </ul>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html","title":"Time series-based change analysis of sandy beach dynamics","text":"Metadata      title: \"E-TRAINEE Research-oriented case study: Time series-based change analysis of sandy beach dynamics\"     description: \"This is the sixth theme within the 3D/4D Geographic Point Cloud Time Series Analysis module.\"     dateCreated: 2023-03     authors: Katharina Anders, Bernhard H\u00f6fle     contributors: Sina Zumstein     estimatedTime: 1.5 hrs In\u00a0[1]: Copied! <pre># import required modules\nimport os\nimport py4dgeo\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime, timedelta\n\n# specify the data path\ndata_path = 'path-to-data'\n\n# sub-directory containing the point clouds\npc_dir = os.path.join(data_path, 'pointclouds')\n\n# check if the data path exists\nif not os.path.exists(pc_dir):\n    raise Exception('Data path does not exist. Please make sure to specify the correct path.')\n</pre> # import required modules import os import py4dgeo import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates from datetime import datetime, timedelta  # specify the data path data_path = 'path-to-data'  # sub-directory containing the point clouds pc_dir = os.path.join(data_path, 'pointclouds')  # check if the data path exists if not os.path.exists(pc_dir):     raise Exception('Data path does not exist. Please make sure to specify the correct path.') In\u00a0[\u00a0]: hide-output Copied! <pre># list of point clouds (time series)\npc_list = os.listdir(pc_dir)\n\n# read the timestamps from file names\ntimestamps = []\nfor f in pc_list:\n    if not f.endswith('.laz'):\n        continue\n\n    # get the timestamp from the file name\n    timestamp_str = '_'.join(f.split('.')[0].split('_')[1:]) # yields YYMMDD_hhmmss\n\n    # convert string to datetime object\n    timestamp = datetime.strptime(timestamp_str, '%y%m%d_%H%M%S')\n    timestamps.append(timestamp)\n\n# create the spatiotemporal analysis object\nanalysis = py4dgeo.SpatiotemporalAnalysis(f'{data_path}/kijkduin_hourly.zip', force=True)\n\n# specify the reference epoch\nreference_epoch_file = os.path.join(pc_dir, pc_list[0])\n\n# read the reference epoch and set the timestamp\nreference_epoch = py4dgeo.read_from_las(reference_epoch_file)\nreference_epoch.timestamp = timestamps[0]\n\n# set the reference epoch in the spatiotemporal analysis object\nanalysis.reference_epoch = reference_epoch\n\n# inherit from the M3C2 algorithm class to define a custom direction algorithm\nclass M3C2_Vertical(py4dgeo.M3C2):\n    def directions(self):\n        return np.array([0, 0, 1]) # vertical vector orientation\n\n# specify corepoints, here all points of the reference epoch\nanalysis.corepoints = reference_epoch.cloud[::]\n\n# specify M3C2 parameters for our custom algorithm class\nanalysis.m3c2 = M3C2_Vertical(cyl_radii=(1.0,), max_distance=10.0, registration_error = 0.019)\n# create a list to collect epoch objects\nepochs = []\nfor e, pc_file in enumerate(pc_list[1:]):\n    epoch_file = os.path.join(pc_dir, pc_file)\n    epoch = py4dgeo.read_from_las(epoch_file)\n    epoch.timestamp = timestamps[e]\n    epochs.append(epoch)\n\n# add epoch objects to the spatiotemporal analysis object\nanalysis.add_epochs(*epochs)\n</pre> # list of point clouds (time series) pc_list = os.listdir(pc_dir)  # read the timestamps from file names timestamps = [] for f in pc_list:     if not f.endswith('.laz'):         continue      # get the timestamp from the file name     timestamp_str = '_'.join(f.split('.')[0].split('_')[1:]) # yields YYMMDD_hhmmss      # convert string to datetime object     timestamp = datetime.strptime(timestamp_str, '%y%m%d_%H%M%S')     timestamps.append(timestamp)  # create the spatiotemporal analysis object analysis = py4dgeo.SpatiotemporalAnalysis(f'{data_path}/kijkduin_hourly.zip', force=True)  # specify the reference epoch reference_epoch_file = os.path.join(pc_dir, pc_list[0])  # read the reference epoch and set the timestamp reference_epoch = py4dgeo.read_from_las(reference_epoch_file) reference_epoch.timestamp = timestamps[0]  # set the reference epoch in the spatiotemporal analysis object analysis.reference_epoch = reference_epoch  # inherit from the M3C2 algorithm class to define a custom direction algorithm class M3C2_Vertical(py4dgeo.M3C2):     def directions(self):         return np.array([0, 0, 1]) # vertical vector orientation  # specify corepoints, here all points of the reference epoch analysis.corepoints = reference_epoch.cloud[::]  # specify M3C2 parameters for our custom algorithm class analysis.m3c2 = M3C2_Vertical(cyl_radii=(1.0,), max_distance=10.0, registration_error = 0.019) # create a list to collect epoch objects epochs = [] for e, pc_file in enumerate(pc_list[1:]):     epoch_file = os.path.join(pc_dir, pc_file)     epoch = py4dgeo.read_from_las(epoch_file)     epoch.timestamp = timestamps[e]     epochs.append(epoch)  # add epoch objects to the spatiotemporal analysis object analysis.add_epochs(*epochs) In\u00a0[3]: Copied! <pre># apply temporal averaging\nanalysis.smoothed_distances = py4dgeo.temporal_averaging(analysis.distances, smoothing_window=6)\n\n# load the smoothed distances\ndistances = analysis.smoothed_distances\n\n# extract the time series at the selected core point\ncp_sel_idx = 205840\nts_sel = distances[cp_sel_idx]\n\n# plot the time series\nplt.plot(timestamps[1:], ts_sel, c='blue', linewidth=1.0)\n\n# format the date labels\ndtFmt = mdates.DateFormatter('%b-%d') \nplt.gca().xaxis.set_major_formatter(dtFmt) \n\n# add plot elements\nplt.xlabel('Date')\nplt.ylabel('Distance [m]')\nplt.ylim(-.01, .01)\n\n# add grid with minor ticks every day\nplt.grid(which='both', linestyle='--')\nplt.gca().xaxis.set_minor_locator(mdates.DayLocator(interval=1))\n\nplt.show()\n</pre> # apply temporal averaging analysis.smoothed_distances = py4dgeo.temporal_averaging(analysis.distances, smoothing_window=6)  # load the smoothed distances distances = analysis.smoothed_distances  # extract the time series at the selected core point cp_sel_idx = 205840 ts_sel = distances[cp_sel_idx]  # plot the time series plt.plot(timestamps[1:], ts_sel, c='blue', linewidth=1.0)  # format the date labels dtFmt = mdates.DateFormatter('%b-%d')  plt.gca().xaxis.set_major_formatter(dtFmt)   # add plot elements plt.xlabel('Date') plt.ylabel('Distance [m]') plt.ylim(-.01, .01)  # add grid with minor ticks every day plt.grid(which='both', linestyle='--') plt.gca().xaxis.set_minor_locator(mdates.DayLocator(interval=1))  plt.show() <pre>[2023-04-13 11:04:12][INFO] Starting: Smoothing temporal data\n</pre> <pre>h:\\conda_envs\\etrainee\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n</pre> <pre>[2023-04-13 11:05:35][INFO] Finished in 83.2294s: Smoothing temporal data\n</pre> <p>The time series plot shows the M3C2 distance at a assumingly stable location, the paved path. The visualization shows that the values mostly vary within a range of 5 mm. Nonetheless, we can observe a fluctuating pattern, which follows a more or less daily course.</p> <p>We do not have external measurements available from the site, but we can assess the relation of the pattern to meteorological data recorded at the nearest regional weather station. Such data is openly provided in The Netherlands by KNMI. For our analysis, you find the prepared file of the nearest station (Hoek van Holland, around 12 km away) with the course data (subdirectory <code>supplementary</code>). We read this data and add temperature information to the plot:</p> In\u00a0[4]: Copied! <pre># read in the meteorological data\nmet = np.genfromtxt(f'{data_path}/supplementary/KNMI_201611_201705_hourly_330_hoekvanholland_formatted.txt',\n                    delimiter=',', skip_header=1, encoding='utf-8',dtype='int')\n\n# convert date string to datetime objects\nmet_dates = [datetime.strptime(d, '%Y%m%d') for d in met[:, 1].astype(str)]\n# add hours to datetime objects\nmet_timestamps = [m + timedelta(hours=h) for m, h in zip(met_dates, met[:, 2].astype(float))]\n\n# get the temperature values (and convert from 0.1 degrees Celsius to degrees Celsius)\ntemps = met[:, 7].astype(float) / 10\n\n# plot the time series\nplt.plot(timestamps[1:], ts_sel, c='blue', linewidth=1.0)\n\n# format the date labels\ndtFmt = mdates.DateFormatter('%b-%d') \nplt.gca().xaxis.set_major_formatter(dtFmt) \n\n# add plot elements\nplt.xlabel('Date')\nplt.ylabel('Distance [m]')\nplt.ylim(-.01, .01)\n\n# add grid with minor ticks every day\nplt.grid(which='both', linestyle='--')\nplt.gca().xaxis.set_minor_locator(mdates.DayLocator(interval=1))\n\n# add temperature values to the plot on the right y-axis\nax2 = plt.twinx()\nax2.plot(met_timestamps, temps, color='red')\nax2.set_ylabel('Temperature [\u00b0C]')\n\n# color the temperature y-axis components red\nfor tl in ax2.get_yticklabels():\n    tl.set_color('red')\nax2.yaxis.label.set_color('red')\n\n# limit the x-axis to the period of distance measurements\nplt.xlim(timestamps[1], timestamps[-1])\n\nplt.show()\n</pre> # read in the meteorological data met = np.genfromtxt(f'{data_path}/supplementary/KNMI_201611_201705_hourly_330_hoekvanholland_formatted.txt',                     delimiter=',', skip_header=1, encoding='utf-8',dtype='int')  # convert date string to datetime objects met_dates = [datetime.strptime(d, '%Y%m%d') for d in met[:, 1].astype(str)] # add hours to datetime objects met_timestamps = [m + timedelta(hours=h) for m, h in zip(met_dates, met[:, 2].astype(float))]  # get the temperature values (and convert from 0.1 degrees Celsius to degrees Celsius) temps = met[:, 7].astype(float) / 10  # plot the time series plt.plot(timestamps[1:], ts_sel, c='blue', linewidth=1.0)  # format the date labels dtFmt = mdates.DateFormatter('%b-%d')  plt.gca().xaxis.set_major_formatter(dtFmt)   # add plot elements plt.xlabel('Date') plt.ylabel('Distance [m]') plt.ylim(-.01, .01)  # add grid with minor ticks every day plt.grid(which='both', linestyle='--') plt.gca().xaxis.set_minor_locator(mdates.DayLocator(interval=1))  # add temperature values to the plot on the right y-axis ax2 = plt.twinx() ax2.plot(met_timestamps, temps, color='red') ax2.set_ylabel('Temperature [\u00b0C]')  # color the temperature y-axis components red for tl in ax2.get_yticklabels():     tl.set_color('red') ax2.yaxis.label.set_color('red')  # limit the x-axis to the period of distance measurements plt.xlim(timestamps[1], timestamps[-1])  plt.show() <p>The comparison shows that the fluctuation of distances coarsely follows the (inverse) course of temperatures. Mind that the point clouds are already aligned through rigid transformations derived for stable parts in the scene (cf. theme 3 of this module to revisit the approach). So we can observe an environmental influence on our measurements, which cannot be corrected by standard preprocessing procedures. We currently have no better way to correct these deviations - even though the effects of environmental influences are known, they cannot be modelled without extensive additional measurements (Friedli et al., 2019).</p> <p>For our analyses of the data, this means we need to be aware of such effects and handle them as uncertainty - to not misinterpret noise as surface change. To further reduce the effect of daily variations in subsequent analysis, we apply a temporal smoothing in a window of one week (cf. Anders et al., 2019).</p> In\u00a0[5]: Copied! <pre># apply stronger temporal averaging (one week window)\nanalysis.invalidate_results(smoothed_distances=True)\nanalysis.smoothed_distances = py4dgeo.temporal_averaging(analysis.distances, smoothing_window=168)\n</pre> # apply stronger temporal averaging (one week window) analysis.invalidate_results(smoothed_distances=True) analysis.smoothed_distances = py4dgeo.temporal_averaging(analysis.distances, smoothing_window=168) <pre>[2023-04-13 11:11:05][INFO] Removing intermediate results from the analysis file I:\\Projects\\2020_ETRAINEE\\Development\\data_final\\kijkduin_hourly/kijkduin_hourly.zip\n[2023-04-13 11:11:05][INFO] Starting: Smoothing temporal data\n[2023-04-13 11:37:26][INFO] Finished in 1580.3869s: Smoothing temporal data\n</pre> <p></p> In\u00a0[12]: Copied! <pre>fig, ax = plt.subplots(1,1, figsize=(15,5))\n\n# get the change magnitude of the last epoch\nchange_vals = analysis.smoothed_distances[:, -1]\n\n# plot coordinates colored by change values \ncloud = analysis.corepoints.cloud\nd = ax.scatter(cloud[:,0], cloud[:,1], c = change_vals, cmap='seismic_r', vmin=-1.0, vmax=1.0, s=1)\nax.set_aspect('equal')\nplt.colorbar(d, format=('%.2f'), label='Change value [m]', ax=ax)\n\n# add plot elements\nax.set_xlabel('X [m]')\nax.set_ylabel('Y [m]')\n\nplt.show()\n</pre> fig, ax = plt.subplots(1,1, figsize=(15,5))  # get the change magnitude of the last epoch change_vals = analysis.smoothed_distances[:, -1]  # plot coordinates colored by change values  cloud = analysis.corepoints.cloud d = ax.scatter(cloud[:,0], cloud[:,1], c = change_vals, cmap='seismic_r', vmin=-1.0, vmax=1.0, s=1) ax.set_aspect('equal') plt.colorbar(d, format=('%.2f'), label='Change value [m]', ax=ax)  # add plot elements ax.set_xlabel('X [m]') ax.set_ylabel('Y [m]')  plt.show() <p>From the plot of changes at the end of the six-week period, we see overall surface increase of the beach. In the area of the pathway and at the foot of the dune, surface decrease is dominant. This surface decrease is caused by human modification, where sand was removed after a storm event (immediately before the onset of our time series data). Paths were therefore freed from sand, and this sand was deposited on the beach. You can see this deposition as attachment to the upper beach part in the linear structure of strong surface increase.</p> <p>Now we can use the higher-frequency change information to find out how much surface changes, i.e. sand transport, we cannot see from this static six-week information. We only perform this for a sub-area on the beach, to exclude the influence of gaps in the data or vegetation on the dunes.</p> In\u00a0[55]: Copied! <pre>distances = analysis.smoothed_distances\ncoords = analysis.corepoints.cloud\n\n# define a bounding box (here 50x50 m)\nminx, miny = [-200.0, -55.0]\nmaxx, maxy = [-150.0, -5.0]\n\n# select core points within the bounding box\ncp_bb_idxs = np.where((coords[:, 0] &gt; minx) &amp; (coords[:, 0] &lt; maxx) &amp; (coords[:, 1] &gt; miny) &amp; (coords[:, 1] &lt; maxy))\n\n# select the corresponding time series\ndistances_bb = distances[cp_bb_idxs]\n\n# loop over temporal intervals\ntints = [1,2,6,12,24,48,96,168,336] # select temporal intervals in hours\ncum_change_tint = []\nfor tint in tints:\n    \n    # select every nth time step, according to the temporal interval\n    distances_tint = distances_bb[:,::tint]\n\n    # get the change rate by subtracting the previous time step from the current time step\n    distances_tint_diff = np.diff(distances_tint, axis=1)\n\n    # calculate the cumulated change\n    change_tint_cum = np.nansum(abs(distances_tint_diff), axis=1)\n    cum_change_tint.append(np.sum(change_tint_cum))\n\n    # store the cumulated change values at tint=1 for plotting\n    if tint == 1:\n        change_tint_cum_sel = change_tint_cum\n\n# plot the cumulated change as a function of temporal interval and as map\nfig, axs = plt.subplots(1,2, figsize=(15,5))\nax1, ax2 = axs\n\nax1.plot(tints, cum_change_tint, c='navy', linewidth=1.0)\n\n# make the y-axis logarithmic\n#ax1.set_yscale('log')\n\n# add plot elements\nax1.set_xlabel('Temporal interval [h]')\nax1.set_ylabel('Cumulated change [m]')\nax1.grid()\n\n# plot the scene as background\nax2.scatter(cloud[:,0], cloud[:,1], c = 'lightgrey', s=1)\n\n# plot the cumulated values as a map\nd = ax2.scatter(coords[cp_bb_idxs,0], coords[cp_bb_idxs,1], c = change_tint_cum_sel, cmap='viridis', s=1)\nax2.set_aspect('equal')\nplt.colorbar(d, format=('%.2f'), label='Cumulated change [m]', ax=ax2)\n\nplt.show()\n</pre> distances = analysis.smoothed_distances coords = analysis.corepoints.cloud  # define a bounding box (here 50x50 m) minx, miny = [-200.0, -55.0] maxx, maxy = [-150.0, -5.0]  # select core points within the bounding box cp_bb_idxs = np.where((coords[:, 0] &gt; minx) &amp; (coords[:, 0] &lt; maxx) &amp; (coords[:, 1] &gt; miny) &amp; (coords[:, 1] &lt; maxy))  # select the corresponding time series distances_bb = distances[cp_bb_idxs]  # loop over temporal intervals tints = [1,2,6,12,24,48,96,168,336] # select temporal intervals in hours cum_change_tint = [] for tint in tints:          # select every nth time step, according to the temporal interval     distances_tint = distances_bb[:,::tint]      # get the change rate by subtracting the previous time step from the current time step     distances_tint_diff = np.diff(distances_tint, axis=1)      # calculate the cumulated change     change_tint_cum = np.nansum(abs(distances_tint_diff), axis=1)     cum_change_tint.append(np.sum(change_tint_cum))      # store the cumulated change values at tint=1 for plotting     if tint == 1:         change_tint_cum_sel = change_tint_cum  # plot the cumulated change as a function of temporal interval and as map fig, axs = plt.subplots(1,2, figsize=(15,5)) ax1, ax2 = axs  ax1.plot(tints, cum_change_tint, c='navy', linewidth=1.0)  # make the y-axis logarithmic #ax1.set_yscale('log')  # add plot elements ax1.set_xlabel('Temporal interval [h]') ax1.set_ylabel('Cumulated change [m]') ax1.grid()  # plot the scene as background ax2.scatter(cloud[:,0], cloud[:,1], c = 'lightgrey', s=1)  # plot the cumulated values as a map d = ax2.scatter(coords[cp_bb_idxs,0], coords[cp_bb_idxs,1], c = change_tint_cum_sel, cmap='viridis', s=1) ax2.set_aspect('equal') plt.colorbar(d, format=('%.2f'), label='Cumulated change [m]', ax=ax2)  plt.show() <p>Above, we derive the cumulated (absolute) surface changes for a sub-area of 50 x 50 m. With increasing temporal interval, the plot shows a strong decrease in the total (positive and negative) surface change that can be observed. We see a particularly sharp decline from hourly to daily (i.e. 24-hourly) analysis. This gives us an indication that a large share of surface variability takes place at these timescales. However, we also need to bear in mind, that also noise is included in these observations. The smaller the temporal interval, the higher the influence of noise, which is present in each epoch of our change analysis.</p> <p>In the map, you see the cumulated change for hourly temporal intervals in our sub-area. You could adapt the selected area (<code>cp_bb_idxs</code>) and see how it influences the result of this assessment, depending on the area on the beach and the processes that occurred there (stronger tidal influence, more/less anthropogenic modification, etc.) at different timescales. Also vary the temporal interval of the visualized cumulated change (<code>change_tint_cum_sel</code>). How does the spatial variation change and what does it tell you about the timescales of observed surface activities?</p> <p></p> In\u00a0[\u00a0]: Copied! <pre># import kmeans clustering module from scikit-learn\nfrom sklearn.cluster import KMeans\n\n# use the smoothed distances for clustering\ndistances = analysis.smoothed_distances\n\n# define the number of clusters\nks = [10]\n\n# create an array to store the labels\nlabels = np.full((distances.shape[0], len(ks)), np.nan)\n\n# perform clustering for each number of clusters\nfor kidx, k in enumerate(ks):\n    print(f'Performing clustering with k={k}...')\n    nan_indicator = np.logical_not(np.isnan(np.sum(distances, axis=1)))\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(distances[nan_indicator, :])\n    labels[nan_indicator, kidx] = kmeans.labels_\n</pre> # import kmeans clustering module from scikit-learn from sklearn.cluster import KMeans  # use the smoothed distances for clustering distances = analysis.smoothed_distances  # define the number of clusters ks = [10]  # create an array to store the labels labels = np.full((distances.shape[0], len(ks)), np.nan)  # perform clustering for each number of clusters for kidx, k in enumerate(ks):     print(f'Performing clustering with k={k}...')     nan_indicator = np.logical_not(np.isnan(np.sum(distances, axis=1)))     kmeans = KMeans(n_clusters=k, random_state=0).fit(distances[nan_indicator, :])     labels[nan_indicator, kidx] = kmeans.labels_ In\u00a0[58]: Copied! <pre># plot the cluster labels as a map\nfig, ax = plt.subplots(1,1, figsize=(7,7))\n\ncmap_clustering = 'tab20'\nsc1 = ax.scatter(cloud[:,0],cloud[:,1],c=labels[:,0],cmap=cmap_clustering,s=1, label=ks[0])\n\nax.set_aspect('equal')\nax.set_title(f'# clusters = {ks[0]}')\n\nplt.tight_layout()\nplt.show()\n</pre> # plot the cluster labels as a map fig, ax = plt.subplots(1,1, figsize=(7,7))  cmap_clustering = 'tab20' sc1 = ax.scatter(cloud[:,0],cloud[:,1],c=labels[:,0],cmap=cmap_clustering,s=1, label=ks[0])  ax.set_aspect('equal') ax.set_title(f'# clusters = {ks[0]}')  plt.tight_layout() plt.show() <p>The obtained clusters can then be related to knowledge about processes on the beach - for example the formation of intertidal bars. As this is an unsupervised method, also processes with a-priori unknown occurrence may be revealed. In this case, one can further look into the time series and properties of surface behaviour, as well as the location on the beach and spatial patterns, to derive an explanation for the observed pattern.</p> <p>Based on time series clustering at Kijkduin beach, Kuschnerus et al. (2021) have uncovered a set of specific processes represented by each cluster, among them anthropogenic modifications through bulldozer works and tidal erosion. The dune area is largely assigned to the same cluster due to the moving vegetation cover as common surface property. Note that the results differ from our analysis above due to a different timespan and temporal resolution being analyzed in their publication.</p> <p>Change patterns identified from time series clustering (number of clusters = 10) for a laser scanning time series at Kijkduin beach. Figure by Kuschnerus et al. (2021) / CC BY 4.0.</p> <p>Beyond the identification of processes from change patterns, the analysis provides information to analysts, for example, as to how to partition the scene into areas of specific relevance. For coastal monitoring, it may be of particular interest in which areas human activities interact with natural sand transport - so that in-depth observation can be performed, or even measures could be taken.</p> <p></p> In\u00a0[59]: Copied! <pre># select a core point to use as seed\ncp_idx_sel = 15162\n\n# parametrize the 4D-OBC extraction\nalgo = py4dgeo.RegionGrowingAlgorithm(window_width=14, \n                                      minperiod=2, \n                                      height_threshold=0.05, \n                                      neighborhood_radius=1.0,\n                                      min_segments=10, \n                                      thresholds=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n                                      seed_candidates=[cp_idx_sel])\n\n# run the algorithm\nanalysis.invalidate_results(seeds=True, objects=True, smoothed_distances=False) # only required if you want to re-run the algorithm\nobjects = algo.run(analysis)\n</pre> # select a core point to use as seed cp_idx_sel = 15162  # parametrize the 4D-OBC extraction algo = py4dgeo.RegionGrowingAlgorithm(window_width=14,                                        minperiod=2,                                        height_threshold=0.05,                                        neighborhood_radius=1.0,                                       min_segments=10,                                        thresholds=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],                                        seed_candidates=[cp_idx_sel])  # run the algorithm analysis.invalidate_results(seeds=True, objects=True, smoothed_distances=False) # only required if you want to re-run the algorithm objects = algo.run(analysis) <pre>[2023-04-13 14:54:49][INFO] Removing intermediate results from the analysis file I:\\Projects\\2020_ETRAINEE\\Development\\data_final\\kijkduin_hourly/kijkduin_hourly.zip\n[2023-04-13 14:54:49][INFO] Starting: Find seed candidates in time series\n[2023-04-13 14:54:49][INFO] Finished in 0.0116s: Find seed candidates in time series\n[2023-04-13 14:54:49][INFO] Starting: Sort seed candidates by priority\n[2023-04-13 14:54:49][INFO] Finished in 0.0642s: Sort seed candidates by priority\n[2023-04-13 14:54:49][INFO] Starting: Performing region growing on seed candidate 1/1\n[2023-04-13 14:55:19][INFO] Finished in 30.3813s: Performing region growing on seed candidate 1/1\n</pre> In\u00a0[61]: Copied! <pre>seed_timeseries = analysis.smoothed_distances[cp_idx_sel]\nplt.plot(timestamps[1:],seed_timeseries, c='black', linestyle='--', linewidth=0.5, label='Seed timeseries')\n\nfor sid, example_seed in enumerate(analysis.seeds):\n    seed_end = example_seed.end_epoch\n    seed_start = example_seed.start_epoch\n    seed_cp_idx = example_seed.index\n\n    plt.plot(timestamps[seed_start:seed_end+1], seed_timeseries[seed_start:seed_end+1], label=f'Seed {sid}')\n\n# format the date labels\ndtFmt = mdates.DateFormatter('%b-%d')\nplt.gca().xaxis.set_major_formatter(dtFmt)\n\n# add plot elements\nplt.xlabel('Date')\nplt.ylabel('Distance [m]')\n\nplt.legend()\nplt.show()\n</pre> seed_timeseries = analysis.smoothed_distances[cp_idx_sel] plt.plot(timestamps[1:],seed_timeseries, c='black', linestyle='--', linewidth=0.5, label='Seed timeseries')  for sid, example_seed in enumerate(analysis.seeds):     seed_end = example_seed.end_epoch     seed_start = example_seed.start_epoch     seed_cp_idx = example_seed.index      plt.plot(timestamps[seed_start:seed_end+1], seed_timeseries[seed_start:seed_end+1], label=f'Seed {sid}')  # format the date labels dtFmt = mdates.DateFormatter('%b-%d') plt.gca().xaxis.set_major_formatter(dtFmt)  # add plot elements plt.xlabel('Date') plt.ylabel('Distance [m]')  plt.legend() plt.show() <p>Above we see the time series of a selected location, where temporary surface increase and decrease - representing a sand bar - was detected as seed. We visualize the corresponding 4D object-by-change, i.e. the time series of all locations belonging to the object and its spatial extent in the scene:</p> In\u00a0[65]: Copied! <pre>fig, axs = plt.subplots(1,2, figsize=(15,7))\nax1,ax2 = axs\n\n# get indices of 4D-OBC\nsel_seed_idx = 0\nidxs = objects[sel_seed_idx].indices\n\n# get change values at end of object for each location\nepoch_of_interest = int((objects[sel_seed_idx].end_epoch - objects[sel_seed_idx].start_epoch)/2 + objects[sel_seed_idx].start_epoch)\ndistances_of_interest = analysis.smoothed_distances[:, epoch_of_interest]\n\n# get the change magnitude between end and start of object for each location\nmagnitudes_of_interest = analysis.smoothed_distances[:, epoch_of_interest] - analysis.smoothed_distances[:, int(objects[sel_seed_idx].start_epoch)]\n\n# set the colormap according to magnitude at each location in the object\ncrange = 1.5\nimport matplotlib.colors as mcolors\ncmap = plt.get_cmap('seismic_r').copy()\nnorm = mcolors.CenteredNorm(halfrange=crange)\ncmapvals = norm(magnitudes_of_interest)\n\n# plot the timeseries of the segmented locations (colored by time series similarity)\nfor idx in idxs[::10]:\n    ax1.plot(timestamps[1:], analysis.smoothed_distances[idx], c=cmap(cmapvals[idx]), linewidth=0.5)\n\n# plot the seed time series\nax1.plot(timestamps[1:], analysis.smoothed_distances[cp_idx_sel], c='black', linewidth=1., label='Seed timeseries')\n\n# fill the area of the object\nax1.axvspan(timestamps[objects[sel_seed_idx].start_epoch], timestamps[objects[sel_seed_idx].end_epoch], alpha=0.3, color='grey', label='4D-OBC timespan')\n\n# add legend and format the date labels\ndtFmt = mdates.DateFormatter('%b-%d')\nplt.gca().xaxis.set_major_formatter(dtFmt)\nax1.legend()\n\n# get subset of core points incorporated in 4D-OBC\ncloud = analysis.corepoints.cloud\nsubset_cloud = cloud[idxs,:2]\n\n# plot coordinates colored by change values at end magnitude of object\nd = ax2.scatter(cloud[:,0], cloud[:,1], c = magnitudes_of_interest, cmap='seismic_r', vmin=-crange, vmax=crange, s=1)\nplt.colorbar(d, format=('%.2f'), label='Change magnitude [m]', ax=ax2)\nax2.set_aspect('equal')\n\n# plot convex hull of 4D-OBC\nfrom scipy.spatial import ConvexHull\nfrom matplotlib.patches import Polygon\nhull = ConvexHull(subset_cloud)\nax2.add_patch(Polygon(subset_cloud[hull.vertices,0:2], label = '4D-OBC hull', fill = False))\n\n# plot seed location of 4D-OBC\nax2.scatter(cloud[cp_idx_sel,0], cloud[cp_idx_sel,1], marker = '*', c = 'black', label = 'Seed')\n\n# add plot elements\nax1.set_title('Time series of segmented 4D-OBC locations')\nax1.set_xlabel('Date')\nax1.set_ylabel('Distance [m]')\nax2.set_title(f'Magnitudes of change in the 4D-OBC timespan\\n({timestamps[epoch_of_interest]-timestamps[analysis.objects[sel_seed_idx].start_epoch]} hours)')\nax2.set_xlabel('X [m]')\nax2.set_ylabel('Y [m]')\nax2.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axs = plt.subplots(1,2, figsize=(15,7)) ax1,ax2 = axs  # get indices of 4D-OBC sel_seed_idx = 0 idxs = objects[sel_seed_idx].indices  # get change values at end of object for each location epoch_of_interest = int((objects[sel_seed_idx].end_epoch - objects[sel_seed_idx].start_epoch)/2 + objects[sel_seed_idx].start_epoch) distances_of_interest = analysis.smoothed_distances[:, epoch_of_interest]  # get the change magnitude between end and start of object for each location magnitudes_of_interest = analysis.smoothed_distances[:, epoch_of_interest] - analysis.smoothed_distances[:, int(objects[sel_seed_idx].start_epoch)]  # set the colormap according to magnitude at each location in the object crange = 1.5 import matplotlib.colors as mcolors cmap = plt.get_cmap('seismic_r').copy() norm = mcolors.CenteredNorm(halfrange=crange) cmapvals = norm(magnitudes_of_interest)  # plot the timeseries of the segmented locations (colored by time series similarity) for idx in idxs[::10]:     ax1.plot(timestamps[1:], analysis.smoothed_distances[idx], c=cmap(cmapvals[idx]), linewidth=0.5)  # plot the seed time series ax1.plot(timestamps[1:], analysis.smoothed_distances[cp_idx_sel], c='black', linewidth=1., label='Seed timeseries')  # fill the area of the object ax1.axvspan(timestamps[objects[sel_seed_idx].start_epoch], timestamps[objects[sel_seed_idx].end_epoch], alpha=0.3, color='grey', label='4D-OBC timespan')  # add legend and format the date labels dtFmt = mdates.DateFormatter('%b-%d') plt.gca().xaxis.set_major_formatter(dtFmt) ax1.legend()  # get subset of core points incorporated in 4D-OBC cloud = analysis.corepoints.cloud subset_cloud = cloud[idxs,:2]  # plot coordinates colored by change values at end magnitude of object d = ax2.scatter(cloud[:,0], cloud[:,1], c = magnitudes_of_interest, cmap='seismic_r', vmin=-crange, vmax=crange, s=1) plt.colorbar(d, format=('%.2f'), label='Change magnitude [m]', ax=ax2) ax2.set_aspect('equal')  # plot convex hull of 4D-OBC from scipy.spatial import ConvexHull from matplotlib.patches import Polygon hull = ConvexHull(subset_cloud) ax2.add_patch(Polygon(subset_cloud[hull.vertices,0:2], label = '4D-OBC hull', fill = False))  # plot seed location of 4D-OBC ax2.scatter(cloud[cp_idx_sel,0], cloud[cp_idx_sel,1], marker = '*', c = 'black', label = 'Seed')  # add plot elements ax1.set_title('Time series of segmented 4D-OBC locations') ax1.set_xlabel('Date') ax1.set_ylabel('Distance [m]') ax2.set_title(f'Magnitudes of change in the 4D-OBC timespan\\n({timestamps[epoch_of_interest]-timestamps[analysis.objects[sel_seed_idx].start_epoch]} hours)') ax2.set_xlabel('X [m]') ax2.set_ylabel('Y [m]') ax2.legend(loc='upper right')  plt.tight_layout() plt.show() <p>This sandbar object is only one example of a surface activity on the beach. There may be more occurrences of sand bars at another location along the beach, and also in the same area during another time span. The automatic spatiotemporal segmentation can be applied to the full dataset to extract all occurrences. The full analysis of the 4D point clouds in Kijkduin (hourly data for six months) is performed in Anders et al. (2021). As a result, 2,021 4D objects-by-change were identified, all representing a surface activity of temporary deposition or erosion.</p> <p>This analysis can be used to assess the occurrence of specific processes. If we have knowledge about the expected properties, such as a sand bar, the objects can be filtered for these and analyzed with respect to their spatiotemporal characteristics, as well as links to external drivers (e.g., tidal information or storms).</p> <p>The full results of 4D objects-by-change of Kijkduin beach was used by Hulskemper et al. (2021) in an unsupervised clustering to automatically assess groups of surface activities. For this, self-organizing maps (SOMs; Kohonen, 1990) are used, a method that can preserve topological order to groups (unlike k-means, or other methods, where the clusters are unrelated). In the resulting SOM, we can therefore also obtain information on neighboring clusters in feature space. This is an advantage for our use case, as the border between objects - i.e. the characteristics of surface processes - are not sharp, and we may be interested in variations of similar (neighboring) clusters. Look into the paper by Hulskemper et al. (2021) to learn more about the method with specific application to 4D objects-by-change of the sandy beach.</p> <p>Here, we will look into the results of their analysis, where different spatial and temporal features were used to obtain groups of similar 4D objects-by-change - among them the spatial extent, the cross-shore position on the beach (a case study-specific feature), magnitude, and duration of change.</p> <p> A) Visualization of a self-organizing map (SOM) for erosion objects. Each plot represents a SOM node with one group of surface activities. The x-axis of each plot represents the mean duration of the 4D objects-by-change (4D-OBCs) in the node. The y-axis of each plot represents the mean height change of the 4D-OBCs in the node. The background colors represent the mean segment size per node (on a logarithmic scale). B) Spatial extent of the 4D-OBCs found in the nodes inside the blue polygon in A. C) Seed time series and timing of the 4D-OBCs in B. Figure by Hulskemper et al. (2022) / CC BY 4.0.</p> <p>The SOM renders the grouping of surface activities visually interpretable. For example, we can see a clear sorting of objects according to their duration, and partly also for the spatial extent. Differences between nodes (i.e. groups) are fairly gradual and not distinct in the visual representation. There are a also a few nodes that seem more exceptional from their neighbors (E(h,c), E(h,d), and E(a,h)), with fairly instantaneous or step-like surface actcivities with a large magnitude (down to -1.5 m). The 4D objects-by-change represented by E(a,h) are distinguishable from the others by their shorter duration.</p> <p>The nodes that are marked by the blue rectangle (subfigure A) are identified as activities related to the intertidal zone of the beach, i.e. erosion of sand in proximity to sand bar formation. Their location is visualized in the map (subfigure B). The time series of these objects (subfigure C) show that they all have similar change behaviour during their existence, even if the timing and duration may be different.</p> <p>In this way, the SOM approach allows identifying groups of surface activities with similar spatiotemporal characteristics, without needing to define them beforehand. Using this, it is also possible to look for a certain type of surface activity, and deriving other occurrences of the same behavior - ultimately relating to occurrences of the same underlying process. For further assessment of beach dynamics, it is possible to relate the occurrence of a type of surface activity to external drivers. This could be, for example, a relation to strong wind events or high tides using respective records from nearby stations. The time series analysis of high-resolution topographic data at the beach thereby holds potential to strongly further our understanding of surface processes and their driving mechanisms. For the application, this provides a powerful tool in the planning of sustainable measures to maintain the coast and mitigating ongoing erosion.</p> <p></p> <p>Analysis of beach width using terrestrial laser scanning point clouds by deriving the shoreline from the border between surface representation and no data (area covered by water). Near-continuous acquisition provides insight into the temporal variability of beach width within tidal cycles and throughout longer-term monitoring. Beach width is marked with orange points where the waterline matches the high and low tides according to records of the Dutch Ministry of Infrastructure and Water Management Rijkswaterstaat. Figures by Kuschnerus et al. (2022) / CC BY-NC-SA 4.0.</p> <p>According to their results (Kuschnerus et al., 2023), beach width varies between 150 m and 250 m, and the observation fits the official records of high and low tides in their timing in many (but not all) cases (around 60 %). This is owed to the situation that missing data at the shoreline can be caused by several factors, not only water cover from the sea. For example, during storm and precipitation, the range of the laser scanner is impeded and causes the effect of missing data towards the sea - which misinterprets the shoreline in the presented approach. Vice versa, reflections from turbulent waves may lead to measurements from the water surface (artifacts in the topographic representation). The beach width will be overestimated in such cases.</p> <p>Apart from possible improvements in the analysis, the assessment of beach width at the local scale and with the high temporal resolution of hourly laser scanning provides a valuable, new scale of monitoring the coastal state.</p> <p>Your turn: You can use a similar approach to assess the beach width for our Kijkduin use case. How? Visually determine the cross-shore location of the dune foot (X coordinate). Then for each epoch in the <code>SpatiotemporalAnalysis</code> object, determine the points furthest seaward which have at least n neighbors (try n=10, following Kuschnerus et al. (2023)). You know this spatial neighborhood search from theme 2 of this module. To assess variability, you can compare the beach width for several hours within a day or week. To assess a longer-term evolution, compare beach width, e.g., at the beginning and end of the available time series (here six weeks). Do you see a similar result as for Noordwijk? How does the beach width behave in Kijkduin?</p> <p></p> <p></p> <p></p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#case-study-time-series-based-change-analysis-of-sandy-beach-dynamics","title":"Case study: Time series-based change analysis of sandy beach dynamics\u00b6","text":"<p>Observation of beach dynamics using near-continuous laser scanning provides insight on surface processes that occur on a large range of spatiotemporal scales. Sandy beaches are highly complex in their morphodynamics, and constantly changing their shape due to sand transport through wind and wave action. Further, use of beaches through humans (for leisure) and measures against, e.g., coastal erosion, lead to anthropogenic modifications, which overlap and interact with natural processes.</p> <p>In this case study, an hourly time series of point clouds acquired by permanent terrestrial laser scanning (TLS) is used to assess surface dynamics on a sandy beach.</p> <p>The objective is to learn about</p> <ul> <li>Getting started with the data (interactive session)</li> <li>Environmental influences on the 3D time series and information extraction</li> <li>Volume change observation depending on acquisition frequency</li> <li>Analysis of coastal dynamics with time series-based change analysis</li> <li>Variation of beach width (Noordwijk)</li> <li>Spatiotemporal variation of surface moisture on the beach (Ostend-Mariakerke)</li> <li>Correlation with SAR (Kijkduin)</li> </ul> <p>Parts of the topics will be demonstrated interactively (in the Jupyter notebook), parts are theoretical based on published research that relates to the case study.</p> <p>After finishing this theme you will be able to:</p> <ul> <li>understand the value of 4D point clouds for observing surface dynamics of different phenomena in coastal environments</li> <li>apply 4D analysis methods for relevant topics in coastal monitoring</li> <li>relate analysis approaches of 4D point clouds to different aspects of understanding human-environment interaction</li> </ul>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#study-area-and-data","title":"Study area and data\u00b6","text":"<p>Before you continue, get familiar with the use case: Permanent laser scanning observation of the sandy beach at Kijkduin (The Netherlands).</p> <p>... and read the key paper accompanying the use case and case study:</p> <p>Vos, S., Anders, K., Kuschnerus, M., Lindenbergh, R., H\u00f6fle, B., Aarninkhof, S., &amp; de Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands. Scientific Data, 9 (1), pp. 191. doi: 10.1038/s41597-022-01291-9.</p> <p>In the case study, we will use hourly point clouds of a timespan of around six weeks (to reduce the required computing capacity). Therefore, use the dataset <code>kijkduin_hourly</code> from the course data repository.</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#getting-started-with-the-data-interactive-session","title":"Getting started with the data (interactive session)\u00b6","text":"<p>In the first step, we will set up the Python environment and load the data for the analysis steps shown interactively throughout this case study. If you are following the session hands-on (highly recommended), you need to adapt the path to the data and continue using the <code>etrainee</code> Python environment of the course. For explanations on the procedure to read in the py4dgeo <code>SpatiotemporalAnalysis</code> object, look into theme 4 of this module.</p> <p>Note: Reading in the data requires around 4 GB of RAM. If you need to reduce the required working memory, you may shorten the time series further by not using some epochs at the end. You can also temporally subsample the data by leaving out every n-th epoch. Be aware that this will yield different results for the analysis than demonstrated throughout the case study.</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#environmental-influences-on-the-3d-time-series-and-information-extraction","title":"Environmental influences on the 3D time series and information extraction\u00b6","text":"<p>As you learnt in the theme about time series-based analysis, environmental influences on LiDAR observation and change analysis become especially notable when acquiring data at high frequency - even when the permanent TLS measurement setup is highly stable. For the analysis of hourly data at the beach, this needs to be accounted for, as such systematic deviations might lead to misinterpretation of the signal as actual surface changes (which of course may also correlate with environmental influences as driving forces).</p> <p>Kuschnerus et al. (2021) have investigated the causes of measurement deviations in high-frequency TLS at a sandy beach with an extensive measurement setup (in Noordwijk, another study site of the CoastScan project). They used internal TLS and external inclination sensors on the measurement frame to assess potential tilting of the instrument and mounting. Further, GNSS receivers were installed in the scene to compare the locally recorded position (30-sec intervals) with the hourly TLS measurements. This comparison showed that indeed a small trend of movemement in the instrument mounting took place and lead to increasing decrease of the measurement height in the TLS time series. Such observations confirm that fine alignment via rigid body transformation is needed even for near-continuous TLS from a fixed position.</p> <p>Further, with the exposed environment at the coast, strong winds and storm events take considerable influence on measurements. By assessing plane fit residuals in stable areas (here, a helicopter landing pad in the scene), Kuschnerus et al. (2021) were able to determine a shaking of the sensor during storm events, leading to a striped pattern of measurement deviations from the planar surface (following the acquisition pattern of vertical lines of the instrument) up to 1.4 cm. This means that actual surface changes - sand being transported by strong wind - can only be assessed considering this larger uncertainty margin.</p> <p>Using stable surfaces or distinct objects in the scene has shown to be a suitable way to assess measurement deviations, uncovering external influences on the seemingly stable observation setup. In practice, however, possibilities are limited by the scene geometry. For coastal settings, this typically means that the area of interest (the beach) are situated at the furthest range, and stable surfaces (e.g., paved areas, surrounding buildings) can only be found at closer range to the sensor (Anders et al., 2019; Vos et al., 2022).</p> <p>At Kijkduin, the measurement variation can be assessed in the paved area above the beach, following Anders et al. (2019). For this, we select a location of interest (manually determined in CloudCompare; X=-61.6 / Y=-69.5 / core point 205840 is suitable) and visualize the time series. To reduce random measurement noise, we average the data in a small temporal window of six hours.</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#volume-change-observation-depending-on-acquisition-frequency","title":"Volume change observation depending on acquisition frequency\u00b6","text":"<p>From coastal dynamics, we know that morphological changes on a sandy beach are highly complex and take place at strongly varying magnitudes and rates. Having near-continuous 3D point clouds available, we can now look into the effect of temporal scales on our observation of surface dynamics. If sand is transported constantly to a local area and away from it, a reduced temporal acquisition interval implies that we are missing changes in our observation.</p> <p>In our analysis object, we quantified the change (point cloud distances) for each epoch to the first epoch as global reference. So the change value at each epoch tell us the volume budget if only two acquisitions had been performed, at at the respective timespan from the epoch to the first epoch. The last epoch in our time series therefore correponds to an observation interval of six weeks.</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#analysis-of-coastal-dynamics-with-time-series-based-change-analysis","title":"Analysis of coastal dynamics with time series-based change analysis\u00b6","text":"<p>As you know from previous themes, the dense temporal data provided by 4D point clouds holds potential for new insights on surface processes. Special methods are being developed to leverage the time series information. Due to the complex surface dynamics induced by variable sand transport, sandy beaches have been an important use case for such method developments. In the following, we will look into the assessment of coastal change patterns via time series clustering, and the characterization of surface activities with 4D objects-by-change and self-organizing maps (SOMs).</p> <p></p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#coastal-change-patterns","title":"Coastal change patterns\u00b6","text":"<p>In the data exploration and analysis so far, we could already observe different processes taking place on the beach. Now, we are going to look into characteristic change patterns that can be observed on Kijkduin beach using time series clustering following Kuschnerus et al. (2021). The approach is explained in a previous theme.</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#characterization-of-morphological-surface-activities","title":"Characterization of morphological surface activities\u00b6","text":"<p>For the identification of individual surface activities in near-continuous 3D time series, you have learnt about the [extraction of 4D objects-by-change in a previous theme], following Anders et al. (2021). Since the method is computationally expensive (long processing time), we will not interactively run the full analysis. Just as a reminder of the process, let's extract an example object at a selected location on the beach:</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#variation-of-beach-width-noordwijk","title":"Variation of beach width (Noordwijk)\u00b6","text":"<p>Zooming out again from the individual surface activities induced by different processes, a relevant variable to observe at the coast is the variation of beach width. Beach width is defined as a coastal state indicator (Van Koningsveld et al., 2005). To derive it from high-frequency laser scanning time series, Kuschnerus et al. (2023) have leveraged a property of LiDAR acquisitions: there is no surface representation (i.e. measurements), where the beach is covered by water. This area varies of course with the tides, so it is especially interesting to consider the temporal variability but also long-term evolution of beach width from the near-continuous, permanent laser scanning acquisition. The presented study was conducted at Noordwijk beach, another site of the CoastScan project.</p> <p>For the analysis, the authors grid the point clouds to digital elevation models (DEMs) and determine how far seawards the beach surface is covered by data. This provides an assessment of the shoreline. Beach width is then derived as the distance from this shoreline to the foot of the dunes. Due to the special setting of permanent laser scanning, and the perpendicular orientation of the beach scene to the scanner origin, the dune foot line along the shore can be determined as a fixed X coordinate in the local coordinate system:</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#spatiotemporal-variation-of-surface-moisture-on-the-beach","title":"Spatiotemporal variation of surface moisture on the beach\u00b6","text":"<p>So far, we have mainly made use of the topographic, i.e. geometric, information provided with our 4D laser scanning point clouds. Thinking back of theme 1, there are other LiDAR attributes we can make use of for analysis - notably the backscatter information. For coastal monitoring, the laser return intensity has been used by Jin et al. (2021) to assess variation in surface moisture. With all other factors (e.g. range and incidence angle) constant, the variability of moisture of the target surface is directly expressed in the intensity of the returned signal with acquisitions repeated from a fixed position. Of course the other factors are not perfectly constant, and especially variable across the scene. The authors have accounted for this by deriving a model for the derivation of surface moisture from the intensity signal (look into Jin et al. (2021)).</p> <p>The results of this assessment show that surface moisture on the beach is linked to the topography, with higher parts of the beach being dryer (at the surface). This can be related to retreat of the tides, where upper parts have more time for drying through evaporation that areas closer to the shoreline. It is also possible to trace the transition from shallow water-covered areas to water-free parts on the beach through their moisture signal.</p> <p>With the available 4D point clouds (here of a beach in Belgium, the assessment has not been performed for Kijkduin), it is now possible to observe surface moisture dynamics at high spatial and temporal resolution. This provides a quantified assessment of tidal fluctuation of surface conditions on the beach. At this observation scale, it was further possible to reveal a relation of surface moisture variation to anthropogenic disturbance, such as tire tracks of vehicles (Jin et al. 2021). This kind of information can be used for completely new assessments of the human-environment interaction in a beach environment. Further, the spatially extensive and temporally dense information on surface moisture can help to improve models on aeolian sand transport, which can only occur under certain surface conditions (which were difficult to measure at the appropriate spatiotemporal scale with other survey strategies).</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#correlation-with-sar","title":"Correlation with SAR\u00b6","text":"<p>As a last aspect of this case study - and also this module - we take a look at the bridge from the close spatial scale of our 4D point clouds to larger-scale observations, as are available from satellite remote sensing. Permanent laser scanning at Noordwijk (a CoastScan site) has provided a pioneer case study to look into a combination with space-borne Synthetic Aperture Radar (SAR) images, presented by Di Biase et al. (2022). SAR provides an active remote sensing method with special advantages for assessing ground surface characteristics. In their study, Di Biase et al. (2022) specifically assess surface roughness in its temporal variability, in relation to wind speed and direction influencing the backscatter behavior. Intensity of the signal was then used as variable to assess the correlation between SAR and near-continuous TLS.</p> <p>The presented study area of 4D point clouds is covered by 12 pixels of Sentinel-1 data obtained from Google Earth Engine in the same period as the permanent laser scanning acquisitions. Therefore, the 4D point clouds are spatially aggregated to the information contained within each of these pixels. Results show a positive correlation between the 4D point cloud data and SAR signal under low wind speeds, though correlation is overall not too high (up to 0.5). Under strong wind conditions (&gt;8 m/s), irregular and low correlations (positive and negative) are derived. This can be related to drier sand under strong winds, which increases the laser return intensity of the dry and hence highly reflective surface (Di Biase et al., 2022).</p> <p>Although presented as preliminary study, the study shows the potential of directing the observation of a complex system to a combination of different spatiotemporal scales - as is required by the broad range of spatiotemporal scales on which processes of sand transport occur. In this context, repeat SAR aquisitions provide a promising Earth observation product to relate local observations at sites like Kijkduin and Noordwijk to larger regions and entire coastlines.</p> <p>As participants of this course, you are becoming experts on a broad set of Earth observation strategies (data and methods). So you may use your gained combined knowledge develop new ideas for investigating complex natural systems and human-environment interaction through time series analysis!</p>"},{"location":"module3/06_casestudy_sandybeach/06_casestudy_sandybeach.html#references","title":"References\u00b6","text":"<ul> <li>Anders, K., Lindenbergh, R. C., Vos, S. E., Mara, H., de Vries, S., &amp; H\u00f6fle, B. (2019). High-Frequency 3D Geomorphic Observation Using Hourly Terrestrial Laser Scanning Data Of A Sandy Beach. ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-2/W5, pp. 317-324. doi: 10.5194/isprs-annals-IV-2-W5-317-2019.</li> <li>Anders, K., Winiwarter, L., Mara, H., Lindenbergh, R., Vos, S. E. &amp; H\u00f6fle, B. (2021). Fully automatic spatiotemporal segmentation of 3D LiDAR time series for the extraction of natural surface changes. ISPRS Journal of Photogrammetry and Remote Sensing, 173, pp. 297-308. doi: 10.1016/j.isprsjprs.2021.01.015.</li> <li>Di Biase, V., Kuschnerus, M. &amp; Lindenbergh, R. (2022). Permanent Laser Scanner and Synthetic Aperture Radar data: correlation characterization at a sandy beach, Sensors, 22(6), 2311 doi: 10.3390/s22062311.</li> <li>Friedli, E., Presl, R., &amp; Wieser, A. (2019). Influence of atmospheric refraction on terrestrial laser scanning at long range. 4th Joint International Symposium on Deformation Monitoring: JISDM, pp. 6.</li> <li>Hulskemper, D., Anders, K., Antol\u00ednez, J.A.\u00c1., Kuschnerus, M., H\u00f6fle, B. &amp; Lindenbergh, R. (2022). Characterization of morphological surface activities derived from near-continuous terrestrial LiDAR time series. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLVIII-2/W2-2022, pp. 53\u201360. doi: 10.5194/isprs-archives-XLVIII-2-W2-2022-53-2022.</li> <li>Jin, J., Verbeurgt, J., De Sloover, L., Stal, C., Deruyter, G., Montreuil, A.-L., Vos, S., De Maeyer, P. &amp; De Wulf, A. (2021). Monitoring spatiotemporal variation in beach surface moisture using a long-range terrestrial laser scanner. ISPRS Journal of Photogrammetry and Remote Sensing, 173, pp. 195-208. doi: 10.1016/j.isprsjprs.2021.01.011.</li> <li>Kohonen, T. (1990): The Self-Organizing Map. Proceedings of the IEEE, 78(9), pp. 1464\u20131480.</li> <li>Kuschnerus, M., Schr\u00f6der, D. &amp; Lindenbergh, R. (2021). Environmental influences on the stability of a permanently installed laser scanner. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLIII-B2-2021, pp. 745\u2013752. doi: 10.5194/isprs-archives-XLIII-B2-2021-745-2021.</li> <li>Kuschnerus, M., Lindenbergh, R. &amp; Vos, S. (2021). Coastal Change Patterns from Time Series Clustering of Permanent Laser Scan Data. Earth Surface Dynamics, 9, pp. 89\u2013103. doi: 10.5194/esurf-9-89-2021.</li> <li>Kuschnerus, M., Lindenbergh, R., Lodder, Q., Brand, E. &amp; Vos, S. (2022). Detecting anthropogenic volume changes in cross sections of a sandy beach with permanent laser scanning. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLIII-B2-2022, pp. 1055\u20131061. doi: 10.5194/isprs-archives-XLIII-B2-2022-1055-2022.</li> <li>Kuschnerus, M., Lindenbergh, R. &amp; de Vries, S. (2022): Assessing sandy beach width variations on intertidal time scales using permanent laser scanning. Proceedings of 5th Joint International Symposium on Deformation Monitoring (JISDM), Valencia. doi: 10.4995/JISDM2022.2022.13729.</li> <li>Van Koningsveld, M., Davidson, M.A., &amp; Huntley, D.A. (2005). Matching Science with Coastal Management Needs: The Search for Appropriate Coastal State Indicators. Journal of Coastal Research, pp. 399\u2013411.</li> <li>Vos, S., Anders, K., Kuschnerus, M., Lindenbergh, R., H\u00f6fle, B., Aarninkhof, S., &amp; de Vries, S. (2022). A high-resolution 4D terrestrial laser scan dataset of the Kijkduin beach-dune system, The Netherlands. Scientific Data, 9 (1), pp. 191. doi: 10.1038/s41597-022-01291-9.</li> </ul>"},{"location":"module4/module4.html","title":"Airborne Imaging Spectroscopy Time Series Analysis","text":"<p>Imaging spectroscopy enables detailed studies of land surface based on its optical properties acquired in many, usually a few hundred, narrow spectral bands. Resulting hyperspectral imagery collected over a selected area and time span can be used, e.g., for studying the responses of vegetation to climate change or other environmental disturbances caused by human activities. In situ and laboratory spectroscopy provide point-based measurements of selected samples, and they are useful for the derivation and assessment of theoretical concepts and for calibration and validation measurements from airborne or satellite sensors. In this module, you will learn about:</p> <ul> <li>retrieval of land surface properties from spectroscopic measurements</li> <li>relating optical properties of vegetation/leaves to their biophysical and biochemical parameters</li> <li>acquisition of airborne and RPAS hyperspectral imagery and its radiometric and geometric pre-processing</li> <li>machine learning classification applied to hyperspectral imagery</li> <li>importance of spectral, spatial, and temporal resolution for vegetation monitoring</li> <li>in situ and laboratory measurements with a spectroradiometer</li> <li>upscaling spectroscopic measurements (from in situ to RPAS/airborne and satellite)</li> <li>processing of image and laboratory spectroscopic datasets in case studies over the arctic-alpine tundra in the Krkono\u0161e Mts. and the Norway Spruce forest in the Ore Mts., Czechia</li> </ul>"},{"location":"module4/module4.html#structure","title":"Structure","text":"<p>The module is structured into the following themes:</p> <ul> <li>Principles of imaging and laboratory spectroscopy</li> <li>Airborne hyperspectral data acquisition and pre-processing</li> <li>In situ and laboratory spectroscopy of vegetation</li> <li>Machine learning in imaging spectroscopy</li> <li>Temporal vs. spatial and spectral resolution</li> <li>Case study: Seasonal spectral separability of selected grass species of the Krkono\u0161e Mts. tundra ecosystem</li> <li>Case study: Discrimination of selected grass species from time series of RPAS hyperspectral imagery</li> <li>Case study: Seasonal dynamics of flood-plain forests</li> </ul>"},{"location":"module4/module4.html#prerequisites-to-perform-this-module","title":"Prerequisites to perform this module","text":"<p>The following skills and background knowledge are required for this module.</p> <ul> <li>Basics of statistics</li> <li>Basics of geoinformation systems and handling raster/vector data</li> <li>Principles of remote sensing</li> <li>Basic programming skills (Python, R)</li> </ul> <p>Follow this link for an overview of the listed prerequisites and recommendations on external material for preparation.</p>"},{"location":"module4/module4.html#software","title":"Software","text":"<p>For this module, you will need the software listed below. Follow the links to the individual software or tools, for help in setting them up.</p> <ul> <li>QGIS + EnMAP-Box</li> <li>R language</li> </ul>"},{"location":"module4/module4.html#use-cases-and-data","title":"Use Cases and Data","text":"<p>In the research-oriented case studies, this module uses the sataset Tundra vegetation monitoring in Krkono\u0161e Mountains.</p>"},{"location":"module4/module4.html#start-the-module","title":"Start the module","text":"<p>... by proceeding to the first theme on Principles of imaging and laboratory spectroscopy.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html","title":"Principles of imaging and laboratory spectroscopy","text":"<p>Imaging and laboratory spectroscopy is a dynamic discipline that allows monitoring the properties of the Earth's surface (and the surfaces of other bodies in the Solar System) in high detail. It has a wide range of applications e.g., in geology and paedology (e.g., rock and soil composition), plant biology and ecology (e.g., physiological status and stress of vegetation), hydrology, or waste management.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#objectives","title":"Objectives","text":"<p>In this theme, you will learn about</p> <ul> <li> <p>fundamentals of imaging and laboratory spectroscopy,</p> </li> <li> <p>hyperspectral (HS) cameras and spectroradiometers,</p> </li> <li> <p>interpretation of a hyperspectral data cube,</p> </li> <li> <p>spectral libraries,</p> </li> <li> <p>examples of imaging and laboratory spectroscopy applications in environmental monitoring.</p> </li> </ul> <p>The theme ends with a self-evaluation quiz, an exercise, and a list of references.</p> <p>In the practical exercise you will explore spectra of selected land cover classes collected with an RPAS hyperspectral camera in the EnMapBox software.</p> <p>After finishing this theme, you will be able to assess advantages and drawbacks of using hyperspectral imaging in comparison to multispectral observations and how field and laboratory spectroscopy supports the aerial/RPAS campaigns.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#imaging-and-laboratory-spectroscopy","title":"Imaging and laboratory spectroscopy","text":"<p>Spectroscopy is the study of forming and looking at spectra using spectrometers, spectroscopes, etc. (Oxford Learner\u2019s Dictionaries, 2023). It has variety of applications in chemistry and physics. In the scope of our course, the focus is on spectra of light reflected from matters forming Earth\u2019s surface or optionally spectra of light transmitted through a thin layer of matter. It is important to remember that both reflectance and transmittance depend on the matter properties and are the function of wavelengths. It worth mentioning that the origin of spectroscopy goes back to the turn of the 17th and 18th century when Isac Newton investigated refraction of light and introduced the word \u201cspectrum\u201d to describe the split of white light into component colours on prism (Wikipedia, 2023). </p> <p>An optical spectrometer is an instrument used for measurement of properties of light over a defined range of wavelengths. Figure 1 shows a schema of a spectrometer. The incoming electromagnetic radiation is dispersed on a diffraction grating and collected on a detector. The spectrometer is characterised with a spectral range (min/max wavelength), spectral bandwidth, spectral sampling, and signal-to-noise ratio (SNR). Due to narrow spectral ranges collected with a detector, SNR is among parameters that should not be neglected when choosing the instrument. </p> <p> </p> <p>Figure 1. Schema of a spectrometer. Figure by Jooja/Wikipedia, CC BY-SA 4.0</p> <p>In general, spectroradiometers measure the radiance in multiple spectral bands (Liang and Wang, 2019). While multispectral spectroradiometers introduced in the Theme 1 of Module 2 of this course collect the measurements over several rather broad, and not continuous spectral bands, hyperspectral spectroradiometers divide the observed part of the spectrum into many (40 - &gt; 200) narrow (5 \u2013 20 nm) adjacent bands with fine spectral resolution. </p> <p>Whiskbroom or pushbroom scanners are used in airborne (i.e., aerial or remotely piloted aircraft systems \u2013 RPAS) or spaceborne hyperspectral imaging. The incident light passes through the optical system and a slit, is dispersed and an array of CCD detectors collect radiance in respective spectral bands (Figure 2). The CCD detectors are fabricated from different materials depending on their spectral sensitivity (e.g., silicon detectors for the visible range, indium gallium arsenide for the NIR, and indium-antimonide detectors for the SWIR parts of the spectrum in the AVIRIS sensor, AVIRIS 2023).</p> <p> </p> <p>Figure 2. Scheme of data acquisition and processing output from the Airborne Visible InfraRed Imaging Spectrometer - Next Generation. AVIRIS-NG. Figure by JPL/Courtesy NASA/JPL-Caltech/Terms of Use</p> <p>The raw spectral measurements have to be radiometrically corrected (see Theme 2 of this Module). A continuous reflectance spectral curve of each pixel within the instrument\u2019s spectral range is obtained and can be used to identify the surface materials and their properties. The whole set of overlaying geometrically coregistered images acquired in continuous spectral bands is called a hyperspectral image cube. It can be used for visualisation (combination of 3 selected spectral bands to create a true/false colour composite) and for depicting pixels which spectral curves are of interest.</p> <p>Laboratory and portable spectroradiometers are used for collecting reference an in-situ data. They provide point measurements (i.e., one spectral curve per measurement) using an optical cable connected to a pistol grip, contact probe, or an integrating sphere (Figure 3). While the first two devices are designed for measurement of reflectance, the integrating sphere can be used for measurement of both reflectance and transmittance of a sample mounted on its holder. Some of so-called leaf clips connected to the spectroradiometer can be used for in situ measurements of both reflectance and transmittance of leaves. </p> <p> </p> <p>Figure 3. Spectroradiometer ADS FieldSpec 4 Hi-Res connected to a contact probe (a), a pistol grip during filed measurements (b), and to a integrating sphere (c).</p> <p>Similar to hyperspectral images, the spectra collected with the laboratory spectroradiometers have to be calibrated to obtain the reflectance of the matter comparable in time and with other measurements. Laboratory spectroscopy is a topic of the Theme 3 of this Module.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#spectral-libraries","title":"Spectral libraries","text":"<p>Some research institutions provide laboratory or imaging measurements of reflectance spectra in a form of spectral libraries. They can be used</p> <ul> <li> <p>as a reference for other measurements with imaging or laboratory spectroradiometers (e.g., when new instruments are tested and calibrated),</p> </li> <li> <p>for deriving spectral composition of materials measured with spectroradiometers,</p> </li> <li> <p>for studies on spectral properties of materials (i.e., absorption bands) for designing new instruments.</p> </li> </ul> <p>There are spectral libraries with open access, such as:</p> <p>USGS spectral library</p> <p>NASA JPL ECOSTRESS spectral library</p> <p>KLUM: Karlsruhe Library of Urban Materials</p> <p>TASK (OPTIONAL): Open the USGS spectral library, select and compare spectra representing soil, fresh and dry vegetation, water (clear, with higher mineral content, etc.). You can sort the items according to the category or use the search by key words.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#examples-of-hyperspectral-cameras-and-spectroradiometers","title":"Examples of hyperspectral cameras and spectroradiometers","text":"<p>There are numerous operational hyperspectral sensors, especially for airborne platform. They differ in their spectral and spatial resolution. Table 1 gives some examples of current and passed hyperspectral sensors carried by spaceborne and airborne platforms. They operate from visible (VIS) to near infrared (NIR) or short-wave infrared (SWIR) part of the spectrum. Notice an increase of spectral resolution from VIS/NIR to SWIR bands.</p> <p>Table 1. Selected HS sensors and platforms.</p> <p> </p> <p>Spectral range [nm]</p> <p>Spectral sampling interval [nm]</p> <p>GSD [m]</p> <p>Spaceborne</p> <p> </p> <p>EO-1/Hyperion</p> <p>357 \u2013 2 576</p> <p>10</p> <p>30</p> <p>Proba/CHRIS</p> <p>400 \u2013 1050</p> <p>11</p> <p>34</p> <p>PRISMA</p> <p>400 \u2013 1010 920 \u2013 2505</p> <p>9</p> <p>30</p> <p>EnMAP</p> <p>420 \u2013 1000  900 \u2013 2450</p> <p>6.5 10</p> <p>30</p> <p>Aerial</p> <p>AVIRIS-NG</p> <p>380 \u2013 2510</p> <p>5</p> <p>0.2 \u2013 10</p> <p> </p> <p>depending on the flying height and focal length</p> <p>APEX</p> <p>380.5 \u2013 971.7 941.2 \u2013 2501.5</p> <p>0.45 \u2013 7.5  5 \u2013 10</p> <p>CASI-1500</p> <p>380 \u2013 1050</p> <p>3.2</p> <p>SASI-600</p> <p>950 \u2013 2450</p> <p>15</p> <p>TASI-600</p> <p>8 000 \u2013 11 500</p> <p>110</p> <p>RPAS</p> <p>Nano-Hyperspec</p> <p>400 \u2013 1 000</p> <p>6</p> <p>0.02 \u2013 0.5</p> <p>FIREFLEYE 185</p> <p>450 - 950</p> <p>4</p> <p>In-situ/lab.</p> <p>ASD</p> <p>FieldSpec 4 HiRES</p> <p>350 \u2013 2500 1000 \u2013 2500</p> <p>3 8</p> <p>point measurement</p> <p> </p> <p>Spectral evolution PSR+</p> <p>350 \u2013 2500</p> <p>2.8nm@700nm 8nm@1500nm</p> <p>6nm@2100nm</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#exercise","title":"Exercise","text":"<p>To explore reflectance spectra acquired with a RPAS hyperspectral sensor using the EnMapBox software package, proceed to the exercise of this theme.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"Which characteristics are typical for a HS sensor:  a few, narrow, no adjacent bands. many, narrow, adjacent bands. many, wide, overlapping bands. many, narrow, adjacent bands.  There are three spectral measurements in the figure. Which of them were possibly acquired with a hyperspectral sensor?  a, b a, c b, c a, b  You have an oak leaf as a sample. Using laboratory spectroscopy, you can determine its  transmittance reflectance transmittance and reflectance transmittance and reflectance  Signal-to-noise ratio is a more critical issue for  hyperspectral sensors multispectral sensors hyperspectral sensors  Spectral libraries can be used for  finding spectral bands suitable for discrimination between selected materials. determination of mineral composition of rocks from spectrometric measurements. finding technical documentation about spectroradiometers. finding spectral bands suitable for discrimination between selected materials.&amp;determination of mineral composition of rocks from spectrometric measurements.  Match the spectra with the material (Source: Kokaly, R.F., Clark, R.N., Swayze, G.A., Livo, K.E., Hoefen, T.M., Pearson, N.C., Wise, R.A., Benzel, W.M., Lowers, H.A., Driscoll, R.L., and Klein, A.J., 2017, USGS Spectral Library Version 7: U.S. Geological Survey Data Series 1035, 61 p., [https://doi.org/10.3133/ds1035](https://doi.org/10.3133/ds1035))  A B C      1. English Oak leaf - fresh A B C      2. Red building brick A B C      3. English Oak leaf - dry B C A"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#next-unit","title":"Next unit","text":"<p>Proceed with Airborne hyperspectral data acquisition and pre-processing</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#references","title":"References","text":""},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#key-literature","title":"Key literature","text":"<p>Rast, M., Painter, T. H. (2019). Earth observation imaging spectroscopy for terrestrial systems: An overview of its history, techniques, and applications of its missions. Surveys in Geophysics, 40(3), 303-331. https://doi.org/10.1007/s10712-019-09517-z</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles.html#further-articles-referenced-literature-and-resources","title":"Further articles, referenced literature and resources","text":"<p>AVIRIS (2023). Airborne Visible InfraRed Imaging Spectrometer, NASA Jet Propulsion Laboratory. https://aviris.jpl.nasa.gov/aviris/index.html</p> <p>Kokaly, R.F., Clark, R.N., Swayze, G.A., Livo, K.E., Hoefen, T.M., Pearson, N.C., Wise, R.A., Benzel, W.M., Lowers, H.A., Driscoll, R.L., and Klein, A.J., 2017, USGS Spectral Library Version 7: U.S. Geological Survey Data Series 1035, 61 p., https://doi.org/10.3133/ds1035.</p> <p>Liang, S., Wang, J. (Eds.). (2019). Advanced remote sensing: terrestrial information extraction and applications. Academic Press.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html","title":"Exploration of hyperspectral data using EnMAP-Box","text":"<p>The aim of this exercise is to get acquainted with data in the EnMAP-Box environment, to understand the possibilities of visualization, the concept of hyperspectral data, and to compare spectral properties of selected vegetation classes.</p> <ul> <li>Prerequisites<ul> <li>Installed EnMAP-Box plugin for QGIS (manual)  </li> <li>Downloaded data (module4/theme1_exercise_hyperspectral_data)   </li> </ul> </li> <li>Tasks<ul> <li>First encounter with data  </li> <li>Comparison of spectral characteristics  </li> <li>Exploration of changes in spectral characteristics in time  </li> </ul> </li> </ul>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#1-first-encounter-with-data","title":"1. First encounter with data","text":"<p>Details about the origin of the data, equipment used for acquisition, and scanned areas can be found here.</p> <p>Multiple regions of interest (ROIs) have been selected from the exemplary area - Lu\u010dn\u00ed hora mountain (LH). In this exercise you will be working with four types of land cover:</p> <ul> <li>scree  </li> <li>mountain pine  </li> <li>grass  <ul> <li>Deschampsia cespitosa (tufted hairgrass)  </li> <li>Nardus stricta (matgrass)  </li> </ul> </li> <li>shrub  <ul> <li>Vaccinium myrtillus (european blueberry)  </li> <li>Calluna vulgaris (common heather)  </li> </ul> </li> </ul> <p> <p>Map of the area with numbered ROIs </p> <p>Names of files in the data folder mostly follow a pattern: <code>area_class_subclass_number_year_month</code>, e.g. <code>LH_wetland_2_2020_08</code> shows the image cutout of the second wetland ROI of Lu\u010dn\u00ed hora mountain captured in August 2020. The subclass tag is only used in the case of grasses and shrubs.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#load-and-visualize-image","title":"Load and visualize image","text":"<p>For the first task, select a ROI of your choice from the folder with classified images (<code>classified</code>). Do not forget to mention the name of the file in your report. Visualize it using the EnMAP-Box plugin for QGIS. You can follow the steps in the manual - Getting Started or work with the provided screenshots:</p> <ul> <li>Open EnMAP-Box in QGIS. </li> </ul> <p> </p> <ul> <li>Open an empty map window. </li> </ul> <p> </p> <ul> <li>Through the dialog in \"Add Data Source\" open the chosen image. Point at a file with the <code>dat</code> extension. </li> </ul> <p> </p> <ul> <li>Drag and drop the raster image from \"Data Sources\" to the open empty map window. </li> </ul> <p> </p> <ul> <li>Edit the \"Symbology\" of the image, located in \"Layer Properties\". The screenshot shows RGB mapping, which should correspond to true color visualization. Minimum and maximum values are dependent on the specific image. </li> </ul> <p> </p> <ul> <li>Due to a short range of wavelengths captured in each band and a significant amount of noise, the image visualization differs from multispectral imagery in true colors.  </li> </ul> <p> </p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#enmap-box-capabilities","title":"EnMAP-Box capabilities","text":"<p>Now you are ready to explore EnMAP-Box capabilities. Add screenshots of the following steps to your report.</p> <ul> <li>The images are only cutouts and cover a relatively small area. To inspect the surroundings, load some of the available WMS layers.  When compared to, for example, Google Satellite Maps, the provided images you are working with in this exercise are of much higher spatial resolution.</li> </ul> <p> <p>Add WMS layer </p> <ul> <li>Right-click on the raster layer in the map and choose \"Image Statistics\" or click on the same option in the menu \"Tools\". Select your image, and feel free to also choose the actual accuracy as the image is very small, and the computation of statistics will be fast.  Write down in your report the number of available bands, range of captured wavelengths, and enclose a few screenshots of the histograms. Similar information can be found in the \"Metadata Viewer\" in the \"Tools\" menu. </li> </ul> <p> <p>Image statistics </p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#spectral-curves","title":"Spectral curves","text":"<p>As the last step of the first task, let's visualize a wavelength spectrum of individual pixels.</p> <ul> <li>Click on \"Open a spectral library window\" next to the map window opening option, and activate the buttons with a cursor logo (\"Identify a cursor location and collect pixels values, spectral profiles and or vector feature attributes.\") and one with a spectral curve logo (\"Identify pixel profiles and show them in a Spectral Library.\").</li> </ul> <p> </p> <ul> <li>Then just click anywhere in the image, and the spectral curve should appear in the spectral library window. Spend time observing the curves of various pixels, play around with the settings for axis \"x\" (wavelength in nanometers, band index), and sampling in the spectral profile sources panel. </li> </ul> <p> </p> <ul> <li>It is possible to visualize more curves at once. Choose option \"Add profiles\" or \"Add profiles automatically\", and curves will remain in the plot. If you want to remove the curves, activate \"Toggle editing mode\" and delete selected rows in the table. In the editing mode, profiles can be renamed, others can be added, etc. </li> </ul> <p> <p>Add profiles </p> <ul> <li>It is also possible to change the colors of the plot, export raw data to CSV, or export the plot as a raster image.</li> </ul> <p>Enclose three screenshots of the whole EnMAP-Box window with different settings for spectral curve visualization in your report.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#2-comparison-of-spectral-characteristics","title":"2. Comparison of spectral characteristics","text":"<p>There are three examples for each class and subclass of land cover from August 2020 in the folder with classified images (<code>classified</code>). Open them one by one (or more at once) in multiple map windows in EnMAP-Box and visualize the spectral curves of particular pixels. It is recommended to use \"Sample5x5Mean\" sampling, due to its ability to reduce noise in the signal by averaging signals from neighbouring pixels.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#spectral-curves-for-classes-and-subclasses","title":"Spectral curves for classes and subclasses","text":"<p>Choose a representative curve for each class and subclass, enclose the plots to your report, and describe typical spectral behaviour of each class, highlighting the differences. Focus on the values of pixels in the wavelength ranges of the visible and near infrared parts of the spectrum, as well as on the shape of the curve. Try to answer the following questions:</p> <pre><code>Which part of the spectrum can be used for distinguishing between the classes?\n\nWhich wavelength ranges are similar for two or more classes and can cause insufficient results of classification?\n\nIs it possible to distinguish between subclasses? Which wavelength ranges might help you?\n</code></pre> <p> <p>Spectral curves for scree and mountain pine </p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#differences-in-spectral-curves-within-classes","title":"Differences in spectral curves within classes","text":"<p>During the previous task, you may have noticed differences in spectral curves between pixels of one class. Select suitable pixels and plot multiple different spectral curves of pixels from one class and image in a one graph. In your report, answer these questions:</p> <pre><code>Are these differences significant from the classification point of view?\n\nCan the differences cause misclassification of some of the pixels?\nIf so, what are some methods used for removing noise in the classification results?\n</code></pre>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#classification-of-additional-images","title":"Classification of additional images","text":"<p>There are six images in the folder with images of unknown class (<code>unknown_1-6</code>). Go through the images and try to assign them to one of the classes (and subclasses, if possible). State your arguments for the classification and include spectral curves that may have aided you.</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#3-exploration-of-changes-in-spectral-characteristics-in-time","title":"3. Exploration of changes in spectral characteristics in time","text":"<p>The folder for task 3 (<code>task_3</code>) contains one ROI of each class and its corresponding multitemporal images from June, July, August, and September 2020. Open images of one ROI in  all four time periods and explore differences in spectral curves. In your report, describe the differences for individual classes. Try to answer the following questions:</p> <pre><code>For which classes are the changes in spectral curves most significant?  \n\nIf the spectral curves differ, which month is the most suitable for the separation of the classes? \nIn which month are the spectral curves of one class most similar to the others?\n</code></pre> <p> <p>Mountain pine in June, July, August, and September </p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#exercise-solution","title":"Exercise solution","text":"<p>Proceed to example solution Exploration of hyperspectral data - report</p>"},{"location":"module4/01_spectroscopy_principles/01_spectroscopy_principles_exercise.html#back-to-theme","title":"Back to theme","text":"<p>Proceed by returning to Principles of imaging and laboratory spectroscopy</p>"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html","title":"E-TRAINEE: Exploration of hyperspectral data - report","text":""},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#report","title":"Report","text":""},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#1-first-encounter-with-data","title":"1. First encounter with data","text":""},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#load-and-visualize-image","title":"Load and visualize image","text":"<p>In the first part of the exercise, I've been working with an image of a mountain pine with the tag <code>LH_mountain_pine_1_2020_08</code>.</p>"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#enmap-box-capabilities","title":"EnMAP-Box capabilities","text":"<p> <p>Added WMS layer of Google Satellite Maps </p> <p> <p>Histogram and statistics of band 108 </p>"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#spectral-curves","title":"Spectral curves","text":"<p> <p>Spectral curve of a light pixel </p> <p> <p>Spectral curve of a shadowed pixel  </p> <p> <p>Multiple spectral curves with 5x5 mean sampling </p>"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#2-comparison-of-spectral-characteristics","title":"2. Comparison of spectral characteristics","text":""},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#spectral-curves-for-classes-and-subclasses","title":"Spectral curves for classes and subclasses","text":"Class Subclass Spectral curve mountain pine scree grass Deschampsia cespitosa grass Nardus stricta shrub Vaccinium myrtillus shrub Calluna vulgaris <p>The most significant difference can be seen between the spectral curve of class scree and all the other classes representing vegetation. The spectral curve of class scree gently increases with wavelength, in contrast to spectral curves of vegetation.  Vegetation classes reach lower values until the break around 700 nm, where the curves turn to even higher values than scree. In general, for the identification of scree, one can use a threshold value of wavelength lower than 700 nm or a ratio of values of wavelength higher and lower than 700 nm. A low ratio indicates scree, a high ratio is typical for vegetation.</p> <p> <p>Comparison of spectral curves of mountain pine (jump in values) and scree (gentle increase) </p> <p>Description of the differences between vegetation classes is much more complicated because the spectral curves are very similar.  Some of them reach lower values in the near infrared part of the spectrum, e.g. mountain pine and shrub Calluna vulgaris.  Comparing the two subclasses of grass, there are less significant differences in the highest values.</p> <p> <p>Comparison of spectral curves of grass Deschampsia cespitosa (higher values) and Nardus stricta (lower values) </p>"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#differences-in-spectral-curves-within-classes","title":"Differences in spectral curves within classes","text":"<p>In this part of the exercise, I've been working with an image of Deschampsia cespitosa with the tag <code>LH_grass_deschampsia_cespitosa_1_2020_08</code>.  As mentioned in the previous task, Deschampsia cespitosa seems to be distinguishable from Nardus stricta due to its higher reflectance values. Although the values are highly dependent on the chosen pixel.</p> <p> <p>Spectral curves for various pixels of grass Deschampsia cespitosa </p>"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#classification-of-additional-images","title":"Classification of additional images","text":"Name of image Class <code>unknown_1</code> scree <code>unknown_2</code> mountain pine <code>unknown_3</code> shrub Calluna vulgaris <code>unknown_4</code> grass Nardus stricta <code>unknown_5</code> grass Deschampsia cespitosa <code>unknown_6</code> shrub Vaccinium myrtillus"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#3-exploration-of-changes-in-spectral-characteristics-in-time","title":"3. Exploration of changes in spectral characteristics in time","text":"Class Spectral curves Comments mountain pine The highest values are from August, the lowest from June. In July and September, the reflectance is comparable. Shapes of the spectral curves are similar. scree Even though seasonal changes should not affect reflectance of scree, there are visible differences. The lowest values are from June. Shapes of the spectral curves are similar. grass Deschampsia cespitosa When compared to other vegetation classes, there is no rise in the spectral curve in June (lowest one) at 700 nm. The other curves are very similar, lower values are from September. grass Nardus stricta Again, the spectral curve from June (lowest one) is suspicious, but the others are almost identical, there are no differences between months. shrub Calluna vulgaris In this case, the spectral curve from June is in the middle, but it differs from the others in that it rises slowly at 700 nm. The lowest one is from July. shrub Vaccinium myrtillus The highest reflectance of the shrub in the near-infrared wavelengths is in July and the lowest in June. It seems as though the shrub has a shorter growing season than Calluna vulgaris. <p>In general, images from June seem to have odd values of reflectance, which is evident from the spectral curves as well as visualization. For distinguishing between the classes, images from July or August are the most suitable.</p>"},{"location":"module4/01_spectroscopy_principles/solution/spetroscopy_principles_exercise_solution.html#back-to-theme","title":"Back to theme","text":"<p>Proceed by returning to Principles of imaging and laboratory spectroscopy</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html","title":"Airborne hyperspectral data acquisition and pre-processing","text":"<p>Acquisition of airborne hyperspectral (HS) images requires careful flight planning supported by field campaigns.  The following preprocessing of data comprises necessary radiometric and geometric corrections.  In addition, spectra transformations and filtering can be carried out to reduce data volume and suppress the noise present in the data.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#objectives","title":"Objectives","text":"<p>In this theme, you will learn about: </p> <ul> <li>flight planning and in-situ measurements for radiometric and geometric correction of images,</li> <li>methods of radiometric correction of HS images,</li> <li>geometric correction,</li> <li>selected spectra transformations (e.g., maximum noise fraction, continuum removal, spectral indices), and spectra smoothing and denoising.</li> </ul> <p>The theme includes an exercise, a self-evaluation quiz, and a list of references.</p> <p>In the practical exercise, you will carry out geometric correction of images acquired with a Nano-Hyperspec\u00ae camera mounted on the DJI Matrice 600 Pro platform.</p> <p>After finishing this theme, you will be aware of flight parameter settings and in-situ measurements to be considered for a successful HS flight mission;  you will understand the principles of radiometric and geometric correction; and you will be familiar with noise and data volume reduction methods.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#planning-airborne-flight-missions","title":"Planning airborne flight missions","text":"<p>Radiometric and geometric quality of acquired images is one of the key factors influencing the success of any application in imaging spectroscopy.  Once the decision on the area of interest and required spectral and spatial resolution is made, it is necessary to choose a suitable sensor and platform \u2013 spaceborne, airborne, or remotely piloted aircraft system (RPAS)/uncrewed aerial vehicle (UAV).  While the orbits and instruments of the spaceborne systems are fixed by their providers, planning of airborne missions is an important part of each use case.  The main sensor and platform parameters to be considered for an airborne flight mission are summarised in Table 1.  Details on sensor parameters, so called parameters of interior orientation (focal length, lens distortion, and position of the principal point), and parameters of exterior orientation (position of the projection centre and rotations of the sensor in space), can be found in photogrammetric literature (e.g., Kraus, 2007, F\u00f6rstner and Wrobel, 2016).</p> <p>Table 1. Selected sensors and airborne platform parameters to be considered for flight mission planning. For an explanation of sensor types and platforms, visit Theme 1.</p> <p> </p> <p>Given the sensor pixel size p<sub>x</sub>, focal length c, and the required ground sampling distance GSD,  the platform flying height above the terrain h can be calculated as \\(h={{GSD}\\over{p_x}} \\ c\\)  , and the absolute flying height above the see level \\(Z = h + Z_{GL}\\), where Z<sub>GL</sub> corresponds to the height of the terrain modelled, e.g., from a digital terrain model (DMT).  The size of the swath S covered with a line scanner is given by the formula \\(S =  {{h}\\over{c}}  \\  s\\) , and the area covered with a frame camera can be expressed as \\(S \\times L \\  [m^{2}]\\) , where \\(S =  {{h}\\over{c}}  \\  s\\) and \\(L =  {{h}\\over{c}}  \\  l\\).  The relation between the above mentioned parameters is shown in Figure 1.</p> <p> </p> <p>Figure 1. Relation between ground sampling distance (GSD), focal length c, flying height above the terrain h, and the area covered on the ground with a linear scanner (left) and a frame camera (right). Figure by course authors.</p> <p>Other important parameters are exposure time and frame rate (FPS), i.e., the number of scanned lines or frame images acquired per second.  They are related \u2013 the exposure time must not exceed the frame period FP=1/FPS [s].  Table 2 gives an example of calculating image acquisition parameters.</p> <p>Table 2. Example of calculating selected parameters for image acquisition with a Nano-Hyperspec\u00ae line sensor</p> <p> </p> <p>To cover the whole area of interest with no gaps, the neighbouring strips are flown with a side overlap q of at least 20%-30% of swath S.  In the case of frame cameras, a forward overlap p must also be considered (Figure 2).  If photogrammetric processing is included, i.e., image orientation based on aerotriangulation and bundle adjustment (Kraus, 2007),  utilising image matching (Remondino et al., 2014) for the derivation of a digital surface model (DSM), overlap values of p = 60% \u2013 80% and q = 30% - 80% are applied.  Smaller values hold for aerial acquisitions with more stable flight control, while larger overlaps are used for RPAS missions.</p> <p> </p> <p>Figure 2. Side overlap between neighbouring image strips collected with a linear scanner (left); forward overlap and side overlap of images acquired with a frame camera (right). Figure by course authors.</p> <p>Both open source and proprietary software tools for flight planning are available.  Their usability is to a certain extent limited to the used platform (drone manufacturer in the case of RPAS).  Due to the dynamic development in this field, a list of the existing software tools is not provided, but you can make your own search based on the key words \u201cdrone mission planning software\u201d or \u201cdrone flight planner app\u201d.  Figure 3  gives a general workflow for airborne HS flight planning.</p> <p> </p> <p>Figure 3. A general workflow for flight mission planning. Figure by course authors.</p> <p>The parameters of the platforms and sensors that are provided in the respective technical data sheets are either pre-defined by a software provider or can be set by the user.  The flight mission planning applications contain graphic tools for drawing an area of interest. Importing a .kmz file is usually also possible.  Further settings comprise flight-specific parameters such as altitude above the mean sea level and above the ground, GSD, image overlaps, platform velocity, etc. (cf. Figure 4*).  Mutually dependent parameters (e.g., GSD and the flying height above the ground) are automatically adjusted after one of them is set up.  Based on the setup parameters, a flight route is automatically planned and visualised.  The created flight plan can be further optimised regarding, e.g., the starting and landing points, the flight line azimuth, overlap, etc.  The final flight plan is then exported and can be uploaded to the flight control system. </p> <p>The possibility of following the terrain during the flight is an important feature of the flight control system to be considered for RPAS.  It allows for a constant GSD in sloped terrain.  On the other hand, it requires a reliable DTM (and information about the height of vegetation and buildings if they are present) to avoid any collisions of the drone. </p> <p> </p> <p>Figure 4. Flight route created in the UgCS drone mission planning software for a Nano-Hyperspec\u00ae line camera mounted on the DJI Matrice 600 Pro. 100 m x 100 m plot at the B\u00edl\u00e1 louka meadow, Krkono\u0161e mountains. Figure by course authors.</p> <p>It is worth mentioning that the radiometric quality of HS images is of high importance.  With respect to the radiometric corrections discussed later in this theme, it is recommended to set the azimuth of the flight lines very close to the current azimuth of the sun.  Moreover, the images shall be acquired under stable illumination conditions, i.e., either a clear sky or a homogeneous cloud cover (enabling to perform the flight). </p> <p>For more details on planning airborne image data acquisition missions, read the paper by Pepe et al. (2018).</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#field-campaigns","title":"Field campaigns","text":"<p>The goal of field campaigns is to collect data necessary for proper geometric and radiometric correction of the acquired images.</p> <p>The ground control points (GCPs) are used for indirect or integrated sensor orientation (for explanation, see the section Geometric correction).  Moreover, check points (CPs) are used for an independent check of the horizontal accuracy of a final image product (orthoimage).  Both kinds of points are represented by signalised or natural distinct points (see Figure 5), and their position is usually determined using a Global Navigation Satellite System (GNSS) receiver enabling Real-Time Kinematic (RTK) measurements (horizontal accuracy of 1 \u2013 2 cm, vertical accuracy of 3 \u2013 5 cm).</p> <p>In-situ spectra of calibration tarps and overselected materials are collected in the field with portable spectroradiometers to improve atmospheric correction of collected data (Figure 5).  Moreover, solar irradiation can be measured with sunphotometers. The section Radiometric correction explains how to use these measurements.</p> <p> </p> <p>Figure 5. Field campaigns supporting the HS image acquisition - signalised ground control points (left), tarp for radiometric calibration (upper right), measurement of reflectance of selected materials with a portable spectroradiometer for atmospheric correction. Photos by course authors.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#radiometric-correction","title":"Radiometric correction","text":"<p>A raw HS data cube consists of layers of pixels, whose digital numbers (DNs) are related to the intensity of electromagnetic energy (incident on cells of a charge-coupled device (CCD)) within a given narrow interval of wavelengths.  DNs do not have any physical unit, and the values are dependent on the processing of the recorded signal of each sensor.  In the first step called sensor calibration, the DNs are converted to spectral (at-sensor) radiance L<sup>s</sup> [Wm<sup>-2</sup>sr<sup>-1</sup>\u03bcm<sup>-1</sup>].  The relation between L and DN is expressed for each sensor\u2019s spectral band b with a linear function \\(L_b^s=G_b DN_b+O_b\\), where G<sub>b</sub> and O<sub>b</sub> are the slope (gain) and offset, respectively.  If the spectral values shall be compared over time with in-situ or laboratory values, further corrections due to variable solar illumination, atmospheric effects, and topography must be considered, as discussed in the theme on satellite image processing workflow in Module 3.  The output of radiometric correction is an image whose pixel values represent spectral reflectance at the surface (Figure 6). </p> <p> </p> <p>Figure 6. General workflow for radiometric correction. Figure by course authors.</p> <p>Before application of the sensor calibration, the dark current must be subtracted from the raw data.  The dark current measurement, i.e., collecting data with a covered objective lens, is performed prior to the flight.  Corrections for scattered light, frame shift smear, and bad/erroneous pixels are introduced as described, e.g., in de Miguel et al. (2015) for the CASI sensor.  The sensor calibration follows. The gain and offset parameters for each spectral band are determined in the laboratory and shall be regularly recalibrated (as a consequence of sensor aging). </p> <p>The water vapour content and the composition and content of aerosols in the atmosphere affect the amount of incident solar radiation and surface reflected radiation captured by the sensor.  They can change in time and across larger locations, which can cause problems in image processing, especially in the case of aerial campaigns.  Thus, it is always an advantage to perform flights on days when the atmospheric conditions are stable over the area of interest.</p> <p>Absorption and scattering are the atmospheric effects we deal with. Molecules in the atmosphere (e.g., water vapor, ozone, and carbon dioxide) absorb incident solar radiation at various wavelengths, as shown in Figure 7.  The fraction of the electromagnetic radiation incident on the Earth\u2019s surface to the solar irradiation at the top of the atmosphere is referred to as transmittance of the atmosphere.  Particles or large gas molecules redirect the electromagnetic radiation from its original path. The result of this interaction is called scattering.  Details on the absorption and scattering mechanisms can be found, e.g., in the CCRS Remote Sensing Tutorial and Stull (2023).</p> <p> </p> <p>Figure 7. Transmittance of the atmosphere from visible to far infrared spectrum. Figure by Even 2021/Wikipedia, CC0 1.0 Universal.</p> <p>The radiance in the solar region (0.4 - 2.5 um) collected from flat terrain in the sensor L<sup>s</sup> basically consists of the following components (ATCOR- 4, Schowengerdt, 2006; see also Figure 8):</p> \\[L^s=L_1^s+L_2^s+L_3^s\\] <p>\\(L_1^s\\): surface reflected radiation \\(L_2^s\\): up-scattered path radiance \\(L_3^s\\): adjacency radiance (radiation reflected from objects adjacent to the measured surface)</p> <p> </p> <p>Figure 8. Components of measured at-sensor radiance. Figure by course authors.</p> <p>Depending on the in-situ measurements and available software tools, atmospheric correction can be carried out in the following ways:</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#1-application-of-radiative-transfer-models","title":"1. Application of radiative transfer models","text":"<p>Radiative Transfer Models (RTM) provide an estimate of the atmospheric effects of absorption and scattering on measured (at-sensor) radiance.  There are several algorithms available for atmospheric correction of HS data acquired during airborne missions:</p> <ul> <li>MODTRAN\u00ae - MODerate resolution atmospheric TRANsmission (Spectral Sciences Int.)  </li> <li>6S - Second Simulation of the Satellite Signal in the Solar Spectrum (Vermote et al., 1997)  </li> <li>Airborne ATCOR\u00ae - ATCOR4 (ATmospheric and topographic CORrection)( ReSe Applications GmbH)  </li> </ul> <p>The amount and composition of atmospheric constituents (aerosols, water vapor) are estimated from in-situ measurements (photometers) or directly from collected data (water vapor bands, aerosol optical thickness retrieval using, e.g., the dark dense vegetation algorithm). Based on the parameters of the atmosphere and the flying height, the transmittance of the atmosphere can be modelled for each wavelength (based on look-up tables generated by the radiative transfer model).  Under the assumption of flat terrain, cloud-free conditions, and neglecting the adjacency radiance,  the surface reflectance \\(\\rho\\) can be estimated from the formula \\(L^{s} = L^{s}_{2}  + \\rho \\tau E_g / \\pi\\) where L<sup>s</sup> is measured at-sensor radiance, \\(\\rho\\) - reflectance of the surface, \\(\\tau\\) - transmittance, and E<sub>g</sub> \u2013 solar irradiance at the surface (sum of direct and scattered solar irradiance).  In the final model, correction of the adjacency radiance must be added.</p> <p>If in-situ spectrometric measurements (surface reflectance) are available, inflight radiometric calibration (gain and offset for each spectral band) or at-sensor radiance validation can be carried out.</p> <p>If flat terrain cannot be assumed, a DTM is a required input in order to correct for topographic effects (Richter, 1997). Moreover, the Bidirectional Reflectance Distribution Function (BRDF) correction is applied (Richter and Schl\u00e4pfer, 2002).  In addition, corrections for cirrus cloud or shadow removal can be applied (Schl\u00e4pfer et al., 2009). A comprehensive evaluation of RTMs for deriving surface reflectance from HS imagery can be found in Zhou et al. (2023).</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#2-empirical-line-correction","title":"2. Empirical line correction","text":"<p>Empirical line correction is based on a linear regression model relating in-situ and image observations (Figure 9).  Its use was justified by several authors (e.g., Baugh and Groeneveld, 2008; Oritz et al., 2017), nevertheless, the combination with RTM models gives better results (Hadley et al., 2008).  In the event that in-situ the dark target measurements are not available, the empirical line passes through the origin (which limits the method in correcting for the effect of transmitted radiation).  Figure 10 shows an example of corrected spectra using the empirical line method.</p> <p> </p> <p>Figure 9. Principle of empirical line correction. Figure by course authors.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#3-dark-object-subtraction","title":"3. Dark object subtraction","text":"<p>This simplest atmospheric correction method is built on the model that the at-sensor radiance \\(L^s\\) has just two components - surface radiance \\(L_1^s\\) and path radiance \\(L_2^s\\): \\(L^s=L_1^s+L_2^s\\).  To model the path radiance, regions of the scene with expected zero surface reflectance (the very darkest objects in the scene) are found.  The radiance of pixels corresponding to dark objects is removed from the whole scene. The method is easy to implement and requires no further data.  The results may not be reliable as the assumption does not hold for real-life scenarios (Shi et al., 2016).  Zhen et al. (2022) propose this method for haze removal from HS imagery.</p> <p>There are other empirical methods applicable to HS imagery, such as Flat Filed correction or Internal Average Relative Reflectance (for comparison of the methods see Merzah and Jaber, 2020).  Optionally, experience with methods for radiometric correction of hyperspectral imagery acquired from RPAS systems was tested, e.g., Suomalainen et al. (2021); Schl\u00e4pfer et al. (2020); Hakala et al. (2018). </p> <p> </p> <p>Figure 10. Example of spectra before (up right) and after (down right) atmospheric correction. The image was acquired with a Nano-Hyperspec\u00ae line camera mounted on the DJI Matrice 600 Pro with a GSD of 3 cm. The empirical line calibration was carried out based on measurements of a pre-calibrated calibration tarp. Figure by course authors.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#geometric-correction","title":"Geometric correction","text":"<p>The goal of geometric correction is to compensate for geometric distortions due to perspective projection, changes in the platform altitude, velocity, and rotations, and uneven terrain.  It results in an orthorectified image georeferenced in the required reference coordinate system. </p> <p>The airborne systems for HS data acquisition are equipped with a GNSS receiver and an inertial measurement unit \u2013 IMU (or an inertial navigation system \u2013 INS) to determine the platform position and rotation with respect to the reference coordinate system (e.g., WGS84).  Such equipment is necessary in the case of a line scanner when each image line has its own parameters of exterior orientation (Pepe et al., 2018).  The method of obtaining the image orientation only from the GNSS/INS measurements is called direct georeferencing (e.g., Toth and J\u00f3zk\u00f3w, 2016; see also Figure 11).  It can be realized using RTK or post-processing kinematic (PPK) measurements. Its use requires high accuracy of the INS, which makes the whole system costly. To carry out quality assessment, a number of CPs is needed.</p> <p> </p> <p>Figure 11. Direct georeferencing of HS images acquired with a line sensor. The triangles correspond to check points. Figure by course authors.</p> <p>In cases where the GNSS/INS equipment is of lower quality, GCPs are used to improve the georeferencing.  Such an approach is called integrated system orientation (ISO, e.g., Toth and J\u00f3zk\u00f3w, 2016).  Indirect georeferencing is then an approach when the GNSS/INS measurements are not used at all and the image orientation relies only on the GCPs \u2013 usually the case of frame cameras.  All three methods of image orientation (direct, integrated, and indirect) are described in photogrammetric literature (e.g., Colomina and Molina, 2014; Kraus, 2007). </p> <p>If the accuracy of the GNSS/INS is low, remaining distortions after direct georeferencing and orthorectification in images collected with a line scanner can be in the order of decimetres or meters (Hruska et al., 2012).  Thus, overlapping images do not match, as shown in Figure 12. </p> <p> </p> <p>Figure 12. Geometric correction of an image acquired with a Nano-Hyperspec\u00ae line camera mounted on the DJI Matrice 600 Pro. The two overlapping image strips after direct georeferencing and orthorectification (top) and after their rectification into RGB orthoimages acquired with a frame camera Sony A7 ILCE-7 from the same platform (bottom). The correction includes transformation from the WGS84 (EPSG 4326) coordinate system to the national coordinate system S-JTSK (EPSG 5514). Figure by course authors.</p> <p>A possible solution is the registration of distorted images into another geographical layer with higher accuracy.  Orthoimages acquired with a RGB frame camera during the same flight or another flight within a short time interval (a few days) are often used for this purpose (Suomalainen et al., 2014; Turner et al., 2017; Habib et al., 2016; Angel et al., 2020; \u010cerven\u00e1 et al. 2020).  Figure 13 depicts a processing workflow for the orientation of images acquired with a line sensor based on fusion with simultaneously acquired images with a frame camera.  The images from the frame camera were georeferenced using GCPs and a standard photogrammetric workflow using structure from motion (Jiang et al., 2020).  Table 3 shows that a combined approach brings a considerable improvement to first-step orientation based solely on GNSS/INS.</p> <p> </p> <p>Figure 13. Geometric processing of HS imagery acquired with a line sensor supported by simultaneously collected images with a frame camera. Figure by Suomalainen et al., (2014), CC-BY 4.0 license.</p> <p>Table 3. Comparison of RMSE on CPs after direct georeferencing and orthorectification of a line sensor (basic correction) and after improvement using orthoimages from a frame camera (advanced correction).</p> Reference GSD [m] RMSE [m] basic correction RMSE [m] advanced correction Angel et al. 2020 0.007 --- 0.05 - 0.08 \u010cerven\u00e1 et al. 2020 0.03 1.74 0.19 Habib et al. 2016 0.05 1.42 - 5.09 0.35 - 0.99 Hruska et al. 2012 0.28 4.63 --- Turner et al. 2017 0.02 --- 0.03 - 0.06"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#exercise","title":"Exercise","text":"<p>At this point, you are ready to work on the exercise.  You will improve georeferencing of a HS image (3 cm GSD, for the purpose of the exercise, reduced only to three spectral bands) by its registration into a RGB orthoimage (2cm GSD).  The HS image was acquired with a Nano-Hyperspec\u00ae line camera mounted on the DJI Matrice 600 Pro georeferenced using onboard GNSS/INS.  The RGB images were acquired with the frame camera Sony A7 ILCE-7, georeferenced using the GCPs and structure from motion.  The GCPs were signalised and measured with a GNSS/RTK receiver with an accuracy of 2 cm in the horizontal direction and 3 cm in the vertical.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#spectra-transformations","title":"Spectra transformations","text":"<p>Due to tge noise present in data and large data volumes, it is practical to carry out further pre-processing steps of radiometrically and geometrically corrected HS imagery before their use for time series statistical analysis or classification. </p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#spectra-smoothening-and-denoising","title":"Spectra smoothening and denoising","text":"<p>Spectral curves collected with an airborne HS sensor contain a relatively high amount of noise due to narrow bandwidth, remaining errors from radiometric correction, etc. (Vaiphasa, 2006).  The collected spectrum \\(s_{0} (\\lambda)\\) can be then expressed as the sum of true signal \\(s_{t} (\\lambda)\\) and noise \\(n (\\lambda)\\):</p> \\[s_{0} (\\lambda) = s_{t} (\\lambda) +n (\\lambda)\\] <p>The true signal can be estimated by convolution:</p> \\[{\\widehat s_{t}} (\\lambda) = s_{0} (\\lambda) *g (\\lambda)\\] <p>where \\({\\widehat s}_{t}\\) is the estimation of the true signal and \\(g (\\lambda)\\) is a convolutional, in our case smoothening filter.</p> <p>One possibility of signal smoothing is the moving average method, when the given spectral value is replaced with a mean or median of L = 2k+1 neighbouring values, where k is the number of preceding and following values, respectively, of the evaluated value.  Figure 14 gives an example of a spectrum smoothed with a mean and a median filter of length L = 5. </p> <p> </p> <p>Figure 14. Example of spectrum smoothing using mean and median filters with the size of the convolution window L = 5. The original spectrum of tufted hairgrass was measured with a Nano-Hyperspec\u00ae line camera in June 2020. Figure by course authors.</p> <p>The Savitzky-Golay filter discussed in Module 1 in connection with smoothing time series data is also applied to measured spectra.</p> <p>Averaging of neighbouring bands followed by subsampling is applied when both data denoising and reduction are desired. Such procedure was used for the HS dataset from the Krkono\u0161e Mts. tundra (see the dataset description) and Figure 15.</p> <p> </p> <p>Figure 15. Example of spectrum smoothing using a median filter with the size of the convolution window  L= 5 followed by sampling using the step size 5. The original spectrum of tufted hairgrass was measured with a Nano-Hyperspec\u00ae line camera in June 2020. It contained 270 bands that were reduced to 54 bands, taking a median of 5 neighbouring measurements. Figure by course authors.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#tutorial","title":"Tutorial","text":"<p>To better understand the Moving Average and Savitzky-Golay filters, go to the Tutorial on Spectra smoothing and denoising.  In a Jupyter Notebook, you can explore their parameters and their influence on the level of spectra smoothing.  To understand the consequences of smoothing on measured spectra, read the by study Vaiphasa, (2006).</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>The spectral bands of HS images are highly correlated, as depicted in Figure 16.  As explained in Module 2, the Principal Component Analysis (PCA) transforms the image into new, uncorrelated bands and gives the possibility to reduce data volume and uncorrelated noise if only components preserving high image variance are used.  It shall be noted that small but significant band differences may appear only in the components of low variance, and thus, their removal must be done with caution (Schowengerdt, 2006).</p> <p> </p> <p>Figure 16. Correlation of 391504 spectral measurements of dominating grass species in the Krkono\u0161e Mts. tundra collected with a Nano-Hyperspec\u00ae line camera in August 2020. The correlation was calculated both for original spectra containing nb = 270 spectral bands and spectra reduced to nb = 54 bands, taking a median of 5 neighbouring measurements. Notice the corresponding patterns in both parts of the figure. Figure by course authors. </p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#maximum-noise-fraction-mnf","title":"Maximum Noise Fraction (MNF)","text":"<p>Noise in HS images can be effectively removed with the maximum noise fraction (MNF) transformation (Green et al., 1988). It is a modification of PCA and can improve the isolation of the noisiest bands in a HS image (Schowengerdt, 2006).  The influence of the MNF transformation on the classification accuracy of grass species from time series of HS imagery is part of the case study on Discrimination of selected grass species from time series of RPAS hyperspectral imagery.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#spectral-indices","title":"Spectral indices","text":"<p>A direct comparison of radiometrically corrected surface reflectance in time series is possible.  In many cases, the relationship between spectral properties of the studied surface and its physical and chemical properties is known.  Thus, instead of studying the whole spectrum, attention can be paid only to selected spectral bands and their combinations.  In Module 2, the term spectral index was introduced, and some examples of indices used for multispectral data were given.  The same approach, i.e., using mathematical operations on measurements from two and more spectral bands to emphasize certain properties of the observed surface can be applied to HS imagery.  Thanks to its narrow spectral bands, more details can be inspected.  The case study on the seasonal dynamics of flood-plain forests gives some examples of indices used in imaging spectroscopy, including their calculation in R.  More examples can be found in, e.g., Roberts et al. (2018); Broge and Mortensen (2002); Envi (2023).</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#continuum-removal","title":"Continuum removal","text":"<p>The method of Continuum Removal (CR) is applied to normalise the shape of the spectral curve on selected wavelength intervals. These intervals are characteristic for the phenomenon under study, typically around the absorption bands.  Figure 17 explains the principle of the method.</p> <p> </p> <p>Figure 17. Principle of the continuum removal method. Original spectrum (left) and the transformed spectrum after continuum removal (right). Figure by Albrechtov\u00e1 et al. (2017)</p> <p>The end points of the interval on the spectral curve determine the continuum line, and their new value is equal to one.  The other transformed reflectance values take values in the interval 0 - 1 and are recalculated as:</p> \\[\\rho^{'}_{\\lambda}  = {{\\rho _{\\lambda}}\\over{\\rho _{c(\\lambda)}}}\\] <p>where \\(\\rho^{'} _{\\lambda}\\) is the reflectance value after the CR transformation at wavelength \u03bb, \\(\\rho _{\\lambda}\\) is the original reflectance value at the same wavelength and \\(\\rho _{c(\\lambda)}\\) is the continuum value,  i.e., the value at wavelength \u03bb given on the line connecting the points of the selected interval on the reflectance spectral curve (Kokaly and Clark, 1999).  Removing the continuum from a selected part of the spectrum allows for the calculation of the depth of the absorption band or its area. This is used, for example, by the vegetation index ANMB<sub>650-725</sub>  (Area under curve Normalized to Maximum Band depth between 650-725 nm, Malenovsk\u00fd et al., 2006).</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"When planning a flight mission,  no overlap between image lines/strips is necessary. overlap is necessary only in the hilly terrain. a higher value of overlap is better in hilly terrain than in flat terrain. a higher value of overlap is better in hilly terrain than in flat terrain.  If possible, how would you choose the flight azimuth?  Perpendicular to the current sun azimuth. Close to the current sun azimuth. Arbitrary. The relations between the azimuths does not influence the image radiometry. Close to the current sun azimuth.  You have a camera with a focal length of 15 mm and a pixel size of 7.5 \u03bcm. What flying height above the terrain do you need to plan to achieve the GSD of 5 cm?  70 m 85 m 100 m 100 m  Radiometric correction includes:  sensor calibration and atmospheric correction. sensor calibration. atmospheric correction. sensor calibration and atmospheric correction.  Which are empirical methods for atmospheric correction?  Empirical line and radiative transfer model. Empirical line and dark object subtraction. Dark object subtraction and radiative transfer model. Empirical line and dark object subtraction.  The platform is equipped with a GNSS/INS. At the same time, two GCPs were measured. What kind of georeferencing can you use?  Direct georeferencing Integrated sensor orientation Indirect georeferencing Direct georeferencing&amp;Integrated sensor orientation"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#next-unit","title":"Next unit","text":"<p>Proceed with In situ and laboratory spectroscopy of vegetation</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#references","title":"References","text":""},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#key-literature","title":"Key literature","text":"<p>Pepe, M., Fregonese, L., Scaioni, M. (2018). Planning airborne photogrammetry and remote-sensing missions with modern platforms and sensors. European Journal of Remote Sensing, 51(1), 412-436. 10.1080/22797254.2018.1444945.</p> <p>Schl\u00e4pfer, D., Richter, R., Hueni, A. (2009). Recent developments in operational atmospheric and radiometric correction of hyperspectral imagery. In Proc. 6th EARSeL SIG IS Workshop (pp. 16-19). researchgate.net.</p> <p>Habib, A., Han, Y., Xiong, W., He, F., Zhang, Z., Crawford, M. (2016). Automated ortho-rectification of UAV-based hyperspectral data over an agricultural field using frame RGB imagery. Remote Sensing, 8(10), 796. 10.3390/rs8100796.  </p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing.html#further-articles-referenced-literature-and-resources","title":"Further articles, referenced literature and resources","text":"<p>Angel, Y., Turner, D., Parkes, S., Malbeteau, Y., Lucieer, A., McCabe, M. F. (2019). Automated georectification and mosaicking of UAV-based hyperspectral imagery from push-broom sensors. Remote Sensing, 12(1), 34. 10.3390/rs12010034. </p> <p>ATCOR-4 (2023): ATCOR 4 - for Airborne Remote Sensing Systems. rese-apps.com. </p> <p>Baugh, W. M., Groeneveld, D. P. (2008). Empirical proof of the empirical line. International Journal of Remote Sensing, 29(3), 665-672. 10.1080/01431160701352162.</p> <p>Broge, N. H., &amp; Mortensen, J. V. (2002). Deriving green crop area index and canopy chlorophyll density of winter wheat from spectral reflectance data. Remote sensing of environment, 81(1), 45-57. 10.1016/S0034-4257(01)00332-7). </p> <p>Canada Centre for Mapping and Earth Observation \u2013 Remote Sensing Tutorial. natural-resources.canada.ca.</p> <p>Albrechtov\u00e1, J., Kupkov\u00e1, L., Campbell, P. K. (2017). Metody hodnocen\u00ed fyziologick\u00e9ho stavu smrkov\u00fdch porost\u016f: p\u0159\u00edpadov\u00e9 studie sledov\u00e1n\u00ed v\u00fdvoje stavu smrkov\u00fdch porost\u016f v Kru\u0161n\u00fdch hor\u00e1ch v letech 1998-2013. \u010cesk\u00e1 geografick\u00e1 spole\u010dnost. </p> <p>\u010cerven\u00e1, L., Lys\u00e1k, J., Pot\u016f\u010dkov\u00e1, M., Kupkov\u00e1, L. (2020). Zku\u0161enosti se zpracov\u00e1n\u00edm hyperspektr\u00e1ln\u00edch dat po\u0159\u00edzen\u00fdch UAV. GIS Ostrava. 10.31490/9788024843988-4. </p> <p>Colomina, I., Molina, P. (2014). Unmanned aerial systems for photogrammetry and remote sensing: a review. ISPRS J. Photogramm. Remote Sens. 92, 79\u201397 10.1016/j.isprsjprs.2014.02.013. </p> <p>ENVI (2023). Vegetation Indices Background. nv5geospatialsoftware.com. </p> <p>F\u00f6rstner, W., Wrobel, B. P. (2016). Photogrammetric computer vision. Springer International Publishing Switzerland.</p> <p>Green, A. A., Berman, M., Switzer, P., &amp; Craig, M. D. (1988). A transformation for ordering multispectral data in terms of image quality with implications for noise removal. IEEE Transactions on geoscience and remote sensing, 26(1), 65-74. 10.1109/36.3001.</p> <p>Hadley, B. C., Garcia-Quijano, M., Jensen, J. R., Tullis, J. A. (2005). Empirical versus model\u2010based atmospheric correction of digital airborne imaging spectrometer hyperspectral data. Geocarto International, 20(4), 21-28. 10.1080/10106040508542360.</p> <p>Hakala, T. et al. (2018). Direct reflectance measurements from drones: Sensor absolute radiometric calibration and system tests for forest reflectance characterization. Sensors, 18(5), 1417. 10.3390/s18051417. </p> <p>Hanu\u0161, J., Fabi\u00e1nek, T., Fajmon, L. (2016). Potential of airborne imaging spectroscopy at CzechGlobe. The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 41, 15-17. 10.5194/isprs-archives-XLI-B1-15-2016. </p> <p>Hruska, R., Mitchell, J., Anderson, M., Glenn, N. F. (2012): Radiometric and Geometric Analysis of Hyperspectral Imagery Acquired from an Unmanned Aerial Vehicle. Remote Sensing. 9, 4, 2736\u20132752. 10.3390/rs4092736. </p> <p>Jiang, S., Jiang, C., Jiang, W. (2020). Efficient structure from motion for large-scale UAV images: A review and a comparison of SfM tools. ISPRS Journal of Photogrammetry and Remote Sensing, 167, 230-251. 10.1016/j.isprsjprs.2020.04.016. </p> <p>Kokaly, R. F., Clark, R. N. (1999). Spectroscopic determination of leaf biochemistry using band-depth analysis of absorption features and stepwise multiple linear regression. Remote sensing of environment, 67(3), 267-287. 10.1016/S0034-4257(98)00084-4. </p> <p>Kraus, K. (2007).  Photogrammetry: Geometry from Images and Laser Scans. de Gryter, 2nd edition.</p> <p>Malenovsky, Z., Ufer, C., Lhot\u00e1kov\u00e1, Z., Clevers, J. G., Schaepman, M. E., Albrechtov\u00e1, J., &amp; Cudl\u00edn, P. (2006). A new hyperspectral index for chlorophyll estimation of a forest canopy: Area under curve normalised to maximal band depth between 650-725 nm. EARSeL eProceedings, 5(2), 161-172. library.wur.nl.</p> <p>Merzah, Z. F.,  Jaber, H. S. (2020). Assessment of Atmospheric Correction Methods for Hyperspectral Remote Sensing Imagery Using Geospatial Techniques. In IOP Conference Series: Materials Science and Engineering (Vol. 745, No. 1, p. 012123). IOP Publishing. 10.1088/1757-899X/745/1/012123.</p> <p>de Miguel, E., Jim\u00e9nez, M., P\u00e9rez, I., de la C\u00c1MARA, \u00d3. G., Mu\u00f1oz, F., G\u00f3mez-S\u00e1nchez, J. A. (2015). AHS and CASI processing for the REFLEX remote sensing campaign: methods and results. Acta Geophysica, 63, 1485-1498. 10.1515/acgeo-2015-0031. </p> <p>Ortiz, J. D., Avouris, D., Schiller, S., Luvall, J. C., Lekki, J. D., Tokars, R. P., Becker, R. (2017). Intercomparison of approaches to the empirical line method for vicarious hyperspectral reflectance calibration. Frontiers in Marine Science, 4, 296. 10.3389/fmars.2017.00296. </p> <p>Remondino, F., Spera, M. G., Nocerino, E., Menna, F., Nex, F. (2014). State of the art in high density image matching. The photogrammetric record, 29(146), 144-166. 10.1111/phor.12063.</p> <p>Richter, R. (1997). Correction of atmospheric and topographic effects for high spatial resolution satellite imagery. International journal of remote sensing, 18(5), 1099-1111. 10.1080/014311697218593.</p> <p>Richter, R., Schl\u00e4pfer, D. (2002). Geo-atmospheric processing of airborne imaging spectrometry data. Part 2: Atmospheric/topographic correction. International Journal of Remote Sensing, 23(13), 2631-2649. 10.1080/01431160110115834. </p> <p>Roberts, D. A., Roth, K. L., Wetherley, E. B., Meerdink, S. K., Perroy, R. L. (2018). Hyperspectral vegetation indices. In Hyperspectral indices and image classifications for agriculture and vegetation (pp. 3-26). CRC press. </p> <p>Schl\u00e4pfer, D., Popp, C., Richter, R. (2020). Drone data atmospheric correction concept for multi-and hyperspectral imagery\u2013the DROACOR model. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLIII-B3-2020, 473\u2013478. 10.5194/isprs-archives-XLIII-B3-2020-473-2020. </p> <p>Schowengerdt, R. A. (2006). Remote sensing: models and methods for image processing. Elsevier, 3rd edition.</p> <p>Stull, R. (2023): Scattering. LibreTexts, Geosciences. 22.4 Scattering. geo.libretexts.org.</p> <p>Suomalainen, J., Anders, N., Iqbal, S., Roerink, G., Franke, J., Wenting, P., ... , Kooistra, L. (2014). A lightweight hyperspectral mapping system and photogrammetric processing chain for unmanned aerial vehicles. Remote Sensing, 6(11), 11013-11030. 10.3390/rs61111013.</p> <p>Suomalainen, J., Oliveira, R. A., Hakala, T., Koivum\u00e4ki, N., Markelin, L., N\u00e4si, R., Honkavaara, E. (2021). Direct reflectance transformation methodology for drone-based hyperspectral imaging. Remote Sensing of Environment, 266, 112691. 10.1016/j.rse.2021.112691.</p> <p>Toth, C., J\u00f3zk\u00f3w, G. (2016). Remote sensing platforms and sensors: A survey. ISPRS Journal of Photogrammetry and Remote Sensing, 115 (2016) 22\u201336. 10.1016/j.isprsjprs.2015.10.004.</p> <p>Turner, D., Lucieer, A., McCabe, M., Parkes, S., Clarke, I. (2017). Pushbroom hyperspectral imaging from an unmanned aircraft system (uas)\u2013geometric processing workflow and accuracy assessment. The International Archives of the Photogrammetry. Remote Sensing and Spatial Information Sciences. XLII-2/W6. 379\u2013384. 10.5194/isprs-archives-xlii-2-w6-379-2017. </p> <p>Vaiphasa, C. (2006). Consideration of smoothing techniques for hyperspectral remote sensing. ISPRS journal of photogrammetry and remote sensing, 60(2), 91-99. 10.1016/j.isprsjprs.2005.11.002. </p> <p>Vermote, E. F., Tanr\u00e9, D., Deuze, J. L., Herman, M., Morcette, J. J. (1997). Second simulation of the satellite signal in the solar spectrum, 6S: An overview. IEEE transactions on geoscience and remote sensing, 35(3), 675-686. 10.1109/36.581987. </p> <p>Zhou, Q., Wang, S., Liu, N., Townsend, P. A., Jiang, C., Peng, B., Verhoef, W., Guan, K. (2023). Towards operational atmospheric correction of airborne hyperspectral imaging spectroscopy: Algorithm evaluation, key parameter analysis, and machine learning emulators. ISPRS Journal of Photogrammetry and Remote Sensing, 196, 386-401. 10.1016/j.isprsjprs.2022.11.016. </p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing_exercise_geometric.html","title":"Geometric correction","text":"<p>The aim of this exercise is to perform geometric correction of hyperspectral image data acquired by an UAV pushbroom scanner.  The insufficient accuracy of IMU/GNSS measurements of low-cost equipment and height differences in terrain can cause shifts and deformations of overlapping strips.  Orthorectified RGB images collected during the same or different flights can be used for coregistration and orthorectification of the UAV hyperspectral images. </p> <ul> <li> <p>Prerequisites</p> <ul> <li>Installed QGIS    </li> <li>Downloaded data (module4/theme2_exercise_geometric_correction)  The dataset consists of:<ul> <li>preprocessed hyperspectral image strips with and without a CRS: 20190815_lineX_RGB_trf.tif (image file) and 20190815_lineX_RGB_trf.tfw (georeference file). </li> <li>check points: CP_GNSS.shp (shapefile) </li> <li>orthorectified RGB image: 20190814_Ortho_RGB.jpg (image file) and 20190814_Ortho_RGB.jgw (georeference file).  </li> </ul> </li> </ul> </li> <li> <p>Tasks</p> <ul> <li>Evaluate relative and absolute accuracy of \u201craw\u201d image strips </li> <li>Collect identical points; register scanned image strips to the orthoimage </li> <li>Accuracy assessment of geometrically corrected image data </li> </ul> </li> </ul>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing_exercise_geometric.html#1-data-preparation-relative-and-absolute-accuracy","title":"1. Data preparation, relative and absolute accuracy","text":"<p>Start with visualizing the deformed hyperspectral image strips <code>20190815_lineX_RGB_trf.tif</code> of B\u00edl\u00e1 louka meadow alpine grasslands (50.729N, 15.683E, Krkono\u0161e mountains) in QGIS. To allow for a smaller image size, the data comprises only a spectral and spatial subset of the original hyperspectral imagery. Mapping the available bands to Red, Green, and Blue and using the min/max cumulative count cut settings should result in an image resembling true colors. To remove the black background, go into the image \u201cProperties -&gt; Transparency\u201d and add a 0 into the empty \u201cAdditional no data value\u201d column. The image strips have a defined coordinate reference system, S-JTSK / Krovak East North (EPSG: 5514), do not use any transformations, and work with the provided CRS.  </p> <p> </p> B\u00edl\u00e1 louka meadow hyperspectral image strips <p>Load the shapefile <code>CP_GNSS.shp</code> containing the positions of the check points. Twelve bright orange round discs were placed evenly throughout the area to signal their location (Figure 2). Have a look at the corresponding check points in the images</p> <p> </p> Illustrational image of the check point; check point appearance in the hyperspectral imagery <p>Evaluate the relative and absolute accuracy of the hyperspectral image strips. All the necessary tools can be found in the georeferencing app in QGIS \u201cLayer -&gt; Georeferencer\u201d. If you are not familiar with the working environment or need help, have a look at the documentation.  </p> <p>To compute the absolute accuracy, determine the residuals on the check points, preferably in map units (m). Load the hyperspectral image strips for quality assessment in the app, find the checkpoints in the hyperspectral image, and compare their coordinates to the ones defined in the provided shapefile. Each check point is located in at least one image strip. For relative accuracy, determine the geometrical distortions between overlaying image strips. Find a minimum of four identical points (distinct edge/corner of grass stand, visible hollow or mound, check point) for each strip pair and compute the difference between their coordinates in the first and second image of the pair. Use the columns Source XY and Destination XY to compute all the differences. Do not use the Residual values in the GCP table from the \"Georeferencer\" app, as they refer to residuals after the transformation!  </p> <p> </p> Example of generated GCP table with source and destination coordinates <p>Apart from the individual residual values obtained, compute commonly used statistics such as horizontal shift, standard deviation, and Root Mean Square Error (RMSE) for each strip (for both relative and absolute accuracy assessment).  </p> <p> </p> <pre><code>Evaluate geometrical distortion between the deformed scanned strips (relative accuracy).  \nEvaluate residuals on the check points (absolute accuracy).\n</code></pre>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing_exercise_geometric.html#2-image-registration","title":"2. Image registration","text":"<p>Open the orthorectified RGB image <code>20190814_Ortho_RGB.jpg</code> covering the whole extent of the hyperspectral image strips. In the georeferencing app in QGIS \u201cLayer -&gt; Georeferencer\u201d load the first image strip without a CRS (coordinate reference system). Collect identical points evenly throughout the area, the minimum required number of points is dependent on the selected transformation. Examples of usable identical points can be found in the figure below. Do not use the check points for the image registration, as the accuracy assessment needs to be unbiased. </p> <p> </p> Examples of identical points in the RGB image (top) and hyperspectral image (bottom) <p>After the identical points have been added to the hyperspectral image strip, the \u201cTransformation settings\u201d need to be defined. Select the Transformation type; the 1st order polynomial (affine) requires at least three identical points, preserves collinearity, and allows scaling, translation, and rotation only. The second transformation type we are going to use is Thin plate spline, which requires at least 10 identical points and is able to introduce local deformations to the data. Choose the Target CRS (EPSG:5514), Output file location, and Resampling method Nearest Neighbour. You can optionally Generate a PDF report or map with the results of the transformation. It is recommended to check the Save GCP points options; the remaining values can be left at their defaults.  Now you are ready to register the image strip by running \u201cStart georeferencing\u201d. </p> <p> </p> Transformation settings <p>Repeat the steps above for the remaining image strips. When searching for identical points, also add some from the previously registered image strip to ensure the correct attachment/connection of the image strips.    </p> <pre><code>Register the hyperspectral image strips one after another to the existing RGB orthoimage. \nTry out different types of transformations: polynomial, spline.\n</code></pre>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing_exercise_geometric.html#3-accuracy-assessment","title":"3. Accuracy assessment","text":"<p>Compute the relative and absolute accuracy of the now orthorectified outputs, follow the steps from the first task.  Include a screenshot of the GCP table from the \"Georeferencer\" app with residual values on the identical points after the transformation.</p> <pre><code>Evaluate geometrical distortion between the orthorectified scanned strips (relative accuracy).\nEvaluate residuals on the check points (absolute accuracy).\n</code></pre> <p>Bonus task: Explore QGIS functions and try to create an orthomosaic from the image strips. </p> <p>Examine the results and try to answer the following questions: </p> <pre><code>Evaluate the absolute and relative accuracy after geometric correction. \nWhat is the maximum error? Is the resulting accuracy sufficient?  \n\nCompare the \u201craw\u201d image strips with the geometrically corrected (orthorectified) image strips in terms of computed accuracy and visual inspection.  \n\nAre there any differences between the results of the spline and polynomial transformations?  \n\nComment on the used resampling method: what does \u201cNearest neighbour\u201d ensure? \nWould using a different resampling type affect the image, and if yes, then how? \n\n</code></pre>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing_exercise_geometric.html#references","title":"References","text":"<p>QGIS Project (2023): QGIS Documentation. docs.qgis.org/3.28/en/docs.</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing_exercise_geometric.html#exercise-solution","title":"Exercise solution","text":"<p>Proceed to example solution Geometric correction - report</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/02_aerial_acquisition_preprocessing_exercise_geometric.html#back-to-theme","title":"Back to theme","text":"<p>Proceed by returning to Aerial/RPAS hyperspectral data acquisition and pre-processing</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/filtering_spectral_curve.html","title":"Filtering spectral curve","text":"Metadata     title: \"E-TRAINEE: Tutorial: Spectra smoothening and denoising\"     description: \"This is a tutorial within the second theme of the Airborne Imaging Spectroscopy Time Series Analysis module.\"     dateCreated: 2023-10-16      authors: Alex Srolleru      contributors: Marketa Potuckova      estimatedTime:  <p>In this tutorial, two filtering methods are discussed: Savitzky-Golay filter, Simple Moving Average filter, and their parameters are explained in detail. They are applied on spectra containing 270 bands with wavelengths ranging from 400 to 1,000 nm. The spectra were acquired with a Nano-Hyperspec\u00ae camera mounted on the DJI Matrice 600 Pro drone (see the dataset description). The filters are implemented in Python, and the tutorial can be downloaded as a Jupyter Notebook.</p> <p>First, we are going to import the needed libraries. SciPy is an open-source Python library used for scientific computing, and matplotlib serves for plotting and visualization in general.</p> In\u00a0[5]: Copied! <pre>import scipy as sp\nimport matplotlib.pyplot as mp\n</pre> import scipy as sp import matplotlib.pyplot as mp <p>The provided data consists of reflectance values extracted for different B\u00edl\u00e1 louka meadow grass species (nard, mol, desch, and cv) from hyperspectral imagery and wavelengths [nm] at which the reflectance values were acquired.</p> In\u00a0[7]: Copied! <pre>wavelength=[398.784,  401.026,  403.268,  405.51,  407.751,  409.993, 412.235,  414.477,  416.719, 418.961,  421.202,  423.444, 425.686,  427.928,  430.17 ,  432.411,  434.653,  436.895, 439.137,  441.379,  443.621,  445.862,  448.104,  450.346, 452.588,  454.83 ,  457.072,  459.313,  461.555,  463.797, 466.039,  468.281,  470.523,  472.764,  475.006,  477.248, 479.49,  481.732,  483.974,  486.215,  488.457,  490.699, 492.941,  495.183,  497.425,  499.666,  501.908,  504.15, 506.392,  508.634,  510.876,  513.117,  515.359,  517.601, 519.843,  522.085,  524.327,  526.568,  528.81 ,  531.052, 533.294,  535.536,  537.778,  540.019,  542.261,  544.503, 546.745,  548.987,  551.229,  553.47 ,  555.712,  557.954, 560.196,  562.438,  564.68,  566.921,  569.163,  571.405, 573.647,  575.889,  578.131,  580.372,  582.614,  584.856, 587.098,  589.34 ,  591.582,  593.823,  596.065,  598.307, 600.549,  602.791,  605.032,  607.274,  609.516,  611.758, 614.000,  616.242,  618.483,  620.725,  622.967,  625.209, 627.451,  629.693,  631.934,  634.176,  636.418,  638.66 , 640.902,  643.144,  645.385,  647.627,  649.869,  652.111, 654.353,  656.595,  658.836,  661.078,  663.32,  665.562, 667.804,  670.046,  672.287,  674.529,  676.771,  679.013, 681.255,  683.497,  685.738,  687.98,  690.222,  692.464, 694.706,  696.948,  699.189,  701.431,  703.673,  705.915, 708.157,  710.399,  712.64,  714.882,  717.124,  719.366, 721.608,  723.85,  726.091,  728.333,  730.575,  732.817, 735.059,  737.301,  739.542,  741.784,  744.026,  746.268, 748.51 ,  750.752,  752.993,  755.235,  757.477,  759.719, 761.961,  764.203,  766.444,  768.686,  770.928,  773.17, 775.412,  777.654,  779.895,  782.137,  784.379,  786.621, 788.863,  791.104,  793.346,  795.588,  797.83 ,  800.072, 802.314,  804.555,  806.797,  809.039,  811.281,  813.523, 815.765,  818.006,  820.248,  822.49,  824.732,  826.974, 829.216,  831.457,  833.699,  835.941,  838.183,  840.425, 842.667,  844.908,  847.15 ,  849.392,  851.634,  853.876, 856.118,  858.359,  860.601,  862.843,  865.085,  867.327, 869.569,  871.810,  874.052,  876.294,  878.536,  880.778, 883.02,  885.261,  887.503,  889.745,  891.987,  894.229, 896.471,  898.712,  900.954,  903.196,  905.438,  907.68, 909.922,  912.163,  914.405,  916.647,  918.889,  921.131, 923.373,  925.614,  927.856,  930.098,  932.34,  934.582, 936.824,  939.065,  941.307,  943.549,  945.791,  948.033, 950.275,  952.516,  954.758,  957.000,  959.242,  961.484, 963.726,  965.967,  968.209,  970.451,  972.693,  974.935, 977.176,  979.418,  981.66 ,  983.902,  986.144,  988.386, 990.627,  992.869,  995.111,  997.353,  999.595, 1001.84]\nnard = [35,89,175,148,144,240,56,79,259,372,100,142,78,148,179,296,259,320,302,379,244,315,370,344,313,351,323,374,356,387,179,416,352,368,287,372,318,440,399,424,311,411,386,399,455,471,463,432,476,435,461,492,498,573,544,593,636,639,658,722,730,692,623,756,764,861,748,860,801,839,791,911,811,791,850,743,730,780,827,783,821,812,785,766,847,784,655,776,831,817,807,850,762,735,817,876,753,740,814,764,750,806,772,735,833,766,767,822,813,782,771,769,766,742,763,752,712,707,781,698,750,708,793,750,645,700,725,726,705,733,820,983,956,1077,1180,1260,1368,1472,1589,1785,1754,1986,2134,2161,2319,2443,2679,2777,2857,3161,3321,3411,3294,3571,3737,3667,3720,3785,3972,4032,4015,4022,4206,3803,4038,3936,4087,4211,3884,4140,4168,4235,4112,4317,4203,4269,4204,4213,4179,3920,4280,4375,4378,4122,4443,4250,4299,4471,4405,4696,4477,4369,4371,4653,4583,4371,4549,4774,4706,4502,4404,4595,4705,4320,4650,4518,4580,4546,4050,4895,4720,4622,4554,4589,4852,4703,4959,4466,4428,4921,4786,4868,5108,4423,4182,4955,4489,4430,4510,4957,5157,4860,4576,4449,4666,4369,4246,5029,4475,5315,3847,4223,3980,4564,4572,5053,4402,2044,4636,3542,3114,3677,4427,4402,3938,3288,4747,4330,4251,3056,4705,3891,4861,4649,4394,5879,6237,5090,4054,5507]\nmol = [49,504,49,209,88,209,54,154,120,63,49,66,57,118,172,224,144,159,89,48,247,204,124,96,187,144,216,149,248,165,137,202,198,188,165,180,165,215,178,257,258,199,215,189,209,161,193,242,214,267,237,305,213,264,248,280,270,329,341,376,365,353,364,405,384,436,384,437,363,504,434,421,465,400,426,424,395,462,407,372,407,392,382,369,382,433,406,367,370,373,389,345,374,405,348,381,312,375,350,345,325,380,331,363,390,190,338,342,299,377,338,317,309,348,332,340,290,345,320,314,319,305,324,344,339,358,347,222,338,355,99,365,413,520,527,634,682,774,810,922,969,1062,1102,1240,1309,1426,1478,1717,1972,2051,2253,2455,2715,2668,2885,2945,3056,3172,3080,3226,3334,3354,3357,3419,3278,3501,3492,3457,3626,3580,3530,3557,3505,3675,3678,3921,3649,3663,3644,3870,4046,3647,3740,3834,3612,3786,3380,3685,3884,3880,3751,3994,3906,3655,3904,3793,4118,3851,3738,3951,3832,3925,4047,3784,3735,3808,4087,3972,3982,4212,3384,3950,3750,4034,4055,3990,4034,3782,4039,4050,4043,3789,4342,4453,4154,3946,4193,4209,4053,3966,4098,4022,4004,3179,4226,3941,4088,3378,2887,4226,3447,3169,4523,3868,2831,2365,3743,2578,3655,3009,3685,3534,3901,3794,3721,3771,4035,3424,3574,3510,2655,4081,4719,4640,2101,4043,3218,4709,2788,4661]\ndesch = [92,23,134,0,8,40,76,268,73,232,154,221,69,212,208,187,167,65,161,187,268,232,304,248,315,236,284,268,249,271,289,262,259,232,262,230,320,336,274,290,334,304,274,298,306,303,272,344,332,333,425,444,419,424,471,530,424,572,605,649,696,767,702,688,759,755,807,794,815,798,850,804,800,788,813,750,710,688,708,661,632,609,608,554,590,635,603,577,469,613,545,594,603,565,593,592,563,592,570,532,489,556,510,571,581,518,582,566,522,555,448,512,491,490,485,498,500,523,432,508,426,398,360,394,461,441,406,444,479,507,567,675,765,927,1078,1311,1354,1613,1774,1971,2219,2529,2504,2789,3280,3448,3641,3981,4162,4551,4624,4821,5019,5400,5406,5572,5846,5875,5942,6049,6022,5932,6135,6926,6645,6750,6592,6425,6366,6809,6621,6641,6475,6547,6629,6624,6558,6718,6651,6661,6652,6871,6535,6915,6974,6958,6864,7112,6885,7052,6599,7346,7074,7000,6761,6956,6876,7124,7077,7256,7051,6876,7079,7044,7302,6761,7018,7194,6921,7278,6861,7094,7019,7127,7469,7148,7482,7122,7450,7582,7144,7166,7455,7364,7497,7394,7421,7291,7664,7766,7199,7365,7882,7249,7930,7101,7926,7840,8597,7614,8665,9367,8057,9395,8450,7926,8879,7814,7643,7640,8548,7259,8261,7419,7598,4786,6974,7634,7155,6286,7161,7468,7527,7244,1946,5800,8092,7010,6707,7435]\ncv = [0,0,7,0,333,0,0,58,46,0,0,54,84,88,89,150,0,154,19,79,117,111,9,128,105,157,134,126,192,143,134,154,136,162,25,138,142,98,101,109,146,121,174,125,135,123,185,161,124,178,225,185,162,210,218,293,288,350,311,340,344,398,353,382,417,363,389,391,422,439,401,423,385,445,389,300,428,390,349,300,335,346,313,333,295,342,293,323,305,309,315,236,288,288,291,296,258,277,308,241,270,280,313,295,288,331,288,288,313,313,249,279,213,200,240,211,226,256,296,227,265,269,232,199,270,232,206,252,265,276,275,368,433,484,516,733,847,834,1071,1218,1307,1381,1648,1656,1951,2166,2329,2615,2743,3025,3206,3338,3750,3863,3843,4251,4202,4460,4349,4629,4594,4553,4730,4832,5026,5171,4829,5000,4993,4915,4992,5082,4828,4950,4943,5064,5075,5045,5168,5313,5145,5407,5307,5252,5195,5347,5199,5268,5213,5294,5476,5232,5458,5321,5558,5484,5254,5429,5653,5360,5238,5537,5685,5332,5172,5653,5486,5095,5475,5709,5731,5449,5396,5896,5696,5618,5543,5705,5191,5957,5446,5726,5328,5786,5854,5874,6487,6100,5867,6043,5880,5952,5987,5745,5991,6488,6134,6077,6427,6496,6366,7060,7820,4759,7637,5966,6328,6091,7054,6127,6339,6414,5327,6231,5501,5661,5646,5545,5629,5692,4952,5928,5432,5232,5477,5735,6299,6342,5032,5080]\n</pre> wavelength=[398.784,  401.026,  403.268,  405.51,  407.751,  409.993, 412.235,  414.477,  416.719, 418.961,  421.202,  423.444, 425.686,  427.928,  430.17 ,  432.411,  434.653,  436.895, 439.137,  441.379,  443.621,  445.862,  448.104,  450.346, 452.588,  454.83 ,  457.072,  459.313,  461.555,  463.797, 466.039,  468.281,  470.523,  472.764,  475.006,  477.248, 479.49,  481.732,  483.974,  486.215,  488.457,  490.699, 492.941,  495.183,  497.425,  499.666,  501.908,  504.15, 506.392,  508.634,  510.876,  513.117,  515.359,  517.601, 519.843,  522.085,  524.327,  526.568,  528.81 ,  531.052, 533.294,  535.536,  537.778,  540.019,  542.261,  544.503, 546.745,  548.987,  551.229,  553.47 ,  555.712,  557.954, 560.196,  562.438,  564.68,  566.921,  569.163,  571.405, 573.647,  575.889,  578.131,  580.372,  582.614,  584.856, 587.098,  589.34 ,  591.582,  593.823,  596.065,  598.307, 600.549,  602.791,  605.032,  607.274,  609.516,  611.758, 614.000,  616.242,  618.483,  620.725,  622.967,  625.209, 627.451,  629.693,  631.934,  634.176,  636.418,  638.66 , 640.902,  643.144,  645.385,  647.627,  649.869,  652.111, 654.353,  656.595,  658.836,  661.078,  663.32,  665.562, 667.804,  670.046,  672.287,  674.529,  676.771,  679.013, 681.255,  683.497,  685.738,  687.98,  690.222,  692.464, 694.706,  696.948,  699.189,  701.431,  703.673,  705.915, 708.157,  710.399,  712.64,  714.882,  717.124,  719.366, 721.608,  723.85,  726.091,  728.333,  730.575,  732.817, 735.059,  737.301,  739.542,  741.784,  744.026,  746.268, 748.51 ,  750.752,  752.993,  755.235,  757.477,  759.719, 761.961,  764.203,  766.444,  768.686,  770.928,  773.17, 775.412,  777.654,  779.895,  782.137,  784.379,  786.621, 788.863,  791.104,  793.346,  795.588,  797.83 ,  800.072, 802.314,  804.555,  806.797,  809.039,  811.281,  813.523, 815.765,  818.006,  820.248,  822.49,  824.732,  826.974, 829.216,  831.457,  833.699,  835.941,  838.183,  840.425, 842.667,  844.908,  847.15 ,  849.392,  851.634,  853.876, 856.118,  858.359,  860.601,  862.843,  865.085,  867.327, 869.569,  871.810,  874.052,  876.294,  878.536,  880.778, 883.02,  885.261,  887.503,  889.745,  891.987,  894.229, 896.471,  898.712,  900.954,  903.196,  905.438,  907.68, 909.922,  912.163,  914.405,  916.647,  918.889,  921.131, 923.373,  925.614,  927.856,  930.098,  932.34,  934.582, 936.824,  939.065,  941.307,  943.549,  945.791,  948.033, 950.275,  952.516,  954.758,  957.000,  959.242,  961.484, 963.726,  965.967,  968.209,  970.451,  972.693,  974.935, 977.176,  979.418,  981.66 ,  983.902,  986.144,  988.386, 990.627,  992.869,  995.111,  997.353,  999.595, 1001.84] nard = [35,89,175,148,144,240,56,79,259,372,100,142,78,148,179,296,259,320,302,379,244,315,370,344,313,351,323,374,356,387,179,416,352,368,287,372,318,440,399,424,311,411,386,399,455,471,463,432,476,435,461,492,498,573,544,593,636,639,658,722,730,692,623,756,764,861,748,860,801,839,791,911,811,791,850,743,730,780,827,783,821,812,785,766,847,784,655,776,831,817,807,850,762,735,817,876,753,740,814,764,750,806,772,735,833,766,767,822,813,782,771,769,766,742,763,752,712,707,781,698,750,708,793,750,645,700,725,726,705,733,820,983,956,1077,1180,1260,1368,1472,1589,1785,1754,1986,2134,2161,2319,2443,2679,2777,2857,3161,3321,3411,3294,3571,3737,3667,3720,3785,3972,4032,4015,4022,4206,3803,4038,3936,4087,4211,3884,4140,4168,4235,4112,4317,4203,4269,4204,4213,4179,3920,4280,4375,4378,4122,4443,4250,4299,4471,4405,4696,4477,4369,4371,4653,4583,4371,4549,4774,4706,4502,4404,4595,4705,4320,4650,4518,4580,4546,4050,4895,4720,4622,4554,4589,4852,4703,4959,4466,4428,4921,4786,4868,5108,4423,4182,4955,4489,4430,4510,4957,5157,4860,4576,4449,4666,4369,4246,5029,4475,5315,3847,4223,3980,4564,4572,5053,4402,2044,4636,3542,3114,3677,4427,4402,3938,3288,4747,4330,4251,3056,4705,3891,4861,4649,4394,5879,6237,5090,4054,5507] mol = [49,504,49,209,88,209,54,154,120,63,49,66,57,118,172,224,144,159,89,48,247,204,124,96,187,144,216,149,248,165,137,202,198,188,165,180,165,215,178,257,258,199,215,189,209,161,193,242,214,267,237,305,213,264,248,280,270,329,341,376,365,353,364,405,384,436,384,437,363,504,434,421,465,400,426,424,395,462,407,372,407,392,382,369,382,433,406,367,370,373,389,345,374,405,348,381,312,375,350,345,325,380,331,363,390,190,338,342,299,377,338,317,309,348,332,340,290,345,320,314,319,305,324,344,339,358,347,222,338,355,99,365,413,520,527,634,682,774,810,922,969,1062,1102,1240,1309,1426,1478,1717,1972,2051,2253,2455,2715,2668,2885,2945,3056,3172,3080,3226,3334,3354,3357,3419,3278,3501,3492,3457,3626,3580,3530,3557,3505,3675,3678,3921,3649,3663,3644,3870,4046,3647,3740,3834,3612,3786,3380,3685,3884,3880,3751,3994,3906,3655,3904,3793,4118,3851,3738,3951,3832,3925,4047,3784,3735,3808,4087,3972,3982,4212,3384,3950,3750,4034,4055,3990,4034,3782,4039,4050,4043,3789,4342,4453,4154,3946,4193,4209,4053,3966,4098,4022,4004,3179,4226,3941,4088,3378,2887,4226,3447,3169,4523,3868,2831,2365,3743,2578,3655,3009,3685,3534,3901,3794,3721,3771,4035,3424,3574,3510,2655,4081,4719,4640,2101,4043,3218,4709,2788,4661] desch = [92,23,134,0,8,40,76,268,73,232,154,221,69,212,208,187,167,65,161,187,268,232,304,248,315,236,284,268,249,271,289,262,259,232,262,230,320,336,274,290,334,304,274,298,306,303,272,344,332,333,425,444,419,424,471,530,424,572,605,649,696,767,702,688,759,755,807,794,815,798,850,804,800,788,813,750,710,688,708,661,632,609,608,554,590,635,603,577,469,613,545,594,603,565,593,592,563,592,570,532,489,556,510,571,581,518,582,566,522,555,448,512,491,490,485,498,500,523,432,508,426,398,360,394,461,441,406,444,479,507,567,675,765,927,1078,1311,1354,1613,1774,1971,2219,2529,2504,2789,3280,3448,3641,3981,4162,4551,4624,4821,5019,5400,5406,5572,5846,5875,5942,6049,6022,5932,6135,6926,6645,6750,6592,6425,6366,6809,6621,6641,6475,6547,6629,6624,6558,6718,6651,6661,6652,6871,6535,6915,6974,6958,6864,7112,6885,7052,6599,7346,7074,7000,6761,6956,6876,7124,7077,7256,7051,6876,7079,7044,7302,6761,7018,7194,6921,7278,6861,7094,7019,7127,7469,7148,7482,7122,7450,7582,7144,7166,7455,7364,7497,7394,7421,7291,7664,7766,7199,7365,7882,7249,7930,7101,7926,7840,8597,7614,8665,9367,8057,9395,8450,7926,8879,7814,7643,7640,8548,7259,8261,7419,7598,4786,6974,7634,7155,6286,7161,7468,7527,7244,1946,5800,8092,7010,6707,7435] cv = [0,0,7,0,333,0,0,58,46,0,0,54,84,88,89,150,0,154,19,79,117,111,9,128,105,157,134,126,192,143,134,154,136,162,25,138,142,98,101,109,146,121,174,125,135,123,185,161,124,178,225,185,162,210,218,293,288,350,311,340,344,398,353,382,417,363,389,391,422,439,401,423,385,445,389,300,428,390,349,300,335,346,313,333,295,342,293,323,305,309,315,236,288,288,291,296,258,277,308,241,270,280,313,295,288,331,288,288,313,313,249,279,213,200,240,211,226,256,296,227,265,269,232,199,270,232,206,252,265,276,275,368,433,484,516,733,847,834,1071,1218,1307,1381,1648,1656,1951,2166,2329,2615,2743,3025,3206,3338,3750,3863,3843,4251,4202,4460,4349,4629,4594,4553,4730,4832,5026,5171,4829,5000,4993,4915,4992,5082,4828,4950,4943,5064,5075,5045,5168,5313,5145,5407,5307,5252,5195,5347,5199,5268,5213,5294,5476,5232,5458,5321,5558,5484,5254,5429,5653,5360,5238,5537,5685,5332,5172,5653,5486,5095,5475,5709,5731,5449,5396,5896,5696,5618,5543,5705,5191,5957,5446,5726,5328,5786,5854,5874,6487,6100,5867,6043,5880,5952,5987,5745,5991,6488,6134,6077,6427,6496,6366,7060,7820,4759,7637,5966,6328,6091,7054,6127,6339,6414,5327,6231,5501,5661,5646,5545,5629,5692,4952,5928,5432,5232,5477,5735,6299,6342,5032,5080] <p>In this tutorial, we are going to work with the reflectance values (multiplied by 10 000) for the grass nard (Nardus stricta). But feel free to play around and change the variable to a different species to observe the various spectra.</p> In\u00a0[9]: Copied! <pre>reflectance = nard\n</pre> reflectance = nard In\u00a0[10]: Copied! <pre>if len(wavelength) == len(reflectance):\n    pass\nelse:\n    print(\"The number of wavelengths is not equivalent to the number of reflectance values\")\n</pre> if len(wavelength) == len(reflectance):     pass else:     print(\"The number of wavelengths is not equivalent to the number of reflectance values\") <p>Next, we are going to visualize the spectral curve extracted from the hyperspectral image, with wavelength on the x axis and reflectance values on the y axis.</p> In\u00a0[12]: Copied! <pre>mp.figure(figsize=(8,4))\nmp.xlabel(\"wavelength [nm]\")\nmp.ylabel(\"reflectance*10000\")\nmp.plot(wavelength, reflectance)\n</pre> mp.figure(figsize=(8,4)) mp.xlabel(\"wavelength [nm]\") mp.ylabel(\"reflectance*10000\") mp.plot(wavelength, reflectance) Out[12]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f48e24b9e50&gt;]</pre> <p>Savitzky and Golay proposed a method of data smoothing based on local least-squares polynomial approximation. They showed that fitting a polynomial to a set of input samples and then evaluating the resulting polynomial at a single point within the approximation interval is equivalent to discrete convolution with a fixed impulse response. The lowpass filters obtained by this method reduce noise while maintaining the shape and height of waveform peaks. (Schafer, 2011).</p> <p>In our case, the filter is applied to the reflectance values, and the parameters of the used polynomial order and window length can be adjusted.</p> In\u00a0[15]: Copied! <pre>filtered_data=sp.signal.savgol_filter(reflectance, window_length=10, polyorder=3, mode=\"nearest\")\n</pre> filtered_data=sp.signal.savgol_filter(reflectance, window_length=10, polyorder=3, mode=\"nearest\") <p>We are going to visualize both the raw spectral curve extracted from the hyperspectral image and the filtered spectral curve.</p> In\u00a0[17]: Copied! <pre>mp.figure(figsize=(8,4))\nmp.xlabel(\"wavelength [nm]\")\nmp.ylabel(\"reflectance*10000\")\nmp.plot(wavelength, reflectance, label=\"raw\")\nmp.plot(wavelength, filtered_data, linewidth=2, label=\"filtered\")\nmp.legend()\n</pre> mp.figure(figsize=(8,4)) mp.xlabel(\"wavelength [nm]\") mp.ylabel(\"reflectance*10000\") mp.plot(wavelength, reflectance, label=\"raw\") mp.plot(wavelength, filtered_data, linewidth=2, label=\"filtered\") mp.legend() Out[17]: <pre>&lt;matplotlib.legend.Legend at 0x7f48d7b39f50&gt;</pre> <p>Observe the effect of changing the polyorder.</p> In\u00a0[19]: Copied! <pre>polyorder=[2,5,8]\nfor num in polyorder:\n    mp.figure(figsize=(6,3))\n    mp.xlabel(\"wavelength [nm]\")\n    mp.ylabel(\"reflectance*10000\")\n    filtered_data=sp.signal.savgol_filter(reflectance, window_length=10, polyorder=num, mode=\"nearest\")\n    mp.plot(wavelength, filtered_data, linewidth=1, color='green')\n    mp.title(\"Polyorder: \" + str(num))\n</pre> polyorder=[2,5,8] for num in polyorder:     mp.figure(figsize=(6,3))     mp.xlabel(\"wavelength [nm]\")     mp.ylabel(\"reflectance*10000\")     filtered_data=sp.signal.savgol_filter(reflectance, window_length=10, polyorder=num, mode=\"nearest\")     mp.plot(wavelength, filtered_data, linewidth=1, color='green')     mp.title(\"Polyorder: \" + str(num)) <p>Observe the effect of changing the window length.</p> In\u00a0[21]: Copied! <pre>window_length=[10,20,30]\nfor num in window_length:\n    mp.figure(figsize=(6,3))\n    mp.xlabel(\"wavelength [nm]\")\n    mp.ylabel(\"reflectance*10000\")\n    filtered_data=sp.signal.savgol_filter(reflectance, window_length=num, polyorder=3, mode=\"nearest\")\n    mp.plot(wavelength, filtered_data, linewidth=1, color='green')\n    mp.title(\"Window length: \" + str(num))\n</pre> window_length=[10,20,30] for num in window_length:     mp.figure(figsize=(6,3))     mp.xlabel(\"wavelength [nm]\")     mp.ylabel(\"reflectance*10000\")     filtered_data=sp.signal.savgol_filter(reflectance, window_length=num, polyorder=3, mode=\"nearest\")     mp.plot(wavelength, filtered_data, linewidth=1, color='green')     mp.title(\"Window length: \" + str(num)) <p>A moving average (rolling average or running average) is a calculation to analyze data points by creating a series of averages of different selections of the full data set. Variations include: simple, cumulative, or weighted forms. It is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles. However, when used with non-time series data, a moving average filters higher frequency components without any specific connection to time, although typically some kind of ordering is implied. Mathematically, a moving average is a type of convolution and so it can be viewed as an example of a lowpass filter used in signal processing. (Wiki contributors, 2023)</p> <p>We are going to apply a simple moving average filter to the input reflectance values. The average is represented by the median, and the local window size is given by kernel_size. Elements of kernel_size should be odd.</p> In\u00a0[24]: Copied! <pre>filtered_data=sp.signal.medfilt(reflectance, kernel_size=3)\n</pre> filtered_data=sp.signal.medfilt(reflectance, kernel_size=3) <p>Both the raw spectral curve extracted from the hyperspectral image and the filtered spectra are visualized.</p> In\u00a0[26]: Copied! <pre>mp.figure(figsize=(8,4))\nmp.xlabel(\"wavelength [nm]\")\nmp.ylabel(\"reflectance*10000\")\nmp.plot(wavelength, reflectance, label=\"raw\")\nmp.plot(wavelength, filtered_data, linewidth=2, label=\"filtered\")\nmp.legend()\n</pre> mp.figure(figsize=(8,4)) mp.xlabel(\"wavelength [nm]\") mp.ylabel(\"reflectance*10000\") mp.plot(wavelength, reflectance, label=\"raw\") mp.plot(wavelength, filtered_data, linewidth=2, label=\"filtered\") mp.legend() Out[26]: <pre>&lt;matplotlib.legend.Legend at 0x7f48d70010d0&gt;</pre> <p>Observe the effect of changing the window (kernel) size.</p> In\u00a0[28]: Copied! <pre>window_size=[3,13,33]\nfor num in window_size:\n    mp.figure(figsize=(6,3))\n    mp.xlabel(\"wavelength [nm]\")\n    mp.ylabel(\"reflectance*10000\")\n    filtered_data=sp.signal.medfilt(reflectance, kernel_size=num)\n    mp.plot(wavelength, filtered_data, linewidth=1, color='green')\n    mp.title(\"Window size: \" + str(num))\n</pre> window_size=[3,13,33] for num in window_size:     mp.figure(figsize=(6,3))     mp.xlabel(\"wavelength [nm]\")     mp.ylabel(\"reflectance*10000\")     filtered_data=sp.signal.medfilt(reflectance, kernel_size=num)     mp.plot(wavelength, filtered_data, linewidth=1, color='green')     mp.title(\"Window size: \" + str(num))"},{"location":"module4/02_aerial_acquisition_preprocessing/filtering_spectral_curve.html#tutorial-spectra-smoothening-and-denoising","title":"Tutorial: Spectra smoothening and denoising\u00b6","text":""},{"location":"module4/02_aerial_acquisition_preprocessing/filtering_spectral_curve.html#savitzky-golay-filter","title":"Savitzky-Golay filter\u00b6","text":""},{"location":"module4/02_aerial_acquisition_preprocessing/filtering_spectral_curve.html#simple-moving-average-filter","title":"Simple Moving Average filter\u00b6","text":""},{"location":"module4/02_aerial_acquisition_preprocessing/filtering_spectral_curve.html#references","title":"References\u00b6","text":"<p>Schafer, R. (2011). \u201cWhat Is a Savitzky-Golay Filter? [Lecture Notes].\u201d IEEE Signal Processing Magazine 28, no. 4, 111\u201317. 10.1109/MSP.2011.941097.</p> <p>Wikipedia contributors. (2023). Moving average. In Wikipedia, The Free Encyclopedia. en.wikipedia.org/wiki/Moving_average</p>"},{"location":"module4/02_aerial_acquisition_preprocessing/solution/02_aerial_acquisition_preprocessing_exercise_geometric_solution.html","title":"E-TRAINEE: Subpixel classification - report","text":""},{"location":"module4/02_aerial_acquisition_preprocessing/solution/02_aerial_acquisition_preprocessing_exercise_geometric_solution.html#report","title":"Report","text":""},{"location":"module4/02_aerial_acquisition_preprocessing/solution/02_aerial_acquisition_preprocessing_exercise_geometric_solution.html#data-preparation-relative-and-absolute-accuracy","title":"Data preparation, relative and absolute accuracy","text":"<p>The provided hyperspectral image strips were loaded and visualized in the QGIS environment, along with the positions of the checkpoints. The difference between the XY coordinates of the checkpoints in the image strips and in the shapefile was determined.  In addition, four identical points were found in strip (line) pairs 2 and 3, 3 and 4, and the difference between their XY coordinates was analyzed. The horizontal shift, standard deviation, and Root Mean Square Error (RMSE) from both relative and absolute residuals were computed.</p> \"Raw\" image data Absolute accuracy shift [m] standard deviation [m] RMSE [m] Relative accuracy shift [m] standard deviation [m] RMSE [m] strip (line) 2 0.35 0.04 0.35 strip (line) 2 and 3 2.01 0.32 2.03 strip (line) 3 1.28 0.51 1.71 strip (line) 3 and 4 1.50 0.12 1.50 strip (line) 4 0.39 0.08 0.40"},{"location":"module4/02_aerial_acquisition_preprocessing/solution/02_aerial_acquisition_preprocessing_exercise_geometric_solution.html#image-registration","title":"Image registration","text":"<p>The orthorectified RGB image with a defined CRS (EPSG:5514) was loaded and used as a base for coregistration and orthorectification of the hyperspectral image strips.  In the \"Georeferencer\" app, at least five identical points were evenly collected throughout the area. The \"Transformation type\" Polynomial 1 (1st order polynomial) and Thin plate spline were tested.  As for the \"Resampling method\", Nearest neighbour was used. The procedure was repeated for each image strip (line). </p> <p> Geometrically corrected hyperspectral image strips - affine transformation <p> </p> Geometrically corrected hyperspectral image strips - spline transformation   The residuals on the GCPs after the transformation were found in the generated PDF report, and their mean was computed.  GCP residuals  |   |                               |   |                       | |:--------------:|:-----------------------------:|:---:|:---------------------:| | **mean [m]**       | 1st polynomial transformation |   | spline transformation | | strip (line) 2 | 0.04                          |   | 0.00                  | | strip (line) 3 | 0.03                          |   | 0.00                  | | strip (line) 4 | 0.01                          |   | 0.00                  |  ##### Accuracy assessment  The relative and absolute accuracy of the orthorectified hyperspectral image strips was assessed using the same procedure as before.   1st polynomial transformation  |                   |           |                        |          |   |                      |           |                        |          | |:-----------------:|:---------:|:----------------------:|:--------:|:---:|:--------------------:|:---------:|:----------------------:|:--------:| | **Absolute accuracy** | shift [m] | standard deviation [m] | RMSE [m] |   | **Relative accuracy**   | shift [m] | standard deviation [m] | RMSE [m] | | strip (line) 2    | 0.08      | 0.05                   | 0.09     |   | strip (line) 2 and 3 | 0.13      | 0.08                   | 0.14     | | strip (line) 3    | 0.06      | 0.03                   | 0.07     |   | strip (line) 3 and 4 | 0.12      | 0.08                   | 0.14     | | strip (line) 4    | 0.04      | 0.02                   | 0.04     |  Spline transformation  |                   |           |                        |          |   |                      |           |                        |          | |:-----------------:|:---------:|:----------------------:|:--------:|:---:|:--------------------:|:---------:|:----------------------:|:--------:| | **Absolute accuracy** | shift [m] | standard deviation [m] | RMSE [m] |   | **Relative accuracy**    | shift [m] | standard deviation [m] | RMSE [m] | | strip (line) 2    | 0.09      | 0.05                   | 0.11     |   | strip (line) 2 and 3 | 0.13      | 0.04                   | 0.14     | | strip (line) 3    | 0.07      | 0.04                   | 0.08     |   | strip (line) 3 and 4 | 0.12      | 0.07                   | 0.13     | | strip (line) 4    | 0.05      | 0.01                   | 0.06     |    ##### Q&amp;A   * Evaluate the absolute and relative accuracy after geometric correction. What is the maximum error? Is the resulting accuracy sufficient?       + using 1st polynomial transformation the shift values reach 0.04-0.08 m with a maximum standard deviation of 0.05 m; shift between the individual strips pairs is 0.12-0.13 m     + using spline transformation the shift values reach 0.05-0.09 m with a maximum standard deviation of 0.05 m; shift between the individual strips pairs is 0.12-0.13 m     + the maximum RMSE on check points is 0.11 m for strip (line) 2 after spline transformation      + resulting accuracy is sufficient for most applications * Compare the \u201craw\u201d image strips with the geometrically corrected (orthorectified) image strips in terms of computed accuracy and visual inspection.       + in terms of absolute accuracy, the shift values decreased from 0.35-1.28 m to sub 0.1 m     + in terms of relative accuracy, the shift values decreased from meter to sub 0.15 m      + visually the shifts and deformations of the overlapping strips are significantly reduced * Are there any differences between the results of the spline and polynomial transformations?       + residuals on the GCPs after spline transformation are 0.00 m as local deformations are introduced to the image     + residuals on the GCPs after 1st polynomial transformation are up to 0.04 m      + however, relative and absolute accuracy assessed on the check points reaches similar values     + visually the orthorectified hyperspectral image strips do not differ significantly   * Comment on the used resampling method: what does \u201cNearest neighbour\u201d ensure? Would using a different resampling type affect the image, and if yes, then how?       + value to a cell/pixel in the output raster is assigned from the closest input cell/pixel     + original input pixel/cell values are retained     + different resampling methods such as bilinear interpolation or cubic convolution introduce new pixel/cell values  ### Back to theme  Proceed by returning to [Aerial/RPAS hyperspectral data acquisition and pre-processing](../02_aerial_acquisition_preprocessing.md)"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html","title":"Optical parameters of foliage \u2013 leaf level","text":"<p>Plant functional traits at the leaf level are commonly used to predict ecosystem responses to environmental factors. Plant functional traits include both leaf biophysical traits (e.g., photosynthetic pigment content and water content) and structural traits (e.g., leaf thickness and proportion of photosynthetic and non-photosynthetic tissues). Optical parameters of foliage \u2012 reflectance, transmittance and absorbance \u2012 are determined by leaf biophysical and structural traits, which can be detected either destructively in the laboratory or non-destructively using leaf optical properties. Estimating chlorophyll and water content from leaf optical properties is a well-established methodology.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#objectives","title":"Objectives","text":"<p>In this theme, you will learn about:</p> <ul> <li>leaf structure and biophysical properties and how they determine foliage spectral reflectance curves</li> <li>identifying characteristics of foliage spectral reflectance curves and interpreting data from them</li> </ul> <p>After finishing this theme you will be able to:</p> <ul> <li>read and understand the spectral reflectance curve of foliage</li> </ul>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#introduction-leaf-structure-and-biophysical-properties","title":"Introduction: leaf structure and biophysical properties","text":"<p>The plant leaf is a complex organ performing a variety of physiological functions. For simplicity, let us call \u201cleaf biophysical properties\u201d the complex of all chemical elements (e.g., nitrogen, carbon), biochemical compounds (e.g., chlorophyll a+b, carotenoids, anthocyanins), proteins, structural biopolymers (e.g., lignin, cellulose) of which the leaf is composed, as well as the way the leaf is built in terms of its anatomy (leaf internal structure) saturated by air and water (Asner, 1998). All these leaf biophysical properties naturally influence leaf optical properties - reflectance, absorbance, and transmittance - together with the contribution of leaf physiological status and phenology.  It should be noted that most plants have flat, dorsiventral leaves (Figure 1A ). This is especially true of trees, thus gaining the name \u201cbroadleaved trees\u201d. By contrast, coniferous trees have evergreen needle-like leaves (Figure 1B ), which are not shed in the autumn and are maintained by the plant for several seasons. Monocotyledonous plants (e.g., grasses) have yet another type of leaf \u2014 long and narrow with bilateral symmetry (Figure 1C ). Leaf morphology, structure, and size must be taken into account when designing measurements of leaf optical properties by laboratory spectroscopy.</p> <p> </p> <p>Figure 1. Leaf morphology and size of three most common leaf shapes. A) flat, dorsiventral leaves of the majority of broadleaved trees, some herbs. B) needle-like leaves of coniferous trees (attached at certain angles to the twig); C) grass leaves, long and narrow with bilateral symmetry.</p> <p>The leaf blade is not always homogeneous in terms of structure and pigment content. Heterogeneity in function, cellular structure, and pigment composition mainly affects photosynthetic function and can be explained by several factors: a) leaf developmental stage or ontogenetic phase corresponding to leaf phenology, leaf senescence; b) the natural appearance of leaf colour determined not only by photosynthetic pigments but also by photoprotective compounds (Figure 2 ). Change in ratios of photosynthetic pigments (chlorophylls) and photoprotective pigments (carotenoids, xanthophylls, anthocyanins) is most visible during leaf senescence (Figure 2, DOY 294 ). Variegated leaves naturally display patches of lighter or darker greens (corresponding to higher and lower chlorophyll concentrations).</p> <p> </p> <p>Figure 2. Variation in leaf pigmentation due to phenology and leaf variegation. Phenology of two common broadleaved temperate tree species - Acer platanoides (Norway maple) and Carpinus betulus (Common hornbeam).  DOY = day of the year. DOY 119 corresponds to juvenile spring leaves, DOY 203 represents mature summer leaves, DOY 294 corresponds to leaf senescence. Variegated leaves of Ficus benjamina (left and central ones) and Tradescantia zebrina (right).</p> <p>Phenology refers to the normal progression of plants in temperate regions through developmental stages during the vegetative season.  Bud dormancy breaks in early spring, then there is budburst, leaf primordia develop into juvenile leaves, then mature leaves function during the majority of vegetation season until the autumn when leaf senescence begins and plants transition into dormancy. Different tree and herb species can have different timing of phenological events what can be used for monitoring plant community during vegetation season. Timing of phenological events is crucial for estimation of vegetation functioning, role of vegetation in carbon cycle. There different online tools for monitoring leaf phenology (e.g., U.S.A. https://usanpn.org/; Europe https://www.eea.europa.eu/data-and-maps/indicators/plant-phenology; Czech republic https://www.fenofaze.cz/cz/). </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#radiation-used-by-plants-on-a-leaf-level-leaf-optical-properties","title":"Radiation used by plants on a leaf level: leaf optical properties.","text":"<p>Leaf optical properties are determined by the fraction of incident electromagnetic radiation that is absorbed (absorption), reflected (reflectance), or transmitted (transmittance) through the leaf (Figure 3 ).  Visible light (400-700 nm) is the most familiar range of electromagnetic radiation, and overlaps with the so-called \u201cphotosynthetically active radiation\u201d (PAR), however other regions of electromagnetic radiation can also be useful in remote sensing. How can leaf optical properties be used to generate data? If we know what wavelengths a particular leaf compound absorbs (i.e., it\u2019s absorption maximum), then we can detect that compound based on the intensity of light reflected from the leaf in the same wavelength/spectral range. For example, chlorophyll a demonstrates absorption maxima at 642 and 392 nm; therefore, higher absorbances at these wavelengths correspond to higher concentrations of chlorophyll a. Light that is not absorbed may be reflected. In the case of chlorophyll, blue and red light are absorbed, leaving green light to be reflected. Not all radiation is absorbed or reflected; light also passes through leaves \u2012 this is called transmittance.</p> <p> </p> <p>Figure 3. Fate of radiation falling on a leaf and its reaction with leaf structure, leaf optical properties depicted on scheme of leaf cross section. Most radiation is absorbed (absorption), particularly in visible region for photosynthetic processes, so called photosynthetically active radiation (PAR). Part of the radiation is reflected (reflectance) and remaining, minor part is transmitted (transmittance). Regarding reflectance, there can be two components of reflectance \u2013 reflectance determined by internal leaf structure (diffuse) and specular reflectance determined by leaf surface structure, see subchapter 2 below for explanation.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#leaf-surface-reflectance-mirror-like-or-specular-reflectance","title":"Leaf surface reflectance: mirror-like or specular reflectance","text":"<p>The earlier assumption that the leaf is a Lambertian reflector (i.e., that it reflects light equally from all angles), has been rejected by many studies, including Gates et al. (1965) and Grant (1987). Nevertheless, there is an amount of the light that is reflected from the leaf surface rather than by the leaf internal structure and its biophysical components. Leaf surface reflectance is controlled by two mechanisms: 1) specular (mirror-like) reflection (Figure 3), in which the angles of incidence and reflection of light are equal  (Vanderbilt et al., 1985), and 2) surface particle scattering, which depends on surface roughness and is overlooked in most plant studies (Grant et al., 1993). The leaf surface is formed by the epidermis, covered by a cuticle and sometimes also hairs (trichomes). In the case of the cuticle Grant et al., (1987) observed that the specular reflectance is completely polarized at 55\u00b0, partially polarized at other angles, and appears white. Trichomes on the epidermis also influence the specular (mirror-like) reflection of the leaf (Grant et al., 1993). </p> <p>Leaf specular reflectance is sometimes considered a potential source of error in the non-destructive estimation of leaf biochemical parameters (Bell and Curran, 1992; Li et al., 2019) and there are difficulties in estimating chlorophyll content in plants with extremely high surface reflectance (Bousquet et al., 2005). However, specular reflectance alone can provide information about the leaf surface (McClendon, 1984; Neuwirthov\u00e1 et al., 2021b) and may be useful for improving RTMs (radiative transfer models) using structural traits (Qiu et al., 2019) - such as \u201cleaf roughness\u201d as a parameter, e.g., the DLM (dorsiventral leaf model) (Stuckens et al., 2009a).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#spectral-reflectance-curve-of-vegetation","title":"Spectral reflectance curve of vegetation","text":"<p>The utilization of light energy for plant physiological processes has long been a topic of interest (Gates et al., 1965; Shull, 1929). Light interacts with various biochemical molecules in plant leaves (notably pigments, although other cellular contents as well), producing measurable absorption spectra, referred to as a spectral signature, i.e., spectral reflectance curve.  Major leaf pigments in plants include chlorophylls, carotenoids, and anthocyanins. Chlorophyll pigments absorb mainly in the blue and red spectral areas (Grant, 1997), while the green spectral area is reflected, resulting in the characteristic green colour of most plants. Carotenoids absorb in the blue and green spectral areas, resulting in yellow-orange reflectance. The specific ranges absorbed by chlorophyll a+b (green) and carotenoids (yellow to orange) can be observed in Figure 4. Anthocyanins also absorb in the blue-green spectral areas, but typically reflect red or purple light.</p> <p> </p> <p>Figure 4. Absorbance of leaf extracts in dimethyl formamide measured by spectrophotometer from different parts of variegated Ficus benjamina leaves (A, B). Differences in absorbance spectra of chlorophyll a, chlorophyll b, and \u03b2-carotene in organic solvent (thin black, green, and orange lines, respectively) and absorbance spectrum of a green plant when the pigments are localized in plastid membranes within mesophyll cells (bold line) (C). (Figure 4C modified from American Society of Plant Biologists 2015 / CC BY-NC 2.0)</p> <p>Other chemical compounds in the leaf also have their own absorption features (reviewed in detail by Curran (1989)). For example, distinctive absorption features in the spectral reflectance curve are demonstrated by proteins, as well as lignin and cellulose (Serrano et al., 2002) and water (Eitel et al., 2006a) (Figure 5 ).  Due to the absorption features of the leaf biophysical components mentioned above, the most commonly studied ranges of electromagnetic spectrum in connection to vegetation are: visible (VIS; (Niglas et al., 2017)) trough near-infrared region (NIR; (Slaton et al., 2001))  to short wave infrared (SWIR; (Cavender-Bares et al., 2016)) with occasional studies focusing on the thermal infrared region (TIR; (Gerber et al., 2011)). Features of the vegetation spectral reflectance curve of vegetation will be further explained in detail in the rest of this chapter.</p> <p> </p> <p>Figure 5. Spectral reflectance curve of vegetation.  Leaf biophysical properties listed above the reflectance curve - photosynthetic pigments, cellular structure, lignin and cellulose - correspond to driving factors determining its course in the range of 350-2500 nm. Red edge corresponds to a steep increase in reflectance on the margin of the red part of the visible spectrum.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#the-reflectance-in-the-visible-part-of-the-electromagnetic-spectrum-is-driven-mainly-by-photosynthetic-pigments","title":"The reflectance in the visible part of the electromagnetic spectrum is driven mainly by photosynthetic pigments","text":"<p>In spectroscopy studies, the visible part of the electromagnetic spectrum (VIS) is the region where the leaf optical properties correspond primarily to pigment content. Typically, the leaf reflectance curve in VIS can be described by a local minimum in the blue region (450-500 nm), a maximum in the green (540-560 nm) and then another minimum in the red (660-680 nm) (Figure 5 ). The decrease in reflectance corresponds to the absorption of chlorophyll: the maximum absorption of Chlorophyll a and Chlorophyll b is between 590-660 nm. Carotenoids have absorption maxima at 425, 450, and 480 nm (Gitelson and Merzlyak, 1994), (Figure 4 ). Anthocyanins have absorption maxima approximately within the interval 510-577 nm (extracted and depending on pH (Fossen et al., 1998). Both Carotenoids and anthocyanins significantly contribute to the change in leaf optical properties (Gitelson et al., 2009; Junker and Ensminger, 2016) during the chlorophyll degradation. These pigments are often seen as the red and orange colour in autumn leaves as they visibly accumulate during the nutrient resorption processes ahead of leaf senescence and tree dormancy in temperate regions (Hoch et al., 2003). These non-photosynthetic pigments have protective benefits for the plant, absorbing excess high energy light and acting as antioxidants (Gould, 2004; Maslova et al., 2021). Carotenoids and anthocyanins can be used as stress and senescence indicators (Junker and Ensminger, 2016) that can be detected non-destructively by optical signal (Gitelson et al., 2009). Leaf surface structure also contributes to reflectance in VIS (Buschmann et al., 2012a; Shull, 1929); For example, hairy and waxy leaves have been found to show greater total reflectance in the VIS compared to the same leaves after hairs or wax removal. Water content also indirectly influences VIS reflectance (Carter, 1991).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#the-red-edge-and-its-inflection-point-of-the-vegetation-spectral-curve-responds-to-stress-state-in-plants","title":"The red edge and its inflection point of the vegetation spectral curve responds to stress state in plants.","text":"<p>The sharp increase in reflectance between VIS and NIR is called the \u201cRed Edge\u201d (RE) and is usually defined by a wavelength range of 680-750 nm (Figure 5 ). The RE is directly related to the chlorophyll content of green leaves (Sims and Gamon, 2002). Specifically, the position of the inflection point of the spectral curve (the extreme of the first derivative of the spectral curve at given wavelengths) serves as an indicator of plant stress (Campbell et al., 2004; Gitelson et al., 1996). A shift of the RE position towards lower wavelengths is called a \u201cblue shift\u201d and corresponds with a worsened physiological status (Rock et al., 1988), whereas its shift towards longer wavelength is called a \u201cred shift\u201d and corresponds to an improved plant physiological status.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#the-reflectance-in-the-near-infra-red-region-is-affected-by-the-leaf-internal-structure","title":"The reflectance in the near infra-red region is affected by the leaf internal structure.","text":"<p>The internal structure of the leaf and the distribution of pigments affect the path of light in the leaf and, thus, determine the optical properties of the leaf. On one hand, the arrangement of leaf tissues and adaxial-abaxial polarity is regulated by a gene network (Conklin et al., 2019; Fukushima and Hasebe, 2014). Thus, leaf thickness (LT) is partly species-specific (Coste et al., 2010; Marenco et al., 2009). On the other hand, leaf anatomy, including LT, is influenced by many environmental factors such as: radiation intensity (Evans et al., 1994), and water availability (Aasamaa et al., 2005). Additional reading on leaf structure influencing its optical properties can be found in (Neuwirhtov\u00e1, 2022).</p> <p> </p> <p>Figure 6. Leaf internal structure shown as cross sections of representative leaf types. Outermost cell layer is the epidermis. Flat leaves (A and B) show an adaxial and abaxial epidermis while needles (C) show a surrounding epidermis. Below the epidermis is the mesophyll; this photosynthetic tissue is made up of cells containing chloroplasts with green chlorophyll pigment. LT = leaf thickness. Details about species and sample preparation: A) Dorsiventral flat leaf (Quercus robur) showing mesophyll differentiated into palisade and spongy. (Light microscopy, bright field, stained with toluidine blue. LT = 100 \u00b5m.) B) Grass leaf (Hordeum vulgare) with undifferentiated mesophyll. (Light microscopy, bright field, no staining. LT = 300 \u00b5m.) C) Coniferous needle (Pinus sylvestris) with undifferentiated mesophyll. (Light microscopy, bright field, stained with phloroglucinol to show lignified call walls (bright red). LT = 400 \u00b5m.)</p> <p>The three most common leaf types are: A) dorsiventral flat leaves, typical for dicotyledonous plants (i.e., deciduous trees), B) long narrow leaves, typical for monocotyledonous plants (i.e., grasses), and C) needle-like leaves, typical for gymnosperms (i.e., coniferous trees) (Figure 1 shows macroscopic view on leaves Figure 6 shows anatomical leaf cross sections). The most external cell layer on leaves is the epidermis, which functions to prevent water loss, protect the plant from excess light, and prevent biological invaders. Internally-adjacent to the epidermis is the mesophyll. Photosynthesis occurs in mesophyll, which is made up of cells with thin cell walls and chloroplasts. In grass leaves (Figure 6B ) and coniferous needles (Figure 6C ), the mesophyll is \u201cundifferentiated\u201d. In broad leaves (Figure 6A ), the mesophyll is differentiated into palisade (near to the top/adaxial side of the leaf) and spongey (near to the bottom/abaxial side of the leaf) parenchyma layers. Leaf optical properties in the NIR are determined by leaf thickness, adaxial and abaxial epidermal properties, and mesophyll architecture. These leaf traits vary depending on the leaf developmental stage (Rapaport et al., 2014) and phenology (i.e., when during the growing season the leaf is being measured)  (Yang et al., 2016), as well as environmental factors such as how much light the leaf is receiving. Leaves receiving more sunlight are known to be thicker than shaded leaves (Hanba et al., 2002); this creates a gradient in leaf structure within a canopy as different leaves receive different amount of light depending on their canopy position (Terashima et al., 2006). It is generally accepted that reflectance in the NIR (750-1350 nm) (Gates et al., 1965) is primarily influenced by the internal structure of the leaves (Figure 6 ) (Buschmann et al., 2012a; Slaton et al., 2001) and water content in leaf tissue, resulting in absorption maxima at approximately 970 and 1200 nm (Sims and Gamon, 2003) (Figure 5, 7C ). </p> <p> </p> <p>Figure 7. Variation in leaf internal structure, leaf thickness and leaf reflectance (examples of dicotyledonous dorsiventral flat leaves with differentiated palisade and spongy parenchyma). A) Anatomical micrographs of cross sections of tree and shrub species with macroscopic leaf photos placed in the left of the microphotograph of a cross section. B) Leaf thickness of measured samples in \u00b5m presented as standard boxplot graph, n = 6. C) Reflectance curves at the leaf level from 350 to 2500 nm for presented woody species in A and B.</p> <p>Epidermal and palisade cells (Figures 6A, 7A ) focus light: the columnar shape and arrangement of the palisade cells and chloroplasts inside of the cells affect light capturing and minimize light scattering within the leaves (Xiao et al., 2016). This enables light to penetrate deeper into the leaf where more chloroplasts are concentrated and intercellular air spaces scatter light and increase the likelihood of light absorption during photosynthesis (Vogelmann and Gorton, 2014).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#the-reflectance-in-the-short-wave-infra-red-is-driven-by-the-cell-structural-compounds-and-water-content","title":"The reflectance in the short-wave infra-red is driven by the cell structural compounds and water content","text":"<p>The spectral curve of vegetation, after its course in VIS and NIR, continues in the mid-infrared region (1350-2500 nm) (Gates et al., 1965). Currently it is referred to as the shortwave infrared region (SWIR). This spectral region is sometimes further subdivided into the SWIR1 (1500-1800 nm) and SWIR2 (2000-2400) regions (Cavender-Bares et al., 2016). Lignin and cellulose, two main cell structural compounds of the leaf, contribute to the reflectance in the SWIR region with specific absorption properties (Serrano et al., 2002) (Figure 4 ). Given their complex polymeric structure, which can be species-specific, the effect of cellulose and lignin on the shape and magnitude of the reflectance curve is not as straightforward as in case of pigment molecules. The SWIR reflectance in combination with the reflectance in NIR is influenced generally by the leaf dry mass per area (LMA). The reflectance of leaves in the SWIR, similarly to the NIR, is largely dependent on the water content of the leaves (i.e., water absorption at 1450, 1940, and 2500 nm (Carter, 1991) (Figure 4 ). Detection of leaf water content LWC, equivalent water thickness, or relative water content (Eitel et al., 2006b; Kokaly et al., 2009) are among the indirect methods to estimate water balance in vegetation, which is one of the main objectives of many remote sensing studies. </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#the-radiation-emitted-and-reflected-in-the-thermal-part-of-the-spectrum-is-affected-by-leaf-biophysical-and-structural-traits","title":"The radiation emitted and reflected in the thermal part of the spectrum is affected by leaf biophysical and structural traits","text":"<p>Compared to the measurement of reflectance in the previously described spectral regions of VIS, NIR and SWIR, acquisition of emission of thermal radiation by a leaf (i.e., thermal infrared region (TIR; 8-14 \u00b5m) (Gerber et al., 2011) Gerber et al., 2011) is not a common methodology in laboratory spectroscopy. However, emission and reflectance in the TIR can also be associated with physical changes in leaves, for example, water, lignin or cellulose contents and leaf area or plant stress (Buitrago Acevedo et al., 2017).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#leaf-optical-properties-differ-on-upper-and-lower-leaf-sides-of-dorsiventral-leaves","title":"Leaf optical properties differ on upper and lower leaf sides of dorsiventral leaves","text":"<p>Leaf optical properties are known to differ from upper and lower leaf side (Buschmann et al., 2012, Luke\u0161 et al., 2020) what is particularly important for flat dorsiventral leaves (Figures 6A, 7A ). Long palisade parenchyma cells, which have an isotropic arrangement (Figures 6A, 7A ), may facilitate light penetration deeper into the leaf interior, whereas spherical spongy mesophyll cells with a more anisotropic structure tend to scatter radiation (Vogelmann, 1993). Dorsiventral flat leaves are adapted for absorption of radiation incident on the upper leaf side. However, some plants change their leaf orientation a lot due to changes in environmental conditions (heliotropic plants) either to track the sunlight or avoid excessive irradiation to prevent overheating (soy, common bean). In such cases, the reflectance spectra at the canopy level will represent a mix of lower and upper leaf side signals at different proportions.  The effect of internal leaf asymmetry and different upper and lower surfaces results in different reflectance if measured from upper and lower leaf side. The magnitude of the difference depends on the structural traits of epidermis (waxes, hairs) and internal mesophyll architecture (intercellular air spaces). Figure 8 shows the phenomenon on example of white poplar (Populus alba) (A,B) and small leaved Linden (Tilia cordata) (C). In both cases the lower reflectance in VIS for the upper side corresponds to adaptation to efficiently absorb the radiation in this orientation. The difference between adaxial (upper) and abaxial (lower) reflectance follows the same trends in both species; however, it is more pronounced in white poplar, where the external leaf surface asymmetry is more pronounced as well.</p> <p> </p> <p>Figure 8. Dorsiventral leaf structure causes differences in leaf optical properties acquired from upper and lower leaf sides. A) Sample of white poplar (Populus alba) branch showing the macroscopic difference between the upper (darker green; blue arrows) and the lower (grey-green; yellow arrows) leaf side caused by the presence of waxes on the upper and trichomes (hairs) on the lower side. B) Reflectance of the upper and lower leaf side. C) Reflectance of the upper and lower leaf side in small leaved Linden (Tilia cordata).</p> <p>In the study by (Luke\u0161 et al., 2020), the optical properties of leaves from both the upper (adaxial) and lower (abaxial) sides were simulated in a leaf-level radiative transfer model called the Dorsiventral Leaf Model (DLM) (Stuckens et al., 2009b). The dorsiventral leaf optical properties that were simulated in this way were upscaled to the whole stand level \u2014 so-called TOC (top-of-canopy) reflectance \u2014 by coupling the simulation results of the leaf-level DLM model with a whole canopy model called Discrete Anisotropic Radiative Transfer model (DART) (Gastellu-Etchegorry et al., 2004). The effect of a simplified parameterization of optical properties (where dorsoventral asymmetry is typically neglected) on the overall reflectance of the forest stand was evaluated. The main conclusions was that neglecting differences in lower (abaxial) side leaf reflectance may introduce relative difference up to 20%, causing the underestimation of \u201cone-sided\u201d scenario compared to \u201ctwo-sided\u201d one (Luke\u0161 et al., 2020).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#leaf-optical-properties-differ-for-leaf-developmental-phases","title":"Leaf optical properties differ for leaf developmental phases","text":"<p>Plants can have leaves of different developmental phases: starting with juvenile leaves in the spring; mature leaves, which are productive during the majority of vegetation season; and finally, senescing leaves with pigment changes in the end of the vegetative season. However, in woody species, there can be foliage formed by two major patterns of crown development: (a) proleptic leaves, result from a rhythmic branching process from buds formed before a period of dormancy, and (b) sylleptically formed leaves, resulting from a continuous branching process during the vegetative season from incompletely formed lateral buds (Halle et al., 2012). Both, proleptic and sylleptic growth can appear in one crown of the same tree, meaning leaves of different developmental stage can appear simultaneously in a tree crown. Thus, in many tree species, two developmental types of leaf occur: (a) pre-formed leaves (also called early leaves) that originate from overwintering buds after a dormancy stage in the beginning of the season, and (b) neo-formed leaves (late leaves) from buds without passing through the dormant period and instead developing entirely during the current growing season (Figure 9 ) (Critchfield, 1960). Sylleptically formed young leaves are often found in fast-growing tree species such as silver birch (B. pendula) (Deepak et al., 2019). Usually, the pre-formed leaves grow on both short and long shoots while the neo-formed leaves develop on long shoots. Since neo-formed leaves on sylleptic branches could form a substantial part of the upper- and external, sun-exposed crown layer (Broeckx et al., 2012), they are thought to have a significant contribution to the top-of-canopy reflectance signal. Leaves of different developmental origins\u2014either proleptic or sylleptic\u2014usually differ quantitatively in leaf thickness or ratio of palisade to spongy parenchyma (Neuwirthov\u00e1 et al., 2021a). Different ratio of palisade to spongy parenchyma then can affect leaf reflectance. The effect of the leaf developmental stage and the differences in pigment content and leaf structure on the reflectance are shown in Figure 10.</p> <p> </p> <p>Figure 9. Pre-formed leaves (also called early leaves) formed by proleptic growth and neo-formed leaves (late leaves) formed by sylleptic growth. Branch of B. pendula during taking ground truth in June (right photo). White circle mark juvenile appearance of the leaf surrounded by mature leaves. In the middle cross-sections of two developmental stages (juvenile and mature) of B. pendula leaves sampled in June (18 June = DOY 169). (Right) description of leaf internal structure with a description of leaf tissues that were quantified in the present study. Palisade and spongy parenchyma comprise the photosynthetic mesophyll tissue. Dermal tissue is represented by the adaxial epidermis on the upper surface and abaxial epidermis on the lower surface. Fresh hand sections stained with toluidine blue, bright field microscopy, magnification 400\u00d7. (Figure by Neuwirthov\u00e1 et al. 2021/ CC BY 4.0)</p> <p> </p> <p>Figure 10. The reflectance of birch leaves sampled in June (18 June = DOY 169) representing two different developmental stages described in Figure 9. Mind the difference in the visible part of the spectrum related to lower chlorophyll content in juvenile leaves. The higher reflectance in NIR and SWIR in mature leaves is determined by more developed intercellular spaces that scatter NIR and higher content of structural compounds (cellulose).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#leaf-optical-properties-differ-with-leaf-stacking","title":"Leaf optical properties differ with leaf stacking.","text":"<p>It should be mentioned that the reflectance curve changes in case of leaf stacking. It is necessary to be aware of differences in shape of spectral reflectance curve, mainly in the NIR (near infrared) region, with the main driving factor being leaf structure for measurements of a single-leaf reflectance or using a leaf stack (Figure 11 ). By piling several layers of leaves, we technically increase chlorophyll content, leaf area index, and leaf mass per area unit (LAI and LMA). In addition, the effect of the internal leaf structure such as both volume and surface of intercellular spaces is enhanced in a leaf stack. However, these effects of a leaf stacking altogether can influence the correlation between leaf biophysical traits and leaf optical properties including vegetation indices, particularly those derived from NIR reflectance values (Neuwirthov\u00e1 et al., 2017). If the vegetation indices are used to assess plant physiological status in various times of the vegetative season, possible changes induced by the particular contact probe measurement setup regarding the leaf stacking should be considered (Neuwirthov\u00e1 et al., 2017).  It is necessary to take in account that the real canopy-scale reflectance is affected by additional factors, such as leaf clumping, leaf angle distribution, presence of non-photosynthetic structures (branches and twigs) and soil/understory reflectance background as summarized, e.g., (Homolov\u00e1 et al., 2013). </p> <p> </p> <p>Figure 11. Averaged reflectance curves measured by a contact probe for a single leaf, a leaf stack and a difference (\u2206R5L\u20131L) between the reflectance measured on a leaf stack (5L) and a single leaf (1L) of Populus tremula (left) and Salix caprea (right). The mean reflectance (%) during the six months. n = 10 trees. (Figure modified from Neuwirthov\u00e1 et al. 2017/ CC BY 4.0)</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"Which leaf compounds affect mostly its optical properties in the visible part of the spectrum?  Water, cellulose, and lignin Chlorophylls, proteins, and cellular structure Chlorophylls, carotenoids, and anthocyanins Water, chlorophylls, carotenoids Chlorophylls, carotenoids, and anthocyanins  The figure shows leaf level reflectance spectra of narrow-leaved ash (common European broadleaf deciduous species) in three different dates within one growing season. Leaves were collected in the floodplain forest, Lan\u017ehot, Czech Republic. Explore the spectral curves, focus on the highlighted regions, and select the correct figure legend. Justify your decision by relating the reflectance in the highlighted regions with leaf biophysical properties and phenology.  Data cubes always have three dimensions. Observations and derivatives of different variables and even from multiple different remote sensing systems can be managed in a well-structured form by a data cube. Data cubes can be deployed on a local machine but most commonly they are hosted on larger (cluster/cloud) infrastructure to serve more users. Observations and derivatives of different variables and even from multiple different remote sensing systems can be managed in a well-structured form by a data cube.&amp;Data cubes can be deployed on a local machine but most commonly they are hosted on larger (cluster/cloud) infrastructure to serve more users.  Which of these statements about data cubes are correct?  Data cubes always have three dimensions. Observations and derivatives of different variables and even from multiple different remote sensing systems can be managed in a well-structured form by a data cube. Data cubes can be deployed on a local machine but most commonly they are hosted on larger (cluster/cloud) infrastructure to serve more users. Observations and derivatives of different variables and even from multiple different remote sensing systems can be managed in a well-structured form by a data cube.&amp;Data cubes can be deployed on a local machine but most commonly they are hosted on larger (cluster/cloud) infrastructure to serve more users."},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#next-unit","title":"Next unit","text":"<p>Proceed with Principles of laboratory spectroscopy</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_01_optical_parameters_of_foliage.html#references","title":"References","text":"<p>Aasamaa, K., Niinemets, \u00dc., S\u00f5ber, A., 2005. Leaf hydraulic conductance in relation to anatomical and functional traits during Populus tremula leaf ontogeny. Tree Physiology 25, 1409\u20131418.</p> <p>Asner, G.P., 1998. Biophysical and biochemical sources of variability in canopy reflectance. Remote sensing of Environment 64, 234\u2013253. Bell, C.C., Curran, P.J., 1992. The effect of specular reflectance on the relationship between reflectance and vegetation amount. International Journal of Remote Sensing 13, 2751\u20132757. https://doi.org/10.1080/01431169208904077</p> <p>Bousquet, L., Lach\u00e9rade, S., Jacquemoud, S., Moya, I., 2005. Leaf BRDF measurements and model for specular and diffuse components differentiation. Remote Sensing of Environment 98, 201\u2013211. https://doi.org/10.1016/j.rse.2005.07.005</p> <p>Broeckx, L.S., Verlinden, M.S., Vangronsveld, J., Ceulemans, R., 2012. Importance of crown architecture for leaf area index of different Populus genotypes in a high-density plantation. Tree Physiology 32, 1214\u20131226. https://doi.org/10.1093/treephys/tps083</p> <p>Buitrago Acevedo, M.F., Groen, T.A., Hecker, C.A., Skidmore, A.K., 2017. Identifying leaf traits that signal stress in TIR spectra. ISPRS Journal of Photogrammetry and Remote Sensing 125, 132\u2013145. https://doi.org/10.1016/j.isprsjprs.2017.01.014</p> <p>Buschmann, C., Lenk, S., Lichtenthaler, H.K., 2012a. Reflectance spectra and images of green leaves with different tissue structure and chlorophyll content. Israel Journal of Plant Sciences 60, 49\u201364. https://doi.org/10.1560/IJPS.60.1-2.49</p> <p>Buschmann, C., Lenk, S., Lichtenthaler, H.K., 2012b. Reflectance spectra and images of green leaves with different tissue structure and chlorophyll content. Israel Journal of Plant Sciences 60, 49\u201364. https://doi.org/10.1560/IJPS.60.1-2.49</p> <p>Campbell, P.K.E., Rock, B.N., Martin, M.E., Neefus, C.D., Irons, J.R., Middleton, E.M., Albrechtova, J., 2004. Detection of initial damage in Norway spruce canopies using hyperspectral airborne data. Int. J. Remote Sens. 25, 5557\u20135583. https://doi.org/10.1080/01431160410001726058</p> <p>Carter, G.A., 1991. Primary and Secondary Effects of Water Content on the Spectral Reflectance of Leaves. American Journal of Botany 78, 916\u2013924. https://doi.org/10.2307/2445170</p> <p>Cavender-Bares, J., Meireles, J.E., Couture, J.J., Kaproth, M.A., Kingdon, C.C., Singh, A., Serbin, S.P., Center, A., Zuniga, E., Pilz, G., Townsend, P.A., 2016. Associations of Leaf Spectra with Genetic and Phylogenetic Variation in Oaks: Prospects for Remote Detection of Biodiversity. Remote Sensing 8, 221. https://doi.org/10.3390/rs8030221</p> <p>Conklin, P.A., Strable, J., Li, S., Scanlon, M.J., 2019. On the mechanisms of development in monocot and eudicot leaves. New Phytologist 221, 706\u2013724. https://doi.org/10.1111/nph.15371</p> <p>Coste, S., Baraloto, C., Leroy, C., Marcon, \u00c9., Renaud, A., Richardson, A.D., Roggy, J.-C., Schimann, H., Uddling, J., H\u00e9rault, B., 2010. Assessing foliar chlorophyll contents with the SPAD-502 chlorophyll meter: a calibration test with thirteen tree species of tropical rainforest in French Guiana. Ann. For. Sci. 5.</p> <p>Critchfield, W.B., 1960. Leaf Dimorphism in Populus Trichocarpa. American Journal of Botany 47, 699\u2013711.</p> <p>Curran, P.J., 1989. Remote sensing of foliar chemistry. Remote Sensing of Environment 30, 271\u2013278. https://doi.org/10.1016/0034-4257(89)90069-2</p> <p>Deepak, M., Keski-Saari, S., Fauch, L., Granlund, L., Oksanen, E., Kein\u00e4nen, M., 2019. Leaf Canopy Layers Affect Spectral Reflectance in Silver Birch. Remote Sensing 11, 2884. https://doi.org/10.3390/rs11242884</p> <p>Eitel, J.U.H., Gessler, P.E., Smith, A.M.S., Robberecht, R., 2006a. Suitability of existing and novel spectral indices to remotely detect water stress in Populus spp. Forest Ecology and Management 229, 170\u2013182. https://doi.org/10.1016/j.foreco.2006.03.027</p> <p>Eitel, J.U.H., Gessler, P.E., Smith, A.M.S., Robberecht, R., 2006b. Suitability of existing and novel spectral indices to remotely detect water stress in Populus spp. Forest Ecology and Management 229, 170\u2013182. https://doi.org/10.1016/j.foreco.2006.03.027</p> <p>Evans, J., Caemmerer, S., Setchell, B., Hudson, G., 1994. The Relationship Between CO2 Transfer Conductance and Leaf Anatomy in Transgenic Tobacco With a Reduced Content of Rubisco. Functional Plant Biol. 21, 475. https://doi.org/10.1071/PP9940475</p> <p>Fossen, T., Cabrita, L., Andersen, O.M., 1998. Colour and stability of pure anthocyanins in\u00afuenced by pH including the alkaline region. Food Chemistry 63, 435\u2013440.</p> <p>Fukushima, K., Hasebe, M., 2014. Adaxial-abaxial polarity: The developmental basis of leaf shape diversity: development and evolution of leaf types. genesis 52, 1\u201318. https://doi.org/10.1002/dvg.22728</p> <p>Gastellu-Etchegorry, J.P., Martin, E., Gascon, F., 2004. DART: a 3D model for simulating satellite images and studying surface radiation budget. International Journal of Remote Sensing 25, 73\u201396. https://doi.org/10.1080/0143116031000115166</p> <p>Gates, D.M., Keegan, H.J., Schleter, J.C., Weidner, V.R., 1965. Spectral properties of plants. Applied optics 4, 11\u201320.</p> <p>Gerber, F., Marion, R., Olioso, A., Jacquemoud, S., Ribeiro da Luz, B., Fabre, S., 2011. Modeling directional\u2013hemispherical reflectance and transmittance of fresh and dry leaves from 0.4\u03bcm to 5.7\u03bcm with the PROSPECT-VISIR model. Remote Sensing of Environment 115, 404\u2013414. https://doi.org/10.1016/j.rse.2010.09.011</p> <p>Gitelson, A., Merzlyak, M.N., 1994. Spectral Reflectance Changes Associated with Autumn Senescence of Aesculus hippocastanum L. and Acer platanoides L. Leaves. Spectral Features and Relation to Chlorophyll Estimation. Journal of Plant Physiology 143, 286\u2013292. https://doi.org/10.1016/S0176-1617(11)81633-0</p> <p>Gitelson, A.A., Chivkunova, O.B., Merzlyak, M.N., 2009. Nondestructive estimation of anthocyanins and chlorophylls in anthocyanic leaves. American Journal of Botany 96, 1861\u20131868. https://doi.org/10.3732/ajb.0800395</p> <p>Gitelson, A.A., Merzlyak, M.N., Lichtenthaler, H.K., 1996. Detection of Red Edge Position and Chlorophyll Content by Reflectance Measurements Near 700 nm. Journal of Plant Physiology 148, 501\u2013508. https://doi.org/10.1016/S0176-1617(96)80285-9</p> <p>Gould, K.S., 2004. Nature\u2019s Swiss Army Knife: The Diverse Protective Roles  of Anthocyanins in Leaves. J Biomed Biotechnol 2004, 314\u2013320. https://doi.org/10.1155/S1110724304406147</p> <p>Grant, L., 1987. Diffuse and specular characteristics of leaf reflectance. Remote Sensing of Environment 22, 309\u2013322. https://doi.org/10.1016/0034-4257(87)90064-2</p> <p>Grant, L., Daughtry, C.S.T., Vanderbilt, V.C., 1993. Polarized and specular reflectance variation with leaf surface features. Physiol Plant 88, 1\u20139. https://doi.org/10.1111/j.1399-3054.1993.tb01753.x</p> <p>Grant, L., Daughtry, C.S.T., Vanderbilt, V.C., 1987. Polarized and non-polarized leaf reflectances of Coleus blumei. Environmental and Experimental Botany 27, 139\u2013145. https://doi.org/10.1016/0098-8472(87)90064-5</p> <p>Grant, R.H., 1997. Partitioning of biologically active radiation in plant canopies. Int J Biometeorol 40, 26\u201340. https://doi.org/10.1007/BF02439408</p> <p>Halle, F., Oldeman, R.A.A., Tomlinson, P.B., 2012. Tropical Trees and Forests: An Architectural Analysis. Springer Science &amp; Business Media. Hanba, Y.T., Kogami, H., Terashima, I., 2002. The effect of growth irradiance on leaf anatomy and photosynthesis in Acer species differing in light demand. Plant, Cell &amp; Environment 25, 1021\u20131030. https://doi.org/10.1046/j.1365-3040.2002.00881.x</p> <p>Hoch, W.A., Singsaas, E.L., McCown, B.H., 2003. Resorption Protection. Anthocyanins Facilitate Nutrient Recovery in Autumn by Shielding Leaves from Potentially Damaging Light Levels. Plant Physiology 133, 1296\u20131305. https://doi.org/10.1104/pp.103.027631</p> <p>Homolov\u00e1, L., Malenovsk\u00fd, Z., Clevers, J.G.P.W., Garc\u00eda-Santos, G., Schaepman, M.E., 2013. Review of optical-based remote sensing for plant trait mapping. Ecological Complexity 15, 1\u201316. https://doi.org/10.1016/j.ecocom.2013.06.003</p> <p>Junker, L.V., Ensminger, I., 2016. Relationship between leaf optical properties, chlorophyll fluorescence and pigment changes in senescing Acer saccharum leaves. Tree Physiology 36, 694\u2013711. https://doi.org/10.1093/treephys/tpv148</p> <p>Kokaly, R.F., Asner, G.P., Ollinger, S.V., Martin, M.E., Wessman, C.A., 2009. Characterizing canopy biochemistry from imaging spectroscopy and its application to ecosystem studies. Remote Sensing of Environment 113, S78\u2013S91. https://doi.org/10.1016/j.rse.2008.10.018</p> <p>Li, Y., Chen, Y., Huang, J., 2019. An Approach to Improve Leaf Pigment Content Retrieval by Removing Specular Reflectance Through Polarization Measurements. IEEE Transactions on Geoscience and Remote Sensing 57, 2173\u20132186. https://doi.org/10.1109/TGRS.2018.2871830</p> <p>Luke\u0161, P., Neuwirthov\u00e1, E., Lhot\u00e1kov\u00e1, Z., Janoutov\u00e1, R., Albrechtov\u00e1, J., 2020. Upscaling seasonal phenological course of leaf dorsiventral reflectance in radiative transfer model. Remote Sensing of Environment 246, 111862. https://doi.org/10.1016/j.rse.2020.111862</p> <p>Marenco, R.A., Antezana-Vera, S.A., Nascimento, H.C.S., 2009. Relationship between specific leaf area, leaf thickness, leaf water content and SPAD-502 readings in six Amazonian tree species. Photosynt. 47, 184\u2013190. https://doi.org/10.1007/s11099-009-0031-6</p> <p>Maslova, T.G., Markovskaya, E.F., Slemnev, N.N., 2021. Functions of Carotenoids in Leaves of Higher Plants (Review). Biol Bull Rev 11, 476\u2013487. https://doi.org/10.1134/S2079086421050078</p> <p>McClendon, J.H., 1984. The Micro-Optics of Leaves. I. Patterns of Reflection from the Epidermis 8.</p> <p>Neuwirhtov\u00e1, E., 2022. Optical properties of the leaf in relation to its anatomical traits. (Ph.D. Thesis). Charles university, Faculty of Science, Prague, Czechia.</p> <p>Neuwirthov\u00e1, E., Kuusk, A., Lhot\u00e1kov\u00e1, Z., Kuusk, J., Albrechtov\u00e1, J., Hallik, L., 2021a. Leaf Age Matters in Remote Sensing: Taking Ground Truth for Spectroscopic Studies in Hemiboreal Deciduous Trees with Continuous Leaf Formation. Remote Sensing 13, 1353. https://doi.org/10.3390/rs13071353</p> <p>Neuwirthov\u00e1, E., Lhot\u00e1kov\u00e1, Z., Albrechtov\u00e1, J., 2017. The Effect of Leaf Stacking on Leaf Reflectance and Vegetation Indices Measured by Contact Probe during the Season. Sensors 17, 1202. https://doi.org/10.3390/s17061202</p> <p>Neuwirthov\u00e1, E., Lhot\u00e1kov\u00e1, Z., Luke\u0161, P., Albrechtov\u00e1, J., 2021b. Leaf Surface Reflectance Does Not Affect Biophysical Traits Modelling from VIS-NIR Spectra in Plants with Sparsely Distributed Trichomes. Remote Sensing 13, 4144. https://doi.org/10.3390/rs13204144</p> <p>Niglas, A., Papp, K., S\u0119kiewicz, M., Sellin, A., 2017. Short-term effects of light quality on leaf gas exchange and hydraulic properties of silver birch (Betula pendula). Tree Physiology 37, 1218\u20131228. https://doi.org/10.1093/treephys/tpx087</p> <p>Qiu, Chen, Croft, Li, Zhang, Zhang, Ju, 2019. Retrieving Leaf Chlorophyll Content by Incorporating Variable Leaf Surface Reflectance in the PROSPECT Model. Remote Sensing 11, 1572. https://doi.org/10.3390/rs11131572</p> <p>Rapaport, T., Hochberg, U., Rachmilevitch, S., Karnieli, A., 2014. The Effect of Differential Growth Rates across Plants on Spectral Predictions of Physiological Parameters. PLoS ONE 9, e88930. https://doi.org/10.1371/journal.pone.0088930</p> <p>Rock, B.N., Hoshizaki, T., Miller, J.R., 1988. Comparison of in situ and airborne spectral measurements of the blue shift associated with forest decline. Remote Sensing of Environment, Imaging Spectrometry 24, 109\u2013127. https://doi.org/10.1016/0034-4257(88)90008-9</p> <p>Serrano, L., Pe\u00f1uelas, J., Ustin, S.L., 2002. Remote sensing of nitrogen and lignin in Mediterranean vegetation from AVIRIS data: Decomposing biochemical from structural signals. Remote Sensing of Environment 81, 355\u2013364. https://doi.org/10.1016/S0034-4257(02)00011-1</p> <p>Shull, C.A., 1929. A Spectrophotometric Study of Reflection of Light from Leaf Surfaces. Botanical Gazette 87, 583\u2013607. https://doi.org/10.1086/333965</p> <p>Sims, D.A., Gamon, J.A., 2003. Estimation of vegetation water content and photosynthetic tissue area from spectral reflectance: a comparison of indices based on liquid water and chlorophyll absorption features. Remote Sensing of Environment 84, 526\u2013537. https://doi.org/10.1016/S0034-4257(02)00151-7</p> <p>Sims, D.A., Gamon, J.A., 2002. Relationships between leaf pigment content and spectral reflectance across a wide range of species, leaf structures and developmental stages. Remote sensing of environment 81, 337\u2013354.</p> <p>Slaton, M.R., Hunt, E.R., Smith, W.K., 2001. Estimating near-infrared leaf reflectance from leaf structural characteristics. American Journal of Botany 88, 278\u2013284.</p> <p>Stuckens, J., Verstraeten, W.W., Delalieux, S., Swennen, R., Coppin, P., 2009a. A dorsiventral leaf radiative transfer model: Development, validation and improved model inversion techniques. Remote Sensing of Environment 113, 2560\u20132573. https://doi.org/10.1016/j.rse.2009.07.014</p> <p>Stuckens, J., Verstraeten, W.W., Delalieux, S., Swennen, R., Coppin, P., 2009b. A dorsiventral leaf radiative transfer model: Development, validation and improved model inversion techniques. Remote Sensing of Environment 113, 2560\u20132573. https://doi.org/10.1016/j.rse.2009.07.014</p> <p>Terashima, I., Hanba, Y.T., Tazoe, Y., Vyas, P., Yano, S., 2006. Irradiance and phenotype: comparative eco-development of sun and shade leaves in relation to photosynthetic CO2 diffusion. Journal of Experimental Botany 57, 343\u2013354.</p> <p>Vanderbilt, V.C., Grant, L., Daughtry, C.S.T., 1985. Polarization of light scattered by vegetation. Proceedings of the IEEE 73, 1012\u20131024. https://doi.org/10.1109/PROC.1985.13232</p> <p>Vogelmann, T.C., 1993. Plant Tissue Optics. Annual review of plant biolog 231\u201351.</p> <p>Vogelmann, T.C., Gorton, H.L., 2014. Leaf: Light Capture in the Photosynthetic Organ.</p> <p>Xiao, Y., Tholen, D., Zhu, X.-G., 2016. The influence of leaf anatomy on the internal light environment and photosynthetic electron transport rate: exploration with a new leaf ray tracing model. J Exp Bot 67, 6021\u20136035. https://doi.org/10.1093/jxb/erw359</p> <p>Yang, X., Tang, J., Mustard, J.F., Wu, J., Zhao, K., Serbin, S., Lee, J.-E., 2016. Seasonal variability of multiple leaf traits captured by leaf spectroscopy at two temperate deciduous forests. Remote Sensing of Environment 179, 1\u201312. https://doi.org/10.1016/j.rse.2016.03.026</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html","title":"Principles of laboratory spectroscopy","text":""},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#objectives","title":"Objectives","text":"<p>In this theme, you will learn about:</p> <ul> <li>the basic principles of laboratory spectroscopy</li> <li>acquisition of optical properties of leaf samples</li> </ul> <p>After finishing this theme you will be able to:</p> <ul> <li>understand examples of using spectroradiometer with two different measurement setups \u2013 the contact probe (CP) and the integrating sphere (IS)</li> <li>discuss the pros and cons of each measurement type</li> </ul>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#spectral-reflectance-acquisition-general-introduction","title":"Spectral reflectance acquisition \u2013 general introduction","text":"<p>The optical property used for interpretation in field and laboratory spectroscopy is reflectance. Reflectance is defined as the fraction of total radiant flux incident upon the surface of a sample that is reflected, which varies according to the wavelength distribution of the incident radiation (Figure 1 ). In laboratory and field spectroscopy, the total incident radiance is usually measured as radiance reflected from very high (99%) diffuse material over the spectral interval recorded by the spectroradiometer. Various polymers are used to manufacture reference panels with high and spectrally constant diffuse reflectance. The most common are Spectralon\u00ae and Zenith Polymer\u00ae.</p> <p> </p> <p> </p> <p>Figure 1. Definition of leaf spectral reflectance as the ratio of measured radiance reflected from a white reference panel and an object (leaf) \u2013 upper part. Solar energy distribution in spectral range between 250 and 2500 nm \u2013 lower part. The interval between 300-2500 nm corresponds to the spectral range of maximal Solar energy and therefore is covered by the most of field and laboratory spectroradiometers. (Figure by Ramos et al. 2020 / CC BY 4.0)</p> <p>Spectroradiometers used for field and laboratory spectroscopy of vegetation usually operate in a spectral interval of 300-2500 nm that coincides with the spectral range of maximal solar energy distribution (Figure 1 ). We use the ASD FieldSpec 4 spectroradiometer (Figure 2 ) as an example to demonstrate spectral reflectance measurement with the main focus on vegetation at various hierarchical levels (leaf and canopy) and in three different measurement setups (Figure 2 A-C ). </p> <p> </p> <p>Figure 2. Three most used vegetation optical properties measurement setups with a field and laboratory spectroradiometer. A) Field measurement of grassland canopy reflectance using an optical cable held by a pistol grip. B) Laboratory measurement of leaf level reflectance using a contact probe. C) Laboratory measurement of leaf level reflectance and transmittance using an integrating sphere.</p> <p>Field spectroradiometers measure reflectance at various hierarchical levels depending on the device connected to the optical cable. Field measurements with bare optical cable fixed in a \u201cpistol\u201d grip are typical for canopy measurements of grasslands, cereal crops with narrow leaves, or data collection from reference surfaces (sand, concrete etc.) used for radiometric correction of airborne data (Figure 2A ). At the leaf level, there are two common attachments that connect to the spectroradiometer to measure vegetative reflectance: the contact probe (CP) (Figure 2B ) and the integrating sphere (IS) (Figure 2C ). Laboratory measurements at the leaf level can serve as input to radiative transfer models and be used for upscaling to the canopy level or for leaf trait retrieval at the leaf level.  However, the reflectance measurements with CP or IS are not interchangeable. The main reason is the difference between geometry of the light source and the sensor, providing specific advantages regarding speed, laboriousness, and field operability of the spectra collection. The radiation quantity measured by the CP is usually termed in the literature as bidirectional reflectance factor (BRF) as the light source and sensor geometry is fixed in stable directional position to each other (Figure 3A ). The radiation quantity acquired by the IS is called directional-hemispherical reflectance factor (DHRF) for having the directional light source and having the reflected or transmitted radiation integrated from the whole hemisphere (Figure 3B ). There are several studies focused on the comparison of those two types of spectral quantities Pot\u016f\u010dkov\u00e1 et al., 2016a), or between DHRF acquired by two types of integrating spheres (Hovi et al., 2020).</p> <p> </p> <p>Figure 3. Two most commonly measured reflectance quantities A) Bidirectional Reflectance Factor by the contact probe or leaf clip and B) Directional-Hemispherical reflectance factor by the integrating sphere. (Figure by the authors).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#spectral-reflectance-acquisition-with-the-spectroradiometer-asd-fieldspec","title":"Spectral reflectance acquisition with the spectroradiometer ASD FieldSpec\u00ae","text":"<p>After assembling the spectroradiometer with a CP, IS, or \u201cpistol\u201d grip and an operating computer (could be via Wifi or Ethernet cable), the spectroradiometer should warmup for at least 30 minutes. While warming up, some preparatory procedures for data acquisition and saving can be done. One of the operating software for ASD FieldSpec is RS3. We will use RS3 to demonstrate how to set measurement settings.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#setting-spectrum-save","title":"Setting Spectrum Save","text":"<p>A folder should be created where the measured spectra will be automatically saved in your favourite file manager. Next, the spectra should be set to be saved here in RS3 (Figure 4 ). The instrument scans the incoming radiance continually and saves the measurement after pressing the space bar (will be shown later). The saved files share the \u201cbase name\u201d that you set in the Spectrum save settings and with each saved spectrum the number in the file name increases. It is recommended to record sample IDs with spectra numbers to an additional protocol. For more details check the RS3 User Guide (can be found on the internet). </p> <p> </p> <p>Figure 4. The screen of Spectrum save menu in RS3 software. Explanation of spectrum save settings is provided in the right box. For more details check the RS3 User Guide.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#adjust-configuration","title":"Adjust configuration","text":"<p>The bane of all electrical data analysis is noise. Noise, in all of its forms, manifests itself in detection equipment of any kind as uncertainty in the measurement. Noise is random. This means it can usually be reduced in the desired spectral signal by a technique called spectrum averaging. The number of scans to average should be a good compromise between noise reduction and time spent on measuring one sample (Figure 5 ). When measuring vegetation, you should consider that the light source heats the leaf. An average of 25 scans should be sufficient. Averaging can be reduced to 15 when dealing with thin, delicate, or young leaves. While measuring in the integrating sphere, the averaging for a sample and white reference should be higher to get a sufficient signal (e.g., 50 for a leaf sample and 100 for white reference - WR), the leaf is not in direct contact with the light source, so the overheating of leaf is less probable. When measuring in the field (Figure 2A ) and using the sunlight as the illumination source, you can use the settings as for the CP. The vegetation won\u2019t overheat, but sometimes you have to deal with the changing illumination caused by clouds, so you have to be fast to get the measurements.</p> <p> </p> <p>Figure 5. The screen of Instrument Configuration menu in RS3. Comparison of preferred configuration for CP (left) and IS (right) spectra acquisition.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#contact-probe-laboratory-spectra-acquisition","title":"Contact probe laboratory spectra acquisition","text":"<p>A contact probe is a device mainly designed for contact measurements of solid raw materials, such as minerals and grains, but also used for vegetation samples (Figure 6 ). A probe has its own light source (typically a krypton halogen bulb) integrated within its body. A CP maintains a constant distance from the probe lens to the sample. A CP allows only reflectance measurements, not transmittance. The CP has some advantages, like avoiding problems with stray light, operating flexibility, and speed. Also, by its design, it allows for repetitive and non-destructive in-situ measurements of samples if used with a leaf clip. The CP is connected to the spectroradiometer by an optical cable for data acquisition and a power cable as an energy source for the light bulb (in the case of laboratory measurement). It is possible to measure in the field as well, however, this setup requires an additional battery for the contact probe light source.</p> <p> </p> <p>Figure 6. The ASD FieldSpec 4 spectroradiometer with operating PC, contact probe, white reference panel and leaf sample on a spectrally black background.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#optimization","title":"Optimization","text":"<p>Spectroradiometer measurements are recorded as a dimensionless integer value called digital numbers (DN). Their relationship to the actual radiometric quantity measured, spectral radiance [ \\(W/m^2/\u00b5m/sr\\) ], is given by the instrument's calibration curve. The spectroradiometer must be optimized before any data is collected (Figure 7). During the optimization process, the contact probe is placed on the Spectralon white reference panel. Optimization sets the proper settings for the light source being used to collect spectra. If the light source does not change substantially and the instrument is warmed up, there should be no reason to re-optimize. After optimization, a graph will display measurements in raw digital numbers (DN) plotted against wavelength in nm.</p> <p> </p> <p>Figure 7. The main RS3 screen during instrument optimization. Three spectral ranges corresponding to three sensors of ASD Fieldspec4 are indicated. Red arrow shows the optimization button or key shortcuts to optimize. The spectral curve on the display is the DN spectrum of a white reference panel after optimization.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#recording-white-reference","title":"Recording white reference","text":"<p>After optimizing the instrument, record the radiance reflected by the calibration panel (Figure 8 ). The white reference is required because the spectroradiometer only measures the intensity of a light field through a given point in space. With the WR available and known, RS3 can compute the reflectance for the material being sampled by the spectroradiometer. This enables the machine to record and display all future measurements as reflectance. The white reference should be done as often as possible, minimally every ten to fifteen minutes when using solar illumination and every thirty minutes when using artificial illumination sources. </p> <p> </p> <p>Figure 8. Taking the spectra of the white reference panel \u2013 instrument calibration. Left \u2013 holding still the contact probe on a reference panel. Right - main RS3 screen after taking white reference spectra.</p> <p> </p> <p> </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#recording-sample-reflectance-at-the-leaf-level-by-cp","title":"Recording sample reflectance at the leaf level by CP","text":"<p>To record spectral measurements of detached leaf samples, place them on a dark background with high absorption along the whole spectral range of the spectroradiometer (Figure 9 ). A remarkable portion of incoming radiation is transmitted by the vegetation samples, particularly in green, NIR, and SWIR wavelengths. The low reflectance background absorbs this transmitted radiation and avoids a bias by additional reflection and transmission from the lower part of the sample. The spectrally black background could be any surface painted with a special low reflectance black coating. Always attach the contact probe in the same orientation to the leaf, the measurement taken by the contact probe is directional.</p> <p> </p> <p>Figure 9. Taking the spectra of the leaf samples. Left \u2013 holding still the contact probe on a leaf sample, beech leaf. Right \u2013 taking the spectra of silver fir needles attached to the shoots \u2013 note the same orientation of shoots with twigs.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#saving-the-spectra","title":"Saving the spectra","text":"<p>After placing the contact probe on a leaf or shoot sample, save the spectrum by pressing the spacebar. Once the spectrum is saved the RS3 gives a sound signal. It is important to hold the probe still until the given number of scans for averaging is completed by the instrument (Figure 10 ). The instrument measures continually. Saving the spectrum before the given number of scans is acquired for averaging results in underestimated sample reflectance. Check the Spectrum Avg bar in the lower left part of the RS3 screen.</p> <p> </p> <p>Figure 10. Sequence of main RS3 screen shots when: A) the contact probe places on a black background. B) during replacement of the contact probe from the black background to the leaf sample but with still insufficient number of scans; C) the contact probe placed on the leaf sample with 100% of leaf scans.</p> <p> </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#visualizing-measurements-in-viewspecpro-and-spectra","title":"Visualizing measurements in ViewSpecPro and spectra","text":"<p>After spectra acquisition, the data could be visualized in ViewSpecPro software as digital numbers (Figure 11A ) or relative reflectance (Figure 11B ). Not much can be inferred from DN visualization. In contrast, with some basic knowledge about leaf biophysical and structural properties determining its optical properties, you can be sure that you deal with vegetation spectra if visualized as reflectance (Figure 11B ). The radiance reflected from the sample is related to the last measured white reference.</p> <p> </p> <p> </p> <p>Figure 11. The screen of ViewSpecPro software Graph visualization. A) white reference and leaf sample spectra in digital numbers, DN. B) white reference and leaf sample spectra expressed as reflectance. A typical spectral reflectance curve of foliage is displayed.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#integrating-sphere-laboratory-spectra-acquisition","title":"Integrating sphere laboratory spectra acquisition","text":"<p>An integrating sphere (IS) is a piece of equipment for measuring leaf optical properties: reflectance and transmittance. The hollow sphere is coated with highly reflecting material in required spectral interval. Usually, the integrating sphere has several ports for light source, detector cable and object samples. Integrating spheres collect reflected light from samples over a full hemisphere. The sphere, by nature of its internal diffuse (Lambertian) reflection and integration of the energy, is insensitive to directional reflectance features coming from the sample, and therefore, gives a very repeatable \u201caveraged\u201d response to the reflectance of the sample placed in the beam at the sphere port.  The basic principle of measuring leaf directional-hemispherical reflectance (Figure 3B ) and transmittance is given by the position of the light source, leaf sample and detector (Figure 12 ). For practical demonstration of measuring leaf sample spectra in the integrating sphere we will show some examples with the ASD RTS-3ZC integrating sphere (Malvern Panalytical, USA). This type of IS has six ports, to which can be attached light source, detector optical cable, samples, light trap or some of the ports can be plugged. For orientation of IS ports see Figure 13.</p> <p> </p> <p>Figure 12. The general scheme of measurement of the directional-hemispherical reflectance (left) and transmittance (right) in the integrating sphere.</p> <p> </p> <p>Figure 13. The ASD RTS-3ZC integrating sphere. Left: Schema of the ASD integrating sphere and its ports: (A) Reflectance input; (B) Reflectance comparison; (C) Reflectance sample; (D) Transmission input; (E) Specular Exclusion Light Trap; (F) Fibre Adapter Port; (G) Collimated Light Source Assembly. The picture shows the setup of sample\u2018s reflectance measurement. For the white reference measurement content of the ports (B) and (C) are changed as described in Table 1, explained below. Right: Integrating sphere with attached sample or reference panel holders in ports (B) and (C) and optic fibre from the spectroradiometer in port (F).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#the-asd-rts-3zc-integrating-sphere-components","title":"The ASD RTS-3ZC Integrating sphere components.","text":"<p>The light source assembly (Figure 14A, B, C ) provides a collimated beam which illuminates the sample, or the reference panel attached to the IS ports. It can be mounted to the port (A) for reflectance measurements or to the port (D) for transmittance measurements, as will be shown later. The light source has a high and low power setting (Figure 14C ). Mind that LOW is equivalent to one solar unit and suitable for leaf level measurements, while the HIGH setting is greater than one solar unit, which may affect vegetation by overheating or drying during the measurements. The reference standard panels (99% reflectance) are necessary for routine reflectance and transmittance measurements, particularly for optimization process and white reference spectra acquisition (Figure 14A ). The use of light trap is important for sample reflectance measurement \u2013 it prevents the ambient light entering the sphere from the back side of the sample. After mounting the light source assembly to the (A) port it is recommended to check that the light beam illuminates the central part of the port (use a paper sheet to see the beam position; see Figure 14E ).</p> <p> </p> <p>Figure 14. The ASD RTS-3ZC integrating sphere components. A) Components that can be mounted to the sample holders and ports of the IS: Collimated light source assembly, reference standard panels (99% reflectance), light trap. B) Collimated light source assembly \u2013 front view, facing the sphere ports. C) Light source assembly power settings \u2013 LOW power is suitable for measuring optical properties of vegetation. D) Light source assembly (red arrow) mounted to the port (A) for instrument optimization and reflectance measurements. E) The central position of the light beam in the reflectance port (C), viewed through a sheet of paper.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#optimization_1","title":"Optimization","text":"<p>Similarly, as for measurements with the contact probe, the spectroradiometer must be optimized before any data from integrating sphere are collected. Optimization sets the proper settings for the light source being used to collect spectra. If the light source does not change substantially and the instrument is warmed up, there should be no reason to re-optimize. During the optimization process, the light source is attached to the port (A) and all other IS ports should be plugged with reference white material (Figure 15). It is recommended to place the uncalibrated reference to the port (B) and the calibrated one to the port (C). After optimization, a graph will display measurements in raw digital numbers (DN) plotted against wavelength in nm (see Figure 6 ).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#saturation","title":"Saturation","text":"<p>If the spectroradiometer receives a higher radiation flux than it is optimized for, it saturates with signal. It can happen while rearranging reference panels, sample, or light source among the ports. RS3 provides a saturation alarm: when saturation occurs, an audible beep will sound, and the Spectrum Avg progress control will display \u201cSaturation\u201d. If this happens, the instrument must be optimized. When measuring in the field, saturation may occur when sunlight conditions change significantly from diffuse to direct sunlight. Saturation is rare when measuring with a contact probe.</p> <p> </p> <p>Figure 15. The ASD RTS-3ZC integrating sphere components port setup for spectroradiometer optimization. The light source assembly is attached to the port (A), the (B) and the (C) ports are plugged with white reference panels (red arrows), the (D) and (E) ports are plugged also (not seen) with their respective white plugs.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#stray-light","title":"Stray light","text":"<p>The beam of light illuminating the sample or reference is not perfectly collimated. This means that some \u201cstray\u201d radiation is directly getting to the sphere walls without first striking the sample or reference. This background or stray light level may have to be accounted for. Sometimes it is not also possible to absolutely prevent the noise caused by the radiation entering the detector from external environment. The good practice with laboratory spectroscopy is to use dark black painted room (Figure 2C ), however it is not always available. In some vegetation studies, the leaf optical properties are acquired directly in the field (Figure 16 ). Therefore, it is necessary to measure the level of stray light spectra for later correction of collected reflectance and/or transmittance spectra (Figure 17 ). The stray light spectra should be measured in both, reflectance, and transmittance mode (with port configuration as listed in Table 1 ). It is not necessary to measure stray light for each sample, the good practice is to measure stray light at the start and finish of each measurement session. Large numbers of scans should be averaged when performing these measurements (200 or more) to minimize the introduction of noise during subsequent calculations. </p> <p> </p> <p>Figure 16. Using the ASD RTS-3ZC integrating sphere (yellow arrows) in the field for vegetation studies. A) Open-air field laboratory. B) Mobile spectroscopic laboratory located in a tent for cases of rain risk or abundant insects. C) Mobile spectroscopic laboratory from inside. Experimental station in beech forest, \u0160t\u00edtn\u00e1 nad Vl\u00e1\u0159\u00ed, Czechia.</p> <p> </p> <p>Figure 17. The main RS3 screen during the reflectance measurement of stray light in ASD RTS-3ZC integrating sphere. Mind that the stray light reflectance is very low (up to 5 % within VIS and NIR), peaking with higher values at the longer SWIR wavelengths (above 2300 nm).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#measurement-of-the-reflectance-and-transmittance-of-the-leaf-samples","title":"Measurement of the reflectance and transmittance of the leaf samples","text":""},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#integrating-sphere-port-configurations","title":"Integrating sphere port configurations","text":"<p>As previously mentioned, the configuration of IS ports enables to measure reflectance and transmittance of the sample. In both modes it is necessary to acquire spectra of the white reference material, and as well as stray light. Specific IS port configurations for spectra acquisition of reference, stray light, and sample, are indicated in Table 1. Ttwo methods for reflectance and transmittance measurements are presented \u2013 the first one ignores the substitution error, the second one (and the recommended one) corrects for the substitution error. What is the substitution error? The presence of the reference, then of the sample, in the sphere ports modifies the throughput (the portion of reflected radiation that is seen by the detector). This phenomenon is known as substitution error and can be corrected for. The recommended method of IS spectra acquisition is to use the configuration that corrects for the substitution error. While measuring the white reference, a \u201cdummy sample\u201d, which has nearly the same reflectance as the sample (within 20 % is close enough) is put to the side port (in ASD sphere port (B) for reflectance measurement \u2013 Figure 13 ). This will correct for substitution error with only one reference scan for use with several similar samples. Similarly, as with contact probe measurements, after setting the desired IS port configuration, the reference or sample reflectance or transmittance could be recorded withing RS3 environment by pressing the spacebar. Once the spectrum is saved, the RS3 gives a sound signal.  While measuring leaf optical properties, one should keep in mind that the work with biological samples in general and leaves in particular requires to avoid their degradation by long manipulation, drying and heating. Thus, it is recommended to measure the reflectance and transmittance of the leaf with the minimal time lag. This requirement results in frequent changing of the sphere configuration and recording the white reference for each sample. The order of measured quantities (e.g., reflectance of WR and sample) should be designed as a trade-off between excessive IS reconfiguration and minimal time-lag between reflectance (R) and transmittance (T) measurements of the same sample. Due to measurement of two different quantities (R and T) the automatic export of reflectance or transmittance (in the ViewSpecPro software) could not be used in contrast to the contact probe measurements. Recording of WR spectra is necessary for further calculations of reflectance and transmittance from measured DN values.</p> <p>Table 1. Port configurations for measurement of optical quantities in reflectance and transmittance mode. DS \u2013 dummy sample (specimen with not more than 20 % reflectance difference from a real sample); LT \u2013 light trap; P \u2013 plug; S \u2013 sample; white reference standard \u2013 W; Calibrated \u2013 cal; Uncalibrated \u2013 uncal.</p> <p> </p> <p>For some examples of IS ports configurations and manipulation, watch the following short movies:</p> <p> </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#measuring-optical-properties-of-needles-challenge-no-1","title":"Measuring optical properties of needles \u2013 challenge no. 1","text":"<p>Flat leaves with sufficient area to cover the entire port of the IS are relatively easy for optical properties measurements. In contrast, the measurement of optical properties of narrow, irregular leaves of conifers \u2013 coniferous needles - is challenging as the narrow needles do not fully cover the sample port in integrating spheres and the most common practice is to fix needles in a special carrier (Figure 18 ) with gaps between them (Daughtry et al., 1989; Mesarch et al., 1999; Y\u00e1\u00f1ez-Rausell et al., 2014). The proportion of the gaps within a field of view of the IS (gap fraction, GF) must be evaluated from scans of needles within the carrier (usually using Image analysis tools). The GF is later used for signal correction while calculating final values of needle reflectance and transmittance (Lhot\u00e1kov\u00e1 et al., 2021), (Figure 21 ).  In particular, needle transmittance measurements are influenced by the additional signal transmitted through gaps, radiation multiple scattering at the edge of needles and light interaction with a carrier, which often results in erroneous negative transmittance values mainly in the VIS range (as shown, e.g., by (Olascoaga et al., 2016)) for Norway spruce and Scots pine. Therefore, results based on spectral transmittance of needles should be interpreted with care. The downside of thin leaves measurement using the IS with special carriers is the laboriousness of the sample preparation. </p> <p> </p> <p>Figure 18. Measuring optical properties of coniferous needles using special metal carriers (with spectrally black coating). A) Norway spruce needles fixed in the carrier, prepared for scanning with double-lamp scanner for gap fraction assessment. B) Preparing the needle samples into the carriers; patient and skilled personnel is the necessity. C) Scan of the needles in the carrier used for gap fraction calculation by image analysis.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#measuring-specular-reflectance-challenge-no-2","title":"Measuring specular reflectance \u2013 challenge no. 2","text":"<p>Some proportion of the radiation is reflected from the leaf surface. The proportion of radiation, which is reflected under the equal angle as the one of incidence is called specular (mirror-like) reflection (Vanderbilt et al., 1985) (for more details see the part Optical parameters of foliage of this course). There were many efforts to measure leaf specular reflectance based on the assumption that it is polarized. It has been shown that the nature of the leaf surface (presence of waxes, hairs, etc.) determines the degree of linearly polarized reflectance (Grant et al., 1993, 1987). Measuring the degree of light polarization (Grant et al., 1993; Li et al., 2019; Maxwell et al., 2017; Vanderbilt and Grant, 1986) is one method to detect changes on the leaf surface (Maxwell et al., 2017). When the leaf surface strongly polarizes radiation, it usually reflects white specular reflections (Vanderbilt et al., 1985). A polarized light beam is usually used to separate the specular component of reflectance from the diffuse reflectance, which is summarized in a study (Lao et al., 2006). </p> <p> </p> <p>Figure 19. Measuring leaf reflectance by two integrating sphere set-ups: total reflectance (RTOT - left) and reflectance with specular component removed (RSR - right). Yellow star in grey circle represents the optical cable sensor, which was placed on the surface of the integrating sphere (Figure modified from (Neuwirthov\u00e1 et al. 2021/ CC BY 4.0)</p> <p>Usually, the integrating sphere is designed for the purpose of measuring the diffuse part of the scattered radiation from the sample. This is also the case of previous description of measurements in the ASD RTS-3ZC Integrating sphere, when only four ports are used (port (A-D) (ASD Inc., 2008; Pot\u016f\u010dkov\u00e1 et al., 2016b). The fifth port (E) is placed at the angle equal to the angle of incidence of the radiation, therefore it is possible to remove most of the specular component of the reflectance using a light trap placed into this port (Figure 19 ). The specular reflectance component itself could be calculated as a difference between the total reflectance (including both diffused and specular reflectance) and the reflectance with the specular component removed (measured with the light trap in port (E)) (Figure 20 ).</p> <p> </p> <p>Figure 20. The example of leaf total reflectance (DHRF, black curve) of the hawkweed (Hieracim nigrescens) decomposed to the specular (blue curve) and diffuse (red curve) components. Left y-axis shows directional\u2013hemispherical reflectance factor (DHRF) of total reflectance (black curve) and the diffuse component (reflectance with the specular component removed, i.e. measured with the light trap in port [E], red curve) and right y-axis corresponds to specular reflectance component. Grey and light red shadows display variance among the samples by standard deviation value. The greenhouse grown juvenile plant with a leaf detail are shown (Figure by (Neuwirthov\u00e1 et al. 2021/ CC BY 4.0)</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#ascii-export-of-the-spectra-measured-by-the-cp-and-is","title":"ASCII export of the spectra measured by the CP and IS","text":"<p>To process the spectral data further it is useful to export them from the .asd format to .txt. It can be done in the ViewSpecPro software. After setting the input and output folders, the files can be marked and prepared to ASCII export. You can choose to work with DN, reflectance (or other quantities, depends on measurement setup). While measuring reflectance with the contact probe you can directly export sample reflectance \u2013 just keeping in mind that the instrument relates the reflectance to the last WR measured. If you want to export multiple spectra to a single .txt file, it is necessary to mark the \u201cOutput to a Single File\u201d box.</p> <p>For data export by the ViewSpecPro software, use the export in DN numbers (Figure 21 ) to later calculate the reflectance and transmittance with the appropriate white reference spectrum using the following formulas (Figure 22 ).</p> <p> </p> <p>Figure 21. The screen of ViewSpecPro software Processing \u2013 ASCII Export. Example of export data into a single output .txt file - in the case of measurement by the contact probe the direct Reflectance export is possible (red ellipse). In the case of measurement by the integrating sphere, both R and T, the export in the form of digital numbers (DN) is necessary (blue ellipse). If you want to export multiple spectra to a single .txt file, it is necessary to mark the \u201cOutput to a Single File\u201d box (red arrow).</p> <p> </p> <p>Figure 22. The formulas for calculation relative reflectance (Rsample) and transmittance (Tsample) of leaf (left) and needle (right) samples measured in the integrating sphere. Mind that radiance values for WR and stray light recorded in appropriate IS port setup should be used for calculation. Rw, the relative reflectance of the IS walls should be close to 100% and should be provided by the IS manufacturer. As the reflectance and transmittance are measured with the sample placed in different ports having slightly different size and shape of the illuminated sample part, the gap fraction (GF) should be assessed separately for R and T.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#the-pros-and-cons-of-two-types-of-laboratory-spectroscopy-measurement-techniques","title":"The pros and cons of two types of laboratory spectroscopy measurement techniques","text":"<p>Laboratory measurements of leaf optical properties are valuable for retrieval of leaf biophysical properties, i.e., leaf functional traits (Burnett et al., 2021; Ely et al., 2019; Lhot\u00e1kov\u00e1 et al., 2021) connected to plant physiological processes, vegetation stress (Cavender-Bares et al., 2016) and also genetic variance and plant phylogenetic relations (Meireles et al., 2020; Stasinski et al., 2021). Optical properties acquired at the leaf level without an interference of the atmosphere and canopy architecture serve as inputs to the radiative transfer models on higher hierarchical level (Luke\u0161 et al., 2020). However, the two introduced types of leaf level reflectance measurements are not fully substitutable, as mentioned before.  On the one hand, the reflectance (DHRF) measured by the integrating sphere is more reproducible, not prone to directional effects caused by leaf surface anisotropy and could be used for inversion of the most used leaf-level radiative transfer model PROSPECT (Jacquemoud and Baret, 1990). However, the operability of the integrating sphere in the field is limited. Nevertheless, several radiative transfer models require not only reflectance but also transmittance, which cannot be measured by the CP. On the other hand, the reflectance (BRF) measured by the contact probe is much faster than using integrating sphere. Moreover, CP is easier to handle and in combination with a leaf clip it is possible to use it directly in the field (Silva-Perez et al., 2018). However, the directional nature of the specular reflectance prevents to use BRF for PROSPECT model inversion to retrieve leaf traits. There have been conducted efforts to combine spectral derivatives and similarity metrics functions with the PROSPECT to eliminate the difference between BRF and DHRF spectra, and retrieve leaf pigment, dry matter and water content (Wan et al., 2021). However, still the two reflectance quantities (BRF and DHRF) are not fully compatible for radiative transfer modelling.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"In a very short movie (18 s) two ways of spectra expression are shown. There are two groups of measured samples represented by spectral curves. Explain the format of the spectra (mind the units) and guess what type of samples they represent.  First the spectral data are expressed as relative reflectance (no units) indicated by the right Y axis. Mind the axis maximum slightly above the value 1.0; later the spectra are visualized as digital numbers (DN), arbitrary units related to the radiation intensity converted to electric signal. The group of samples with higher values in all wavelengths in both formats correspond to samples of highly reflective Spectralon panel, which correspond to the 100% reflectance (value 1.0). The second group of samples with lower values in DN and reflectance as well corresponds to some vegetation samples. The shape of the spectra expressed as reflectance shows characteristic features: low reflectance in blue and red, peak in green (resulting from chlorophyll absorption), sharp increase in the red edge, NIR plateau and distinctive water absorption bands in the SWIR.  You are going to acquire reflectance of detached leaf samples in the laboratory using the contact probe and a spectroradiometer with 400-2400 nm spectral range. What do you use as a background and why?  White cardboard box Glass Petri dish Black velvet cloth Matte black-coated metal tray Matte black-coated metal tray  You are going to collect a spectral library of leaf level reflectance (BRF) for different cultivars of ornamental species in an arboretum. Your target genera are Rhododendron sp. (Rhododendron) and Corylus sp. (Hazel). How many white reference scans and leaf sample scans should you make?  20 reference samples vs. 100 leaf samples 30 reference samples vs. 30 leaf samples 100 reference samples vs. 20 leaf samples 100 reference samples vs. 20 leaf samples"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#exercise","title":"Exercise","text":""},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#comparison-of-spectra-measured-by-integrating-sphere-and-contact-probe","title":"Comparison of spectra measured by integrating sphere and contact probe","text":"<p>The aim of this exercise is to visualize and compare reflectance spectra acquired by a contact probe (CP) and an integrating sphere (IS) attached to the spectroradiometer ASD FieldSpec 4 Hi-Res. </p> <p>Please proceed to the exercise: Comparison of spectra measured by integrating sphere and contact probe.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#next-unit","title":"Next unit","text":"<p>Proceed with Relating imaging spectroscopy and in-situ or laboratory measurements of vegetation properties</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_02_principles_of_laboratory_spectroscopy.html#references","title":"References","text":"<p>ASD Inc., 2008. Integrating Sphere User Manual; https://www.mapping-solutions.co.uk/downloads/data/ASD/Accessories_Brochure/A1A15.pdf.</p> <p>Burnett, A.C., Serbin, S.P., Davidson, K.J., Ely, K.S., Rogers, A., 2021. Detection of the metabolic response to drought stress using hyperspectral reflectance. Journal of Experimental Botany 72, 6474\u20136489. https://doi.org/10.1093/jxb/erab255</p> <p>Cavender-Bares, J., Meireles, J., Couture, J., Kaproth, M., Kingdon, C., Singh, A., Serbin, S., Center, A., Zuniga, E., Pilz, G., Townsend, P., 2016. Associations of Leaf Spectra with Genetic and Phylogenetic Variation in Oaks: Prospects for Remote Detection of Biodiversity. Remote Sensing 8, 221. https://doi.org/10.3390/rs8030221</p> <p>Daughtry, C.S.T., Biehl, L.L., Ranson, K.J., 1989. A new technique to measure the spectral properties of conifer needles. Remote Sensing of Environment 27, 81\u201391. https://doi.org/10.1016/0034-4257(89)90039-4</p> <p>Ely, K.S., Burnett, A.C., Lieberman-Cribbin, W., Serbin, S.P., Rogers, A., 2019. Spectroscopy can predict key leaf traits associated with source\u2013sink balance and carbon\u2013nitrogen status. Journal of Experimental Botany 70, 1789\u20131799. https://doi.org/10.1093/jxb/erz061</p> <p>Grant, L., Daughtry, C.S.T., Vanderbilt, V.C., 1993. Polarized and specular reflectance variation with leaf surface features. Physiol Plant 88, 1\u20139. https://doi.org/10.1111/j.1399-3054.1993.tb01753.x</p> <p>Grant, L., Daughtry, C.S.T., Vanderbilt, V.C., 1987. Polarized and non-polarized leaf reflectances of Coleus blumei. Environmental and Experimental Botany 27, 139\u2013145. https://doi.org/10.1016/0098-8472(87)90064-5</p> <p>Hovi, A., M\u00f5ttus, M., Juola, J., Manoocheri, F., Ikonen, E., Rautiainen, M., 2020. Evaluating the performance of a double integrating sphere in measurement of reflectance, transmittance, and albedo of coniferous needles. Silva Fenn. 54. https://doi.org/10.14214/sf.10270</p> <p>Jacquemoud, S., Baret, F., 1990. PROSPECT: A model of leaf optical properties spectra. Remote Sensing of Environment 34, 75\u201391. https://doi.org/10.1016/0034-4257(90)90100-Z</p> <p>Jacquemoud, S., Ustin, S., 2019. Leaf Optical Properties. Cambridge University Press, Cambridge. https://doi.org/10.1017/9781108686457</p> <p>Lao, C., Guo, Y., Li, B., 2006. Parameterization of Bidirectional Reflection from Maize Leaves with Measurement in the Principal Plane, in: 2006 Second International Symposium on Plant Growth Modeling and Applications. Presented at the 2006 International Symposium on Plant Growth Modeling, Simulation, Visualization and Applications (PMA), IEEE, Beijing, China, pp. 109\u2013115. https://doi.org/10.1109/PMA.2006.26</p> <p>Lhot\u00e1kov\u00e1, Z., Kopa\u010dkov\u00e1-Strnadov\u00e1, V., Oulehle, F., Homolov\u00e1, L., Neuwirthov\u00e1, E., \u0160vik, M., Janoutov\u00e1, R., Albrechtov\u00e1, J., 2021. Foliage Biophysical Trait Prediction from Laboratory Spectra in Norway Spruce Is More Affected by Needle Age Than by Site Soil Conditions. Remote Sensing 13, 391. https://doi.org/10.3390/rs13030391</p> <p>Li, Y., Chen, Y., Huang, J., 2019. An Approach to Improve Leaf Pigment Content Retrieval by Removing Specular Reflectance Through Polarization Measurements. IEEE Transactions on Geoscience and Remote Sensing 57, 2173\u20132186. https://doi.org/10.1109/TGRS.2018.2871830</p> <p>Luke\u0161, P., Neuwirthov\u00e1, E., Lhot\u00e1kov\u00e1, Z., Janoutov\u00e1, R., Albrechtov\u00e1, J., 2020. Upscaling seasonal phenological course of leaf dorsiventral reflectance in radiative transfer model. Remote Sensing of Environment 246, 111862. https://doi.org/10.1016/j.rse.2020.111862</p> <p>Maxwell, D.J., Partridge, J.C., Roberts, N.W., Boonham, N., Foster, G.D., 2017. The effects of surface structure mutations in Arabidopsis thaliana on the polarization of reflections from virus-infected leaves. PLoS ONE 12, e0174014. https://doi.org/10.1371/journal.pone.0174014</p> <p>Meireles, J.E., Cavender\u2010Bares, J., Townsend, P.A., Ustin, S., Gamon, J.A., Schweiger, A.K., Schaepman, M.E., Asner, G.P., Martin, R.E., Singh, A., Schrodt, F., Chlus, A., O\u2019Meara, B.C., 2020. Leaf reflectance spectra capture the evolutionary history of seed plants. New Phytologist 228, 485\u2013493. https://doi.org/10.1111/nph.16771</p> <p>Mesarch, M.A., Walter-Shea, E.A., Asner, G.P., Middleton, E.M., Chan, S.S., 1999. A Revised Measurement Methodology for Conifer Needles Spectral Optical Properties: Evaluating the Influence of Gaps between Elements. Remote Sensing of Environment 177_192.</p> <p>Neuwirthov\u00e1, E., Lhot\u00e1kov\u00e1, Z., Luke\u0161, P., Albrechtov\u00e1, J., 2021. Leaf Surface Reflectance Does Not Affect Biophysical Traits Modelling from VIS-NIR Spectra in Plants with Sparsely Distributed Trichomes. Remote Sensing 13, 4144. https://doi.org/10.3390/rs13204144</p> <p>Olascoaga, B., Mac Arthur, A., Atherton, J., Porcar-Castell, A., 2016. A comparison of methods to estimate photosynthetic light absorption in leaves with contrasting morphology. Tree Physiol 36, 368\u2013379. https://doi.org/10.1093/treephys/tpv133</p> <p>Pot\u016f\u010dkov\u00e1, M., \u010cerven\u00e1, L., Kupkov\u00e1, L., Lhot\u00e1kov\u00e1, Z., Luke\u0161, P., Hanu\u0161, J., Novotn\u00fd, J., Albrechtov\u00e1, J., 2016a. Comparison of Reflectance Measurements Acquired with a Contact Probe and an Integration Sphere: Implications for the Spectral Properties of Vegetation at a Leaf Level. Sensors 16, 1801. https://doi.org/10.3390/s16111801</p> <p>Pot\u016f\u010dkov\u00e1, M., \u010cerven\u00e1, L., Kupkov\u00e1, L., Lhot\u00e1kov\u00e1, Z., Luke\u0161, P., Hanu\u0161, J., Novotn\u00fd, J., Albrechtov\u00e1, J., 2016b. Comparison of Reflectance Measurements Acquired with a Contact Probe and an Integration Sphere: Implications for the Spectral Properties of Vegetation at a Leaf Level. Sensors 16, 1801. https://doi.org/10.3390/s16111801</p> <p>Schaepman-Strub, G., Schaepman, M.E., Painter, T.H., Dangel, S., Martonchik, J.V., 2006. Reflectance quantities in optical remote sensing\u2014definitions and case studies. Remote Sensing of Environment 103, 27\u201342. https://doi.org/10.1016/j.rse.2006.03.002</p> <p>Silva-Perez, V., Molero, G., Serbin, S.P., Condon, A.G., Reynolds, M.P., Furbank, R.T., Evans, J.R., 2018. Hyperspectral reflectance as a tool to measure biochemical and physiological traits in wheat. Journal of Experimental Botany 69, 483\u2013496. https://doi.org/10.1093/jxb/erx421</p> <p>Stasinski, L., White, D.M., Nelson, P.R., Ree, R.H., Meireles, J.E., 2021. Reading light: leaf spectra capture fine\u2010scale diversity of closely related, hybridizing arctic shrubs. New Phytologist 232, 2283\u20132294. https://doi.org/10.1111/nph.17731</p> <p>Vanderbilt, V.C., Grant, L., 1986. Polarization Photometer To Measure Bidirectional Reflectance Factor R(55\u00b0, 0\u00b0; 55\u00b0,180\u00b0) Of Leaves. Optical Engineering 24, 566\u2013571. https://doi.org/10.1117/12.7973861</p> <p>Vanderbilt, V.C., Grant, L., Daughtry, C.S.T., 1985. Polarization of light scattered by vegetation. Proceedings of the IEEE 73, 1012\u20131024. https://doi.org/10.1109/PROC.1985.13232</p> <p>Wan, L., Zhang, J., Xu, Y., Huang, Y., Zhou, W., Jiang, L., He, Y., Cen, H., 2021. PROSDM: Applicability of PROSPECT model coupled with spectral derivatives and similarity metrics to retrieve leaf biochemical traits from bidirectional reflectance. Remote Sensing of Environment 267, 112761. https://doi.org/10.1016/j.rse.2021.112761</p> <p>Y\u00e1\u00f1ez-Rausell, L., Malenovsky, Z., Clevers, J.G.P.W., Schaepman, M.E., 2014. Minimizing Measurement Uncertainties of Coniferous Needle-Leaf Optical Properties. Part II: Experimental Setup and Error Analysis. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 7, 406\u2013420. https://doi.org/10.1109/JSTARS.2013.2292817</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html","title":"Relating imaging spectroscopy and in-situ or laboratory measurements of vegetation properties","text":"<p>Imaging spectroscopy enables us to characterise the properties of vegetation in a certain area of an ecosystem via electromagnetic radiation spectra derived from image data. Imaging spectroscopy at the hyperspectral level allows for a detailed study of vegetation\u2019s spectral characteristics, providing more data than the multispectral level. Each pixel captured by imaging spectroscopy contains information on the spectral signature of objects it contains. The spectral signature of objects covered by the pixel are expressed by a spectral curve, as described in the principles of laboratory spectroscopy.  The detail provided by this method makes it possible to retrieve data about the biophysical properties of vegetation (a subset of leaf functional traits) on a large scale, such as an entire ecosystem or geographic region. Leaf functional traits, such as pigment and water content, serve as important indicators of plant physiological status and can be used for monitoring vegetation states over time (\u0160vik et al., 2023).  Imaging spectroscopy allows for the non-destructive and repeatable monitoring of vegetation and, thus, offers an efficient way to map and monitor leaf functional traits. Satellite-based retrieval of functional traits can provide a direct link between local-scale functional trait variation and regional-scale ecosystem function estimation since such retrieval can be repeated across time and space, and is capable of producing fine-resolution data across broad areas (see the review articles for more reading (Asner et al., 2015; Hill et al., 2019; Homolov\u00e1 et al., 2013; Ustin et al., 2009; Ustin and Gamon, 2010). Retrieval of leaf functional traits from optical data instead of in-situ or laboratory analyses of sampled foliage brings remarkable time and labour cost savings and has been intensively used in recent decades (see rev. (Hill et al., 2019). Optical methods for detecting functional vegetation traits are tightly coupled to the phylogenetic and genetic backgrounds of plants, enabling the study of plant biodiversity (Cavender-Bares et al., 2020). Moreover, functional vegetation traits are also linked to belowground processes and the structure and function of microbial communities, which can be a  challenge for current multidisciplinary research (Cavender-Bares et al., 2022).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#objectives","title":"Objectives","text":"<p>In this theme, you will learn about:</p> <ul> <li>leaf functional traits: biophysical and structural parameters of vegetation and their ecological significance</li> <li>laboratory and in-situ measurements of leaf functional traits: vegetation parameters (chlorophyll, LMA, water content)</li> </ul> <p>After finishing this theme you will be able to:</p> <ul> <li>understand significance of functional leaf traits retrieved from imaging spectroscopy </li> <li>understand how to design \u201cground truth\u201d sampling of vegetation (trees)</li> <li>measure leaf functional traits in-situ or in the laboratory for ground truth acquisition</li> </ul>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#introduction-to-leaf-functional-traits-and-their-ecological-significance","title":"Introduction to leaf functional traits and their ecological significance","text":"<p>Leaf functional traits refer to key chemical, structural, morphological, and physiological characteristics of plant leaves that contribute to indirectly plant fitness (growth, survival, and reproduction) (Violle et al., 2007). Investigations into functional traits aim to recognize the ecological and evolutionary underpinnings of why plants \u201cbehave\u201d as they do: why they grow in some places and not others, and how they interact with their biotic and abiotic environment (Reich, 2014). Some commonly investigated leaf functional traits include chlorophyll content, leaf mass per area (LMA), non-photosynthetic pigment accumulation, stomatal density, nitrogen content, phenology, and morphology (Figure 1 ).</p> <ul> <li>Chlorophyll content functions as a proxy for leaf photosynthetic capacity. High chlorophyll content is positively correlated with high photosynthetic capacity and changes in chlorophyll content may reflect stress or nutrient deficiencies (such as N, or Mg). </li> <li>LMA refers to the ratio between leaf dry mass and leaf area. This morphological trait works well as an indicator of where a plant falls on the \u201cleaf economic spectrum\u201d in terms of resource acquisition. LMA is correlated with maximum photosynthetic rate, growth potential, and even decomposition rate (de la Riva et al., 2016). </li> <li>Non-photosynthetic pigments include anthocyanins (typically red, purple, or blue, depending on pH), carotenoids (orange) and flavonoids (yellow). The concentration of non-photosynthetic pigments is indicative of a leaf\u2019s capacity to endure certain types of environmental stress, such as high light, UV radiation, temperature extremes, and even pathogen attack. Carotenoids function to dissipate excess light energy (e.g., via the xanthophyll cycle) while anthocyanins and flavonoids provide a range of protective functions, notably absorbing UV radiation and acting as antioxidants. </li> <li>Stomatal density is influenced by genotype, light conditions, and atmospheric CO2 levels. Low stomatal density may reduce transpiration water loss, however, high stomatal density allows for a more calibrated response to environmental conditions. </li> <li>Nitrogen content is essential for producing major cellular components of photosynthesis, such as chlorophyll, thylakoid proteins, and enzymes involved in CO2 fixation (notably ribulose 1-5 bisphosphate carboxylase/oxygenase). Nitrogen content is correlated with photosynthetic capacity, and can also be analysed to diagnose nutrient deficiencies. </li> <li>Phenology refers to the movement of plants through stages in their growing season. How early or late a plant accumulates secondary metabolites or begins to flower can have implications for that plant\u2019s success. Phenology is particularly important with remote sensing as accurate species identification depends on knowing how a plant changes over its life cycle. </li> <li>Morphology is a deceptively simple trait that plays a role in how well-suited a species is to its environment, and changes in morphology can occur as a result of specific stressors influencing the phenotypic plasticity of a species. Moreover, leaf morphology should be taken into account when planning experiments as not all physiological instruments are suited to all leaf morphologies.</li> </ul> <p>Each functional trait represents a physiological or morphological trade-off the plant is making. The theory of the \u201cplant economic spectrum\u201d provides a framework for evaluating species according to their trade-offs \u2014 strategies of rapid acquisition or resource conservation (Freschet et al., 2010). In general, resource-limiting environments favour slower growth rates, long leaf lifespans, high LMA. Conversely, in environments where water, light, or nutrients are available, the selective pressure of competition favours short lifespans, high photosynthetic rates, low LMA and leaf nitrogen content. Functional traits help explain the distributions of plant species and communities across gradients of light, water, and nutrient availability. Analysis of leaf functional traits may also highlight specific adaptations that benefit a given plant in its environment by comparing stress levels among plants exposed to the same conditions. Investigating sites with known constraints, such as low nutrient availability or high UV exposure, can reveal which leaf characteristics are preferentially selected by that environmental stressor. Locally adapted genotypes show a minimized range of potential leaf traits (Read et al., 2014).</p> <p> </p> <p>Figure 1: Overview of common leaf function traits which exist on a spectrum. From top to bottom, LMA (leaf mass per area) refers to how dense a leaf is, i.e., how much biomass it has accumulated per unit of area. Leaves with low LMA tend to grow faster and require less investment from the plant, while leaves with a higher LMA tend to are more long-lasting and have more structural compounds, such as lignin and cellulose, bound up in them. Plant height affects how a plant can compete in its environment. Tall plants have an advantage for capturing light for photosynthesis, however, low growing plants may have other advantages such as rapid horizontal spread. Stomatal density is an integral component to gas exchange and water relations in plants and is a trait which shows a lot of plasticity depending on environmental conditions and genotypic background. Chlorophyll content is directly related to the photosynthetic capacity of plants while non-photosynthetic pigments (such as anthocyanins, carotenoids, and flavonoids) protect leaves from high energy radiation and other stressors. Nitrogen content is also related to photosynthetic capacity and can be indicative of nutrient deficiencies. Phenolic profiles refer to the content of flavonoids and phenolic acids, which confer induvial protective benefits to plants. Phenology refers to how the plant changes as it moves through its growing season. Phenology is an essential consideration for remote sensing as spectral signatures may change. Phenology is dependent on environmental cues and can be affected by climate change. Morphology is the physical form of the plant, which can provide advantages or disadvantages in a given environment. Typically, plants with compact morphology perform better in extreme environments (e.g., high elevation, low moisture, high irradiance) where survival is a priority. By contrast, in plentiful environments, competition is more important, thus rapid, expansive growth is more advantageous.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#canopy-composition-and-architecture-affects-imaging-spectroscopy","title":"Canopy composition and architecture affects imaging spectroscopy","text":"<p>When analysing image data, never assume ideal conditions (i.e., one layer of uniformly oriented leaves from a single species in a pixel). It is necessary to take in account different leaf structures, canopy positions, arrangements of leaves on branches, angles of branching, and crown architecture. When analysing data from a whole forest stand, additional parameters corresponding to the characteristics of the whole plant community should be considered (i.e., different layers of vegetation within a canopy, species abundance, and vegetation density). Therefore, the study of vegetation at the level of tree crown, canopy, whole forest stand, or whole ecosystem brings the challenge of upscaling/downscaling laboratory and in-situ analyses and spectroscopy to imaging spectroscopy. One example is using radiative transfer models to upscale/downscale (Figure 2 [REMOVED TEMPORARILY] )  (Schaepman et al., 2009). Laboratory spectroscopy brings individual spectral curves obtained from the field of view of the equipment attached to laboratory spectroradiometer \u2013 contact probe or integration sphere, while image spectroscopy brings data reflected on top of a canopy.  For this reason, it is important to think carefully about the design of leaf sampling \u2014 from which laboratory measurements of biophysical leaf traits will be derived and then used for up- or downscaling to image data.</p> <p>[REMOVED TEMPORARILY] Figure 2: Coupled states, processes and scales ranging from cellular architecture to global biogeochemical cycles. The contribution of linked radiative transfer models in down-and upscaling ranges from leaves to biomes.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#ground-truth-for-imaging-spectroscopy-vegetation-sampling","title":"Ground truth for imaging spectroscopy: vegetation sampling","text":"<p>Ground truth data collection or \u201cground truthing\u201d is the measurement in-situ of selected foliage samples or measurement of sampled foliage material in a lab. Ground truth data then enable calibration of image data, and aid in the interpretation and analysis of what is being sensed. When preparing to acquire ground truth data for the interpretation of imaging spectroscopic data, it is necessary to take in account all heterogeneity of the vegetation cover to be studied in a given area/ region.  First of all, a design of foliage sampling must be established, which corresponds to the desired outcome.  Design has to be applied on several hierarchical levels, starting from number of sites / plots / stands to be studied, number of plant species to be studied per site, number of leaves of foliage samples per one plant.</p> <ul> <li>Number of sites \u2013 two contrasting sites are not enough for interpretation of imaging data, at least three or more (4-5) per studied area are needed to show gradients in observed properties of vegetation. </li> <li>Selection of plant species to be studied per site \u2013 depends on the purpose of the study. For example, when studying one plant species or monoculture, then dominant as well as co-dominant individuals should be included in the ratio they appear in a stand. However, when studying forest biodiversity-ecosystem functioning relationships, limiting tree sampling to dominant trees when analysing stand-level relationships (\u03b413C response of trees to drought) may bias these outcomes (Martin-Blangy et al., 2021).</li> <li>Selection of foliage samples per one individual - again depends on a purpose of the study. There is usually a strong vertical gradient on radiation quality and quantity throughout the canopy or individual tree crown. For example, functional leaf traits show high variation with average light condition and quality within a canopy, where lower canopy leaves tend to be thinner and have lower photosynthetic rates and altered pigment pools to account for the lower light quality and intensity (Niinemets, 2007). Thus, to sample ground truth for image data from the top of a canopy (TOC), it is necessary to sample sunlit parts of a crown and then it can be independently of a azimuth orientation of a branch (Lhot\u00e1kov\u00e1 et al., 2007). In case of evergreen trees, such as conifers, it is necessary to take in account different needle age classes represented on a sunlit part of a branch since their functional leaf traits and correspondingly leaf optical properties change with age remarkably (Figure 3 ). Variability in leaf traits related to the leaf age should be taken into account in tropical trees (Chavana-Bryant et al., 2019) or species with continuous leaf development (Neuwirthov\u00e1 et al., 2021(Figure 4 )).</li> </ul> <p> </p> <p>Figure 3: Needle age class affects functional and optical leaf traits. A) Photosynthetic pigment \u2013 chlorophyll a+b content (Cab), B) needle water content expressed as equivalent water thickness (EWT) and needle (leaf) mass per area (LMA) of different needle age classes (NAC). NAC1 corresponds to the current year needles, NAC2 to previous year needles and NAC4 to the mixed sample of four and more seasons old needles. The needle traits are shown separately for exposed (sunlit) and shaded part of the crown of mature Norway spruce. The difference between leaf traits attributed to needle age within the respective crown position were tested by one-way ANOVA and Tukey-Kramer multiple comparison test. Different letters above the boxplots correspond to significant differences at \u03b1 = 0.05. Mind that in some traits (Cab) the differences among NACs are equal at both crown vertical positions, meanwhile in others (EWT, LMA) not. The differences between needle optical properties \u2013 D) reflectance and E) transmittance correspond to the trend observed in needle traits. The dark green line corresponds to significant differences among NACs in reflectance or transmittance (p-value, right axis). (Figure modified from Lhot\u00e1kov\u00e1 et al., 2021 / CC BY 4.0.</p> <p> </p> <p>Figure 4: Leaves of different age and developmental stage. A) Needle age classes in evergreen conifers, example of Norway spruce (Picea abies). The colour coding and numbers correspond to the age of needles. Mind that the youngest needle age classes are most abundant and located at the edge of the branch. Adapted after (Lhot\u00e1kov\u00e1 et al., 2021 B) Presence of juvenile leaves on the branch of Silver birch (Betula pendula) \u2014marked by white ellipses \u2014 with prevailing mature leaves. Sampled in June (18 June = DOY 169). (Figure modified from Neuwirthov\u00e1 et al. 2021/ CC BY 4.0)</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#measurements-of-functional-leaf-traits","title":"Measurements of functional leaf traits","text":"<p>Variations in relatively easy-to-measure plant traits (stable in time, related to individual appearance, e. g.: height, weight, LMA, N content) are tightly coupled to hard-to-measure functions (responsible for actual physiological status, which is also more variable in time, e. g., light capture and utilisation, photosynthetic rate, water content) (Cavender-Bares et al., 2020).  Traits such as LMA and N are readily detectable via spectroscopy. Other traits\u2014such as leaf lifespan or photosynthetic rates, reveal more about how a plant invests and allocates resources over time, are harder to measure. However, hard-to measure traits, correlated with these readily detectable traits, can thus be inferred, permitting greater insight into ecological processes and also reflect about the survival in different kinds of environments (Cavender-Bares et al., 2020).</p> <p>Below there are listed and described leaf-level foliage parameters related to photosynthetic capacity, production, physiological status.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#chlorophyll-content","title":"Chlorophyll content","text":"<p>Leaf chlorophyll content is arguably the most important photosynthetic indicator of vegetation function and physiological condition. Chlorophyll absorbs photosynthetically active radiation and defines the potential of vegetation for photosynthesis and is of high importance in determining ecosystem function and productivity (Houborg et al., 2009). Chlorophyll content further provides an estimate of nutrient status as a substantial proportion of leaf nitrogen is tied up in chlorophyll and is thus an important variable for agricultural remote sensing (Daughtry, 2000; Gitelson, 2011; Gitelson and Solovchenko, 2018; Haboudane et al., 2002).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#laboratory-assessment-of-chlorophyll-content","title":"Laboratory assessment of chlorophyll content","text":"<p>Extraction of the pigment into a selected organic solvent and subsequent spectrophotometric evaluation is accepted as the standard procedure for the determination of chlorophyll content in leaves. Different solvents with different extraction efficiencies can be used and appropriate equations are assigned to convert absorbance to chlorophyll concentration (Minocha et al., 2009; Porra et al., 1989; Wellburn, 1994). An example of chlorophyll extraction process is shown in Figure 5. In case of using dimethylformamide as a solvent, it is not necessary to homogenize the samples. The downside of this solvent is that it is a carcinogenic compound and should be handled within a chemical hood and protective gloves. Several other organic solvents (e.g., acetone, ethanol, dimethyl sulfoxide) can be used for pigment extraction with various efficiency, for details see (Minocha et al., 2009). The destructive nature of the biochemical assessment does not allow monitoring of developmental or seasonal changes in chlorophyll content in the same leaf or plant. Therefore, rapid, and non-destructive optical methods for assessing chlorophyll content have been proposed and are now increasingly used when acquiring ground truth for remote sensing studies.</p> <p> </p> <p>Figure 5: Main steps of biochemical assessment of chlorophyll. A) Cutting the leaf samples by a puncher of known inner diameter (leaf sample area). B) Leaf samples in a known volume of solvent. C) Leaf pigment extracts after 7 days in 4\u00b0C and darkness. Mind that the colour of extract corresponds with the green and white part of the variegated leaves. D) Sample extract moved to the spectrophotometry cuvette for measurement. E) Internal chamber of a spectrophotometer with cuvette position and the beam path through the sample.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#non-destructive-optical-chlorophyll-assessment-by-portable-devices","title":"Non-destructive optical chlorophyll assessment by portable devices","text":"<p>Various portable devices are available for non-destructive optical assessment of chlorophyll content (Figure 6 ). Since the 1990s, the SPAD (Soil plant analysis development, Konica-Minolta, Inc., Osaka, Japan) has been the most widely used, followed by the CCM-200 (OptiSciences, Inc., Hudson, NH, USA) (reviewed in (Donnelly et al., 2020; Parry et al., 2014)) or the MultispeQ (PhotosynQ Inc., East Lansing, MI, USA) (Kuhlgert et al., 2016). The Dualex-4 Scientific instrument (FORCE-A, Orsay, France) is designed to measure leaf chlorophyll content including other leaf pigments (flavonols and anthocyanins by analysing their screening effect on chlorophyll fluorescence) (Cerovic et al., 2012; Goulas et al., 2004). All these portable chlorophyll meters measure leaf transmittance at two different wavelengths: the \"index band\", where transmittance is more sensitive to changes in leaf chlorophyll content, and the \"reference band\", where transmittance is more stable regardless of changing chlorophyll content.  Chlorophyll above 700 nm could be related the chlorophyll content of the leaf (Gitelson et al., 1999). This finding is used by another chlorophyll meter that works by measuring the ratio of fluorescence in the 735 and 700 nm region, the CCM-300 (OptiSciences, Inc., Hudson, NH, USA). The advantage of a fluorescence-based chlorophyll meter is the ability to measure narrow or small leaves without the requirement to cover the entire measurement area in contrast to the transmittance-based devices mentioned above. </p> <p> </p> <p>Figure 6: Examples of handheld optical instruments for chlorophyll assessment with the principle of measurement indicated.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#calibration-of-chlorophyll-meter-readings-to-absolute-values-of-chlorophyll-content","title":"Calibration of chlorophyll meter readings to absolute values of chlorophyll content","text":"<p>Chlorophyll meter readings can be used as relative values to compare plants under different treatments or at different developmental stages. Some instruments have built-in calibration equations (CCM-300, Dualex-4 Scientific) or such equations can be constructed to obtain absolute chlorophyll content (Coste et al., 2010; Dong et al., 2019; Donnelly et al., 2020; Parry et al., 2014). Relationships between chlorophyll meter data and absolute chlorophyll content tend to be non-linear and species-specific due to species differences in leaf morphology and structure (Coste et al., 2010; Donnelly et al., 2020; Parry et al., 2014; Uddling et al., 2007). For examples see Figure 7.</p> <p> </p> <p>Figure 7: Examples of relationships between chlorophyll content assessed in the laboratory (X-axes, Total chlorophyll in \u00b5g.cm-2 in A and B) and chlorophyll content assessed by handheld optical instruments. A) Fluorescence-based chlorophyll meter CCM-300. Relative units of chlorophyll fluorescence ratio (CFR) on Y-axis. The best model fits are shown \u2013 thick blue = logarithmic fit for all studied species and leaf morphologies. Black line = polynomial fit for laminar leaves (e.g. leaves of temperate trees), cyan line = logarithmic fit for grasses (four different species from Arctic-alpine grassland), red line = polynomial fit for Norway spruce needles. B) Transmittance-based chlorophyll meter SPAD for grass leaves (three different species from Arctic-alpine grassland). The best model fits are shown \u2013 thick blue line = polynomial fit for all three species, green line = polynomial fit for D. cespitosa and C. villosa \u2013 two species with similar leaf microstructure (shown in C). C) Examples of leaf structure: laminar leaf of temperate trees and shrubs (example on hazel \u2013 Corylus avellana); needle leaf of Norway spruce (Picea abies); grass leaves of three species from Arctic-alpine grassland: Deschampsia cespitosa, Calamagrostis villosa and Mollinia caerulea. Hand-microtome sections, bright field, light microscopy.</p> <p>As mentioned above, SPAD chlorophyll meter has been the most widely used including remote sensing e.g., (Darvishzadeh et al., 2019). We present a relationship (linear model training) constructed on a dataset of 19 woody species sampled four times during the growing season in the botanical garden as an example of successful calibration between SPAD relative readings and absolute chlorophyll values. Regarding the leaf structure leaves represented three groups 1) deciduous species, 2) evergreens and 3) leaves represented by indoor grown Ficus species corresponding to the legend in Figure 8. In this case, the performance of polynomial and linear model training was similar, showing equal \\(R^2\\) of 0.86. The equations describing the polynomial and linear relationship between absolute chlorophyll content and SPADvalues (i.e., trained model) are shown in Table 1 including \\(R^2\\) and root mean squared error in relative SPAD units.</p> <p> </p> <p>Figure 8: The relationship between Total chlorophyll extracted in the lab (units \u00b5g\u00b7cm^(-2)) and measured SPADvalues with the chlorophyll meter. Blue line corresponds to polynomial model fit, black line corresponds to linear model. Dashed lines of respective colours correspond to 95% confidence intervals. Laminar leaves are symbol-coded by leaf type: circle for deciduous species, triangle for evergreen species and star for Ficus species.</p> Table 1. The relationships between measured SPAD*values* and Total chlorophyll extracted in the lab from *Figure 8* expressed as polynomial and linear equations. Chlorophyll meter Model fit Equation \\(R^2\\) RMSE [ SPADvalues ] SPAD Polynom SPAD= -0.0026TC^2 + 1.0587TC + 7.4766 0.86 7.585 SPAD Linear SPAD= 0.8577TC+10.1521 0.86 7.698 <p>Linear equations are easy to invert or the relationship between the training data (biochemically assessed chlorophyll content and chlorophyll meter readings) can be constructed directly using the chlorophyll meter reading as independent and chlorophyll content as dependent variable, which is the usual case of most of the studies  (Casa et al., 2015; Cerovic et al., 2012; Parry et al., 2014; Uddling et al., 2007). However, regarding the biophysical principles, the chlorophyll meter readings represent the leaf optical properties (transmittance), which are determined by are the physical and chemical properties of the leaf and not vice versa.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#leaf-anthocyanin-content","title":"Leaf anthocyanin content:","text":"<p>Anthocyanins are common water-soluble pigments of higher plants that are responsible for the red, purple, and blue colouration of various parts of plants, including leaves, flowers and fruits. During autumn senescence, anthocyanins accumulate in the leaves of higher plants (Junker and Ensminger, 2016) and are responsible for their red coloration (Gitelson et al., 2009). During leaf senescence, anthocyanin synthesis provides leaf protection, delays leaf fall, and enhances nutrient resorption (Hoch et al., 2003). Anthocyanins are antioxidants (Hoch et al., 2001) and their main function in plants is protection against environmental stress factors such as UV radiation (by shielding the photosynthetic apparatus), photooxidative stress, extreme temperatures, and pathogen attack.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#laboratory-assessment-of-anthocyanins-content","title":"Laboratory assessment of anthocyanins content","text":"<p>There are several methods for anthocyanin content detection: one of the simplest ways is to use paper chromatography. Frequently used high performance liquid chromatography (HPLC) enables identify individual chemical compounds as anthocyanins are very diverse plant protective pigment group. Total anthocyanins can be assessed also by simple spectrophotometry similarly to chlorophyll and carotenoids mentioned above. The peak of anthocyanin absorbance in organic extracts (e.g., methanol) is typically in the range of 500-550 nm and the height of the peak indicates the concentration of anthocyanin in the extract. The extract absorbance is spectrophotometrically determined at 524 and 653 nm and leaf anthocyanins content could be calculated based on the equation A524 \u2212 0.24A653 (Junker and Ensminger, 2016; Murray et al., 1994). Then the absorbance values should be converted to molar concentration using the Beer-Lambert equation and the universal molar extinction coefficient \u03b5 = 30,000 L \u00b7 mol^(-1)\u00b7 cm^(-1) could be used for general anthocyanin without the knowledge of individual compounds (Merzlyak et al., 2008) and finally related to leaf area. This method provides a precise and accurate measurement of anthocyanin content in the sample, and it is commonly used in laboratory settings. </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#non-destructive-optical-anthocyanins-assessment-by-portable-devices","title":"Non-destructive optical anthocyanins assessment by portable devices","text":"<p>Anthocyanins absorb light at specific wavelengths, which affects the reflectance of light by the plant leaves. Based on the reflected light from the leaves, is possible to estimate the concentration of anthocyanins in the plant leaves by spectroradiometer. Based on spectral reflectance data from 450 to 700 nm was used vegetation indices ARI and RGR were to track anthocyanins as indicators for plant stress and senescence and also indices ExG and GRVI contrasting yellow and orange from green in autumn leaves (Junker and Ensminger, 2016). Both spectroscopic methods are based on the same principle of light absorption by anthocyanin pigments. Measuring reflectance is non-destructive and can be used to measure anthocyanin content in leaves without removing them from the plant, making it useful for field applications. </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#leaf-mass-per-area","title":"Leaf mass per area","text":"<p>Leaf mass per area (LMA) \u2014 the ratio of a leaf dry mass to its surface area \u2014 and its reciprocal, specific leaf area (SLA), correspond to a fundamental trade-off of leaf construction costs versus light harvesting potential (Niinemets, 2007). LMA also changes with phenology and increases during the growing season as the trees accumulate structural compound to the leaves (Figure 9, (Neuwirthov\u00e1 et al., 2021). Foliar nitrogen and LMA form a key axis of variation that describes end-members between \u201ccheap\u201d thinner, low-LMA leaves with high leaf nitrogen, higher photosynthetic rates and faster turnover, versus thick, \u201cexpensive\u201d leaves with high LMA, low nitrogen, slower turnover. The gradient in LMA in leaves with various morphology, internal structure and longevity is demonstrated in three types of leaves: laminar leaves of temperate deciduous  woody species, grasses of relict alpine-arctic tundra and conifer needles (Figure 10 ). One of the main factors affecting LMA in shade intolerant plants is solar irradiation, and its intensity and spectral composition. Due to the gradient in irradiance, the LMA and palisade parenchyma thickness increases with canopy height (Zhang et al., 2019).</p> <p> </p> <p>Figure 9: Examples of change in leaf mass per area (LMA) and water content per leaf area (WCLA) during the growing season in three deciduous hemiboreal species (silver birch, black alder and common aspen). Numbers correspond to the day of the year, starting May 21 and ending October 1. Difference among the values for different DOYs in one group was tested by the analysis of variance, \u03b1 = 0.05. Different colors correspond to individual DOYs during the season; line in boxes corresponds to the median value, error bars show inter-quartile range (IQR); dots correspond to outliers defined as three times IQR; same letters above boxes indicate no significance, different letters correspond to significant difference among DOYs within one graph at \u03b1 = 0.05. (Figure modified from Neuwirthov\u00e1 et al. 2021 / CC BY 4.0)</p> <p> </p> <p>Figure 10: Examples of the gradient in leaf mass per area (LMA) for leaf types with different morphology, photosynthetic capacity and longevity represented by three leaf types (the internal leaf structure is shown below the group axis). The difference among leaf types were tested by one-way ANOVA and Tukey-Kramer multiple comparison test. Different letters above the boxplots correspond to significant differences at \u03b1 = 0.05.</p> <p>After the leaf collection, the fresh mass is assessed. The area of fresh leaves than can be assessed by scanning. In field conditions, leaves can be photographed on contrasting background with a scale bar and leaf area could be assessed by image analysis later, similarly as from leaf scans. With thin leaves and needles it is recommended to use a scanner with upper lamp. Leaves are dried in an oven at 60\u00b0C for at least 48 hours until the dry weight becomes constant. Finally leaf (dry) mass per area (LMA) and its reciprocal (SLA) is calculated. From the difference between the fresh and dry leaf mass the water mass can be calculated. The water content can be expressed as percentage of water in fresh leaf mass or as equivalent water thickness (EWT) or water content per leaf area (WCLA) \u2013 the water mass per leaf area. </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#leaf-water-content","title":"Leaf water content","text":"<p>Water represents a major fresh leaf chemical constituent, usually exceeds 50% of the fresh leaf mass. Water is released from leaves as vapor via transpiration and continually replenished by xylem from roots. In the conditions of drought (which can mean either atmospheric demand for transpiration or soil water scarcity or both), the leaf water content decreases. The extreme decrease in leaf water content manifests as leaf wilting. It is not appropriate to include the water content of the leaf as a biochemical or structural property; although it is closely linked, it is rather a biophysical property. For the simplest estimation of plant water content, the ratio of the fresh (FW) to dry weight (DW) of a given sample is used. It is given by the relationship: FW/DW. In remote sensing and spectroscopy studies, leaf water content is usually expressed as the  equivalent water thickness (EWT) (Mobasheri and Fatemi, 2013), which is calculated from the fresh dry weight of the leaf and its area. It is given by the relationship: (FW-DW)/Area The usual leaf water metric in plant physiology is relative water content (RWC).  Relative water content is defined as the amount of water in the leaf at the time of sampling in relation to the maximum water content that the leaf can hold (water saturation). To assess RWC, first the  fresh weight of the leaves (FW) is determined. Then, a freshly weighed leaf is placed in a tube with water in a well-lit place for 5 hours \u2014 that enables the leaf to fully saturate with water. After the leaf is fully saturated, its turgescent weight (TW) is recorded. Finally, the leaf is dried, and the dry mass is recorded (DW). RWC calculation is based on equation: RWC (%) = (FW\u2212DW/TW\u2212DW)\u00d7 100 Water uptake and conduction in the plant is mainly due to osmosis and water potential gradients between soil, plant, and atmosphere. The uptake of water by the plant is only due to the difference in water potentials from higher water potential sites to lower water potential sites. The water potential can inform us about the plant's water management and individual physiological state. The water potential of the xylem (water transport tissue) in a leaf is dependent on environmental conditions such as irradiance, evaporation, and soil moisture (Pallardy and Kozlowski, 1981). Humidity and temperature, which vary significantly throughout the day, determine the water potential of the surrounding atmosphere.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#seasonality-of-vegetation-traits-leaf-phenology","title":"Seasonality of vegetation traits, leaf phenology","text":"<p>Leaf functional traits serve as important indicators of plant physiological status and can be used for monitoring vegetation state across longer time periods (\u0160vik et al., 2023).  Depending on the season and leaf phenology, there is visible variation in internal structure among species and an increasing ratio of palisade to spongy parenchyma (Luke\u0161 et al., 2020). Not only season, but also position on the twig plays a role in leaf development, the rate of leaf development on the twig is determined by the intensity of irradiance, which is also determined by the crown architecture (Kikuzawa, 1995). Leaves in exposed canopy layers are better adapted to strong light than leaves in lower canopy layers (Niinemets et al., 2015).  Temperature and CO2 levels are also related to leaf development during the growing season. Increased temperature and CO2 levels of islands bring an earlier start of the growing season, prolonging the growing season and delaying leaf senescence (Nezval et al., 2020; Wang et al., 2019).</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"Name three pigments found in leaves. Which pigment is responsible for photosynthesis?  Plant pigments include chlorophyll, carotenoids, anthocyanins, and flavonoids. Chlorophyll is the pigment responsible for photosynthesis.  What is LMA and how can you expect the LMA of a plant to change during the growing season?  LMA is the ratio of leaf dry mass to surface area. It is expected to increase during the growing season as juvenile leaves mature and use the products of photosynthesis to accumulate structural compounds, such as cellulose and lignin.  What are the two ways to measure the chlorophyll content in leaves described in the text and which way is more useful for studying phenological changes and why?  The text describes 1) a laboratory method for measuring chlorophyll, where chlorophyll is extracted into a solvent and then the absorbance of the sample at a specific wavelength is measured using a spectrophotometer, and 2) an optical method for measuring chlorophyll using a portable device that assessed transmittance at specific wavelengths. Because phenological changes occur within a plant over a growing season, the optical method is better suited because the same plant can be measured repeatedly as it moves through phenological stages. By contrast, the laboratory method is destructive and so a plant can be measured only once.   Variation in leaf traits can be found within the canopy of an individual tree. How does canopy position relate to LMA and what is the cause of this variation?  Leaves that are higher in a tree canopy will likely show greater LMA values than leaves at the bottom of a canopy. This is due to the fact that sun exposure increases the thickness of the mesophyll. As sunlight is transmitted through a canopy, leaves of the lower layers received less sunlight and have correspondingly thinner mesophylls \u2013 thinner mesophylls require less structural material (e.g. cellulose), and thus the LMA is lower."},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#tutorial","title":"Tutorial","text":"<p>Relating imaging spectroscopy and in-situ spectroscopy </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#next-unit","title":"Next unit","text":"<p>Proceed with Machine learning in imaging spectroscopy</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_03_relating_imagery_lab_vegetation.html#references","title":"References","text":"<p>Asner, G.P., Martin, R.E., Anderson, C.B., Knapp, D.E., 2015. Quantifying forest canopy traits: Imaging spectroscopy versus field survey. Remote Sensing of Environment 158, 15\u201327. https://doi.org/10.1016/j.rse.2014.11.011</p> <p>Casa, R., Castaldi, F., Pascucci, S., Pignatti, S., 2015. Chlorophyll estimation in field crops: an assessment of handheld leaf meters and spectral reflectance measurements. J. Agric. Sci. 153, 876\u2013890. https://doi.org/10.1017/S0021859614000483</p> <p>Cavender-Bares, J., Gamon, J.A., Townsend, P.A. (Eds.), 2020. Remote Sensing of Plant Biodiversity. Springer International Publishing, Cham. https://doi.org/10.1007/978-3-030-33157-3</p> <p>Cavender-Bares, J., Schweiger, A.K., Gamon, J.A., Gholizadeh, H., Helzer, K., Lapadat, C., Madritch, M.D., Townsend, P.A., Wang, Z., Hobbie, S.E., 2022. Remotely detected aboveground plant function predicts belowground processes in two prairie diversity experiments. Ecological Monographs 92, e01488. https://doi.org/10.1002/ecm.1488</p> <p>Cerovic, Z.G., Masdoumier, G., Ghozlen, N.B., Latouche, G., 2012. A new optical leaf\u2010clip meter for simultaneous non\u2010destructive assessment of leaf chlorophyll and epidermal flavonoids. Physiologia Plantarum 146, 251\u2013260. https://doi.org/10.1111/j.1399-3054.2012.01639.x</p> <p>Chavana-Bryant, C., Malhi, Y., Anastasiou, A., Enquist, B.J., Cosio, E.G., Keenan, T.F., Gerard, F.F., 2019. Leaf age effects on the spectral predictability of leaf traits in Amazonian canopy trees. Science of The Total Environment 666, 1301\u20131315. https://doi.org/10.1016/j.scitotenv.2019.01.379</p> <p>Coste, S., Baraloto, C., Leroy, C., Marcon, \u00c9., Renaud, A., Richardson, A.D., Roggy, J.-C., Schimann, H., Uddling, J., H\u00e9rault, B., 2010. Assessing foliar chlorophyll contents with the SPAD-502 chlorophyll meter: a calibration test with thirteen tree species of tropical rainforest in French Guiana. Ann. For. Sci. 5.</p> <p>Darvishzadeh, R., Skidmore, A., Abdullah, H., Cherenet, E., Ali, A., Wang, T., Nieuwenhuis, W., Heurich, M., Vrieling, A., O\u2019Connor, B., Paganini, M., 2019. Mapping leaf chlorophyll content from Sentinel-2 and RapidEye data in spruce stands using the invertible forest reflectance model. International Journal of Applied Earth Observation and Geoinformation 79, 58\u201370. https://doi.org/10.1016/j.jag.2019.03.003</p> <p>Daughtry, C., 2000. Estimating Corn Leaf Chlorophyll Concentration from Leaf and Canopy Reflectance. Remote Sensing of Environment 74, 229\u2013239. https://doi.org/10.1016/S0034-4257(00)00113-9</p> <p>de la Riva, E.G., Olmo, M., Poorter, H., Ubera, J.L., Villar, R., 2016. Leaf Mass per Area (LMA) and Its Relationship with Leaf Structure and Anatomy in 34 Mediterranean Woody Species along a Water Availability Gradient. PLoS One 11, e0148788. https://doi.org/10.1371/journal.pone.0148788</p> <p>Dong, T., Shang, J., Chen, J.M., Liu, J., Qian, B., Ma, B., Morrison, M.J., Zhang, C., Liu, Y., Shi, Y., Pan, H., Zhou, G., 2019. Assessment of Portable Chlorophyll Meters for Measuring Crop Leaf Chlorophyll Concentration. Remote Sensing 11, 2706. https://doi.org/10.3390/rs11222706</p> <p>Donnelly, A., Yu, R., Rehberg, C., Meyer, G., Young, E.B., 2020. Leaf chlorophyll estimates of temperate deciduous shrubs during autumn senescence using a SPAD-502 meter and calibration with extracted chlorophyll. Annals of Forest Science 77, 30. https://doi.org/10.1007/s13595-020-00940-6</p> <p>Freschet, G.T., Cornelissen, J.H.C., Van Logtestijn, R.S.P., Aerts, R., 2010. Evidence of the \u2018plant economics spectrum\u2019 in a subarctic flora. Journal of Ecology 98, 362\u2013373. https://doi.org/10.1111/j.1365-2745.2009.01615.x</p> <p>Gitelson, A.A., 2011. Nondestrictive estimation of foliar pigment (chlorophylls, carotenoids, and anthocyanins) contents: evaluating a semianalytical three-band model., in: Hyperspectral Remote Sensing of Vegetation. CRC Press, p. 141.</p> <p>Gitelson, A.A., Buschmann, C., Lichtenthaler, H.K., 1999. The Chlorophyll Fluorescence Ratio F735/F700 as an Accurate Measure of the Chlorophyll Content in Plants. Remote Sensing of Environment 69, 296\u2013302. https://doi.org/10.1016/S0034-4257(99)00023-1</p> <p>Gitelson, A.A., Chivkunova, O.B., Merzlyak, M.N., 2009. Nondestructive estimation of anthocyanins and chlorophylls in anthocyanic leaves. American Journal of Botany 96, 1861\u20131868. https://doi.org/10.3732/ajb.0800395</p> <p>Gitelson, A.A., Solovchenko, A., 2018. Noninvasive Quantification of Foliar Pigments, Principles and Implementation. CRC Press, p. 28.</p> <p>Goulas, Y., Cerovic, Z.G., Cartelat, A., Moya, I., 2004. Dualex: a new instrument for field measurements of epidermal ultraviolet absorbance by chlorophyll fluorescence. Appl. Opt. 43, 4488. https://doi.org/10.1364/AO.43.004488</p> <p>Haboudane, D., Miller, J.R., Tremblay, N., Zarco-Tejada, P.J., Dextraze, L., 2002. Integrated narrow-band vegetation indices for prediction of crop chlorophyll content for application to precision agriculture. Remote Sensing of Environment 81, 416\u2013426. https://doi.org/10.1016/S0034-4257(02)00018-4</p> <p>Hill, J., Buddenbaum, H., Townsend, P.A., 2019. Imaging Spectroscopy of Forest Ecosystems: Perspectives for the Use of Space-borne Hyperspectral Earth Observation Systems. Surv Geophys 40, 553\u2013588. https://doi.org/10.1007/s10712-019-09514-2</p> <p>Hoch, W.A., Singsaas, E.L., McCown, B.H., 2003. Resorption Protection. Anthocyanins Facilitate Nutrient Recovery in Autumn by Shielding Leaves from Potentially Damaging Light Levels. Plant Physiology 133, 1296\u20131305. https://doi.org/10.1104/pp.103.027631</p> <p>Hoch, W.A., Zeldin, E.L., McCown, B.H., 2001. Physiological significance of anthocyanins during autumnal leaf senescence. Tree Physiology 21, 1\u20138. https://doi.org/10.1093/treephys/21.1.1</p> <p>Homolov\u00e1, L., Luke\u0161, P., Malenovsk\u00fd, Z., Lhot\u00e1kov\u00e1, Z., Kaplan, V., Hanu\u0161, J., 2013. Measurement methods and variability assessment of the Norway spruce total leaf area: implications for remote sensing. Trees 27, 111\u2013121. https://doi.org/10.1007/s00468-012-0774-8</p> <p>Houborg, R., Anderson, M., Daughtry, C., 2009. Utility of an image-based canopy reflectance modeling tool for remote estimation of LAI and leaf chlorophyll content at the field scale. Remote Sensing of Environment 113, 259\u2013274. https://doi.org/10.1016/j.rse.2008.09.014</p> <p>Junker, L.V., Ensminger, I., 2016. Relationship between leaf optical properties, chlorophyll fluorescence and pigment changes in senescing Acer saccharum leaves. Tree Physiology 36, 694\u2013711. https://doi.org/10.1093/treephys/tpv148</p> <p>Kikuzawa, K., 1995. Leaf phenology as an optimal strategy for carbon gain in plants. Canadian Journal of Botany 73, 158\u2013163.</p> <p>Kuhlgert, S., Austic, G., Zegarac, R., Osei-Bonsu, I., Hoh, D., Chilvers, M.I., Roth, M.G., Bi, K., TerAvest, D., Weebadde, P., Kramer, D.M., 2016. MultispeQ Beta: a tool for large-scale plant phenotyping connected to the open PhotosynQ network. R. Soc. open sci. 3, 160592. https://doi.org/10.1098/rsos.160592</p> <p>Lhot\u00e1kov\u00e1, Z., Albrechtov\u00e1, J., Malenovsk\u00fd, Z., Rock, B.N., Pol\u00e1k, T., Cudl\u00edn, P., 2007. Does the azimuth orientation of Norway spruce (Picea abies/L./Karst.) branches within sunlit crown part influence the heterogeneity of biochemical, structural and spectral characteristics of needles? Environmental and Experimental Botany 59, 283\u2013292. https://doi.org/10.1016/j.envexpbot.2006.02.003</p> <p>Lhot\u00e1kov\u00e1, Z., Kopa\u010dkov\u00e1-Strnadov\u00e1, V., Oulehle, F., Homolov\u00e1, L., Neuwirthov\u00e1, E., \u0160vik, M., Janoutov\u00e1, R., Albrechtov\u00e1, J., 2021. Foliage Biophysical Trait Prediction from Laboratory Spectra in Norway Spruce Is More Affected by Needle Age Than by Site Soil Conditions. Remote Sensing 13, 391. https://doi.org/10.3390/rs13030391</p> <p>Luke\u0161, P., Neuwirthov\u00e1, E., Lhot\u00e1kov\u00e1, Z., Janoutov\u00e1, R., Albrechtov\u00e1, J., 2020. Upscaling seasonal phenological course of leaf dorsiventral reflectance in radiative transfer model. Remote Sensing of Environment 246, 111862. https://doi.org/10.1016/j.rse.2020.111862</p> <p>Martin-Blangy, S., Charru, M., G\u00e9rard, S., Jactel, H., Jourdan, M., Morin, X., Bonal, D., 2021. Mixing beech with fir or pubescent oak does not help mitigate drought exposure at the limit of its climatic range. Forest Ecology and Management 482, 118840. https://doi.org/10.1016/j.foreco.2020.118840</p> <p>Merzlyak, M.N., Chivkunova, O.B., Solovchenko, A.E., Naqvi, K.R., 2008. Light absorption by anthocyanins in juvenile, stressed, and senescing leaves. Journal of Experimental Botany 59, 3903\u20133911. https://doi.org/10.1093/jxb/ern230</p> <p>Minocha, R., Martinez, G., Lyons, B., Long, S., 2009. Development of a standardized methodology for quantifying total chlorophyll and carotenoids from foliage of hardwood and conifer tree species. Canadian Journal of Forest Research 39, 849\u2013861. https://doi.org/10.1139/X09-015</p> <p>Mobasheri, M.R., Fatemi, S.B., 2013. Leaf Equivalent Water Thickness assessment using reflectance at optimum wavelengths. Theoretical and Experimental Plant Physiology 25, 196\u2013202.</p> <p>Murray, J.R., Smith, A.G., Hackett, W.P., 1994. Differential dihydroflavonol reductase transcription and anthocyanin pigmentation in the juvenile and mature phases of ivy (Hedera helix L.). Planta 194, 102\u2013109. https://doi.org/10.1007/BF00201040</p> <p>Neuwirthov\u00e1, E., Kuusk, A., Lhot\u00e1kov\u00e1, Z., Kuusk, J., Albrechtov\u00e1, J., Hallik, L., 2021. Leaf Age Matters in Remote Sensing: Taking Ground Truth for Spectroscopic Studies in Hemiboreal Deciduous Trees with Continuous Leaf Formation. Remote Sensing 13, 1353. https://doi.org/10.3390/rs13071353</p> <p>Nezval, O., Krejza, J., Sv\u011btl\u00edk, J., \u0160igut, L., Hor\u00e1\u010dek, P., 2020. Comparison of traditional ground-based observations and digital remote sensing of phenological transitions in a floodplain forest. Agricultural and Forest Meteorology 291, 108079. https://doi.org/10.1016/j.agrformet.2020.108079</p> <p>Niinemets,  \u00fcLo, 2007. Photosynthesis and resource distribution through plant canopies. Plant, Cell &amp; Environment 30, 1052\u20131071. https://doi.org/10.1111/j.1365-3040.2007.01683.x</p> <p>Niinemets, \u00dc., Keenan, T.F., Hallik, L., 2015. A worldwide analysis of within-canopy variations in leaf structural, chemical and physiological traits across plant functional types. New Phytologist 205, 973\u2013993. https://doi.org/10.1111/nph.13096</p> <p>Pallardy, S.G., Kozlowski, T.T., 1981. Water Relations of Populus Clones. Ecology 62, 159. https://doi.org/10.2307/1936679</p> <p>Parry, C., Blonquist, J.M., Bugbee, B., 2014. In situ measurement of leaf chlorophyll concentration: analysis of the optical/absolute relationship: The optical/absolute chlorophyll relationship. Plant Cell Environ 37, 2508\u20132520. https://doi.org/10.1111/pce.12324</p> <p>Porra, R., Thompson, W., Kriedemann, P., 1989. Determination of Accurate Extinction Coefficients and Simultaneous-Equations for Assaying Chlorophyll-a and Chlorophyll-B Extracted with 4 Different Solvents - Verification of the Concentration. Biochimica Et Biophysica Acta 975, 384\u2013394. https://doi.org/10.1016/S0005-2728(89)80347-0</p> <p>Read, Q.D., Moorhead, L.C., Swenson, N.G., Bailey, J.K., Sanders, N.J., 2014. Convergent effects of elevation on functional leaf traits within and among species. Functional Ecology 28, 37\u201345.</p> <p>Reich, P.B., 2014. The world-wide \u2018fast\u2013slow\u2019 plant economics spectrum: a traits manifesto. Journal of Ecology 102, 275\u2013301. https://doi.org/10.1111/1365-2745.12211</p> <p>Schaepman, M.E., Ustin, S.L., Plaza, A.J., Painter, T.H., Verrelst, J., Liang, S., 2009. Earth system science related imaging spectroscopy\u2014An assessment. Remote Sensing of Environment 113, S123\u2013S137. https://doi.org/10.1016/j.rse.2009.03.001</p> <p>\u0160vik, M., Luke\u0161, P., Lhot\u00e1kov\u00e1, Z., Neuwirthov\u00e1, E., Albrechtov\u00e1, J., Campbell, P.E., Homolov\u00e1, L., 2023. Retrieving plant functional traits through time series analysis of satellite observations using machine learning methods. International Journal of Remote Sensing 44, 3083\u20133105. https://doi.org/10.1080/01431161.2023.2216847</p> <p>Uddling, J., Gelang-Alfredsson, J., Piikki, K., Pleijel, H., 2007. Evaluating the relationship between leaf chlorophyll concentration and SPAD-502 chlorophyll meter readings. Photosynth. Res. 91, 37\u201346. https://doi.org/10.1007/s11120-006-9077-5</p> <p>Ustin, S.L., Gamon, J.A., 2010. Remote sensing of plant functional types. New Phytologist 186, 795\u2013816. https://doi.org/10.1111/j.1469-8137.2010.03284.x</p> <p>Ustin, S.L., Gitelson, A.A., Jacquemoud, S., Schaepman, M., Asner, G.P., Gamon, J.A., Zarco-Tejada, P., 2009. Retrieval of foliar information about plant pigment systems from high resolution spectroscopy. Remote Sensing of Environment 113, S67\u2013S77. https://doi.org/10.1016/j.rse.2008.10.019</p> <p>Violle, C., Navas, M.-L., Vile, D., Kazakou, E., Fortunel, C., Hummel, I., Garnier, E., 2007. Let the concept of trait be functional. Oikos 116, 882\u2013892.</p> <p>Wang, S., Ju, W., Pe\u00f1uelas, J., Cescatti, A., Zhou, Y., Fu, Y., Huete, A., Liu, M., Zhang, Y., 2019. Urban\u2212rural gradients reveal joint control of elevated CO2 and temperature on extended photosynthetic seasons. Nat Ecol Evol 3, 1076\u20131085. https://doi.org/10.1038/s41559-019-0931-1</p> <p>Wellburn, A.R., 1994. The Spectral Determination of Chlorophylls a and b, as well as Total Carotenoids, Using Various Solvents with Spectrophotometers of Different Resolution. Journal of Plant Physiology 307\u2013313. Zhang, X., Jin, G., Liu, Z., 2019. Contribution of leaf anatomical traits to leaf mass per area among canopy layers for five coexisting broadleaf species across shade tolerances at a regional scale. Forest Ecology and Management 452, 117569. https://doi.org/10.1016/j.foreco.2019.117569</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_navig.html","title":"In-situ and laboratory spectroscopy of vegetation","text":"<p>In-situ and laboratory spectroscopy provide point-based measurements of reflectance of selected samples using portable or laboratory spectroradiometers. This theme is devoted to spectral properties of vegetation, their relation to selected biochemical parameters such as leaf pigments and water content, and their changes due to stress caused, e.g., by drought or pollution. Methods of vegetation spectral properties measurement using spectroradiometers and airborne hyperspectral sensors are explained. The theme is divided into three theoretical lessons, concluded with a quiz and a list of literature:</p> <ul> <li>Optical parameters of foliage \u2013 leaf level</li> <li>Principles of laboratory spectroscopy</li> <li>Relating imaging spectroscopy and in-situ or laboratory measurements of vegetation properties</li> </ul> <p>A practical exercise is devoted to the comparison of spectra measured by an integrating sphere and a contact probe. An additional tutorial points to the issue of relating spectra obtained from imaging and in-situ spectroscopy.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_navig.html#next-unit","title":"Next unit","text":"<p>Proceed with Machine learning in imaging spectroscopy</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_image_insitu_tutorial.html","title":"Tutorial: Relating imaging spectroscopy and in-situ spectroscopy.","text":""},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_image_insitu_tutorial.html#comparison-of-spectra-acquired-by-field-spectroradiometer-and-by-hyperspectral-camera-attached-to-the-drone","title":"Comparison of spectra acquired by field spectroradiometer and by hyperspectral camera attached to the drone","text":"<p>Are the spectra measured by the spectroradiometer ASD FieldSpec4 Wide-Res in the field and spectra extracted from Headwall Nano-Hyperspec\u00ae images comparable? Are the vegetation indices calculated from both datasets comparable? We will demonstrate this comparison on measurements of Calamagrostis villosa in June, July and August 2020. We had four stable plots (diameter app. 2 m) where two subplots (a, b) were randomly selected every month and measured by the spectroradiometer and by geodetic GNSS instrument for the precise location (for the details of the study design see \u010cerven\u00e1 et al. (2022)). In the same dayas, flights for acquisition of hyperspectral data were performed. From the pixels of hyperspectral images corresponding to measured GNSS positions of sublots the spectra were extracted. However, both spectra sources have different spectral resolution, so data from spectroradiometer with 2151 bands had to be resampled to the same 269 bands as image data has. You can see all the resulting spectra in Figure 1.</p> <p> </p> <p>Figure 1: Spectra acquired by spectroradiometer ASD FieldSpec4 Wide-Res in the field and spectra extracted from Headwall Nano-Hyperspec\u00ae images for four plots with two subplots and three months.</p> <p>You can see from Figure 1 that spectra are similar but not the same: * Spectra extracted from the image data are noisier (without further processing).  * The difference between the spectra is more visible at near infrared. It can be influenced by different ground sample area which was measured. Spectra in the field were measured from approximately 0.5 m height above the canopy, i.e. a circle with a diameter of 22 cm on the ground. Image data has pixel size of only 9 cm (resampled from original 3cm data). * Some of the spectra can be also influenced by changing weather conditions in the mountains</p> <p>To quantify the differences, the paired t-tests were computed for all the wavelengths and the p-values were saved using script 1. The results can be seen in Figure 2. </p> <pre><code>data=read.delim(\"Calamagrostis_drone_spectroradiometer_269bands.txt\")\nfix(data)\ndim(data)\n\nresults &lt;- vector()\n\nfor (I in 5:273)\n{\n    model&lt;-t.test(data[data$origin==\"drone\",I],data[data$origin==\"spectroradiometer\",I],paired=TRUE)\n    out&lt;-capture.output(model);\n    results &lt;- append(results,grep (\"p-value\", out, value = TRUE) );\n}\n\nwrite(results, \"ttest_ Calamagrostis.txt\") \n\n</code></pre> <p>Script 1: Paired T-test calculation in R</p> <p> </p> <p>Figure 2: P-values of t-test comparing the reflectances from image data and spectroradiometer</p> <p>The null hypothesis of paired t-test says that the reflectance means of both instruments are equal at the given wavelength or in the other words used in R: \u201ctrue mean difference is equal to 0\u201d. The null hypothesis is rejected when the p-value is less than a pre-specified significance level which is usually set to 0.05.  At most of the wavelengths the null hypothesis was not rejected, so the reflectances of both methods of measurement should be comparable. But it can be clearly seen, that p-values are considerably lower in near infrared than in visible part of spectra. In the next step we try to answer the second question if the vegetation indices calculated from both datasets are also comparable. We had selected four indices suitable for chlorophyll content estimations: two of them are normalized difference indices (NDVI, NDVI2), one is simple ratio (Carter4) and the last one (TCARI) is more complicated formula with some constants. For formulas and references see Table 1.</p> <p> </p> <p>Table 1: Formulas for selected vegetation indices</p> <p>Firstly, we performed paired t-test and here are the results from R for all four indices:</p> <p>NDVI: t = -0.79486, df = 23, p-value = 0.4348 NDVI2: t = -0.57476, df = 23, p-value = 0.571 Carter4: t = 1.8077, df = 23, p-value = 0.08375 TCARI: t = -1.3071, df = 23, p-value = 0.204 For all four indices the null hypothesis was not rejected based on the p-values. To find out more about the indices we plotted indices calculated from image data against indices calculated from spectroradiometer data (Figure 3) and added the linear trend which says how well they are correlated. We can see that both normalized difference indices perform the best, simple ratio is only slightly worse and TCARI index has the worst correlation. </p> <p> </p> <p>Figure 3: Correlations of selected indices calculated from image data and spectroradiometer data.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_image_insitu_tutorial.html#conclusions","title":"Conclusions","text":"<p>Two questions were raised at the beginning of this tutorial: Are the spectra measured by the spectroradiometer ASD FieldSpec4 Wide-Res in the field and spectra extracted from Headwall Nano-Hyperspec\u00ae images comparable? Are the vegetation indices calculated from both datasets comparable? The answer to both questions is based on our example yes, however, there are also differences between them which can be caused by different factors \u2013 mostly it is the height from which the spectra were acquired. In our case the area on the ground is not of absolutely the same extend for both measurements\u2019 methods. Also, the atmosphere can influence data acquired from higher altitudes more. The noise especially in the image data is another issue and application of some smoothening filter could be helpful before further data processing. Ratio-based indices and normalized difference indices calculated based on data from different devices can be more comparable because they also eliminate different illumination of data which can easily happen in the real word (especially in the mountainous areas with very changeable weather conditions).  </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_image_insitu_tutorial.html#references","title":"References","text":"<p>CASTER, G. A. (1994): Ratios of leaf reflectance in narrow wavebands as indicator of plant stress. International Journal of Remote Sensing, 3, 15, 697\u2013704.</p> <p>\u010cERVEN\u00c1, L., KUPKOV\u00c1, L., POT\u016e\u010cKOV\u00c1, M., LYS\u00c1K, J., 2020. Seasonal Spectral Separability of Selected Grasses: Case Study From The Krkono\u0161e Mts. Tundra Ecosystem. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. RLIII-B3-2020, 371\u2013376. https://doi.org/10.5194/isprs-archives-RLIII-B3-2020-371-2020</p> <p>GITELSON, A., MERZLYAK, M. N. (1994): Quantitative estimation of chlorophyll-a using reflectance spectra: eRperiments with autumn chestnut and maple leaves. Journal of Photochemistry and Photobiology B: Biology, 3, 22, 247\u2013252. </p> <p>HABOUDANE, D., JOHN, R., MILLER, J. R., TREMBLAY, N., ZARCO-TEJADA, P. J. DERTRAZE, L. (2002): Integrated narrow-band vegetation indices for prediction of crop chlorophyll content for application to precision agriculture. Remote Sensing of Environment, 2\u20133, 81, 416\u2013426.</p> <p>TUCKER, C. J. (1979): Red and photographic infrared linear combinations for monitoring vegetation. Remote Sensing of Environment, 2, 8, 127\u2013150. </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_probe_sphere_exercise.html","title":"Comparison of spectra measured by integrating sphere and contact probe","text":"<p>The aim of this exercise is to visualize and compare reflectance spectra acquired by a contact probe (CP) and an integrating sphere (IS) attached to the spectroradiometer ASD FieldSpec 4 Hi-Res.  The spectra were collected for homogenous material represented by green colored paper and natural material represented by a green leaf. </p> <ul> <li> <p>Prerequisites</p> <ul> <li>Installed spreadsheet editor (MS Office Excel, LibreOffice Calc or other) or R      </li> <li>Downloaded data (module4/theme3_exercise_spectra_comparision); based on your preferred working environment, use either the file<code>Exercise_M4_T3_data.xlsx</code> (excel spreadsheet) or <code>Exercise_M4_T3_data_CP.txt</code> and <code>Exercise_M4_T3_data_IS.txt</code> (text file) The dataset consists of:<ul> <li>wavelength (column <code>Wavelength</code>)</li> <li>three spectra measured by contact probe (CP) at three different places of the sample (column <code>paper_CP_XXXXX</code>, <code>leaf_CP_XXXXX</code>)</li> <li>one spectrum measured by integrating sphere (IS) (column <code>IS_green_paper</code>, <code>IS_green_leaf</code>).</li> </ul> </li> </ul> </li> <li> <p>Tasks</p> <ul> <li>Data visualization</li> <li>Compare the reflectance spectra from the integrating sphere and contact probe </li> </ul> </li> </ul>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_probe_sphere_exercise.html#1-data-visualization","title":"1. Data visualization","text":"<p>Prepare graphs displaying all of the reflectance spectra acquired by the contact probe. Do measurements of spectral variability on leaf and paper differ in any way? And if yes, then why? Calculate the median spectra for both materials.</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_probe_sphere_exercise.html#2-comparision-of-reflectance-spectra","title":"2. Comparision of reflectance spectra","text":"<p>Compare the reflectance spectra acquired by the integrating sphere with the computed median spectra from the contact probe (for both materials). What variations can you find between the spectra? </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_probe_sphere_exercise.html#exercise-solution","title":"Exercise solution","text":"<p>Proceed to example solution Comparison of spectra measured by integrating sphere and contact probe - report</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/03_spectra_probe_sphere_exercise.html#back-to-theme","title":"Back to theme","text":"<p>Proceed by returning to Principles of laboratory spectroscopy</p>"},{"location":"module4/03_relating_imagery_lab_vegetation/solution/03_spectra_probe_sphere_solution.html","title":"Comparison of spectra measured by integrating sphere and contact probe  - report","text":""},{"location":"module4/03_relating_imagery_lab_vegetation/solution/03_spectra_probe_sphere_solution.html#report","title":"Report","text":""},{"location":"module4/03_relating_imagery_lab_vegetation/solution/03_spectra_probe_sphere_solution.html#_1","title":"Comparison of spectra measured by integrating sphere and contact probe  - report","text":""},{"location":"module4/03_relating_imagery_lab_vegetation/solution/03_spectra_probe_sphere_solution.html#data-visualization","title":"Data visualization","text":"<p>The graphs show that the three spectra measured for green paper are more similar than the three spectra measured for green leaf.  It is caused by the paper's homogeneity; the leaf varies more. However, the differences are very small for both materials,  so there are no obvious measurement errors, and the leaf shows no signs of senescence or disease.</p> <p> </p> Green leaf spectra measured by contact probe <p> </p> Green paper spectra measured by contact probe <p>Median spectra for both materials were calculated.</p> <p> </p> Example of calculated median spectra (green column) for contact probe measurements"},{"location":"module4/03_relating_imagery_lab_vegetation/solution/03_spectra_probe_sphere_solution.html#comparision-of-reflectance-spectra","title":"Comparision of reflectance spectra","text":"<p>Although the spectra are similar, there are noticeable differences.  The spectra measured by the integrating sphere exhibit noise at the lowest and highest wavelengths. In addition,  different measurement principles cause distinctions, which seem to be more visible for  leaf measurements.  If you are interested in the comparison of spectra acquired by different devices, have a look at Pot\u016f\u010dkov\u00e1 et al., 2016,  especially Section 2.3 for spectra comparison methods and Section 3.1. for results.</p> <p> </p> Comparision of green leaf spectra measured by contact probe and integrating sphere <p> </p> Comparision of green paper spectra measured by contact probe and integrating sphere"},{"location":"module4/03_relating_imagery_lab_vegetation/solution/03_spectra_probe_sphere_solution.html#references","title":"References","text":"<p>Pot\u016f\u010dkov\u00e1, M., \u010cerven\u00e1, L., Kupkov\u00e1, L., Lhot\u00e1kov\u00e1, Z., Luke\u0161, P., Hanu\u0161, J., Novotn\u00fd, J., Albrechtov\u00e1, J. (2016): Comparison of Reflectance Measurements Acquired with a Contact Probe and an Integration Sphere: Implications for the Spectral Properties of Vegetation at a Leaf Level. Sensors, 16, 1801. 10.3390/s16111801 </p>"},{"location":"module4/03_relating_imagery_lab_vegetation/solution/03_spectra_probe_sphere_solution.html#back-to-theme","title":"Back to theme","text":"<p>Proceed by returning to Principles of laboratory spectroscopy</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html","title":"04 exercise cnn 1d","text":"<ul> <li><p>Prerequisities (This notebook can be run either online using Google Colab or on your local machine)</p> <ul> <li>A Google account for accessing Google Colab (link to notebook). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.</li> </ul> <p>or</p> <ul> <li><p>A Python environment with the necessary libraries (manual). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.</p> </li> <li><p>Downloaded data (module4.zip/theme4_exercise_machine_learning) The dataset consists of:</p> <ul> <li>Hyperspectral RPAS imagery of B\u00edl\u00e1 Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: BL_202008_imagery.tif</li> <li>a raster with reference data: BL_202008_reference.tif</li> <li>Pretrained models and corresponding classified rasters: /sample_results/*</li> </ul> </li> </ul> </li> <li><p>Tasks</p> <ul> <li>Preprocess imagery for Deep Learning</li> <li>Classify the hyperspectral image using 1D CNN</li> <li>Observe how hyperparameter values alter classification results</li> <li>Evaluate your results and compare to our pretrained classifier</li> <li>Optional: Classify a urban scene</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchnet as tnt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom os.path import join\nfrom time import perf_counter\nfrom tqdm import notebook as tqdm\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom etrainee_m4_utils import image_preprocessing\nfrom etrainee_m4_utils import inference_utils\nfrom etrainee_m4_utils import visualisation_utils\n\n# GLOBAL SETTINGS\nplt.rcParams['figure.figsize'] = [4, 4]\nnp.set_printoptions(precision=2, suppress=True)  # Array print precision\n</pre> import torch import torch.nn as nn import torch.optim as optim import torchnet as tnt  import numpy as np import matplotlib.pyplot as plt  from os.path import join from time import perf_counter from tqdm import notebook as tqdm  from sklearn.model_selection import KFold, StratifiedKFold, train_test_split from sklearn.metrics import classification_report  from etrainee_m4_utils import image_preprocessing from etrainee_m4_utils import inference_utils from etrainee_m4_utils import visualisation_utils  # GLOBAL SETTINGS plt.rcParams['figure.figsize'] = [4, 4] np.set_printoptions(precision=2, suppress=True)  # Array print precision In\u00a0[\u00a0]: Copied! <pre># Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n# default: 'pavia_centre'\nds_name = 'bila_louka'\n\n# Get a list of class names\n_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)\n</pre> # Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre' # default: 'pavia_centre' ds_name = 'bila_louka'  # Get a list of class names _, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name) <p>Please fill correct paths to your training and reference rasters (just pointing the root_path variable to the project folder should do):</p> In\u00a0[\u00a0]: Copied! <pre>root_path = 'C:/folder/where/this/project/is/saved'\n\n# PATHS TO TRAINING DATA\n# Krkonose\nimagery_path   = join(root_path, 'BL_202008_imagery.tif')\nreference_path = join(root_path, 'BL_202008_reference.tif')\n# Pavia\n#imagery_path   = join(root_path, 'Pavia.mat')\n#reference_path = join(root_path, 'Pavia_gt.mat')\n\n# PATH TO SAVE MODELS\nmodel_save_folder = join(root_path, 'models')\n\n# PATH TO SAVE CLASSIFIED IMAGE\nout_path = join(root_path, 'results/Krkonose_1D_CNN.tif')\n\n# PATH TO THE SAMPLE RESULTS\nsample_result_path = join(root_path, 'sample_results/1D_CNN_sample_result.tif')\n</pre> root_path = 'C:/folder/where/this/project/is/saved'  # PATHS TO TRAINING DATA # Krkonose imagery_path   = join(root_path, 'BL_202008_imagery.tif') reference_path = join(root_path, 'BL_202008_reference.tif') # Pavia #imagery_path   = join(root_path, 'Pavia.mat') #reference_path = join(root_path, 'Pavia_gt.mat')  # PATH TO SAVE MODELS model_save_folder = join(root_path, 'models')  # PATH TO SAVE CLASSIFIED IMAGE out_path = join(root_path, 'results/Krkonose_1D_CNN.tif')  # PATH TO THE SAMPLE RESULTS sample_result_path = join(root_path, 'sample_results/1D_CNN_sample_result.tif') In\u00a0[\u00a0]: Copied! <pre>loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n# loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n    # reference_path, out_shape=(1088, 1088, 102))\n\nprint(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\nprint(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')\n</pre> loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path) # loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,     # reference_path, out_shape=(1088, 1088, 102))  print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}') print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],\n                                 loaded_raster[\"reference\"],\n                                 ds_name=ds_name)\n</pre> visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],                                  loaded_raster[\"reference\"],                                  ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>tile_shape = (1, 1)\noverlap = 0\n\ndataset_tiles = image_preprocessing.tile_training(loaded_raster,\n                                                  tile_shape,\n                                                  overlap)\n\nprint(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}')\nprint(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}')\n</pre> tile_shape = (1, 1) overlap = 0  dataset_tiles = image_preprocessing.tile_training(loaded_raster,                                                   tile_shape,                                                   overlap)  print(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}') print(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles,\n                                                         nodata_vals=[0],\n                                                         is_training=True)\n\nprint(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}')\nprint(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}')\n</pre> filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles,                                                          nodata_vals=[0],                                                          is_training=True)  print(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}') print(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}') <p>Let's have a look at spectral curves of the individual pixels, change tile_number to see different pixels.</p> In\u00a0[\u00a0]: Copied! <pre>tile_number = 400\nvisualisation_utils.show_spectral_curve(filtered_tiles, tile_number,\n                                        ds_name=ds_name)\n</pre> tile_number = 400 visualisation_utils.show_spectral_curve(filtered_tiles, tile_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>normalized_tiles, unique, counts = image_preprocessing.normalize_tiles_1d(filtered_tiles, nodata_vals=[65535], is_training=True)\nprint(f'Preprocessed imagery shape {normalized_tiles[\"imagery\"].shape}')\nprint(f'Preprocessed reference shape {normalized_tiles[\"reference\"].shape}')\n</pre> normalized_tiles, unique, counts = image_preprocessing.normalize_tiles_1d(filtered_tiles, nodata_vals=[65535], is_training=True) print(f'Preprocessed imagery shape {normalized_tiles[\"imagery\"].shape}') print(f'Preprocessed reference shape {normalized_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>tile_number = 400\nvisualisation_utils.show_spectral_curve(normalized_tiles,\n                                        tile_number,\n                                        ds_name=ds_name)\n</pre> tile_number = 400 visualisation_utils.show_spectral_curve(normalized_tiles,                                         tile_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>train_fraction = 1/3\n\nX_train, X_test, y_train, y_test = train_test_split(normalized_tiles['imagery'], normalized_tiles['reference'], train_size=train_fraction)\n\ntraining = {'imagery': X_train, 'reference': y_train}\ntest  = {'imagery': X_test,  'reference': y_test}\n</pre> train_fraction = 1/3  X_train, X_test, y_train, y_test = train_test_split(normalized_tiles['imagery'], normalized_tiles['reference'], train_size=train_fraction)  training = {'imagery': X_train, 'reference': y_train} test  = {'imagery': X_test,  'reference': y_test} In\u00a0[\u00a0]: Copied! <pre>dataset = tnt.dataset.TensorDataset([training['imagery'], training['reference']])\nprint(dataset)\n\nprint(f'Class labels: \\n{unique}\\n')\nprint(f'Number of pixels in a class: \\n{counts}')\n</pre> dataset = tnt.dataset.TensorDataset([training['imagery'], training['reference']]) print(dataset)  print(f'Class labels: \\n{unique}\\n') print(f'Number of pixels in a class: \\n{counts}') In\u00a0[\u00a0]: Copied! <pre>class SpectralNet(nn.Module):\n    \"\"\"1D CNN for classifying pixels based on the spectral response.\"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Initialize the SpectralNet model.\n\n        n_channels, int, number of input channel\n        size, int list, size of the feature maps of convs for the encoder\n        n_class = int,  the number of classes\n        \"\"\"\n        # necessary for all classes extending the module class\n        super(SpectralNet, self).__init__()\n\n        self.maxpool = nn.MaxPool1d(2, return_indices=False)\n        self.dropout = nn.Dropout(p=0.5, inplace=False)\n\n        self.n_channels = args['n_channel']\n        self.size = args['layer_width']\n        self.n_class = args['n_class']\n\n        # Encoder layer definitions\n        def conv_layer_1d(in_ch, out_ch, k_size=3, conv_bias=False):\n            \"\"\"Create default conv layer.\"\"\"\n            return nn.Sequential(nn.Conv1d(in_ch, out_ch, kernel_size=k_size,\n                                           bias=conv_bias),\n                                 nn.BatchNorm1d(out_ch), nn.ReLU())\n\n        def fconnected_layer(in_ch, out_ch, mlp_bias=False):\n            \"\"\"Create default linear layer.\"\"\"\n            return nn.Sequential(nn.Linear(in_ch, out_ch, bias=mlp_bias),\n                                 nn.BatchNorm1d(out_ch), nn.ReLU())\n\n        self.c1 = conv_layer_1d(self.n_channels, self.size[0])\n        self.c2 = conv_layer_1d(self.size[0], self.size[1])\n        self.c3 = conv_layer_1d(self.size[1], self.size[2])\n\n        self.c4 = conv_layer_1d(self.size[2], self.size[3])\n        self.c5 = conv_layer_1d(self.size[3], self.size[4])\n        self.c6 = conv_layer_1d(self.size[4], self.size[5])\n        \n        self.c7 = conv_layer_1d(self.size[5], self.size[6])\n        self.c8 = conv_layer_1d(self.size[6], self.size[7])\n        self.c9 = conv_layer_1d(self.size[7], self.size[8])\n\n        self.flatten = nn.Flatten()\n\n        self.l1 = fconnected_layer(self.size[9], self.size[10])\n        self.l2 = fconnected_layer(self.size[10], self.size[11])\n        self.l3 = fconnected_layer(self.size[11], self.size[12])\n        # Final classifying layer\n        self.classifier = nn.Linear(self.size[12], self.n_class)\n\n        # Weight initialization\n        self.c1[0].apply(self.init_weights)\n        self.c2[0].apply(self.init_weights)\n        self.c3[0].apply(self.init_weights)\n        self.c4[0].apply(self.init_weights)\n        self.c5[0].apply(self.init_weights)\n        self.c6[0].apply(self.init_weights)\n        self.c7[0].apply(self.init_weights)\n        self.c8[0].apply(self.init_weights)\n        self.c9[0].apply(self.init_weights)\n        self.l1[0].apply(self.init_weights)\n        self.l2[0].apply(self.init_weights)\n        self.l3[0].apply(self.init_weights)\n\n        self.classifier.apply(self.init_weights)\n\n        if torch.cuda.is_available():  # Put the model on GPU memory\n            self.cuda()\n            torch.backends.cudnn.enabled = True\n            torch.backends.cudnn.benchmark = True\n\n    def init_weights(self, layer):\n        \"\"\"Initialise layer weights from a gaussian.\"\"\"\n        nn.init.kaiming_normal_(\n            layer.weight, mode='fan_out', nonlinearity='relu')\n\n    def forward(self, input_data):\n        \"\"\"Define model structure for the forward pass.\"\"\"\n        # Encoder\n        # Level 1\n        x1 = self.c1(input_data)\n        x2 = self.c2(x1)\n        x3 = self.c3(x2)\n        x4 = self.maxpool(x3)\n        # Level 2\n        x5 = self.c4(x4)\n        x6 = self.c5(x5)\n        x7 = self.c6(x6)\n        x8 = self.maxpool(x7)\n        # Level 3\n        x9 = self.c7(x8)\n        x10 = self.c8(x9)\n        x11 = self.c9(x10)\n        # Fully connected portion of the network (MLP)\n        x12 = self.flatten(x11)\n        x13 = self.l1(x12)\n        x14 = self.l2(x13)\n        x15 = self.l3(x14)\n        # Output\n        out = self.classifier(self.dropout(x15))\n\n        return out\n</pre> class SpectralNet(nn.Module):     \"\"\"1D CNN for classifying pixels based on the spectral response.\"\"\"      def __init__(self, args):         \"\"\"         Initialize the SpectralNet model.          n_channels, int, number of input channel         size, int list, size of the feature maps of convs for the encoder         n_class = int,  the number of classes         \"\"\"         # necessary for all classes extending the module class         super(SpectralNet, self).__init__()          self.maxpool = nn.MaxPool1d(2, return_indices=False)         self.dropout = nn.Dropout(p=0.5, inplace=False)          self.n_channels = args['n_channel']         self.size = args['layer_width']         self.n_class = args['n_class']          # Encoder layer definitions         def conv_layer_1d(in_ch, out_ch, k_size=3, conv_bias=False):             \"\"\"Create default conv layer.\"\"\"             return nn.Sequential(nn.Conv1d(in_ch, out_ch, kernel_size=k_size,                                            bias=conv_bias),                                  nn.BatchNorm1d(out_ch), nn.ReLU())          def fconnected_layer(in_ch, out_ch, mlp_bias=False):             \"\"\"Create default linear layer.\"\"\"             return nn.Sequential(nn.Linear(in_ch, out_ch, bias=mlp_bias),                                  nn.BatchNorm1d(out_ch), nn.ReLU())          self.c1 = conv_layer_1d(self.n_channels, self.size[0])         self.c2 = conv_layer_1d(self.size[0], self.size[1])         self.c3 = conv_layer_1d(self.size[1], self.size[2])          self.c4 = conv_layer_1d(self.size[2], self.size[3])         self.c5 = conv_layer_1d(self.size[3], self.size[4])         self.c6 = conv_layer_1d(self.size[4], self.size[5])                  self.c7 = conv_layer_1d(self.size[5], self.size[6])         self.c8 = conv_layer_1d(self.size[6], self.size[7])         self.c9 = conv_layer_1d(self.size[7], self.size[8])          self.flatten = nn.Flatten()          self.l1 = fconnected_layer(self.size[9], self.size[10])         self.l2 = fconnected_layer(self.size[10], self.size[11])         self.l3 = fconnected_layer(self.size[11], self.size[12])         # Final classifying layer         self.classifier = nn.Linear(self.size[12], self.n_class)          # Weight initialization         self.c1[0].apply(self.init_weights)         self.c2[0].apply(self.init_weights)         self.c3[0].apply(self.init_weights)         self.c4[0].apply(self.init_weights)         self.c5[0].apply(self.init_weights)         self.c6[0].apply(self.init_weights)         self.c7[0].apply(self.init_weights)         self.c8[0].apply(self.init_weights)         self.c9[0].apply(self.init_weights)         self.l1[0].apply(self.init_weights)         self.l2[0].apply(self.init_weights)         self.l3[0].apply(self.init_weights)          self.classifier.apply(self.init_weights)          if torch.cuda.is_available():  # Put the model on GPU memory             self.cuda()             torch.backends.cudnn.enabled = True             torch.backends.cudnn.benchmark = True      def init_weights(self, layer):         \"\"\"Initialise layer weights from a gaussian.\"\"\"         nn.init.kaiming_normal_(             layer.weight, mode='fan_out', nonlinearity='relu')      def forward(self, input_data):         \"\"\"Define model structure for the forward pass.\"\"\"         # Encoder         # Level 1         x1 = self.c1(input_data)         x2 = self.c2(x1)         x3 = self.c3(x2)         x4 = self.maxpool(x3)         # Level 2         x5 = self.c4(x4)         x6 = self.c5(x5)         x7 = self.c6(x6)         x8 = self.maxpool(x7)         # Level 3         x9 = self.c7(x8)         x10 = self.c8(x9)         x11 = self.c9(x10)         # Fully connected portion of the network (MLP)         x12 = self.flatten(x11)         x13 = self.l1(x12)         x14 = self.l2(x13)         x15 = self.l3(x14)         # Output         out = self.classifier(self.dropout(x15))          return out <p>augment takes in the training tile and the corresponding reference labels. It then adds a random value (taken from a normal distribution) at each wavelength and thus slightly modifies the training data. Change tile_number to see the augmentation effect for different spectral curves.</p> In\u00a0[\u00a0]: Copied! <pre>def augment(obs, g_t):\n    \"\"\"the data augmentation function, introduces random noise.\"\"\"\n    sigma, clip= 0.002, 0.01\n    rand = torch.clamp(torch.mul(sigma, torch.randn([1,1,54])), -clip, clip)\n    obs = torch.add(obs, rand)\n\n    return obs, g_t\n</pre> def augment(obs, g_t):     \"\"\"the data augmentation function, introduces random noise.\"\"\"     sigma, clip= 0.002, 0.01     rand = torch.clamp(torch.mul(sigma, torch.randn([1,1,54])), -clip, clip)     obs = torch.add(obs, rand)      return obs, g_t In\u00a0[\u00a0]: Copied! <pre>tile_number = 400\nvisualisation_utils.show_augment_spectral(training, tile_number,\n                                          augment, ds_name=ds_name)\n</pre> tile_number = 400 visualisation_utils.show_augment_spectral(training, tile_number,                                           augment, ds_name=ds_name) <p>train trains the network for one epoch. This function contains a for loop, which loads the training data in individual batches. Each batch of training data goes through the network, after which we compute the loss function (cross-entropy). Last step of training is performing an optimiser step, which changes the networks heights.</p> <p>eval evaluates the results on a validation set, should be done periodically during training to check for overfitting.</p> <p>train_full performs the full training loop. It first initialises the model and optimiser. Then the train function is called in a loop, with periodic evaluation on the validation set.</p> In\u00a0[\u00a0]: Copied! <pre>def train(model, optimizer, args):\n    \"\"\"train for one epoch\"\"\"\n    model.train() #switch the model in training mode\n  \n    #the DataLoader class will take care of the batching\n    loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])\n    loader = tqdm.tqdm(loader, ncols=500)\n  \n    #will keep track of the loss\n    loss_meter = tnt.meter.AverageValueMeter()\n\n    for index, (tiles, gt) in enumerate(loader):\n    \n        optimizer.zero_grad() #put gradient to zero\n\n        tiles, gt = augment(tiles, gt)\n\n        if torch.cuda.is_available():\n            pred = model(tiles.cuda()) #compute the prediction\n        else:\n            pred = model(tiles)\n\n        loss = nn.functional.cross_entropy(pred.cpu(), gt, weight=args['class_weights'])\n        loss.backward() #compute gradients\n\n        optimizer.step() #one SGD step\n        loss_meter.add(loss.item())\n        \n    return loss_meter.value()[0]\n\ndef eval(model, sampler):\n    \"\"\"evaluate results on the validation set\"\"\"\n  \n    model.eval() #switch in eval mode\n  \n    loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)\n    loader = tqdm.tqdm(loader, ncols=500)\n  \n    loss_meter = tnt.meter.AverageValueMeter()\n\n    with torch.no_grad():\n        for index, (tiles, gt) in enumerate(loader):\n            if torch.cuda.is_available():\n                pred = model(tiles.cuda())\n                loss = nn.functional.cross_entropy(pred.cpu(), gt)\n            else:\n                pred = model(tiles)\n                loss = nn.functional.cross_entropy(pred, gt)\n            loss_meter.add(loss.item())\n\n    return loss_meter.value()[0]\n\n\ndef train_full(args):\n    \"\"\"The full training loop\"\"\"\n\n    #initialize the model\n    model = SpectralNet(args)\n\n    print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')\n  \n    #define the Adam optimizer\n    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],\n                                               gamma=args['scheduler_gamma'])\n  \n    train_loss = np.empty(args['n_epoch'])\n    test_epochs = []\n    test_loss = []\n\n    for i_epoch in range(args['n_epoch']):\n        #train one epoch\n        print(f'Epoch #{str(i_epoch+1)}')\n        train_loss[i_epoch] = train(model, optimizer, args)\n        scheduler.step()\n\n        # Periodic testing on the validation set\n        if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):\n            print('Evaluation')\n            loss_test = eval(model, args['test_subsampler'])\n            test_epochs.append(i_epoch + 1)\n            test_loss.append(loss_test)\n\n    plt.figure(figsize=(10, 10))\n    plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')\n    plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')\n    plt.plot(test_epochs, test_loss, label='Validation loss')\n    plt.legend()\n    plt.show()\n    print(train_loss)\n    print(test_loss)\n    args['loss_test'] = test_loss[-1]\n    \n    return model\n</pre> def train(model, optimizer, args):     \"\"\"train for one epoch\"\"\"     model.train() #switch the model in training mode        #the DataLoader class will take care of the batching     loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])     loader = tqdm.tqdm(loader, ncols=500)        #will keep track of the loss     loss_meter = tnt.meter.AverageValueMeter()      for index, (tiles, gt) in enumerate(loader):              optimizer.zero_grad() #put gradient to zero          tiles, gt = augment(tiles, gt)          if torch.cuda.is_available():             pred = model(tiles.cuda()) #compute the prediction         else:             pred = model(tiles)          loss = nn.functional.cross_entropy(pred.cpu(), gt, weight=args['class_weights'])         loss.backward() #compute gradients          optimizer.step() #one SGD step         loss_meter.add(loss.item())              return loss_meter.value()[0]  def eval(model, sampler):     \"\"\"evaluate results on the validation set\"\"\"        model.eval() #switch in eval mode        loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)     loader = tqdm.tqdm(loader, ncols=500)        loss_meter = tnt.meter.AverageValueMeter()      with torch.no_grad():         for index, (tiles, gt) in enumerate(loader):             if torch.cuda.is_available():                 pred = model(tiles.cuda())                 loss = nn.functional.cross_entropy(pred.cpu(), gt)             else:                 pred = model(tiles)                 loss = nn.functional.cross_entropy(pred, gt)             loss_meter.add(loss.item())      return loss_meter.value()[0]   def train_full(args):     \"\"\"The full training loop\"\"\"      #initialize the model     model = SpectralNet(args)      print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')        #define the Adam optimizer     optimizer = optim.Adam(model.parameters(), lr=args['lr'])     scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],                                                gamma=args['scheduler_gamma'])        train_loss = np.empty(args['n_epoch'])     test_epochs = []     test_loss = []      for i_epoch in range(args['n_epoch']):         #train one epoch         print(f'Epoch #{str(i_epoch+1)}')         train_loss[i_epoch] = train(model, optimizer, args)         scheduler.step()          # Periodic testing on the validation set         if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):             print('Evaluation')             loss_test = eval(model, args['test_subsampler'])             test_epochs.append(i_epoch + 1)             test_loss.append(loss_test)      plt.figure(figsize=(10, 10))     plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')     plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')     plt.plot(test_epochs, test_loss, label='Validation loss')     plt.legend()     plt.show()     print(train_loss)     print(test_loss)     args['loss_test'] = test_loss[-1]          return model In\u00a0[\u00a0]: Copied! <pre>args = { #Dict to store all model parameters\n    'n_channel': 1,\n    'n_class': len(unique),\n    'layer_width': [32,32,32,64,64,64,128,128,128,384,384,192,96],    # krkonose\n    #'layer_width': [16,16,16,32,32,32,64,64,64,960,480,240, 120],     # pavia city centre\n\n    'crossval_nfolds': 3,\n    'n_epoch_test': 10,          #periodicity of evaluation on test set\n    'scheduler_milestones': [60,80,95],\n    'scheduler_gamma': 0.3,\n    \n    #'class_weights': torch.tensor([0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),\n    'class_weights': torch.tensor([.15, .05, .3, .05, .05, .1, .3]),\n\n    'n_epoch': 50,\n    'lr': 1e-5,\n    'batch_size': 256,\n}\n\nprint(f'''Number of models to be trained:\n    {args['crossval_nfolds']}\nInitial learning rate:\n    {args['lr']}\nBatch size:\n    {args['batch_size']}\nNumber of training epochs:\n    {args['n_epoch']}''')\n</pre> args = { #Dict to store all model parameters     'n_channel': 1,     'n_class': len(unique),     'layer_width': [32,32,32,64,64,64,128,128,128,384,384,192,96],    # krkonose     #'layer_width': [16,16,16,32,32,32,64,64,64,960,480,240, 120],     # pavia city centre      'crossval_nfolds': 3,     'n_epoch_test': 10,          #periodicity of evaluation on test set     'scheduler_milestones': [60,80,95],     'scheduler_gamma': 0.3,          #'class_weights': torch.tensor([0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),     'class_weights': torch.tensor([.15, .05, .3, .05, .05, .1, .3]),      'n_epoch': 50,     'lr': 1e-5,     'batch_size': 256, }  print(f'''Number of models to be trained:     {args['crossval_nfolds']} Initial learning rate:     {args['lr']} Batch size:     {args['batch_size']} Number of training epochs:     {args['n_epoch']}''') In\u00a0[\u00a0]: Copied! <pre>kfold = StratifiedKFold(n_splits = args['crossval_nfolds'], shuffle=True)\ntrained_models = []\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset, training['reference'])):\n    print(f'Training starts for model number {str(fold+1)}')\n    \n    a = perf_counter()\n    args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)\n    args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)\n    \n    trained_models.append((train_full(args), args['loss_test']))\n    \n    state_dict_path = join(model_save_folder, f'1D_CNN_fold_{str(fold)}.pt')\n    torch.save(trained_models[fold][0].state_dict(), state_dict_path)\n    print(f'Model saved to: {state_dict_path}')\n    \n    print(f'Training finished in {visualisation_utils.sec_to_hms(perf_counter()-a)}.')\n    print('\\n\\n')\n\nprint(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}')\nprint(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}')\n</pre> kfold = StratifiedKFold(n_splits = args['crossval_nfolds'], shuffle=True) trained_models = [] for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset, training['reference'])):     print(f'Training starts for model number {str(fold+1)}')          a = perf_counter()     args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)     args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)          trained_models.append((train_full(args), args['loss_test']))          state_dict_path = join(model_save_folder, f'1D_CNN_fold_{str(fold)}.pt')     torch.save(trained_models[fold][0].state_dict(), state_dict_path)     print(f'Model saved to: {state_dict_path}')          print(f'Training finished in {visualisation_utils.sec_to_hms(perf_counter()-a)}.')     print('\\n\\n')  print(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}') print(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}') In\u00a0[\u00a0]: Copied! <pre># Parameters for model definition\nargs = {\n    'n_channel': 1,\n    'n_class': 7,\n    'layer_width': [32,32,32,64,64,64,128,128,128,384,384,192,96],\n    #'layer_width': [16,16,16,32,32,32,64,64,64,960,480,240,120],     # pavia city centre\n}\n\n# Select which model to load by using a different filename index\nstate_dict_path = join(model_save_folder, '1D_CNN_fold_0.pt')\n</pre> # Parameters for model definition args = {     'n_channel': 1,     'n_class': 7,     'layer_width': [32,32,32,64,64,64,128,128,128,384,384,192,96],     #'layer_width': [16,16,16,32,32,32,64,64,64,960,480,240,120],     # pavia city centre }  # Select which model to load by using a different filename index state_dict_path = join(model_save_folder, '1D_CNN_fold_0.pt') In\u00a0[\u00a0]: Copied! <pre># Load the model\nmodel = SpectralNet(args)\nmodel.load_state_dict(torch.load(state_dict_path))\nmodel.eval()\n</pre> # Load the model model = SpectralNet(args) model.load_state_dict(torch.load(state_dict_path)) model.eval() In\u00a0[\u00a0]: Copied! <pre>tile_shape = (1, 1)\noverlap = 0\n</pre> tile_shape = (1, 1) overlap = 0 In\u00a0[\u00a0]: Copied! <pre>start = perf_counter()\n# Load raster\nraster_orig = image_preprocessing.read_gdal_with_geoinfo(imagery_path) # Krkonose\n#raster_orig = image_preprocessing.read_pavia_centre(imagery_path, out_shape=(1088, 1088, 102)) # Pavia\n\n# Split raster into tiles\ndataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'],\n                        out_shape=tile_shape, out_overlap=overlap)\n# Normalize tiles\ndataset_full = image_preprocessing.normalize_tiles_1d(dataset_full_tiles, nodata_vals=[0])\n# Convert to Pytorch TensorDataset\ndataset = tnt.dataset.TensorDataset(dataset_full['imagery'])\n\nprint('')\nprint(f'Loading and preprocessing the imagery took {visualisation_utils.sec_to_hms(perf_counter() - start)}.')\n</pre> start = perf_counter() # Load raster raster_orig = image_preprocessing.read_gdal_with_geoinfo(imagery_path) # Krkonose #raster_orig = image_preprocessing.read_pavia_centre(imagery_path, out_shape=(1088, 1088, 102)) # Pavia  # Split raster into tiles dataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'],                         out_shape=tile_shape, out_overlap=overlap) # Normalize tiles dataset_full = image_preprocessing.normalize_tiles_1d(dataset_full_tiles, nodata_vals=[0]) # Convert to Pytorch TensorDataset dataset = tnt.dataset.TensorDataset(dataset_full['imagery'])  print('') print(f'Loading and preprocessing the imagery took {visualisation_utils.sec_to_hms(perf_counter() - start)}.') In\u00a0[\u00a0]: Copied! <pre>timer_start = perf_counter()\narr_class = inference_utils.combine_tiles_1d(model, dataset, tile_shape, overlap, dataset_full_tiles['dimensions'])\nprint(f'Model applied to all tiles in {visualisation_utils.sec_to_hms(perf_counter() - timer_start)}.')\n</pre> timer_start = perf_counter() arr_class = inference_utils.combine_tiles_1d(model, dataset, tile_shape, overlap, dataset_full_tiles['dimensions']) print(f'Model applied to all tiles in {visualisation_utils.sec_to_hms(perf_counter() - timer_start)}.') <p>You can also visualise the result:</p> In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    np.add(arr_class, 1), ds_name=ds_name)\n</pre> visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     np.add(arr_class, 1), ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>print(f'The classified result gets saved to {out_path}')\ninference_utils.export_result(out_path, arr_class, raster_orig['geoinfo'])\n</pre> print(f'The classified result gets saved to {out_path}') inference_utils.export_result(out_path, arr_class, raster_orig['geoinfo']) In\u00a0[\u00a0]: Copied! <pre>predicted_arr = inference_utils.classify_tiles(model, test['imagery'], mode='1D')\n</pre> predicted_arr = inference_utils.classify_tiles(model, test['imagery'], mode='1D') In\u00a0[\u00a0]: Copied! <pre>print(classification_report(test['reference'], predicted_arr,\n                            target_names=class_names[1:]))\n</pre> print(classification_report(test['reference'], predicted_arr,                             target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_confusion_matrix(test_filtered, predicted_filtered,\n                                          ds_name=ds_name)\n</pre> visualisation_utils.show_confusion_matrix(test_filtered, predicted_filtered,                                           ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Read test reference\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\ntest_flat = test_arr['reference'].reshape(\n    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\ntest_filtered = test_flat[test_flat &gt; 0]\n\n# Read sample result\nsample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\nsample_flat = sample_arr['reference'].reshape(sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\nsample_filtered = sample_flat[test_flat &gt; 0]\n</pre> # Read test reference test_arr = image_preprocessing.read_gdal(imagery_path, test_path) test_flat = test_arr['reference'].reshape(     test_arr['reference'].shape[0]*test_arr['reference'].shape[1]) test_filtered = test_flat[test_flat &gt; 0]  # Read sample result sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path) sample_flat = sample_arr['reference'].reshape(sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1]) sample_filtered = sample_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre># Visualise the sample result\nvisualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    sample_arr['reference'],\n                                    ds_name=ds_name)\n</pre> # Visualise the sample result visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     sample_arr['reference'],                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Print a classification report for the sample result\nprint(classification_report(test_filtered, sample_filtered,\n                            target_names=class_names[1:]))\n</pre> # Print a classification report for the sample result print(classification_report(test_filtered, sample_filtered,                             target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre># Show a Confusion matrix for the sample result\nvisualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n                                          ds_name=ds_name)\n</pre> # Show a Confusion matrix for the sample result visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,                                           ds_name=ds_name) <p>Return to exercises</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#training-a-1d-convolutional-neural-network-for-hyperspectral-data-classification","title":"Training a 1D Convolutional Neural Network for hyperspectral data classification\u00b6","text":"<p>In this notebook, you will train and apply a one-dimensional convolutional neural network for classification of hyperspectral data from B\u00edl\u00e1 Louka, Krkono\u0161e mountains, Czechia. Please start by reading about the dataset and area of interest here.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#structure-of-this-exercise","title":"Structure of this exercise\u00b6","text":"<p>What are you going to encounter during this exercise.</p> <ol> <li>Load libraries, set paths</li> <li>Load and Preprocess training data</li> <li>Neural Network Definition / Training</li> <li>Apply Network</li> <li>Evaluate Result</li> <li>Sample Solution</li> </ol>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#0-load-external-libraries-and-set-paths","title":"0. Load external libraries and set paths\u00b6","text":"<p>First, we need to import external libraries:</p> <ul> <li><p>torch, torch.nn, torch.optim, torchnet - Pytorch related libraries for deep learning</p> </li> <li><p>numpy - Arrays to hold our data</p> </li> <li><p>matplotlib.pyplot - Draw images</p> </li> <li><p>sklearn.model_selection - Cross-validation implemented in scikit-learn</p> </li> <li><p>sklearn.metrics - Compute accuracy metrics using scikit-learn</p> </li> <li><p>time.perf_counter - Track how long individual functions take to run</p> </li> <li><p>os.path - Path manipulation</p> </li> <li><p>tqdm - show progress bars during training</p> </li> <li><p>etrainee_m4_utils.image_preprocessing - Our library holding functions for image tiling, preprocessing, etc.</p> </li> <li><p>etrainee_m4_utils.inference_utils - Our library for correctly exporting classifed images</p> </li> <li><p>etrainee_m4_utils.visualisation_utils - Our library for visualising the data</p> </li> </ul> <p>Two external libraries are not imported directly in this notebook, but are used by functions in image_preprocessing and inference_utils:</p> <ul> <li>gdal - Manipulates spatial data</li> <li>scipy.io - Reads .mat files</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#1-loading-and-preprocessing-training-data","title":"1. Loading and preprocessing training data\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#11-data-loading-into-numpy","title":"1.1. Data loading into NumPy\u00b6","text":"<p>Let's start by reading an image into a numpy array, we do this in the background using GDAL.</p> <p>The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data for training has the same height and width.</p> <p>For loading most raster datasets, we created a read_gdal() function in the image_preprocessing module. But loading .mat files for the Pavia City Centre requires a specific function (read_pavia_centre()). Both read_pavia_centre() and read_gdal() return a dictionary containing two numpy arrays with keys imagery and reference.</p> <p>If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile for neural networks, we crop the image to (1088, 1088, 102) here.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#12-image-tiling","title":"1.2. Image tiling\u00b6","text":"<p>We have our data loaded into a numpy array, the next step is to divide the image into individual tiles, which will be the input for our neural network.</p> <p>As we want to perform convolution only in the spectral dimension, we need to divide the hyperspectral image into tiles of individual pixels, this is ensured by setting the variable tile_shape as (1, 1). overlap is not needed for one-dimensional processing.</p> <p>This process creates 1 183 744 \"tiles\" of 1 by 1 pixels, with the same amount of spectral bands as earlier.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#13-tile-filtration","title":"1.3. Tile filtration\u00b6","text":"<p>However, most of the created tiles do not contain training data, we therefore need to filter them and only keep the tiles with a field-collected reference.</p> <p>This process significantly reduces the size of our dataset from 1 183 744 to 48 370 - training data is available on less than 2 percent of the dataset.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#14-data-normalization","title":"1.4. Data normalization\u00b6","text":"<p>After filtering the tiles to only include training data, we can move onto a final part of the preprocessing - data normalization. In Machine Learning, it is common to normalize all data before classification.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#15-splitting-data-for-trainingtesting","title":"1.5. Splitting data for training/testing\u00b6","text":"<p>Our reference dataset has to be split into three groups:</p> <p>training / validation / test</p> <p>In this step we divide the data into train+val and test groups. The variable train_fraction is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function train_test_split, which performs a stratified sampling from all classes.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#16-conversion-to-pytorch-tensors","title":"1.6. Conversion to Pytorch Tensors\u00b6","text":"<p>The resulting preprocessed tiles are subsequently transformed from numpy arrays into pytorch tensors for training.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#2-neural-network-definition","title":"2. Neural network definition\u00b6","text":"<p>After preprocessing our data, we can move onto defining our neural network and functions for training. You can either train your own neural network or use the one we already trained for you (sample_results/1D_CNN_sample_trained.pt). In case you are using the pretrained network, please run only the following code snippet (2.1.) and skip ahead to section 3.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#21-network-structure","title":"2.1. Network structure\u00b6","text":"<p>Our network is named SpectralNet, and it's a fairly simple network with 1D convolutions followed by maxpooling and a set of fully-connected layers at the end. The structure of our network is defined in the SpectralNet class, which has three methods:</p> <ul> <li>__init__ - This method runs automatically when defining an instance of the class, it defines indiviudal layers of the networks (1D convolutions, fully connected layers, maxpooling and also a dropout layer).</li> <li>init_weights - Randomly initialising network weights based on a normal distribution.</li> <li>forward - Defining how data should flow through the network during a forward pass (network structure definition). The PyTorch library automatically creates a method for backward passes based on this structure.</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#22-functions-for-network-training","title":"2.2. Functions for network training\u00b6","text":"<p>Training the network is handled by four functions:</p> <ul> <li>augment - Augments the training data by adding random noise.</li> <li>train - Trains the network for one epoch.</li> <li>eval - Evaluates the results on a validation set.</li> <li>train_full - Performs the full training loop.</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#23-hyperparameter-definition","title":"2.3. Hyperparameter definition\u00b6","text":"<p>Training networks requires first setting several hyperparameters, please feel free to play around with them and try different values for the number of training epochs, learning rate or batch size.</p> <ul> <li><p>n_channel - number of channels, set to 1 for our task</p> </li> <li><p>n_class - number of classification classes</p> </li> <li><p>layer_width - number of filters in each NN layer</p> </li> <li><p>crossval_nfolds - Number of folds for crossvalidation</p> </li> <li><p>n_epoch_test - after how many training epochs do we validate on the validation set</p> </li> <li><p>scheduler_milestones - after how many epochs do we reduce the training rate</p> </li> <li><p>scheduler_gamma - by what factor do we reduce the training rate</p> </li> <li><p>class_weights - training weights for individual classes, used to offset imbalanced class distribution</p> </li> <li><p>n_epoch - how many epochs are performed during training</p> </li> <li><p>lr - how fast can individual network parameters change during one training epoch</p> </li> <li><p>batch_size - how many tiles should be included in each gradient descent step</p> </li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#24-network-training","title":"2.4 Network training\u00b6","text":"<p>Run the training procedure using set hyperparameters, evaluate the training/validation loss graphs produced during training to adjust hyperparameter values.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#3-applying-the-network","title":"3. Applying the network\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#31-loading-a-trained-model","title":"3.1. Loading a trained model\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#32-loading-and-preprocessing-the-data","title":"3.2. Loading and preprocessing the data\u00b6","text":"<p>Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.</p> <p>By default, the training raster (imagery_path) is used.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#33-applying-the-cnn","title":"3.3. Applying the CNN\u00b6","text":"<p>The following snippet applies the CNN. This may take several hours, given that the network has to be applied to each pixel separately - almost 1.2 million times (1088 * 1088 = 1 183 744).</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#34-export-resulting-raster","title":"3.4. Export Resulting Raster\u00b6","text":"<p>Export the resulting classified raster into out_path for distribution or further analysis (e.g. validation in GIS).</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#4-evaluate-classification-result","title":"4. Evaluate Classification Result\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#41-apply-classifier-to-the-test-dataset","title":"4.1. Apply classifier to the test dataset\u00b6","text":"<p>The classified raster is comapred to the test reference data. The reference data is saved in the test dictionary with keys imagery and reference.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#42-compute-accuracy-metrics","title":"4.2. Compute Accuracy Metrics\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#43-show-confusion-matrix","title":"4.3. Show Confusion Matrix\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#5-sample-solution","title":"5. Sample Solution\u00b6","text":"<p>We have generated this result using these training parameters (please note that just using the same training parameters will not yield the same result, as the network is randomly initailised):</p> <ul> <li>Number of epochs: 100</li> <li>Batch Size: 256</li> <li>Learning Rate: 1e-5</li> <li>Learning Rate reduced after Epochs #: [60,80,95]</li> <li>Learning Rate reduced by: 0.3</li> <li>Class Weights: [0.197, 0.343, 0.032, 0.166, 0.141, 0.029, 0.013, 0.022, 0.057]</li> <li>Number of filters in each layer: [32,32,32,64,64,64,128,128,128,384,384,192,96]</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_1d.html#optional-classify-a-urban-scene","title":"Optional: Classify a urban scene\u00b6","text":"<p>Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.</p> <p>Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University. You will need to change the paths to input data and use the read_pavia_centre method to load the matlab matrices into numpy.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html","title":"04 exercise cnn 2d","text":"<ul> <li><p>Prerequisities (This notebook can be run either online using Google Colab or on your local machine)</p> <ul> <li>A Google account for accessing Google Colab (link to notebook). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.</li> </ul> <p>or</p> <ul> <li><p>A Python environment with the necessary libraries (manual). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.</p> </li> <li><p>Downloaded data (module4.zip/theme4_exercise_machine_learning) The dataset consists of:</p> <ul> <li>Hyperspectral RPAS imagery of B\u00edl\u00e1 Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: BL_202008_imagery.tif</li> <li>a raster with reference data: BL_202008_reference.tif</li> <li>Pretrained models and corresponding classified rasters: /sample_results/*</li> </ul> </li> </ul> </li> <li><p>Tasks</p> <ul> <li>Preprocess imagery for Deep Learning</li> <li>Classify the hyperspectral image using 2D CNN</li> <li>Observe how hyperparameter values alter classification results</li> <li>Evaluate your results and compare to our pretrained classifier</li> <li>Optional: Classify a urban scene</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchnet as tnt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom os.path import join\nfrom time import perf_counter\nfrom tqdm import notebook as tqdm\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n\nfrom etrainee_m4_utils import image_preprocessing\nfrom etrainee_m4_utils import inference_utils\nfrom etrainee_m4_utils import visualisation_utils\n\n# GLOBAL SETTINGS\nplt.rcParams['figure.figsize'] = [5, 5]\nnp.set_printoptions(precision=2, suppress=True)  # Array print precision\n</pre> import torch import torch.nn as nn import torch.optim as optim import torchnet as tnt  import numpy as np import matplotlib.pyplot as plt  from os.path import join from time import perf_counter from tqdm import notebook as tqdm  from sklearn.metrics import classification_report from sklearn.model_selection import KFold, StratifiedKFold, train_test_split  from etrainee_m4_utils import image_preprocessing from etrainee_m4_utils import inference_utils from etrainee_m4_utils import visualisation_utils  # GLOBAL SETTINGS plt.rcParams['figure.figsize'] = [5, 5] np.set_printoptions(precision=2, suppress=True)  # Array print precision In\u00a0[\u00a0]: Copied! <pre># Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n# default: 'pavia_centre'\nds_name = 'bila_louka'\n\n# Get a list of class names\n_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)\n</pre> # Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre' # default: 'pavia_centre' ds_name = 'bila_louka'  # Get a list of class names _, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name) <p>Please fill correct paths to your training and reference rasters (just pointing the root_path variable to the project folder should do):</p> In\u00a0[\u00a0]: Copied! <pre>root_path = 'C:/folder/where/this/project/is/saved'\n\n# PATHS TO TRAINING DATA\n# Krkonose\nimagery_path   = join(root_path, 'BL_202008_imagery.tif')\nreference_path = join(root_path, 'BL_202008_reference.tif')\n# Pavia\n#imagery_path   = join(root_path, 'Pavia.mat')\n#reference_path = join(root_path, 'Pavia_gt.mat')\n\n# PATH TO SAVE MODELS\nmodel_save_folder = join(root_path, 'models')\n\n# PATH TO SAVE CLASSIFIED IMAGE\nout_path = join(root_path, 'results/Krkonose_2D_CNN.tif')\n\n# PATH TO THE SAMPLE RESULTS\nsample_result_path = join(root_path, 'sample_results/2D_CNN_sample_result.tif')\n</pre> root_path = 'C:/folder/where/this/project/is/saved'  # PATHS TO TRAINING DATA # Krkonose imagery_path   = join(root_path, 'BL_202008_imagery.tif') reference_path = join(root_path, 'BL_202008_reference.tif') # Pavia #imagery_path   = join(root_path, 'Pavia.mat') #reference_path = join(root_path, 'Pavia_gt.mat')  # PATH TO SAVE MODELS model_save_folder = join(root_path, 'models')  # PATH TO SAVE CLASSIFIED IMAGE out_path = join(root_path, 'results/Krkonose_2D_CNN.tif')  # PATH TO THE SAMPLE RESULTS sample_result_path = join(root_path, 'sample_results/2D_CNN_sample_result.tif') In\u00a0[\u00a0]: Copied! <pre>loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n# loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n    # reference_path, out_shape=(1088, 1088, 102))\n\nprint(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\nprint(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')\n</pre> loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path) # loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,     # reference_path, out_shape=(1088, 1088, 102))  print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}') print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_img_ref(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                 loaded_raster['reference'],\n                                 ds_name=ds_name)\n</pre> visualisation_utils.show_img_ref(loaded_raster['imagery'][:, :, [25, 15, 5]],                                  loaded_raster['reference'],                                  ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>tile_shape = (128, 128)\noverlap = 64\n\ndataset_tiles = image_preprocessing.tile_training(loaded_raster,\n                                                  tile_shape,\n                                                  overlap)\nprint(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}')\nprint(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}')\n</pre> tile_shape = (128, 128) overlap = 64  dataset_tiles = image_preprocessing.tile_training(loaded_raster,                                                   tile_shape,                                                   overlap) print(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}') print(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles,\n                                                         nodata_vals=[0],\n                                                         is_training=True)\nprint(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}')\nprint(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}')\n</pre> filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles,                                                          nodata_vals=[0],                                                          is_training=True) print(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}') print(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>normalized_tiles, unique, counts = image_preprocessing.normalize_tiles(filtered_tiles, nodata_vals=[0], is_training=True)\nprint(f'Preprocessed imagery shape {normalized_tiles[\"imagery\"].shape}')\nprint(f'Preprocessed reference shape {normalized_tiles[\"reference\"].shape}')\n</pre> normalized_tiles, unique, counts = image_preprocessing.normalize_tiles(filtered_tiles, nodata_vals=[0], is_training=True) print(f'Preprocessed imagery shape {normalized_tiles[\"imagery\"].shape}') print(f'Preprocessed reference shape {normalized_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>train_fraction = 1/3\n\nX_train, X_test, y_train, y_test = train_test_split(normalized_tiles['imagery'], normalized_tiles['reference'], train_size=train_fraction)\n\ntraining = {'imagery': X_train, 'reference': y_train}\ntest  = {'imagery': X_test,  'reference': y_test}\n</pre> train_fraction = 1/3  X_train, X_test, y_train, y_test = train_test_split(normalized_tiles['imagery'], normalized_tiles['reference'], train_size=train_fraction)  training = {'imagery': X_train, 'reference': y_train} test  = {'imagery': X_test,  'reference': y_test} In\u00a0[\u00a0]: Copied! <pre>dataset = tnt.dataset.TensorDataset([training['imagery'],\n                                     training['reference']])\n\nprint(f'Class labels: \\n{unique}\\n')\nprint(f'Number of pixels in a class: \\n{counts}')\n</pre> dataset = tnt.dataset.TensorDataset([training['imagery'],                                      training['reference']])  print(f'Class labels: \\n{unique}\\n') print(f'Number of pixels in a class: \\n{counts}') In\u00a0[\u00a0]: Copied! <pre>class SpatialNet(nn.Module):\n    \"\"\"U-Net for semantic segmentation.\"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Initialize the U-Net model.\n\n        n_channels, int, number of input channel\n        size_e, int list, size of the feature maps of convs for the encoder\n        size_d, int list, size of the feature maps of convs for the decoder\n        n_class = int,  the number of classes\n        \"\"\"\n        super(SpatialNet, self).__init__(\n        )  # necessary for all classes extending the module class\n\n        self.maxpool = nn.MaxPool2d(2, 2, return_indices=False)\n        self.dropout = nn.Dropout2d(p=0.5, inplace=True)\n\n        self.n_channels = args['n_channel']\n        self.size_e = args['size_e']\n        self.size_d = args['size_d']\n        self.n_class = args['n_class']\n        \n        def conv_layer_2d(in_ch, out_ch, k_size=3, conv_bias=False):\n            \"\"\"Create default conv layer.\"\"\"\n            return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,\n                                           padding=1, padding_mode='reflect',\n                                           bias=conv_bias),\n                                 nn.BatchNorm2d(out_ch), nn.ReLU())\n\n        # Encoder layer definitions\n        self.c1 = conv_layer_2d(self.n_channels, self.size_e[0])\n        self.c2 = conv_layer_2d(self.size_e[0], self.size_e[1])\n        self.c3 = conv_layer_2d(self.size_e[1], self.size_e[2])\n        self.c4 = conv_layer_2d(self.size_e[2], self.size_e[3])\n        self.c5 = conv_layer_2d(self.size_e[3], self.size_e[4])\n        self.c6 = conv_layer_2d(self.size_e[4], self.size_e[5])\n        \n        # Decoder layer definitions\n        self.trans1 = nn.ConvTranspose2d(self.size_e[5], self.size_d[0],\n                                         kernel_size=2, stride=2)\n        self.c7 = conv_layer_2d(self.size_d[0], self.size_d[1])\n        self.c8 = conv_layer_2d(self.size_d[1], self.size_d[2])\n        self.trans2 = nn.ConvTranspose2d(self.size_d[2], self.size_d[3],\n                                         kernel_size=2, stride=2)\n        self.c9 = conv_layer_2d(self.size_d[3], self.size_d[4])\n        self.c10 = conv_layer_2d(self.size_d[4], self.size_d[5])\n\n        # Final classifying layer\n        self.classifier = nn.Conv2d(self.size_d[5], self.n_class,\n                                    1, padding=0)\n\n        # Weight initialization\n        self.c1[0].apply(self.init_weights)\n        self.c2[0].apply(self.init_weights)\n        self.c3[0].apply(self.init_weights)\n        self.c4[0].apply(self.init_weights)\n        self.c5[0].apply(self.init_weights)\n        self.c6[0].apply(self.init_weights)\n\n        self.c7[0].apply(self.init_weights)\n        self.c8[0].apply(self.init_weights)\n\n        self.c9[0].apply(self.init_weights)\n        self.c10[0].apply(self.init_weights)\n        self.classifier.apply(self.init_weights)\n\n        # Put the model on GPU memory\n        if torch.cuda.is_available():\n            self.cuda()\n            torch.backends.cudnn.enabled = True\n            torch.backends.cudnn.benchmark = True\n\n    def init_weights(self, layer):  # gaussian init for the conv layers\n        \"\"\"Initialise layer weights.\"\"\"\n        nn.init.kaiming_normal_(\n            layer.weight, mode='fan_out', nonlinearity='relu')\n\n    def forward(self, input_data):\n        \"\"\"Define model structure.\"\"\"\n        # Encoder\n        # level 1\n        x1 = self.c2(self.c1(input_data))\n        x2 = self.maxpool(x1.clone())\n        # level 2\n        x3 = self.c4(self.c3(x2.clone()))\n        x4 = self.maxpool(x3.clone())\n        # level 3\n        x5 = self.c6(self.c5(x4.clone()))\n\n        # Decoder\n        # level 2\n        y4 = self.trans1(x5.clone())\n        y3 = self.c8(self.c7(y4.clone()))\n        # level 1\n        y2 = self.trans2(y3.clone())\n        y1 = self.c10(self.c9(y2.clone()))\n        # Output\n        out = self.classifier(self.dropout(y1.clone()))\n        return out\n</pre> class SpatialNet(nn.Module):     \"\"\"U-Net for semantic segmentation.\"\"\"      def __init__(self, args):         \"\"\"         Initialize the U-Net model.          n_channels, int, number of input channel         size_e, int list, size of the feature maps of convs for the encoder         size_d, int list, size of the feature maps of convs for the decoder         n_class = int,  the number of classes         \"\"\"         super(SpatialNet, self).__init__(         )  # necessary for all classes extending the module class          self.maxpool = nn.MaxPool2d(2, 2, return_indices=False)         self.dropout = nn.Dropout2d(p=0.5, inplace=True)          self.n_channels = args['n_channel']         self.size_e = args['size_e']         self.size_d = args['size_d']         self.n_class = args['n_class']                  def conv_layer_2d(in_ch, out_ch, k_size=3, conv_bias=False):             \"\"\"Create default conv layer.\"\"\"             return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,                                            padding=1, padding_mode='reflect',                                            bias=conv_bias),                                  nn.BatchNorm2d(out_ch), nn.ReLU())          # Encoder layer definitions         self.c1 = conv_layer_2d(self.n_channels, self.size_e[0])         self.c2 = conv_layer_2d(self.size_e[0], self.size_e[1])         self.c3 = conv_layer_2d(self.size_e[1], self.size_e[2])         self.c4 = conv_layer_2d(self.size_e[2], self.size_e[3])         self.c5 = conv_layer_2d(self.size_e[3], self.size_e[4])         self.c6 = conv_layer_2d(self.size_e[4], self.size_e[5])                  # Decoder layer definitions         self.trans1 = nn.ConvTranspose2d(self.size_e[5], self.size_d[0],                                          kernel_size=2, stride=2)         self.c7 = conv_layer_2d(self.size_d[0], self.size_d[1])         self.c8 = conv_layer_2d(self.size_d[1], self.size_d[2])         self.trans2 = nn.ConvTranspose2d(self.size_d[2], self.size_d[3],                                          kernel_size=2, stride=2)         self.c9 = conv_layer_2d(self.size_d[3], self.size_d[4])         self.c10 = conv_layer_2d(self.size_d[4], self.size_d[5])          # Final classifying layer         self.classifier = nn.Conv2d(self.size_d[5], self.n_class,                                     1, padding=0)          # Weight initialization         self.c1[0].apply(self.init_weights)         self.c2[0].apply(self.init_weights)         self.c3[0].apply(self.init_weights)         self.c4[0].apply(self.init_weights)         self.c5[0].apply(self.init_weights)         self.c6[0].apply(self.init_weights)          self.c7[0].apply(self.init_weights)         self.c8[0].apply(self.init_weights)          self.c9[0].apply(self.init_weights)         self.c10[0].apply(self.init_weights)         self.classifier.apply(self.init_weights)          # Put the model on GPU memory         if torch.cuda.is_available():             self.cuda()             torch.backends.cudnn.enabled = True             torch.backends.cudnn.benchmark = True      def init_weights(self, layer):  # gaussian init for the conv layers         \"\"\"Initialise layer weights.\"\"\"         nn.init.kaiming_normal_(             layer.weight, mode='fan_out', nonlinearity='relu')      def forward(self, input_data):         \"\"\"Define model structure.\"\"\"         # Encoder         # level 1         x1 = self.c2(self.c1(input_data))         x2 = self.maxpool(x1.clone())         # level 2         x3 = self.c4(self.c3(x2.clone()))         x4 = self.maxpool(x3.clone())         # level 3         x5 = self.c6(self.c5(x4.clone()))          # Decoder         # level 2         y4 = self.trans1(x5.clone())         y3 = self.c8(self.c7(y4.clone()))         # level 1         y2 = self.trans2(y3.clone())         y1 = self.c10(self.c9(y2.clone()))         # Output         out = self.classifier(self.dropout(y1.clone()))         return out <p>augment takes in the training tile and the corresponding reference labels. It then adds a random value (taken from a normal distribution) at each wavelength and thus slightly modifies the training data. Change tile_number to see the augmentation effect for different tiles.</p> In\u00a0[\u00a0]: Copied! <pre>def augment(obs, g_t):\n    \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"\n    sigma, clip= 0.005, 0.02\n\n    # Random noise\n    rand = torch.clamp(torch.mul(sigma, torch.randn([1, 54, tile_shape[0],tile_shape[1]])), -clip, clip)\n    obs = torch.add(obs, rand)\n\n    # Random rotation 0 90 180 270 degree\n    n_turn = np.random.randint(4) #number of 90 degree turns, random int between 0 and 3\n    obs = torch.rot90(obs, n_turn, dims=(2,3))\n    g_t = torch.rot90(g_t, n_turn, dims=(1,2))\n\n    return obs, g_t\n</pre> def augment(obs, g_t):     \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"     sigma, clip= 0.005, 0.02      # Random noise     rand = torch.clamp(torch.mul(sigma, torch.randn([1, 54, tile_shape[0],tile_shape[1]])), -clip, clip)     obs = torch.add(obs, rand)      # Random rotation 0 90 180 270 degree     n_turn = np.random.randint(4) #number of 90 degree turns, random int between 0 and 3     obs = torch.rot90(obs, n_turn, dims=(2,3))     g_t = torch.rot90(g_t, n_turn, dims=(1,2))      return obs, g_t In\u00a0[\u00a0]: Copied! <pre>tile_number = 60\nvisualisation_utils.show_augment_spatial(training, tile_number,\n                                         augment, ds_name=ds_name)\n</pre> tile_number = 60 visualisation_utils.show_augment_spatial(training, tile_number,                                          augment, ds_name=ds_name) <p>train trains the network for one epoch. This function contains a for loop, which loads the training data in individual batches. Each batch of training data goes through the network, after which we compute the loss function (cross-entropy). Last step of training is performing an optimiser step, which changes the networks heights.</p> <p>eval evaluates the results on a validation set, should be done periodically during training to check for overfitting.</p> <p>train_full performs the full training loop. It first initialises the model and optimiser. Then the train function is called in a loop, with periodic evaluation on the validation set.</p> In\u00a0[\u00a0]: Copied! <pre>def train(model, optimizer, args):\n    \"\"\"train for one epoch\"\"\"\n    model.train() #switch the model in training mode\n  \n    #the loader function will take care of the batching\n    loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])\n    loader = tqdm.tqdm(loader, ncols=500)\n  \n    #will keep track of the loss\n    loss_meter = tnt.meter.AverageValueMeter()\n\n    for index, (tiles, gt) in enumerate(loader):\n    \n        optimizer.zero_grad() #put gradient to zero\n\n        tiles, gt = augment(tiles, gt)\n\n        if torch.cuda.is_available():\n            pred = model(tiles.cuda()) #compute the prediction\n        else:\n            pred = model(tiles)\n\n        loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=args['class_weights'])\n        loss.backward() #compute gradients\n\n        for p in model.parameters(): #we clip the gradient at norm 1\n            p.grad.data.clamp_(-1, 1) #this helps learning faster\n\n        optimizer.step() #one SGD step\n        loss_meter.add(loss.item())\n        \n    return loss_meter.value()[0]\n\ndef eval(model, sampler):\n    \"\"\"eval on test/validation set\"\"\"\n  \n    model.eval() #switch in eval mode\n  \n    loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)\n    loader = tqdm.tqdm(loader, ncols=500)\n  \n    loss_meter = tnt.meter.AverageValueMeter()\n\n    with torch.no_grad():\n        for index, (tiles, gt) in enumerate(loader):\n            if torch.cuda.is_available():\n                pred = model(tiles.cuda()) #compute the prediction\n            else:\n                pred = model(tiles)\n\n            loss = nn.functional.cross_entropy(pred.cpu(), gt)\n            loss_meter.add(loss.item())\n\n    return loss_meter.value()[0]\n\n\ndef train_full(args):\n    \"\"\"The full training loop\"\"\"\n\n    #initialize the model\n    model = SpatialNet(args)\n\n    print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')\n  \n    #define the Adam optimizer\n    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],\n                                               gamma=args['scheduler_gamma'])\n  \n    train_loss = np.empty(args['n_epoch'])\n    test_epochs = []\n    test_loss = []\n\n    for i_epoch in range(args['n_epoch']):\n        #train one epoch\n        print(f'Epoch #{str(i_epoch+1)}')\n        train_loss[i_epoch] = train(model, optimizer, args)\n        scheduler.step()\n\n        # Periodic testing on the validation set\n        if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):\n            print('Evaluation')\n            loss_test = eval(model, args['test_subsampler'])\n            test_epochs.append(i_epoch + 1)\n            test_loss.append(loss_test)\n            \n    plt.figure(figsize=(10, 10))\n    plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')\n    plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')\n    plt.plot(test_epochs, test_loss, label='Validation loss')\n    plt.legend()\n    plt.show()\n    print(train_loss)\n    print(test_loss)\n    args['loss_test'] = test_loss[-1]\n    \n    return model\n</pre> def train(model, optimizer, args):     \"\"\"train for one epoch\"\"\"     model.train() #switch the model in training mode        #the loader function will take care of the batching     loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])     loader = tqdm.tqdm(loader, ncols=500)        #will keep track of the loss     loss_meter = tnt.meter.AverageValueMeter()      for index, (tiles, gt) in enumerate(loader):              optimizer.zero_grad() #put gradient to zero          tiles, gt = augment(tiles, gt)          if torch.cuda.is_available():             pred = model(tiles.cuda()) #compute the prediction         else:             pred = model(tiles)          loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=args['class_weights'])         loss.backward() #compute gradients          for p in model.parameters(): #we clip the gradient at norm 1             p.grad.data.clamp_(-1, 1) #this helps learning faster          optimizer.step() #one SGD step         loss_meter.add(loss.item())              return loss_meter.value()[0]  def eval(model, sampler):     \"\"\"eval on test/validation set\"\"\"        model.eval() #switch in eval mode        loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)     loader = tqdm.tqdm(loader, ncols=500)        loss_meter = tnt.meter.AverageValueMeter()      with torch.no_grad():         for index, (tiles, gt) in enumerate(loader):             if torch.cuda.is_available():                 pred = model(tiles.cuda()) #compute the prediction             else:                 pred = model(tiles)              loss = nn.functional.cross_entropy(pred.cpu(), gt)             loss_meter.add(loss.item())      return loss_meter.value()[0]   def train_full(args):     \"\"\"The full training loop\"\"\"      #initialize the model     model = SpatialNet(args)      print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')        #define the Adam optimizer     optimizer = optim.Adam(model.parameters(), lr=args['lr'])     scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],                                                gamma=args['scheduler_gamma'])        train_loss = np.empty(args['n_epoch'])     test_epochs = []     test_loss = []      for i_epoch in range(args['n_epoch']):         #train one epoch         print(f'Epoch #{str(i_epoch+1)}')         train_loss[i_epoch] = train(model, optimizer, args)         scheduler.step()          # Periodic testing on the validation set         if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):             print('Evaluation')             loss_test = eval(model, args['test_subsampler'])             test_epochs.append(i_epoch + 1)             test_loss.append(loss_test)                  plt.figure(figsize=(10, 10))     plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')     plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')     plt.plot(test_epochs, test_loss, label='Validation loss')     plt.legend()     plt.show()     print(train_loss)     print(test_loss)     args['loss_test'] = test_loss[-1]          return model In\u00a0[\u00a0]: Copied! <pre>args = { # Dict to store all model parameters\n    'n_channel': 54,\n    'n_class': len(unique),\n    'size_e': [64,64,128,128,256,256],\n    'size_d': [256,128,128,128,64,64],\n    \n    'crossval_nfolds': 3,\n    'n_epoch_test': 2,\n    'scheduler_milestones': [50,75,90],\n    'scheduler_gamma': 0.3,\n    #'class_weights': torch.tensor([0.0, 0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),\n    'class_weights': torch.tensor([.0, .15, .05, .3, .05, .05, .1, .3]),\n    \n    'n_epoch': 50,\n    'lr': 1e-6,\n    'batch_size': 4,\n}\n\nprint(f'''Number of models to be trained:\n    {args['crossval_nfolds']}\nNumber of spectral channels:\n    {args['n_channel']}\nInitial learning rate:\n    {args['lr']}\nBatch size:\n    {args['batch_size']}\nNumber of training epochs:\n    {args['n_epoch']}''')\n</pre> args = { # Dict to store all model parameters     'n_channel': 54,     'n_class': len(unique),     'size_e': [64,64,128,128,256,256],     'size_d': [256,128,128,128,64,64],          'crossval_nfolds': 3,     'n_epoch_test': 2,     'scheduler_milestones': [50,75,90],     'scheduler_gamma': 0.3,     #'class_weights': torch.tensor([0.0, 0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),     'class_weights': torch.tensor([.0, .15, .05, .3, .05, .05, .1, .3]),          'n_epoch': 50,     'lr': 1e-6,     'batch_size': 4, }  print(f'''Number of models to be trained:     {args['crossval_nfolds']} Number of spectral channels:     {args['n_channel']} Initial learning rate:     {args['lr']} Batch size:     {args['batch_size']} Number of training epochs:     {args['n_epoch']}''') In\u00a0[\u00a0]: Copied! <pre>## Training a 2D network\nkfold = KFold(n_splits = args['crossval_nfolds'], shuffle=True)\ntrained_models = []\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    print(f'Training starts for model number {str(fold+1)}')\n    \n    a = perf_counter()\n    args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)\n    args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)\n    \n    trained_models.append((train_full(args), args['loss_test']))\n    \n    state_dict_path = join(model_save_folder, f'2D_CNN_fold_{str(fold)}.pt')\n    torch.save(trained_models[fold][0].state_dict(), state_dict_path)\n    print(f'Model saved to: {state_dict_path}')\n    print(f'Training finished in {str(perf_counter()-a)}s')\n    print('\\n\\n')\n\nprint(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}')\nprint(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}')\n</pre> ## Training a 2D network kfold = KFold(n_splits = args['crossval_nfolds'], shuffle=True) trained_models = [] for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):     print(f'Training starts for model number {str(fold+1)}')          a = perf_counter()     args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)     args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)          trained_models.append((train_full(args), args['loss_test']))          state_dict_path = join(model_save_folder, f'2D_CNN_fold_{str(fold)}.pt')     torch.save(trained_models[fold][0].state_dict(), state_dict_path)     print(f'Model saved to: {state_dict_path}')     print(f'Training finished in {str(perf_counter()-a)}s')     print('\\n\\n')  print(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}') print(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}') In\u00a0[\u00a0]: Copied! <pre># Parameters for model definition\nargs = {\n    'n_class': 8,\n    'n_channel': 54,\n    'size_e': [64,64,128,128,256,256],\n    'size_d': [256,128,128,128,64,64]\n}\n\n# Select which model to load by using a different filename index\nstate_dict_path = join(model_save_folder, '2D_CNN_fold_0.pt')\n</pre> # Parameters for model definition args = {     'n_class': 8,     'n_channel': 54,     'size_e': [64,64,128,128,256,256],     'size_d': [256,128,128,128,64,64] }  # Select which model to load by using a different filename index state_dict_path = join(model_save_folder, '2D_CNN_fold_0.pt') In\u00a0[\u00a0]: Copied! <pre># Load the model\nmodel = SpatialNet(args)\nmodel.load_state_dict(torch.load(state_dict_path))\nmodel.eval()\n</pre> # Load the model model = SpatialNet(args) model.load_state_dict(torch.load(state_dict_path)) model.eval() In\u00a0[\u00a0]: Copied! <pre>tile_shape = (128, 128)\noverlap = 64\n</pre> tile_shape = (128, 128) overlap = 64 In\u00a0[\u00a0]: Copied! <pre>start = perf_counter()\n# Load raster\nraster_orig = image_preprocessing.read_gdal_with_geoinfo(imagery_path)\n#raster_orig = image_preprocessing.read_pavia_centre(imagery_path, out_shape=(1088, 1088, 102))\n\n# Split raster into tiles\ndataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'],\n                         out_shape=tile_shape, out_overlap=overlap)\n# Normalize tiles\ndataset_full = image_preprocessing.normalize_tiles(dataset_full_tiles,\n                                                   nodata_vals=[0])\n# Convert to Pytorch TensorDataset\ndataset = tnt.dataset.TensorDataset(dataset_full['imagery'])\n\nprint('')\nprint(f'Loading and preprocessing the imagery took {visualisation_utils.sec_to_hms(perf_counter() - start)}.')\n</pre> start = perf_counter() # Load raster raster_orig = image_preprocessing.read_gdal_with_geoinfo(imagery_path) #raster_orig = image_preprocessing.read_pavia_centre(imagery_path, out_shape=(1088, 1088, 102))  # Split raster into tiles dataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'],                          out_shape=tile_shape, out_overlap=overlap) # Normalize tiles dataset_full = image_preprocessing.normalize_tiles(dataset_full_tiles,                                                    nodata_vals=[0]) # Convert to Pytorch TensorDataset dataset = tnt.dataset.TensorDataset(dataset_full['imagery'])  print('') print(f'Loading and preprocessing the imagery took {visualisation_utils.sec_to_hms(perf_counter() - start)}.') In\u00a0[\u00a0]: Copied! <pre>timer_start = perf_counter()\narr_class = inference_utils.combine_tiles_2d(model, dataset,\n                                             tile_shape, overlap,\n                                             dataset_full_tiles['dimensions'])\nprint(f'Model applied to all tiles in {visualisation_utils.sec_to_hms(perf_counter() - timer_start)}.')\n</pre> timer_start = perf_counter() arr_class = inference_utils.combine_tiles_2d(model, dataset,                                              tile_shape, overlap,                                              dataset_full_tiles['dimensions']) print(f'Model applied to all tiles in {visualisation_utils.sec_to_hms(perf_counter() - timer_start)}.') <p>You can also visualise the result:</p> In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'], arr_class,\n                                    ds_name=ds_name)\n</pre> visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'], arr_class,                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>print(f'The classified result gets saved to {out_path}')\ninference_utils.export_result(out_path, arr_class, raster_orig['geoinfo'])\n</pre> print(f'The classified result gets saved to {out_path}') inference_utils.export_result(out_path, arr_class, raster_orig['geoinfo']) In\u00a0[\u00a0]: Copied! <pre>predicted_arr = inference_utils.classify_tiles(model, test['imagery'], mode='2D')\n</pre> predicted_arr = inference_utils.classify_tiles(model, test['imagery'], mode='2D') In\u00a0[\u00a0]: Copied! <pre># Reshaping both rasters to 1D\ntest_flat = test['reference'].reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])\npred_flat = predicted_arr.reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])\n\n# Filtering to only include pixels with reference data\npred_filtered = pred_flat[test_flat &gt; 0]\ntest_filtered = test_flat[test_flat &gt; 0]\n</pre> # Reshaping both rasters to 1D test_flat = test['reference'].reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2]) pred_flat = predicted_arr.reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])  # Filtering to only include pixels with reference data pred_filtered = pred_flat[test_flat &gt; 0] test_filtered = test_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre>print(classification_report(test_filtered, pred_filtered, zero_division=0,\n                            target_names=class_names))\n</pre> print(classification_report(test_filtered, pred_filtered, zero_division=0,                             target_names=class_names)) In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_confusion_matrix(test_filtered, pred_filtered,\n                                          ds_name=ds_name)\n</pre> visualisation_utils.show_confusion_matrix(test_filtered, pred_filtered,                                           ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Read test reference\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\ntest_flat = test_arr['reference'].reshape(\n    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\ntest_filtered = test_flat[test_flat &gt; 0]\n\n# Read sample result\nsample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\nsample_flat = sample_arr['reference'].reshape(sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\nsample_filtered = sample_flat[test_flat &gt; 0]\n</pre> # Read test reference test_arr = image_preprocessing.read_gdal(imagery_path, test_path) test_flat = test_arr['reference'].reshape(     test_arr['reference'].shape[0]*test_arr['reference'].shape[1]) test_filtered = test_flat[test_flat &gt; 0]  # Read sample result sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path) sample_flat = sample_arr['reference'].reshape(sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1]) sample_filtered = sample_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre># Visualise the sample result\nvisualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    sample_arr['reference'],\n                                    ds_name=ds_name)\n</pre> # Visualise the sample result visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     sample_arr['reference'],                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Print a classification report for the sample result\nprint(classification_report(test_filtered, sample_filtered,\n                            target_names=class_names[1:]))\n</pre> # Print a classification report for the sample result print(classification_report(test_filtered, sample_filtered,                             target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre># Show a Confusion matrix for the sample result\nvisualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n                                          ds_name=ds_name)\n</pre> # Show a Confusion matrix for the sample result visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,                                           ds_name=ds_name) <p>Return to exercises</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#training-a-2d-convolutional-neural-network-for-hyperspectral-data-classification","title":"Training a 2D Convolutional Neural Network for hyperspectral data classification\u00b6","text":"<p>In this notebook, you will train and apply a two-dimensional convolutional neural network for classification of hyperspectral data from B\u00edl\u00e1 Louka, Krkono\u0161e mountains, Czechia. Please start by reading about the dataset and area of interest here.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#structure-of-this-exercise","title":"Structure of this exercise\u00b6","text":"<p>What are you going to encounter during this exercise.</p> <ol> <li>Load libraries, set paths</li> <li>Load and Preprocess training data</li> <li>Neural Network Definition / Training</li> <li>Apply Network</li> <li>Evaluate Result</li> <li>Sample Solution</li> </ol>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#0-load-external-libraries-and-set-paths","title":"0. Load external libraries and set paths\u00b6","text":"<p>First, we need to import external libraries:</p> <ul> <li><p>torch, torch.nn, torch.optim, torchnet - Pytorch related libraries for deep learning</p> </li> <li><p>numpy - Arrays to hold our data</p> </li> <li><p>matplotlib.pyplot - Draw images</p> </li> <li><p>sklearn.model_selection - Cross-validation implemented in scikit-learn</p> </li> <li><p>sklearn.metrics - Compute accuracy metrics using scikit-learn</p> </li> <li><p>time.perf_counter - Track how long individual functions take to run</p> </li> <li><p>os.path - Path manipulation</p> </li> <li><p>tqdm - show progress bars during training</p> </li> <li><p>etrainee_m4_utils.image_preprocessing - Our library holding functions for image tiling, preprocessing, etc.</p> </li> <li><p>etrainee_m4_utils.inference_utils - Our library for correctly exporting classifed images</p> </li> <li><p>etrainee_m4_utils.visualisation_utils - Our library for visualising the data</p> </li> </ul> <p>Two external libraries are not imported directly in this notebook, but are used by functions in image_preprocessing and inference_utils:</p> <ul> <li>gdal - Manipulates spatial data</li> <li>scipy.io - Reads .mat files</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#1-loading-and-preprocessing-training-data","title":"1. Loading and preprocessing training data\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#11-data-loading-into-numpy","title":"1.1. Data loading into NumPy\u00b6","text":"<p>Let's start by reading an image into a numpy array, we do this in the background using either scipy or GDAL.</p> <p>The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.</p> <p>For loading most raster datasets, we created a read_gdal() function in the image_preprocessing module. But loading .mat files for the Pavia City Centre requires a specific function (read_pavia_centre()). Both read_pavia_centre() and read_gdal() return a dictionary containing two numpy arrays with keys imagery and reference.</p> <p>If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile, we crop the image to (1088, 1088, 102) here.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#12-image-tiling","title":"1.2. Image tiling\u00b6","text":"<p>We have our data loaded into a numpy array, the next step is to divide the image into individual tiles, which will be the input for our neural network.</p> <p>As we want to perform convolution only in the spatial dimensions, we need to divide the hyperspectral image into tiles of a given shape. Standard tile sizes are based on multiples of two, for example 2^8 = 256.</p> <p>Overlap is important as there could be inconsistencies (\"jagged\" edges) on boundaries of classified tiles, we therefore combine results of overlapping tiles to avoid the inconsistencies.</p> <p>As you can see, tiling procedure transformed the original 1088x1088 image into 256 tiles of 128x128 pixels, with the original number of spectral bands.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#13-tile-filtration","title":"1.3. Tile filtration\u00b6","text":"<p>However, some of the created tiles do not contain training data, we therefore need to filter them and only keep the tiles with a field-collected reference.</p> <p>This process reduces the size of our dataset from 256 to 226 tiles.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#14-data-normalization","title":"1.4. Data normalization\u00b6","text":"<p>After filtering the tiles to only include training data, we can move onto a final part of the preprocessing - data normalization. In Machine Learning, it is common to normalize all data before classification.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#15-splitting-data-for-trainingtesting","title":"1.5. Splitting data for training/testing\u00b6","text":"<p>Our reference dataset has to be split into three groups:</p> <p>training / validation / test</p> <p>In this step we divide the data into train+val and test groups. The variable train_fraction is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function train_test_split, which performs a stratified sampling from all classes.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#16-conversion-to-pytorch-tensors","title":"1.6. Conversion to Pytorch Tensors\u00b6","text":"<p>The resulting preprocessed tiles are subsequently transformed from numpy arrays into pytorch tensors for the training.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#2-neural-network-definition","title":"2. Neural network definition\u00b6","text":"<p>After preprocessing our data, we can move onto defining our neural network and functions for training. You can either train your own neural network or use the one we already trained for you (sample_results/2D_CNN_sample_trained.pt). In case you are using the pretrained network, please run only the following code snippet (2.1.) and skip ahead to section 3.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#21-network-structure","title":"2.1. Network structure\u00b6","text":"<p>Our network is named SpatialNet, and it's based on the popular U-Net / SegNet architectures. The Structure of our network is defined in the SpatialNet class, which has three methods:</p> <ul> <li>__init__ - This method runs automatically when defining an instance of the class, it defines individual layers of the networks (2D convolutions, transposed convolutions, maxpooling and also a dropout layer).</li> <li>init_weights - Randomly initialising network weights based on a normal distribution.</li> <li>forward - Defining how data should flow through the network during a forward pass (network structure definition). The PyTorch library automatically creates a method for backward passes based on this structure.</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#22-functions-for-network-training","title":"2.2. Functions for network training\u00b6","text":"<p>Training the network is handled by four functions:</p> <ul> <li>augment - Augments the training data by adding random noise.</li> <li>train - Trains the network for one epoch.</li> <li>eval - Evaluates the results on a validation set.</li> <li>train_full - Performs the full training loop.</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#23-hyperparameter-definition","title":"2.3. Hyperparameter definition\u00b6","text":"<p>Training networks requires first setting several hyperparameters, please feel free to play around with them and try different values for the number of training epochs, learning rate or batch size.</p> <ul> <li><p>n_channel - number of channels, set to 1 for our task</p> </li> <li><p>n_class - number of classification classes</p> </li> <li><p>size_e - number of filters in each NN layer of the encoder</p> </li> <li><p>size_d - number of filters in each NN layer of the decoder</p> </li> <li><p>crossval_nfolds - Number of folds for crossvalidation</p> </li> <li><p>n_epoch_test - after how many training epochs to evaluate on the validation set</p> </li> <li><p>scheduler_milestones - after how many epochs do we reduce the training rate</p> </li> <li><p>scheduler_gamma - by what factor do we reduce the training rate</p> </li> <li><p>class_weights - training weights for individual classes, used to offset imbalanced class distribution</p> </li> <li><p>n_epoch - how many epochs are performed during training</p> </li> <li><p>lr - how fast can individual network parameters change during one training epoch</p> </li> <li><p>batch_size - how many tiles should be included in each gradient descent step</p> </li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#24-network-training","title":"2.4 Network training\u00b6","text":"<p>Run the training procedure using set hyperparameters, evaluate the training/validation loss graphs produced during training to adjust hyperparameter values.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#3-applying-the-network","title":"3. Applying the network\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#31-loading-a-trained-model","title":"3.1. Loading a trained model\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#32-loading-and-preprocessing-the-data","title":"3.2. Loading and preprocessing the data\u00b6","text":"<p>Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.</p> <p>By default, the training raster (imagery_path) is used.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#33-applying-the-cnn","title":"3.3. Applying the CNN\u00b6","text":"<p>The following snippet applies the CNN.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#34-export-resulting-raster","title":"3.4. Export Resulting Raster\u00b6","text":"<p>Export the resulting classified raster into out_path for distribution or further analysis (e.g. validation in GIS).</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#4-evaluate-classification-result","title":"4. Evaluate Classification Result\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#41-apply-classifier-to-the-test-dataset","title":"4.1. Apply classifier to the test dataset\u00b6","text":"<p>The classified raster is comapred to the test reference data. The reference data is saved in the test dictionary with keys imagery and reference.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#42-compute-accuracy-metrics","title":"4.2. Compute Accuracy Metrics\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#43-show-confusion-matrix","title":"4.3. Show Confusion Matrix\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#5-sample-solution","title":"5. Sample Solution\u00b6","text":"<p>We have generated this result using these training parameters (please note that just using the same training parameters will not yield the same result, as the network is randomly initailised):</p> <ul> <li>Number of epochs: 100</li> <li>Batch Size: 4</li> <li>Learning Rate: 1e-6</li> <li>Learning Rate reduced after Epochs #: [50,75,90]</li> <li>Learning Rate reduced by (scheduler_gamma): 0.3</li> <li>Class Weights: [0, 0.197, 0.343, 0.032, 0.166, 0.141, 0.029, 0.013, 0.022, 0.057]</li> <li>Number of Convolutional filters in the Encoder (size_e): [256,256,512,512,1024,1024]</li> <li>Number of Convolutional filters in the Decoder (size_d): [1024,512,512,512,256,256]</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_2d.html#optional-classify-a-urban-scene","title":"Optional: Classify a urban scene\u00b6","text":"<p>Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.</p> <p>Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University. You will need to change the paths to input data and use the read_pavia_centre method to load the matlab matrices into numpy.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html","title":"04 exercise cnn 3d","text":"<ul> <li><p>Prerequisities (This notebook can be run either online using Google Colab or on your local machine)</p> <ul> <li>A Google account for accessing Google Colab (link to notebook). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.</li> </ul> <p>or</p> <ul> <li><p>A Python environment with the necessary libraries (manual). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.</p> </li> <li><p>Downloaded data (module4.zip/theme4_exercise_machine_learning) The dataset consists of:</p> <ul> <li>Hyperspectral RPAS imagery of B\u00edl\u00e1 Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: BL_202008_imagery.tif</li> <li>a raster with reference data: BL_202008_reference.tif</li> <li>Pretrained models and corresponding classified rasters: /sample_results/*</li> </ul> </li> </ul> </li> <li><p>Tasks</p> <ul> <li>Preprocess imagery for Deep Learning</li> <li>Classify the hyperspectral image using 3D CNN</li> <li>Observe how hyperparameter values alter classification results</li> <li>Evaluate your results and compare to our pretrained classifier</li> <li>Optional: Classify a urban scene</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom os.path import join\nfrom time import perf_counter\n\nfrom sklearn.metrics import classification_report\nimport torchnet as tnt\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom tqdm import notebook as tqdm\n\nfrom etrainee_m4_utils import image_preprocessing\nfrom etrainee_m4_utils import inference_utils\nfrom etrainee_m4_utils import visualisation_utils\n\n# GLOBAL SETTINGS\nplt.rcParams['figure.figsize'] = [5, 5]  \nnp.set_printoptions(precision=2, suppress=True)  # Array print precision\n</pre> import torch import torch.nn as nn import torch.optim as optim  import numpy as np import matplotlib.pyplot as plt  from os.path import join from time import perf_counter  from sklearn.metrics import classification_report import torchnet as tnt  from sklearn.model_selection import KFold, StratifiedKFold, train_test_split from tqdm import notebook as tqdm  from etrainee_m4_utils import image_preprocessing from etrainee_m4_utils import inference_utils from etrainee_m4_utils import visualisation_utils  # GLOBAL SETTINGS plt.rcParams['figure.figsize'] = [5, 5]   np.set_printoptions(precision=2, suppress=True)  # Array print precision In\u00a0[\u00a0]: Copied! <pre># Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n# default: 'pavia_centre'\nds_name = 'bila_louka'\n\n# Get a list of class names\n_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)\n</pre> # Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre' # default: 'pavia_centre' ds_name = 'bila_louka'  # Get a list of class names _, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name) <p>Please fill correct paths to your training and reference rasters (just pointing the root_path variable to the project folder should do):</p> In\u00a0[\u00a0]: Copied! <pre>root_path = 'C:/folder/where/this/project/is/saved'\n\n# PATHS TO TRAINING DATA\n# Krkonose\nimagery_path   = join(root_path, 'BL_202008_imagery.tif')\nreference_path = join(root_path, 'BL_202008_reference.tif')\n# Pavia\n#imagery_path   = join(root_path, 'Pavia.mat')\n#reference_path = join(root_path, 'Pavia_gt.mat')\n\n# PATH TO SAVE MODELS\nmodel_save_folder = join(root_path, 'models')\n\n# PATH TO SAVE CLASSIFIED IMAGE\nout_path = join(root_path, 'results/Krkonose_3D_CNN.tif')\n\n# PATH TO THE SAMPLE RESULTS\nsample_result_path = join(root_path, 'sample_results/3D_CNN_sample_result.tif')\n</pre> root_path = 'C:/folder/where/this/project/is/saved'  # PATHS TO TRAINING DATA # Krkonose imagery_path   = join(root_path, 'BL_202008_imagery.tif') reference_path = join(root_path, 'BL_202008_reference.tif') # Pavia #imagery_path   = join(root_path, 'Pavia.mat') #reference_path = join(root_path, 'Pavia_gt.mat')  # PATH TO SAVE MODELS model_save_folder = join(root_path, 'models')  # PATH TO SAVE CLASSIFIED IMAGE out_path = join(root_path, 'results/Krkonose_3D_CNN.tif')  # PATH TO THE SAMPLE RESULTS sample_result_path = join(root_path, 'sample_results/3D_CNN_sample_result.tif') In\u00a0[\u00a0]: Copied! <pre>loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n# loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n    # reference_path, out_shape=(1088, 1088, 102))\n\nprint(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\nprint(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')\n</pre> loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path) # loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,     # reference_path, out_shape=(1088, 1088, 102))  print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}') print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_img_ref(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                 loaded_raster['reference'], ds_name=ds_name)\n</pre> visualisation_utils.show_img_ref(loaded_raster['imagery'][:, :, [25, 15, 5]],                                  loaded_raster['reference'], ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>tile_shape = (64, 64)\noverlap = 32\n\ndataset_tiles = image_preprocessing.tile_training(loaded_raster, tile_shape, overlap)\nprint(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}')\nprint(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}')\n</pre> tile_shape = (64, 64) overlap = 32  dataset_tiles = image_preprocessing.tile_training(loaded_raster, tile_shape, overlap) print(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}') print(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles, nodata_vals=[65535], is_training=True)\nprint(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}')\nprint(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}')\n</pre> filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles, nodata_vals=[65535], is_training=True) print(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}') print(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>normalized_tiles, unique, counts = image_preprocessing.normalize_tiles_3d(filtered_tiles, nodata_vals=[65535], is_training=True)\nprint(f'Preprocessed imagery shape {normalized_tiles[\"imagery\"].shape}')\nprint(f'Preprocessed reference shape {normalized_tiles[\"reference\"].shape}')\n</pre> normalized_tiles, unique, counts = image_preprocessing.normalize_tiles_3d(filtered_tiles, nodata_vals=[65535], is_training=True) print(f'Preprocessed imagery shape {normalized_tiles[\"imagery\"].shape}') print(f'Preprocessed reference shape {normalized_tiles[\"reference\"].shape}') In\u00a0[\u00a0]: Copied! <pre>train_fraction = 1/3\n\nX_train, X_test, y_train, y_test = train_test_split(normalized_tiles['imagery'], normalized_tiles['reference'], train_size=train_fraction)\n\ntraining = {'imagery': X_train, 'reference': y_train}\ntest  = {'imagery': X_test,  'reference': y_test}\n</pre> train_fraction = 1/3  X_train, X_test, y_train, y_test = train_test_split(normalized_tiles['imagery'], normalized_tiles['reference'], train_size=train_fraction)  training = {'imagery': X_train, 'reference': y_train} test  = {'imagery': X_test,  'reference': y_test} In\u00a0[\u00a0]: Copied! <pre>dataset = tnt.dataset.TensorDataset([training['imagery'],\n                                     training['reference']])\n\nprint(f'Class labels: \\n{unique}\\n')\nprint(f'Number of pixels in a class: \\n{counts}')\n</pre> dataset = tnt.dataset.TensorDataset([training['imagery'],                                      training['reference']])  print(f'Class labels: \\n{unique}\\n') print(f'Number of pixels in a class: \\n{counts}') In\u00a0[\u00a0]: Copied! <pre>class SpectroSpatialNet(nn.Module):\n    \"\"\"3D Spectral-Spatial CNN for semantic segmentation.\"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Initialize the SpectroSpatial model.\n\n        n_channels, int, number of input channel\n        size_e, int list, size of the feature maps of convs for the encoder\n        size_d, int list, size of the feature maps of convs for the decoder\n        n_class = int,  the number of classes\n        \"\"\"\n        # necessary for all classes extending the module class\n        super(SpectroSpatialNet, self).__init__()\n\n        self.maxpool = nn.MaxPool3d(2, 2, return_indices=False)\n        self.dropout = nn.Dropout3d(p=0.5, inplace=True)\n\n        self.n_channels = args['n_channel']\n        self.size_e = args['size_e']\n        self.size_d = args['size_d']\n        self.n_class = args['n_class']\n\n        # Encoder layer definitions\n        def c_en_3d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n                    bias=False):\n            \"\"\"Create default conv layer for the encoder.\"\"\"\n            return nn.Sequential(nn.Conv3d(in_ch, out_ch, kernel_size=k_size,\n                                           padding=pad, padding_mode=pad_mode,\n                                           bias=bias),\n                                 nn.BatchNorm3d(out_ch), nn.ReLU())\n\n        def c_de_2d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',\n                    bias=False):\n            \"\"\"Create default conv layer for the decoder.\"\"\"\n            return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,\n                                           padding=pad, padding_mode=pad_mode,\n                                           bias=bias),\n                                 nn.BatchNorm2d(out_ch), nn.ReLU())\n\n        self.c1 = c_en_3d(self.n_channels, self.size_e[0])\n        self.c2 = c_en_3d(self.size_e[0], self.size_e[1])\n        self.c3 = c_en_3d(self.size_e[1], self.size_e[2])\n        self.c4 = c_en_3d(self.size_e[2], self.size_e[3])\n        self.c5 = c_en_3d(self.size_e[3], self.size_e[4])\n        self.c6 = c_en_3d(self.size_e[4], self.size_e[5])\n\n        self.trans1 = nn.ConvTranspose2d(self.size_d[0], self.size_d[1],\n                                      kernel_size=2, stride=2)\n        self.c7 = c_de_2d(self.size_d[1], self.size_d[2])\n        self.c8 = c_de_2d(self.size_d[2], self.size_d[3])\n        self.trans2 = nn.ConvTranspose2d(self.size_d[3], self.size_d[4],\n                                      kernel_size=2, stride=2)\n        self.c9 = c_de_2d(self.size_d[4], self.size_d[5])\n        self.c10 = c_de_2d(self.size_d[5], self.size_d[6])\n\n        # Final classifying layer\n        self.classifier = nn.Conv2d(self.size_d[6], self.n_class,\n                                    1, padding=0)\n\n        # Weight initialization\n        self.c1[0].apply(self.init_weights)\n        self.c2[0].apply(self.init_weights)\n        self.c3[0].apply(self.init_weights)\n        self.c4[0].apply(self.init_weights)\n        self.c5[0].apply(self.init_weights)\n        self.c6[0].apply(self.init_weights)\n\n        self.c7[0].apply(self.init_weights)\n        self.c8[0].apply(self.init_weights)\n\n        self.c9[0].apply(self.init_weights)\n        self.c10[0].apply(self.init_weights)\n        self.classifier.apply(self.init_weights)\n\n        # Put the model on GPU memory\n        if torch.cuda.is_available():\n            self.cuda()\n            torch.backends.cudnn.enabled = True\n            torch.backends.cudnn.benchmark = True\n\n    def init_weights(self, layer):  # gaussian init for the conv layers\n        \"\"\"Initialise layer weights.\"\"\"\n        nn.init.kaiming_normal_(\n            layer.weight, mode='fan_out', nonlinearity='relu')\n\n    def forward(self, input_data):\n        \"\"\"Define model structure.\"\"\"\n        # Encoder\n        # level 1\n        x1 = self.c2(self.c1(input_data))\n        x2 = self.maxpool(x1.clone())\n        # level 2\n        x3 = self.c4(self.c3(x2.clone()))\n        x4 = self.maxpool(x3.clone())\n        # level 3\n        x5 = self.c6(self.c5(x4.clone()))\n        # Decoder\n        # Level 3\n        y5 = torch.flatten(x5.clone(), start_dim=1, end_dim=2)\n        # level 2\n        y4 = self.trans1(y5.clone())\n        y3 = self.c8(self.c7(y4.clone()))\n        # level 1\n        y2 = self.trans2(y3.clone())\n        y1 = self.c10(self.c9(y2.clone()))\n        # Output\n        out = self.classifier(self.dropout(y1.clone()))\n        return out\n</pre> class SpectroSpatialNet(nn.Module):     \"\"\"3D Spectral-Spatial CNN for semantic segmentation.\"\"\"      def __init__(self, args):         \"\"\"         Initialize the SpectroSpatial model.          n_channels, int, number of input channel         size_e, int list, size of the feature maps of convs for the encoder         size_d, int list, size of the feature maps of convs for the decoder         n_class = int,  the number of classes         \"\"\"         # necessary for all classes extending the module class         super(SpectroSpatialNet, self).__init__()          self.maxpool = nn.MaxPool3d(2, 2, return_indices=False)         self.dropout = nn.Dropout3d(p=0.5, inplace=True)          self.n_channels = args['n_channel']         self.size_e = args['size_e']         self.size_d = args['size_d']         self.n_class = args['n_class']          # Encoder layer definitions         def c_en_3d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',                     bias=False):             \"\"\"Create default conv layer for the encoder.\"\"\"             return nn.Sequential(nn.Conv3d(in_ch, out_ch, kernel_size=k_size,                                            padding=pad, padding_mode=pad_mode,                                            bias=bias),                                  nn.BatchNorm3d(out_ch), nn.ReLU())          def c_de_2d(in_ch, out_ch, k_size=3, pad=1, pad_mode='zeros',                     bias=False):             \"\"\"Create default conv layer for the decoder.\"\"\"             return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,                                            padding=pad, padding_mode=pad_mode,                                            bias=bias),                                  nn.BatchNorm2d(out_ch), nn.ReLU())          self.c1 = c_en_3d(self.n_channels, self.size_e[0])         self.c2 = c_en_3d(self.size_e[0], self.size_e[1])         self.c3 = c_en_3d(self.size_e[1], self.size_e[2])         self.c4 = c_en_3d(self.size_e[2], self.size_e[3])         self.c5 = c_en_3d(self.size_e[3], self.size_e[4])         self.c6 = c_en_3d(self.size_e[4], self.size_e[5])          self.trans1 = nn.ConvTranspose2d(self.size_d[0], self.size_d[1],                                       kernel_size=2, stride=2)         self.c7 = c_de_2d(self.size_d[1], self.size_d[2])         self.c8 = c_de_2d(self.size_d[2], self.size_d[3])         self.trans2 = nn.ConvTranspose2d(self.size_d[3], self.size_d[4],                                       kernel_size=2, stride=2)         self.c9 = c_de_2d(self.size_d[4], self.size_d[5])         self.c10 = c_de_2d(self.size_d[5], self.size_d[6])          # Final classifying layer         self.classifier = nn.Conv2d(self.size_d[6], self.n_class,                                     1, padding=0)          # Weight initialization         self.c1[0].apply(self.init_weights)         self.c2[0].apply(self.init_weights)         self.c3[0].apply(self.init_weights)         self.c4[0].apply(self.init_weights)         self.c5[0].apply(self.init_weights)         self.c6[0].apply(self.init_weights)          self.c7[0].apply(self.init_weights)         self.c8[0].apply(self.init_weights)          self.c9[0].apply(self.init_weights)         self.c10[0].apply(self.init_weights)         self.classifier.apply(self.init_weights)          # Put the model on GPU memory         if torch.cuda.is_available():             self.cuda()             torch.backends.cudnn.enabled = True             torch.backends.cudnn.benchmark = True      def init_weights(self, layer):  # gaussian init for the conv layers         \"\"\"Initialise layer weights.\"\"\"         nn.init.kaiming_normal_(             layer.weight, mode='fan_out', nonlinearity='relu')      def forward(self, input_data):         \"\"\"Define model structure.\"\"\"         # Encoder         # level 1         x1 = self.c2(self.c1(input_data))         x2 = self.maxpool(x1.clone())         # level 2         x3 = self.c4(self.c3(x2.clone()))         x4 = self.maxpool(x3.clone())         # level 3         x5 = self.c6(self.c5(x4.clone()))         # Decoder         # Level 3         y5 = torch.flatten(x5.clone(), start_dim=1, end_dim=2)         # level 2         y4 = self.trans1(y5.clone())         y3 = self.c8(self.c7(y4.clone()))         # level 1         y2 = self.trans2(y3.clone())         y1 = self.c10(self.c9(y2.clone()))         # Output         out = self.classifier(self.dropout(y1.clone()))         return out <p>augment takes in the training tile and the corresponding reference labels. It then adds a random value (taken from a normal distribution) at each wavelength and thus slightly modifies the training data. Change tile_number to see the augmentation effect for different tiles.</p> In\u00a0[\u00a0]: Copied! <pre>def augment(obs, g_t):\n    \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"\n    sigma, clip= 0.005, 0.02 \n\n    # Random noise\n    rand = torch.clamp(torch.mul(sigma, torch.randn([1, 1, 54, tile_shape[0],tile_shape[1]])), -clip, clip)\n    obs = torch.add(obs, rand)\n\n    # Random rotation 0 90 180 270 degree\n    n_turn = np.random.randint(4) #number of 90 degree turns, random int between 0 and 3\n    obs = torch.rot90(obs, n_turn, dims=(3,4))\n    g_t = torch.rot90(g_t, n_turn, dims=(1,2))\n\n    return obs, g_t\n</pre> def augment(obs, g_t):     \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"     sigma, clip= 0.005, 0.02       # Random noise     rand = torch.clamp(torch.mul(sigma, torch.randn([1, 1, 54, tile_shape[0],tile_shape[1]])), -clip, clip)     obs = torch.add(obs, rand)      # Random rotation 0 90 180 270 degree     n_turn = np.random.randint(4) #number of 90 degree turns, random int between 0 and 3     obs = torch.rot90(obs, n_turn, dims=(3,4))     g_t = torch.rot90(g_t, n_turn, dims=(1,2))      return obs, g_t In\u00a0[\u00a0]: Copied! <pre>tile_number = 200\nvisualisation_utils.show_augment_spectro_spatial(training,\n                                                 tile_number,\n                                                 augment, ds_name=ds_name)\n</pre> tile_number = 200 visualisation_utils.show_augment_spectro_spatial(training,                                                  tile_number,                                                  augment, ds_name=ds_name) <p>train trains the network for one epoch. This function contains a for loop, which loads the training data in individual batches. Each batch of training data goes through the network, after which we compute the loss function (cross-entropy). Last step of training is performing an optimiser step, which changes the networks heights.</p> <p>eval evaluates the results on a validation set, should be done periodically during training to check for overfitting.</p> <p>train_full performs the full training loop. It first initialises the model and optimiser. Then the train function is called in a loop, with periodic evaluation on the validation set.</p> In\u00a0[\u00a0]: Copied! <pre>def train(model, optimizer, args):\n    \"\"\"train for one epoch\"\"\"\n    model.train() #switch the model in training mode\n  \n    #the loader function will take care of the batching\n    loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])\n    loader = tqdm.tqdm(loader, ncols=500)\n  \n    #will keep track of the loss\n    loss_meter = tnt.meter.AverageValueMeter()\n\n    for index, (tiles, gt) in enumerate(loader):\n    \n        optimizer.zero_grad() #put gradient to zero\n\n        tiles, gt = augment(tiles, gt)\n\n        if torch.cuda.is_available():\n            pred = model(tiles.cuda()) #compute the prediction\n        else:\n            pred = model(tiles)\n\n        loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=args['class_weights'])\n        loss.backward() #compute gradients\n\n        for p in model.parameters(): #we clip the gradient at norm 1\n            p.grad.data.clamp_(-1, 1) #this helps learning faster\n\n        optimizer.step() #one SGD step\n        loss_meter.add(loss.item())\n        \n    return loss_meter.value()[0]\n\ndef eval(model, sampler):\n    \"\"\"eval on test/validation set\"\"\"\n  \n    model.eval() #switch in eval mode\n  \n    loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)\n    loader = tqdm.tqdm(loader, ncols=500)\n  \n    loss_meter = tnt.meter.AverageValueMeter()\n\n    with torch.no_grad():\n        for index, (tiles, gt) in enumerate(loader):\n            if torch.cuda.is_available():\n                pred = model(tiles.cuda()) #compute the prediction\n            else:\n                pred = model(tiles)\n\n            loss = nn.functional.cross_entropy(pred.cpu(), gt)\n            loss_meter.add(loss.item())\n\n    return loss_meter.value()[0]\n\n\ndef train_full(args):\n    \"\"\"The full training loop\"\"\"\n\n    #initialize the model\n    model = SpectroSpatialNet(args)\n\n    print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')\n  \n    #define the Adam optimizer\n    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],\n                                               gamma=args['scheduler_gamma'])\n  \n    train_loss = np.empty(args['n_epoch'])\n    test_epochs = []\n    test_loss = []\n\n    for i_epoch in range(args['n_epoch']):\n        #train one epoch\n        print(f'Epoch #{str(i_epoch+1)}')\n        train_loss[i_epoch] = train(model, optimizer, args)\n        scheduler.step()\n\n        # Periodic testing on the validation set\n        if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):\n            print('Evaluation')\n            loss_test = eval(model, args['test_subsampler'])\n            test_epochs.append(i_epoch + 1)\n            test_loss.append(loss_test)\n            \n    plt.figure(figsize=(10, 10))\n    plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')\n    plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')\n    plt.plot(test_epochs, test_loss, label='Validation loss')\n    plt.legend()\n    plt.show()\n    print(train_loss)\n    print(test_loss)\n    args['loss_test'] = test_loss[-1]\n    \n    return model\n</pre> def train(model, optimizer, args):     \"\"\"train for one epoch\"\"\"     model.train() #switch the model in training mode        #the loader function will take care of the batching     loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])     loader = tqdm.tqdm(loader, ncols=500)        #will keep track of the loss     loss_meter = tnt.meter.AverageValueMeter()      for index, (tiles, gt) in enumerate(loader):              optimizer.zero_grad() #put gradient to zero          tiles, gt = augment(tiles, gt)          if torch.cuda.is_available():             pred = model(tiles.cuda()) #compute the prediction         else:             pred = model(tiles)          loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=args['class_weights'])         loss.backward() #compute gradients          for p in model.parameters(): #we clip the gradient at norm 1             p.grad.data.clamp_(-1, 1) #this helps learning faster          optimizer.step() #one SGD step         loss_meter.add(loss.item())              return loss_meter.value()[0]  def eval(model, sampler):     \"\"\"eval on test/validation set\"\"\"        model.eval() #switch in eval mode        loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)     loader = tqdm.tqdm(loader, ncols=500)        loss_meter = tnt.meter.AverageValueMeter()      with torch.no_grad():         for index, (tiles, gt) in enumerate(loader):             if torch.cuda.is_available():                 pred = model(tiles.cuda()) #compute the prediction             else:                 pred = model(tiles)              loss = nn.functional.cross_entropy(pred.cpu(), gt)             loss_meter.add(loss.item())      return loss_meter.value()[0]   def train_full(args):     \"\"\"The full training loop\"\"\"      #initialize the model     model = SpectroSpatialNet(args)      print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')        #define the Adam optimizer     optimizer = optim.Adam(model.parameters(), lr=args['lr'])     scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],                                                gamma=args['scheduler_gamma'])        train_loss = np.empty(args['n_epoch'])     test_epochs = []     test_loss = []      for i_epoch in range(args['n_epoch']):         #train one epoch         print(f'Epoch #{str(i_epoch+1)}')         train_loss[i_epoch] = train(model, optimizer, args)         scheduler.step()          # Periodic testing on the validation set         if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):             print('Evaluation')             loss_test = eval(model, args['test_subsampler'])             test_epochs.append(i_epoch + 1)             test_loss.append(loss_test)                  plt.figure(figsize=(10, 10))     plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')     plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')     plt.plot(test_epochs, test_loss, label='Validation loss')     plt.legend()     plt.show()     print(train_loss)     print(test_loss)     args['loss_test'] = test_loss[-1]          return model In\u00a0[\u00a0]: Copied! <pre>args = { #Dict to store all model parameters\n    'n_channel': 1,\n    'n_class': len(unique),\n    'size_e': [16,16,16,16,16,16,32,32,32],\n    'size_d': [208,32,32,32,32,32,32,32,32,32],\n    \n    'crossval_nfolds': 3,\n    'n_epoch_test': 2,          #periodicity of evaluation on test set\n    'scheduler_milestones': [60,80,90],\n    'scheduler_gamma': 0.3,\n    #'class_weights': torch.tensor([0.0, 0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),\n    'class_weights': torch.tensor([.0, .15, .05, .3, .05, .05, .1, .3]),\n\n    'n_epoch': 50,\n    'lr': 1e-6,\n    'batch_size': 4,\n}\n\nprint(f'''Number of models to be trained:\n    {args['crossval_nfolds']}\nNumber of spectral channels:\n    {args['n_channel']}\nInitial learning rate:\n    {args['lr']}\nBatch size:\n    {args['batch_size']}\nNumber of training epochs:\n    {args['n_epoch']}''')\n</pre> args = { #Dict to store all model parameters     'n_channel': 1,     'n_class': len(unique),     'size_e': [16,16,16,16,16,16,32,32,32],     'size_d': [208,32,32,32,32,32,32,32,32,32],          'crossval_nfolds': 3,     'n_epoch_test': 2,          #periodicity of evaluation on test set     'scheduler_milestones': [60,80,90],     'scheduler_gamma': 0.3,     #'class_weights': torch.tensor([0.0, 0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),     'class_weights': torch.tensor([.0, .15, .05, .3, .05, .05, .1, .3]),      'n_epoch': 50,     'lr': 1e-6,     'batch_size': 4, }  print(f'''Number of models to be trained:     {args['crossval_nfolds']} Number of spectral channels:     {args['n_channel']} Initial learning rate:     {args['lr']} Batch size:     {args['batch_size']} Number of training epochs:     {args['n_epoch']}''') In\u00a0[\u00a0]: Copied! <pre>## Training a 3D network\nkfold = KFold(n_splits = args['crossval_nfolds'], shuffle=True)\ntrained_models = []\nfor fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n    print(f'Training starts for model number {str(fold+1)}')\n    \n    a = perf_counter()\n    args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)\n    args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)\n    \n    trained_models.append((train_full(args), args['loss_test']))\n    \n    state_dict_path = join(model_save_folder, f'3D_CNN_fold_{str(fold)}.pt')\n    torch.save(trained_models[fold][0].state_dict(), state_dict_path)\n    print(f'Model saved to: {state_dict_path}')\n    print(f'Training finished in {str(perf_counter()-a)}s')\n    print('\\n\\n')\n\nprint(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}')\nprint(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}')\n</pre> ## Training a 3D network kfold = KFold(n_splits = args['crossval_nfolds'], shuffle=True) trained_models = [] for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):     print(f'Training starts for model number {str(fold+1)}')          a = perf_counter()     args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)     args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)          trained_models.append((train_full(args), args['loss_test']))          state_dict_path = join(model_save_folder, f'3D_CNN_fold_{str(fold)}.pt')     torch.save(trained_models[fold][0].state_dict(), state_dict_path)     print(f'Model saved to: {state_dict_path}')     print(f'Training finished in {str(perf_counter()-a)}s')     print('\\n\\n')  print(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}') print(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}') In\u00a0[\u00a0]: Copied! <pre># Parameters for model definition\nargs = {\n    'n_class': 8,\n    'n_channel': 1,\n    'size_e': [16,16,16,16,16,16,32,32,32],\n    'size_d': [208,32,32,32,32,32,32,32,32,32],\n}\n# Path to the state_dictionary\nstate_dict_path = join(model_save_folder, '3D_CNN_fold_0.pt')\n</pre> # Parameters for model definition args = {     'n_class': 8,     'n_channel': 1,     'size_e': [16,16,16,16,16,16,32,32,32],     'size_d': [208,32,32,32,32,32,32,32,32,32], } # Path to the state_dictionary state_dict_path = join(model_save_folder, '3D_CNN_fold_0.pt') In\u00a0[\u00a0]: Copied! <pre># Load the model\nmodel = SpectroSpatialNet(args)\nmodel.load_state_dict(torch.load(state_dict_path))\nmodel.eval()\n</pre> # Load the model model = SpectroSpatialNet(args) model.load_state_dict(torch.load(state_dict_path)) model.eval() In\u00a0[\u00a0]: Copied! <pre>tile_shape = (128, 128)\noverlap = 64\n</pre> tile_shape = (128, 128) overlap = 64 In\u00a0[\u00a0]: Copied! <pre>start = perf_counter()\n# Load raster\nraster_orig = image_preprocessing.read_gdal_with_geoinfo(imagery_path)\n#raster_orig = image_preprocessing.read_pavia_centre(imagery_path, out_shape=(1088, 1088, 102))\n\n# Split raster into tiles\ndataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'],\n                         out_shape=tile_shape, out_overlap=overlap)\n# Normalize tiles\ndataset_full = image_preprocessing.normalize_tiles_3d(dataset_full_tiles,\n                                                      nodata_vals=[0])\n# Convert to Pytorch TensorDataset\ndataset = tnt.dataset.TensorDataset(dataset_full['imagery'])\nend = perf_counter()\n\nprint('')\nprint(f'Loading and preprocessing the imagery took {visualisation_utils.sec_to_hms(perf_counter() - start)}.')\n</pre> start = perf_counter() # Load raster raster_orig = image_preprocessing.read_gdal_with_geoinfo(imagery_path) #raster_orig = image_preprocessing.read_pavia_centre(imagery_path, out_shape=(1088, 1088, 102))  # Split raster into tiles dataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'],                          out_shape=tile_shape, out_overlap=overlap) # Normalize tiles dataset_full = image_preprocessing.normalize_tiles_3d(dataset_full_tiles,                                                       nodata_vals=[0]) # Convert to Pytorch TensorDataset dataset = tnt.dataset.TensorDataset(dataset_full['imagery']) end = perf_counter()  print('') print(f'Loading and preprocessing the imagery took {visualisation_utils.sec_to_hms(perf_counter() - start)}.') In\u00a0[\u00a0]: Copied! <pre>timer_start = perf_counter()\narr_class = inference_utils.combine_tiles_2d(model, dataset,\n                                             tile_shape, overlap,\n                                             dataset_full_tiles['dimensions'])\nprint(f'Model applied to all tiles in {visualisation_utils.sec_to_hms(perf_counter() - timer_start)}.')\n</pre> timer_start = perf_counter() arr_class = inference_utils.combine_tiles_2d(model, dataset,                                              tile_shape, overlap,                                              dataset_full_tiles['dimensions']) print(f'Model applied to all tiles in {visualisation_utils.sec_to_hms(perf_counter() - timer_start)}.') <p>You can also visualise the result:</p> In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'], arr_class,\n                                    ds_name=ds_name)\n</pre> visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'], arr_class,                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>print(f'The classified result gets saved to {out_path}')\ninference_utils.export_result(out_path, arr_class, raster['geoinfo'])\n</pre> print(f'The classified result gets saved to {out_path}') inference_utils.export_result(out_path, arr_class, raster['geoinfo']) In\u00a0[\u00a0]: Copied! <pre>predicted_arr = inference_utils.classify_tiles(model, test['imagery'], mode='3D')\n</pre> predicted_arr = inference_utils.classify_tiles(model, test['imagery'], mode='3D') In\u00a0[\u00a0]: Copied! <pre>print(f'The classification is evaluated using the test raster: {test_path}')\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\n</pre> print(f'The classification is evaluated using the test raster: {test_path}') test_arr = image_preprocessing.read_gdal(imagery_path, test_path) In\u00a0[\u00a0]: Copied! <pre># Reshaping both rasters to 1D\ntest_flat = test['reference'].reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])\npred_flat = predicted_arr.reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])\n\n# Filtering to only include pixels with reference data\npredicted_filtered = predicted_flat[test_flat &gt; 0]\ntest_filtered = test_flat[test_flat &gt; 0]\n</pre> # Reshaping both rasters to 1D test_flat = test['reference'].reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2]) pred_flat = predicted_arr.reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])  # Filtering to only include pixels with reference data predicted_filtered = predicted_flat[test_flat &gt; 0] test_filtered = test_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre>print(classification_report(test_filtered, predicted_filtered, zero_division=0,\n                            target_names=class_names[1:]))\n</pre> print(classification_report(test_filtered, predicted_filtered, zero_division=0,                             target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_confusion_matrix(test_filtered, predicted_filtered,\n                                          ds_name=ds_name)\n</pre> visualisation_utils.show_confusion_matrix(test_filtered, predicted_filtered,                                           ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Read test reference\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\ntest_flat = test_arr['reference'].reshape(\n    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\ntest_filtered = test_flat[test_flat &gt; 0]\n\n# Read sample result\nsample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\nsample_flat = sample_arr['reference'].reshape(sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\nsample_filtered = sample_flat[test_flat &gt; 0]\n</pre> # Read test reference test_arr = image_preprocessing.read_gdal(imagery_path, test_path) test_flat = test_arr['reference'].reshape(     test_arr['reference'].shape[0]*test_arr['reference'].shape[1]) test_filtered = test_flat[test_flat &gt; 0]  # Read sample result sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path) sample_flat = sample_arr['reference'].reshape(sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1]) sample_filtered = sample_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre># Visualise the sample result\nvisualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    sample_arr['reference'],\n                                    ds_name=ds_name)\n</pre> # Visualise the sample result visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     sample_arr['reference'],                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Print a classification report for the sample result\nprint(classification_report(test_filtered, sample_filtered,\n                            target_names=class_names[1:]))\n</pre> # Print a classification report for the sample result print(classification_report(test_filtered, sample_filtered,                             target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre># Show a Confusion matrix for the sample result\nvisualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n                                          ds_name=ds_name)\n</pre> # Show a Confusion matrix for the sample result visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,                                           ds_name=ds_name) <p>Return to exercises</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#training-a-3d-convolutional-neural-network-for-hyperspectral-data-classification","title":"Training a 3D Convolutional Neural Network for hyperspectral data classification\u00b6","text":"<p>In this notebook, you will train and apply a three-dimensional convolutional neural network for classification of hyperspectral data from B\u00edl\u00e1 Louka, Krkono\u0161e mountains, Czechia. Please start by reading about the dataset and area of interest here.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#structure-of-this-exercise","title":"Structure of this exercise\u00b6","text":"<p>What are you going to encounter during this exercise.</p> <ol> <li>Load libraries, set paths</li> <li>Load and Preprocess training data</li> <li>Neural Network Definition / Training</li> <li>Apply Network</li> <li>Evaluate Result</li> <li>Sample Solution</li> </ol>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#0-load-external-libraries-and-set-paths","title":"0. Load external libraries and set paths\u00b6","text":"<p>First, we need to import external libraries:</p> <ul> <li><p>torch, torch.nn, torch.optim, torchnet - Pytorch related libraries for deep learning</p> </li> <li><p>numpy - Arrays to hold our data</p> </li> <li><p>matplotlib.pyplot - Draw images</p> </li> <li><p>sklearn.model_selection - Cross-validation implemented in scikit-learn</p> </li> <li><p>sklearn.metrics - Compute accuracy metrics using scikit-learn</p> </li> <li><p>time.perf_counter - Track how long individual functions take to run</p> </li> <li><p>os.path - Path manipulation</p> </li> <li><p>tqdm - show progress bars during training</p> </li> <li><p>etrainee_m4_utils.image_preprocessing - Our library holding functions for image tiling, preprocessing, etc.</p> </li> <li><p>etrainee_m4_utils.inference_utils - Our library for correctly exporting classifed images</p> </li> <li><p>etrainee_m4_utils.visualisation_utils - Our library for visualising the data</p> </li> </ul> <p>Two external libraries are not imported directly in this notebook, but are used by functions in image_preprocessing and inference_utils:</p> <ul> <li>gdal - Manipulates spatial data</li> <li>scipy.io - Reads .mat files</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#1-loading-and-preprocessing-training-data","title":"1. Loading and preprocessing training data\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#11-data-loading-into-numpy","title":"1.1. Data loading into NumPy\u00b6","text":"<p>Let's start by reading an image into a numpy array, we do this in the background using GDAL.</p> <p>The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.</p> <p>For loading most raster datasets, we created a read_gdal() function in the image_preprocessing module. But loading .mat files for the Pavia City Centre requires a specific function (read_pavia_centre()). Both read_pavia_centre() and read_gdal() return a dictionary containing two numpy arrays with keys imagery and reference.</p> <p>If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile, we crop the image to (1088, 1088, 102) here.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#12-image-tiling","title":"1.2. Image tiling\u00b6","text":"<p>We have our data loaded into a numpy array, the next step is to divide the image into individual tiles, which will be the input for our neural network.</p> <p>As we want to perform convolution only in the spatial dimensions, we need to divide the hyperspectral image into tiles of a given shape. Standardly used tile sizes are multiplies of two, for example 2^8 = 256. This tile shape is ensured by setting the variable tile_shape as (256, 256).</p> <p>Overlap is important as there could be inconsistencies (\"jagged\" edges) on boundaries of classified tiles, we therefore combine results of overlapping tiles to avoid the inconsistencies.</p> <p>This process creates 1089 tiles of 64 by 64 pixels, with the same amount of spectral bands as earlier.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#13-tile-filtration","title":"1.3. Tile filtration\u00b6","text":"<p>However, most of the created tiles do not contain training data, we therefore need to filter them and only keep the tiles with a field-collected reference.</p> <p>This process significantly reduces the size of our dataset from 1 089 to 528 - training data is available for less than half of the dataset.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#14-data-normalization","title":"1.4. Data normalization\u00b6","text":"<p>After filtering the tiles to only include training data, we can move onto a final part of the preprocessing - data normalization. In Machine Learning, it is common to normalize all data before classification.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#15-splitting-data-for-trainingtesting","title":"1.5. Splitting data for training/testing\u00b6","text":"<p>Our reference dataset has to be split into three groups:</p> <p>training / validation / test</p> <p>In this step we divide the data into train+val and test groups. The variable train_fraction is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function train_test_split, which performs a stratified sampling from all classes.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#16-conversion-to-pytorch-tensors","title":"1.6. Conversion to Pytorch Tensors\u00b6","text":"<p>The resulting preprocessed tiles are subsequently transformed from numpy arrays into pytorch tensors for the training.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#2-neural-network-definition","title":"2. Neural network definition\u00b6","text":"<p>After preprocessing our data, we can move onto defining our neural network and functions for training. You can either train your own neural network or use the one we already trained for you (sample_results/3D_CNN_sample_trained.pt). In case you are using the pretrained network, please run only the following code snippet (2.1.) and skip ahead to section 3.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#21-network-structure","title":"2.1. Network structure\u00b6","text":"<p>Our network is named SpectralNet, and its structure is defined in the SpectralNet class, which has three methods:</p> <ul> <li>__init__ - This method runs automatically when defining an instance of the class, it defines indiviudal layers of the networks (3D convolutions, transposed convolutions, maxpooling and also a dropout layer).</li> <li>init_weights - Randomly initialising network weights based on a normal distribution.</li> <li>forward - Defining how data should flow through the network during a forward pass (network structure definition). The PyTorch library automatically creates a method for backward passes based on this structure.</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#22-functions-for-network-training","title":"2.2. Functions for network training\u00b6","text":"<p>Training the network is handled by four functions:</p> <ul> <li>augment - Augments the training data by adding random noise.</li> <li>train - Trains the network for one epoch.</li> <li>eval - Evaluates the results on a validation set.</li> <li>train_full - Performs the full training loop.</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#23-hyperparameter-definition","title":"2.3. Hyperparameter definition\u00b6","text":"<p>Training networks requires first setting several hyperparameters, please feel free to play around with them and try different values for the number of training epochs, learning rate or batch size.</p> <ul> <li><p>n_channel - number of channels, set to 1 for our task</p> </li> <li><p>n_class - number of classification classes</p> </li> <li><p>size_e - number of filters in each NN layer of the encoder</p> </li> <li><p>size_d - number of filters in each NN layer of the decoder</p> </li> <li><p>crossval_nfolds - Number of folds for crossvalidation</p> </li> <li><p>n_epoch_test - after how many training epochs do we validate on the validation set</p> </li> <li><p>scheduler_milestones - after how many epochs do we reduce the training rate</p> </li> <li><p>scheduler_gamma - by what factor do we reduce the training rate</p> </li> <li><p>class_weights - training weights for individual classes, used to offset imbalanced class distribution</p> </li> <li><p>n_epoch - how many epochs are performed during training</p> </li> <li><p>lr - how fast can individual network parameters change during one training epoch</p> </li> <li><p>batch_size - how many tiles should be included in each gradient descent step</p> </li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#24-network-training","title":"2.4 Network training\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#3-applying-the-network","title":"3. Applying the network\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#31-loading-a-trained-model","title":"3.1. Loading a trained model\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#32-loading-and-preprocessing-the-data","title":"3.2. Loading and preprocessing the data\u00b6","text":"<p>Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.</p> <p>By default, the training raster (imagery_path) is used.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#33-applying-the-cnn","title":"3.3. Applying the CNN\u00b6","text":"<p>The following snippet applies the CNN.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#34-export-resulting-raster","title":"3.4. Export Resulting Raster\u00b6","text":"<p>Export the resulting classified raster into out_path for distribution or further analysis (e.g. validation in GIS).</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#4-evaluate-classification-result","title":"4. Evaluate Classification Result\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#41-apply-classifier-to-the-test-dataset","title":"4.1. Apply classifier to the test dataset\u00b6","text":"<p>The classified raster is comapred to the test reference data. The reference data is saved in the test dictionary with keys imagery and reference.</p>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#42-compute-accuracy-metrics","title":"4.2. Compute Accuracy Metrics\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#43-show-confusion-matrix","title":"4.3. Show Confusion Matrix\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#5-sample-solution","title":"5. Sample Solution\u00b6","text":"<p>We have generated this result using these training parameters (please note that just using the same training parameters will not yield the same result, as the network is randomly initailised):</p> <ul> <li>Number of epochs: 100</li> <li>Batch Size: 8</li> <li>Learning Rate: 1e-6</li> <li>Learning Rate reduced after Epochs #: [60,80,90]</li> <li>Learning Rate reduced by: 0.3</li> <li>Class Weights: [0, 0.197, 0.343, 0.032, 0.166, 0.141, 0.029, 0.013, 0.022, 0.057]</li> <li>Number of convolutional filters in the encoder (size_e): [128,128,128,128,128,128,256,256,256]</li> <li>Number of convolutional filters in the decoder (size_d): [1664,256,256,256,256,256,256,256,256,256]</li> </ul>"},{"location":"module4/04_time_series_specifics/04_exercise_cnn_3d.html#optional-classify-a-urban-scene","title":"Optional: Classify a urban scene\u00b6","text":"<p>Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.</p> <p>Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University. You will need to change the paths to input data and use the read_pavia_centre method to load the matlab matrices into numpy.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html","title":"Machine learning in imaging spectroscopy","text":"<p>Image classification is one of basic processing steps in remote sensing aiming at the retrieval of thematic information (e.g., land cover types, vegetation species) from imagery. General introduction to image classification was given in Module 1 Time series analysis based on classification and Module 2 Multitemporal classification. In this theme these concepts will be extended to a linear unmixing model basically allowing for a single-date classification with subpixel accuracy, spectral angle mapper (SAM), spectral feature fitting (SFF), and a convolutional neural network (CNN) model to classify mono- or multitemporal hyperspectral (HS) imagery.  </p> <p>The objective of this theme is to learn about:</p> <ul> <li> <p>examples of classification methods suitable for HS imagery, namely linear spectral unmixing, SAM, and SID,</p> </li> <li> <p>definition of spectral endmembers.</p> </li> </ul> <p>The theme ends with a self-evaluation quiz, a set of exercises, and a list of references.</p> <p>In the practical exercise you will use linear spectral unmixing, CNN, and optionally also earlier learned random forest and support vector machine classifiers on an example of HS data collected at the B\u00edl\u00e1 louka plot of the Krkono\u0161e Mountains National Park study site from an RPAS platform.</p> <p>After finishing this theme, you will understand the principles of HS data classification and you will be able implement them in Python, R, or EnMapBox.</p> <p>This theme is focused on supervised methods used for HS classification. Existence of high-resolution spectral information opens up for methods based on spectra comparison. The spectra may originate from spectral libraries or can be derived directly from imagery. For purpose of this text, such methods are further called as \u201cbased on reference spectra\u201d. Other option is to use training data in the same manner as you learned in the Module 1 and Module 2 mentioned above. These methods will be referred as \u201cbased on training samples\u201d.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#supervised-classification-of-hyperspectral-images-based-on-reference-spectra","title":"Supervised classification of hyperspectral images based on reference spectra","text":"<p>Classification legend comprises (in ideal case all) materials/surfaces existing in the scene. Thus, each of the material shall be specified with a reference spectrum. Available spectral libraries and in-situ measurements do not always contain all spectra needed. This problem can be solved by extracting the reference, so called endmember spectra for each class/material from image itself. In hyperspectral imaging, an endmember is understood as an idealized, pure signature for a class which represents only single material within the scene. Not every endmember needs to be a pure pixel, it is a spectral signature that is considered to be pure. Thus, only pixel which spectral signature is an endmember can be called as pure (Kale at al., 2017).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#endmember-extraction-from-hs-imagery","title":"Endmember extraction from HS imagery","text":"<p>There are several possibilities how to extract endmembers from the image:</p> <p>Manual picking from image data</p> <ul> <li> <p>Assumption of homogeneity of the area</p> </li> <li> <p>Some endmembers may be missed</p> </li> </ul> <p>Manual picking from existing library spectra</p> <ul> <li> <p>Requires knowledge of ground composition</p> </li> <li> <p>Library spectra may differ from image spectra</p> </li> </ul> <p>Using endmember extraction algorithm</p> <ul> <li>Output endmembers represent \u201cspectral\u201d classes that might differ from \u201cuser\u201d classes</li> </ul> <p>There are several endmember extraction algorithms such as Pixel Purity Index (PPI), N-FINDR, Automatic Morphological Endmember Extraction (AMEE). The often used PPI is based on the Minimum Noise Fraction transformation (see Theme 2 of this Module). For a detailed explanation and comparison refer to Mart\u00ednez et al. (2006) and Kale at al., 2017. </p> <p>Once the endmember spectra are defined, you can proceed with classification.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#linear-spectral-unmixing","title":"Linear spectral unmixing","text":"<p>Spectral mixture analysis or spectral unmixing assumes that a spectrum of in a pixel might be a result of a composition (mixure) of several endmembers. The goal of unmixing is to find a fraction of endmembers in a pixel (Adams et al., 1986). If the materials in the field of view are optically separated (there is no multiple scattering between components), it can be assumed that the final signal is a linear combination of the endmembers. The relation can be expressed as:</p> <p> </p> <p>where r<sub>i</sub> is the measured spectrum, a<sub>ij</sub> is the endmember of the spectrum j, f<sub>ij</sub> is the respective endmember fraction, and e<sub>i</sub> is the error between the measurement and the derived model. The unknown fractions are determined by adjustment (e.g., by the least squares method). It shall be mentioned that spectral unmixing is only possible when the number of endmembers of the image is lower than the number of spectral bands (leads to over-dimensioning of the system of equations). The spectral differences between the original and modelled spectral values in each pixel can be calculated and statistically expressed as the root mean square error.</p> <p>Numerous examples of applications of linear unmixing can be found e.g., Okujeni etal. (2021), Cooper et al. (2020).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#spectral-angle-mapper","title":"Spectral angle mapper","text":"<p>The endmember and measured spectra can be considered as vectors in the spectral feature space. Spectral angle mapper (SAM) calculates an n-D angle to match pixels to endmembers. </p> <p> </p> <p>where \u0398 is the calculated spectral angle, t is the measured spectrum, r is the reference (endmember), i is the number of the spectral band. The pixel is assigned to an endmember with smallest angle (Yuhas et al., 1992). For each endmember, the spectral angle to the image pixels can be calculated. After visualisation brighter pixel means a larger spectral angle to the endmember.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#spectral-information-divergence","title":"Spectral information divergence","text":"<p>A comparison of endmember and measured spectra based on probability theory and entropy was suggested by proposed by Chang (2000). Du et al. (2004) uses this concept and shows advantages to SAM (especially in case of small spectral angles).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#supervised-classification-of-hyperspectral-images-based-training-data","title":"Supervised classification of hyperspectral images based training data","text":"<p>Support vector machines (SVM), random forest (RF) and CNN classifiers are used for hyperspectral data in same manner as explained for multispectral datasets in the Module 1 and Module 2 of this course. They are further practiced and in the case of the CNN also explained in practical the exercises. RF classification applied on a multitemporal dataset of HS imagery is the topic of the case study on Discrimination of selected grass species from time series of RPAS hyperspectral imagery.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#self-evaluation-quiz","title":"Self-evaluation quiz","text":"<p> Spectra unmixing  reveals information on spectral composition inside the pixel. provides information on spatial distribution of materials within the pixel.</p> reveals information on spectral composition inside the pixel. <p></p> <p> The abbreviation SAM in image spectra classification stands for  Spatial Airborne Mapper Spectral Angle Mapper Spectral Average Mapper</p> Spectral Angle Mapper <p></p> <p> The similarity measure between the reference and measured spectra in the SID classificator is based on  correlation Euclidian distance entropy</p> entropy <p></p> <p> The purpose of the kernel function used with the SVM classifier is to  increase the data dimension to improve separability between classes. reduce the number of outliers in the training dataset. produce new training data.</p> increase the data dimension to improve separability between classes. <p></p> <p></p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#exercise","title":"Exercise","text":"<p>A set of classification algoritms is provided. While the algorithms based on reference spectra (linear unmixing, SAM, SID) are suitable for single date datasets and multitemporal analysis must be carried out on the classification results, the multitemporal datasets can be an input to present methods based on training samples (RF, SVM, CNN).</p> <p>Proceed to the exercises: Hyperspectral data classification.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#next-unit","title":"Next unit","text":"<p>Proceed with Temporal vs. spatial and spectral resolution</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics.html#references","title":"References","text":"<p>Adams, J. B., Smith, M. O., Johnson, P. E. (1986). Spectral mixture modeling: A new analysis of rock and soil types at the Viking Lander 1 site. Journal of Geophysical Research: Solid Earth, 91(B8), 8098-8112. https://doi.org/10.1029/JB091iB08p08098</p> <p>Dvo\u0159\u00e1k, J., Pot\u016f\u010dkov\u00e1, M., Treml, V. (2022). Weakly supervised learning for treeline ecotone classification based on aerial orthoimages and an ancillary DSM. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences,  Volume V-3-2022. https://doi.org/10.5194/isprs-annals-V-3-2022-33-2022</p> <p>Chang, C. I. (2000). An information-theoretic approach to spectral variability, similarity, and discrimination for hyperspectral image analysis. IEEE Transactions on information theory, 46(5), 1927-1932. https://ieeexplore.ieee.org/abstract/document/857802</p> <p>Cooper, S., Okujeni, A., J\u00e4nicke, C., Clark, M., van der Linden, S., Hostert, P. (2020). Disentangling fractional vegetation cover: Regression-based unmixing of simulated spaceborne imaging spectroscopy data. Remote Sensing of Environment, 246, 111856. https://doi.org/10.1016/j.rse.2020.111856</p> <p>Du, Y., Chang, C. I., Ren, H., Chang, C. C., Jensen, J. O., D\u2019Amico, F. M. (2004). New hyperspectral discrimination measure for spectral characterization. Optical engineering, 43(8), 1777-1786. https://doi.org/10.1117/1.1766301</p> <p>Kale, K. V., Solankar, M. M., Nalawade, D. B., Dhumal, R. K., Gite, H. R. (2017). A research review on hyperspectral data processing and analysis algorithms. Proceedings of the national academy of sciences, India section a: physical sciences, 87, 541-555. https://doi.org/10.1007/s40010-017-0433-y</p> <p>Mart\u00ednez, P. J., P\u00e9rez, R. M., Plaza, A., Aguilar, P. L., Cantero, M. C., Plaza, J. (2006). Endmember extraction algorithms from hyperspectral images. http://hdl.handle.net/2122/1963</p> <p>Okujeni, A., J\u00e4nicke, C., Cooper, S., Frantz, D., Hostert, P., Clark, M., ...  van der Linden, S. (2021). Multi-season unmixing of vegetation class fractions across diverse Californian ecoregions using simulated spaceborne imaging spectroscopy data. Remote Sensing of Environment, 264, 112558. https://doi.org/10.1016/j.rse.2021.112558</p> <p>Yuhas, R. H., Goetz, A. F., Boardman, J. W. (1992). Discrimination among semi-arid landscape endmembers using the spectral angle mapper (SAM) algorithm. In JPL, Summaries of the Third Annual JPL Airborne Geoscience Workshop. Volume 1: AVIRIS Workshop. https://ntrs.nasa.gov/citations/19940012238</p> <p>Wang, X., Liu, J., Chi, W., Wang, W., &amp; Ni, Y. (2023). Advances in Hyperspectral Image Classification Methods with Small Samples: A Review. Remote Sensing, 15(15), 3795. https://doi.org/10.3390/rs15153795</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise.html","title":"Exercise: Machine learning in imaging spectroscopy","text":"<p>Objective of this exercise is to compare results of different supervised classification methods applied on a hyperspectral (HS) image acquired with a Nano-Hyperspec\u00ae camera from an RPAS platform.</p> <p>Methods based on reference spectra</p> <ul> <li>Linear spectral unmixing</li> <li>Spectral angle mapper (SAM)</li> <li>Spectral information divergence (SID \u2013 optional)</li> </ul> <p>A reference spectrum of each class in the image must be extracted/provided prior to classification. These methods are typically used in spectroscopy for data acquired with both spectroradiometers and imaging sensors.</p> <p>Methods based on training samples</p> <ul> <li>Random forest (RF)</li> <li>Support vector machine (SVM \u2013 optional)</li> <li>Convolutional neural network (CNN)</li> <li>3D CNN based on spectro-spatial convolution </li> <li>2D CNN based on spatial convolution (optional)</li> <li>1D CNN based on spectral convolution (optional)</li> </ul> <p>SAM, SID, RF, SVM, and CNN classifications are provided as a Jupyter notebook or can be run through Google Colab, the EnMapBox is used for linear spectral unmixing. Each exercise can be done independently and comprises a set of tasks and a sample solution. Comparison of results of different classifications in QGIS is recommended.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise.html#dataset","title":"Dataset","text":"<p>Temporarily, the datasets for SAM, SID, SVM, RF, CNN classifications are available separately as imagery and reference data.</p> <p>The original dataset comprising 270 spectral bands and 3 cm ground sampling distance (GSD) acquired with a Nano-Hyperspec\u00ae camera mounted on DJI Matrice 600 Pro drone was resampled to 54 bands and 9cm GSD for the purpose of this exercise. It covers a 100 m x 100 m plot at the B\u00edl\u00e1 louka meadow in the Krkono\u0161e Mountains. The training and validation data were collected during the field campaign by botanists. The dataset was collected in August of 2020.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise.html#next-unit","title":"Next unit","text":"<p>Proceed with Temporal vs. spatial and spectral resolution</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html","title":"04 time series specifics exercise rf","text":"<ul> <li><p>Prerequisities (This notebook can be run either online using Google Colab or on your local machine)</p> <ul> <li>A Google account for accessing Google Colab (link to notebook). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.</li> </ul> <p>or</p> <ul> <li><p>A Python environment with the necessary libraries (manual). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.</p> </li> <li><p>Downloaded data (module4.zip/theme4_exercise_machine_learning) The dataset consists of:</p> <ul> <li>Hyperspectral RPAS imagery of B\u00edl\u00e1 Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: BL_202008_imagery.tif</li> <li>a raster with reference data: BL_202008_reference.tif</li> <li>Pretrained models and corresponding classified rasters: /sample_results/*</li> </ul> </li> </ul> </li> <li><p>Tasks</p> <ul> <li>Preprocess imagery for Machine Learning</li> <li>Classify the hyperspectral image using RF</li> <li>Observe how hyperparameter values alter classification results</li> <li>Evaluate your results and compare to our pretrained classifier</li> <li>Optional: Classify a urban scene</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom time import perf_counter\nfrom os.path import join\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\n\nfrom tqdm import notebook as tqdm\nfrom joblib import dump, load\n\nfrom etrainee_m4_utils import image_preprocessing\nfrom etrainee_m4_utils import inference_utils\nfrom etrainee_m4_utils import visualisation_utils\n\n# GLOBAL SETTINGS\nplt.rcParams['figure.figsize'] = [5, 5]\nnp.set_printoptions(precision=2, suppress=True)  # Array print precision\n</pre> import numpy as np import matplotlib.pyplot as plt  from time import perf_counter from os.path import join  from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.model_selection import KFold, StratifiedKFold, train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report  from tqdm import notebook as tqdm from joblib import dump, load  from etrainee_m4_utils import image_preprocessing from etrainee_m4_utils import inference_utils from etrainee_m4_utils import visualisation_utils  # GLOBAL SETTINGS plt.rcParams['figure.figsize'] = [5, 5] np.set_printoptions(precision=2, suppress=True)  # Array print precision In\u00a0[\u00a0]: Copied! <pre># Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n# default: 'pavia_centre'\nds_name = 'bila_louka'\n\n# Get a list of class names\n_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)\n</pre> # Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre' # default: 'pavia_centre' ds_name = 'bila_louka'  # Get a list of class names _, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name) <p>Please fill correct paths to your training and reference rasters (just pointing the root_path variable to the project folder should do):</p> In\u00a0[\u00a0]: Copied! <pre>root_path = 'C:/folder/where/this/project/is/saved'\n\n# PATHS TO TRAINING DATA\n# Bila Louka\nimagery_path   = join(root_path, 'BL_202008_imagery.tif')\nreference_path = join(root_path, 'BL_202008_reference.tif')\n# Pavia\n#imagery_path   = join(root_path, 'Pavia.mat')\n#reference_path = join(root_path, 'Pavia_gt.mat')\n\n# PATH TO SAVE MODELS\nmodel_save_folder = join(root_path, 'models')\n\n# PATH TO SAVE CLASSIFIED IMAGE\nout_path  = join(root_path, 'results/Krkonose_RF.tif')\n\n# PATH TO THE SAMPLE RESULTS\nsample_result_path  = join(root_path, 'sample_results/RF_sample_result.tif')\n</pre> root_path = 'C:/folder/where/this/project/is/saved'  # PATHS TO TRAINING DATA # Bila Louka imagery_path   = join(root_path, 'BL_202008_imagery.tif') reference_path = join(root_path, 'BL_202008_reference.tif') # Pavia #imagery_path   = join(root_path, 'Pavia.mat') #reference_path = join(root_path, 'Pavia_gt.mat')  # PATH TO SAVE MODELS model_save_folder = join(root_path, 'models')  # PATH TO SAVE CLASSIFIED IMAGE out_path  = join(root_path, 'results/Krkonose_RF.tif')  # PATH TO THE SAMPLE RESULTS sample_result_path  = join(root_path, 'sample_results/RF_sample_result.tif') In\u00a0[\u00a0]: Copied! <pre>loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n#loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n    # reference_path, out_shape=(1088, 1088, 102))\n\nprint(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\nprint(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')\n</pre> loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path) #loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,     # reference_path, out_shape=(1088, 1088, 102))  print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}') print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}') In\u00a0[5]: Copied! <pre>visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],\n                                 loaded_raster[\"reference\"], ds_name=ds_name)\n</pre> visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],                                  loaded_raster[\"reference\"], ds_name=ds_name) <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> In\u00a0[\u00a0]: Copied! <pre>orig_shape = loaded_raster['imagery'].shape\n\nflat_arrs = {}\nflat_arrs['imagery'] = loaded_raster['imagery'].reshape(\n    orig_shape[0]*orig_shape[1], orig_shape[2])\nflat_arrs['reference'] = loaded_raster['reference'].reshape(\n    orig_shape[0]*orig_shape[1])\n\nprint(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}')\n</pre> orig_shape = loaded_raster['imagery'].shape  flat_arrs = {} flat_arrs['imagery'] = loaded_raster['imagery'].reshape(     orig_shape[0]*orig_shape[1], orig_shape[2]) flat_arrs['reference'] = loaded_raster['reference'].reshape(     orig_shape[0]*orig_shape[1])  print(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>filtered_arrs = {}\nfiltered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0]\nfiltered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]\n\nprint(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}')\n</pre> filtered_arrs = {} filtered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0] filtered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]  print(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>pixel_number = 1000\nvisualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,\n                                        ds_name=ds_name)\n</pre> pixel_number = 1000 visualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>scaler = StandardScaler()\nscaler.fit(flat_arrs['imagery'])\n\nscaled_arrs = {\n    'imagery': scaler.transform(filtered_arrs['imagery']),\n    'reference': filtered_arrs['reference']}\n</pre> scaler = StandardScaler() scaler.fit(flat_arrs['imagery'])  scaled_arrs = {     'imagery': scaler.transform(filtered_arrs['imagery']),     'reference': filtered_arrs['reference']} In\u00a0[\u00a0]: Copied! <pre>pixel_number = 5\nvisualisation_utils.show_spectral_curve(scaled_arrs, pixel_number,\n                                        ds_name=ds_name)\n</pre> pixel_number = 5 visualisation_utils.show_spectral_curve(scaled_arrs, pixel_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>train_fraction = 1/3\n\nX_train, X_test, y_train, y_test = train_test_split(scaled_arrs['imagery'], scaled_arrs['reference'], train_size=train_fraction)\n\ntrain = {'imagery': X_train, 'reference': y_train}\ntest  = {'imagery': X_test,  'reference': y_test}\n</pre> train_fraction = 1/3  X_train, X_test, y_train, y_test = train_test_split(scaled_arrs['imagery'], scaled_arrs['reference'], train_size=train_fraction)  train = {'imagery': X_train, 'reference': y_train} test  = {'imagery': X_test,  'reference': y_test} In\u00a0[\u00a0]: Copied! <pre># How many training samples do we have for each class?\nunique, counts = np.unique(train['reference'], return_counts=True)\nprint(f'The individual classes contain {counts} training pixels.')\n</pre> # How many training samples do we have for each class? unique, counts = np.unique(train['reference'], return_counts=True) print(f'The individual classes contain {counts} training pixels.') In\u00a0[\u00a0]: Copied! <pre># define potential parameter values for the RF\nparameters_rf = {\n    'n_estimators': [50, 100, 250, 500, 750], # Number of trees in the forest\n    'max_depth': [3, 5, 10, 20, 50],          # Maximum depth of a tree\n}\n</pre> # define potential parameter values for the RF parameters_rf = {     'n_estimators': [50, 100, 250, 500, 750], # Number of trees in the forest     'max_depth': [3, 5, 10, 20, 50],          # Maximum depth of a tree } In\u00a0[\u00a0]: Copied! <pre># Create the optimizer and run the optimization\nopt = RandomizedSearchCV(RandomForestClassifier(), parameters_rf, cv=5, scoring=\"jaccard_micro\", n_iter=8, refit=False, n_jobs=-2, verbose=4)\nopt.fit(X=train['imagery'], y=train['reference'])\nprint(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}')\n</pre> # Create the optimizer and run the optimization opt = RandomizedSearchCV(RandomForestClassifier(), parameters_rf, cv=5, scoring=\"jaccard_micro\", n_iter=8, refit=False, n_jobs=-2, verbose=4) opt.fit(X=train['imagery'], y=train['reference']) print(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}') In\u00a0[\u00a0]: Copied! <pre># Create the optimizer and run the GridSerach optimization\nopt = GridSearchCV(RandomForestClassifier(), parameters_rf, cv=5, scoring=\"jaccard_micro\", refit=False, n_jobs=-2, verbose=4)\nopt.fit(X=train['imagery'], y=train['reference'])\nprint(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}')\n</pre> # Create the optimizer and run the GridSerach optimization opt = GridSearchCV(RandomForestClassifier(), parameters_rf, cv=5, scoring=\"jaccard_micro\", refit=False, n_jobs=-2, verbose=4) opt.fit(X=train['imagery'], y=train['reference']) print(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}') In\u00a0[\u00a0]: Copied! <pre>rf = RandomForestClassifier(**opt.best_params_)\n\nrf.fit(X=train['imagery'], y=train['reference'])\nrf.score(train['imagery'], train['reference'])\n</pre> rf = RandomForestClassifier(**opt.best_params_)  rf.fit(X=train['imagery'], y=train['reference']) rf.score(train['imagery'], train['reference']) In\u00a0[\u00a0]: Copied! <pre># save using joblib.dump(object, filename)\nmodel_path = join(model_save_folder, 'RF.joblib')\ndump(rf, model_path)\n</pre> # save using joblib.dump(object, filename) model_path = join(model_save_folder, 'RF.joblib') dump(rf, model_path) In\u00a0[\u00a0]: Copied! <pre># load using joblib.load(filename)\nrf = load(model_path)\n</pre> # load using joblib.load(filename) rf = load(model_path) In\u00a0[\u00a0]: Copied! <pre># Load raster\nraster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))\n\n# Flattern spatial dimension of the raster\nraster_shape = raster['imagery'].shape\nraster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],\n                                        raster_shape[2])\n\n# Scale the input data\nscaler = StandardScaler()\nscaler.fit(raster_flat)\nraster_scaled = scaler.transform(raster_flat)\n</pre> # Load raster raster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))  # Flattern spatial dimension of the raster raster_shape = raster['imagery'].shape raster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],                                         raster_shape[2])  # Scale the input data scaler = StandardScaler() scaler.fit(raster_flat) raster_scaled = scaler.transform(raster_flat) In\u00a0[\u00a0]: Copied! <pre>predicted_flat = rf.predict(raster_scaled)\n</pre> predicted_flat = rf.predict(raster_scaled) In\u00a0[\u00a0]: Copied! <pre>predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1])\n</pre> predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1]) <p>You can also visualise the result:</p> In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    predicted_raster, ds_name=ds_name)\n</pre> visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     predicted_raster, ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>inference_utils.export_result(out_path, predicted_raster, raster['geoinfo'])\n</pre> inference_utils.export_result(out_path, predicted_raster, raster['geoinfo']) In\u00a0[\u00a0]: Copied! <pre>test_predicted = rf.predict(test['imagery'])\n</pre> test_predicted = rf.predict(test['imagery']) In\u00a0[\u00a0]: Copied! <pre>print(classification_report(test['reference'], test_predicted,\n      target_names=class_names[1:]))\n</pre> print(classification_report(test['reference'], test_predicted,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_confusion_matrix(test['reference'], test_predicted,\n                                          ds_name=ds_name)\n</pre> visualisation_utils.show_confusion_matrix(test['reference'], test_predicted,                                           ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Read test reference\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\ntest_flat = test_arr['reference'].reshape(\n    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\ntest_filtered = test_flat[test_flat &gt; 0]\n\n# Read sample result\nsample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\nsample_flat = sample_arr['reference'].reshape(\n    sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\nsample_filtered = sample_flat[test_flat &gt; 0]\n</pre> # Read test reference test_arr = image_preprocessing.read_gdal(imagery_path, test_path) test_flat = test_arr['reference'].reshape(     test_arr['reference'].shape[0]*test_arr['reference'].shape[1]) test_filtered = test_flat[test_flat &gt; 0]  # Read sample result sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path) sample_flat = sample_arr['reference'].reshape(     sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1]) sample_filtered = sample_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre># Visualise the sample result\nvisualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    sample_arr['reference'],\n                                    ds_name=ds_name)\n</pre> # Visualise the sample result visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     sample_arr['reference'],                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Print a classification report for the sample result\nprint(classification_report(test_filtered, sample_filtered,\n      target_names=class_names[1:]))\n</pre> # Print a classification report for the sample result print(classification_report(test_filtered, sample_filtered,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre># Show a Confusion matrix for the sample result\nvisualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n                                          ds_name=ds_name)\n</pre> # Show a Confusion matrix for the sample result visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,                                           ds_name=ds_name) <p>Return to exercises</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#random-forest-rf-hyperspectral-data-classification","title":"Random Forest (RF) hyperspectral data classification\u00b6","text":"<p>In this notebook, you will train and apply a common Machine Learning classifier - Random Forest for classification of hyperspectral data from B\u00edl\u00e1 Louka, Krkono\u0161e mountains, Czechia. Please start by reading about the dataset and area of interest here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#structure-of-this-exercise","title":"Structure of this exercise\u00b6","text":"<p>What are you going to encounter during this exercise.</p> <ol> <li>Load libraries, set paths</li> <li>Load and Preprocess training data</li> <li>Create/Fit Random Forest</li> <li>Apply Classifier</li> <li>Evaluate Result</li> <li>Sample Solution</li> </ol>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#0-load-external-libraries-and-set-paths","title":"0. Load external libraries and set paths\u00b6","text":"<p>First, we need to import external libraries:</p> <ul> <li><p>numpy - Arrays to hold our data</p> </li> <li><p>matplotlib.pyplot - Draw images</p> </li> <li><p>sklearn.ensemble - Random forest classifier</p> </li> <li><p>sklearn.model_selection - Cross-validation and hyperparameter tuning implemented in scikit-learn</p> </li> <li><p>sklearn.metrics - Compute accuracy metrics using scikit-learn</p> </li> <li><p>sklearn.preprocessing - Normalizing input data using scikit-learn</p> </li> <li><p>time.perf_counter - Track how long individual functions take to run</p> </li> <li><p>os.path - Path manipulation</p> </li> <li><p>tqdm - show progress bars during training</p> </li> <li><p>joblib - Saving and loading trained classifiers</p> </li> <li><p>etrainee_m4_utils.image_preprocessing - Our library holding functions for image tiling, preprocessing, etc.</p> </li> <li><p>etrainee_m4_utils.inference_utils - Our library for correctly exporting classifed images</p> </li> <li><p>etrainee_m4_utils.visualisation_utils - Our library for visualising the data</p> </li> </ul> <p>Two external libraries are not imported directly in this notebook, but are used by functions in image_preprocessing and inference_utils:</p> <ul> <li>gdal - Manipulates spatial data</li> <li>scipy.io - Reads .mat files</li> </ul>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#1-load-and-preprocess-training-data","title":"1. Load and preprocess training data\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#11-data-loading-into-numpy","title":"1.1. Data loading into NumPy\u00b6","text":"<p>Let's start by reading an image into a numpy array, we do this in the background using GDAL.</p> <p>The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.</p> <p>For loading most raster datasets, we created a read_gdal() function in the image_preprocessing module. But loading .mat files for the Pavia City Centre requires a specific function (read_pavia_centre()). Both read_pavia_centre() and read_gdal() return a dictionary containing two numpy arrays with keys imagery and reference.</p> <p>If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile for neural networks, we crop the image to (1088, 1088, 102) here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#12-flatten-array","title":"1.2. Flatten array\u00b6","text":"<p>We will be classifying individual pixels, therefore we can transform the 3D image (height, width, spectral bands) to a 2D array (length, spectral bands). This transformation destroys spatial relationships within the image, however the classifiers can only use 1D features anyway and it simplifies the next step (filtering NoData).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#13-filter-out-nodata-pixels","title":"1.3. Filter out NoData pixels\u00b6","text":"<p>We can only train the classifier on pixels with a reference value, therefore we remove all pixels belonging to class 0 (NoData). This operation reduces our training dataset from ~1.18 milion to ~130 thousand pixels. We then visualise the spectral curves of individual pixels.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#14-data-scaling","title":"1.4. Data scaling\u00b6","text":"<p>After filtering the training data, we can move onto data scaling. In Machine Learning, it is common to scale all features before classification, because many classifiers assume that all features vary on comparable scales  and that each feature has values close to zero.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#15-splitting-data-for-trainingtesting","title":"1.5. Splitting data for training/testing\u00b6","text":"<p>Our reference dataset has to be split into three groups:</p> <p>training / validation / test</p> <p>In this step we divide the data into train+val and test groups. The variable train_fraction is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function train_test_split, which performs a stratified sampling from all classes.</p> <p>Please experiment with different fractions of training data.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#2-random-forest-definition-and-training","title":"2. Random Forest definition and training\u00b6","text":"<p>After preprocessing our data, we can move onto defining a machine learning model. You can either train your own classifiers or use ones we already trained for you (sample_results/RF_sample_trained.joblib). In case you are using the pretrained RF, skip ahead to section 2.3.</p> <p>This training uses a Random Forest implementation from scikit-learn, a popular Machine Learning library for Python. The documentation is available at https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#21-find-most-suitable-parameters","title":"2.1. Find most suitable parameters\u00b6","text":"<p>To function proprely, RF has to have suitable values for some hyperparameters. A common approach is to train classifiers with different hyperparameter values and select the most suitable ones.</p> <p>Scikit-Learn makes this easy using RandomizedSearch or GridSearch, these functions train the classifier multiple times using different hyperparameter values and determine the most suitable combination. Each combination of hyperparameter values is tried multiple times using cross-validation (out-of-sample testing).</p> <p>Run either the cell with RandomizedSearchCV or with GridSearchCV, while Grid Search may be able to find better hyperparameters, Randomized Search will likely also find good solutions in a much shorter amount of time.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#22-fit","title":"2.2. Fit\u00b6","text":"<p>The best hyperparameter values identified during cross-validation are then used for training the model on the whole training dataset.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#23-saveload-trained-rf-for-potential-future-use","title":"2.3. Save/load trained RF for potential future use\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#3-model-application-evaluation","title":"3. Model application &amp; evaluation\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#31-loading-and-preprocessing-the-data","title":"3.1. Loading and preprocessing the data\u00b6","text":"<p>Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.</p> <p>By default, the training raster (imagery_path) is used.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#32-applying-the-classifier","title":"3.2. Applying the classifier\u00b6","text":"<p>The following snippet applies the classifier to the loaded imagery, and then transforms the flattened array back into a raster.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#33-export-resulting-raster","title":"3.3. Export resulting raster\u00b6","text":"<p>Export the resulting classified raster into out_path for distribution or further analysis (e.g. validation in GIS).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#4-evaluate-classification-result","title":"4. Evaluate Classification Result\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#41-apply-classifier-to-the-test-dastaset","title":"4.1. Apply classifier to the test dastaset\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#42-compute-accuracy-metrics","title":"4.2. Compute accuracy metrics\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#43-show-confusion-matrix","title":"4.3. Show Confusion Matrix\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#5-sample-solution","title":"5. Sample Solution\u00b6","text":"<p>We have generated this result using these training parameters (please note that just using the same training parameters will not yield the same result):</p> <ul> <li>Number of trees: 750</li> <li>Maximium tree depth: 50</li> </ul>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_rf.html#optional-classify-a-urban-scene","title":"Optional: Classify a urban scene\u00b6","text":"<p>Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.</p> <p>Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University. You will need to change the paths to input data and use the read_pavia_centre method to load the matlab matrices into numpy.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html","title":"04 time series specifics exercise sam","text":"<ul> <li><p>Prerequisities (This notebook can be run either online using Google Colab or on your local machine)</p> <ul> <li>A Google account for accessing Google Colab (link to notebook). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.</li> </ul> <p>or</p> <ul> <li><p>A Python environment with the necessary libraries (manual). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.</p> </li> <li><p>Downloaded data (module4.zip/theme4_exercise_machine_learning) The dataset consists of:</p> <ul> <li>Hyperspectral RPAS imagery of B\u00edl\u00e1 Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: BL_202008_imagery.tif</li> <li>a raster with reference data: BL_202008_reference.tif</li> <li>Pretrained models and corresponding classified rasters: /sample_results/*</li> </ul> </li> </ul> </li> <li><p>Tasks</p> <ul> <li>Preprocess hyperspectral imagery</li> <li>Classify the hyperspectral image using SAM</li> <li>Observe how changing the training set size changes the resulting classification</li> <li>Evaluate your results and compare to our pretrained classifier</li> <li>Optional: Classify a urban scene</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom time import perf_counter\nfrom os.path import join\nimport scipy\n\nfrom pysptools.classification import SAM\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report\n\nfrom tqdm import notebook as tqdm\nfrom joblib import dump, load\n\nfrom etrainee_m4_utils import image_preprocessing\nfrom etrainee_m4_utils import inference_utils\nfrom etrainee_m4_utils import visualisation_utils\n\n# GLOBAL SETTINGS\nplt.rcParams['figure.figsize'] = [5, 5]\nnp.set_printoptions(precision=2, suppress=True)  # Array print precision\n</pre> import numpy as np import matplotlib.pyplot as plt  from time import perf_counter from os.path import join import scipy  from pysptools.classification import SAM  from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.model_selection import KFold, StratifiedKFold, train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import classification_report  from tqdm import notebook as tqdm from joblib import dump, load  from etrainee_m4_utils import image_preprocessing from etrainee_m4_utils import inference_utils from etrainee_m4_utils import visualisation_utils  # GLOBAL SETTINGS plt.rcParams['figure.figsize'] = [5, 5] np.set_printoptions(precision=2, suppress=True)  # Array print precision In\u00a0[\u00a0]: Copied! <pre># Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n# default: 'pavia_centre'\nds_name = 'bila_louka'\n\n# Get a list of class names\n_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)\n</pre> # Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre' # default: 'pavia_centre' ds_name = 'bila_louka'  # Get a list of class names _, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name) <p>Please fill correct paths to your training and reference rasters (just pointing the root_path variable to the project folder should do):</p> In\u00a0[\u00a0]: Copied! <pre>root_path = 'C:/folder/where/this/project/is/saved'\n\n# PATHS TO TRAINING DATA\n# Bila Louka\nimagery_path   = join(root_path, 'BL_202008_imagery.tif')\nreference_path = join(root_path, 'BL_202008_reference.tif')\n# Pavia\n#imagery_path   = join(root_path, 'Pavia.mat')\n#reference_path = join(root_path, 'Pavia_gt.mat')\n\n# PATH TO SAVE MODELS\nmodel_save_folder = join(root_path, 'models')\n\n# PATH TO SAVE CLASSIFIED IMAGE\nout_path  = join(root_path, 'results/Krkonose_SAM.tif')\n\n# PATH TO THE SAMPLE RESULTS\nsample_result_path  = join(root_path, 'sample_results/SAM_sample_result.tif')\n</pre> root_path = 'C:/folder/where/this/project/is/saved'  # PATHS TO TRAINING DATA # Bila Louka imagery_path   = join(root_path, 'BL_202008_imagery.tif') reference_path = join(root_path, 'BL_202008_reference.tif') # Pavia #imagery_path   = join(root_path, 'Pavia.mat') #reference_path = join(root_path, 'Pavia_gt.mat')  # PATH TO SAVE MODELS model_save_folder = join(root_path, 'models')  # PATH TO SAVE CLASSIFIED IMAGE out_path  = join(root_path, 'results/Krkonose_SAM.tif')  # PATH TO THE SAMPLE RESULTS sample_result_path  = join(root_path, 'sample_results/SAM_sample_result.tif') In\u00a0[\u00a0]: Copied! <pre>loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n#loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n    # reference_path, out_shape=(1088, 1088, 102))\n\nprint(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\nprint(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')\n</pre> loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path) #loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,     # reference_path, out_shape=(1088, 1088, 102))  print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}') print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}') In\u00a0[5]: Copied! <pre>visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],\n                                 loaded_raster[\"reference\"], ds_name=ds_name)\n</pre> visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],                                  loaded_raster[\"reference\"], ds_name=ds_name) <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> In\u00a0[\u00a0]: Copied! <pre>orig_shape = loaded_raster['imagery'].shape\n\nflat_arrs = {}\nflat_arrs['imagery'] = loaded_raster['imagery'].reshape(\n    orig_shape[0]*orig_shape[1], orig_shape[2])\nflat_arrs['reference'] = loaded_raster['reference'].reshape(\n    orig_shape[0]*orig_shape[1])\n\nprint(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}')\n</pre> orig_shape = loaded_raster['imagery'].shape  flat_arrs = {} flat_arrs['imagery'] = loaded_raster['imagery'].reshape(     orig_shape[0]*orig_shape[1], orig_shape[2]) flat_arrs['reference'] = loaded_raster['reference'].reshape(     orig_shape[0]*orig_shape[1])  print(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>filtered_arrs = {}\nfiltered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0]\nfiltered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]\n\nprint(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}')\n</pre> filtered_arrs = {} filtered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0] filtered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]  print(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>pixel_number = 1000\nvisualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,\n                                        ds_name=ds_name)\n</pre> pixel_number = 1000 visualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>train_fraction = 1/3\n\nX_train, X_test, y_train, y_test = train_test_split(filtered_arrs['imagery'], filtered_arrs['reference'], train_size=train_fraction)\n\n\ntrain = {'imagery': X_train, 'reference': y_train}\ntest  = {'imagery': X_test,  'reference': y_test}\n</pre> train_fraction = 1/3  X_train, X_test, y_train, y_test = train_test_split(filtered_arrs['imagery'], filtered_arrs['reference'], train_size=train_fraction)   train = {'imagery': X_train, 'reference': y_train} test  = {'imagery': X_test,  'reference': y_test} In\u00a0[\u00a0]: Copied! <pre># How many training samples do we have for each class?\nunique, counts = np.unique(train['reference'], return_counts=True)\nprint(f'The individual classes contain {counts} training pixels.')\n</pre> # How many training samples do we have for each class? unique, counts = np.unique(train['reference'], return_counts=True) print(f'The individual classes contain {counts} training pixels.') In\u00a0[\u00a0]: Copied! <pre>num_classes = np.unique(train['reference']).shape[0]\nnum_bands   = train['imagery'].shape[1]\nmean_bands = np.empty((num_classes, num_bands))\n\nfor class_val in np.unique(train['reference']):\n    mean_bands[class_val-1, :] = np.mean(train['imagery'][train['reference'] == class_val], axis=0)\n\nspectral_lib = {'imagery': mean_bands, 'reference': np.unique(train['reference'])}\n</pre> num_classes = np.unique(train['reference']).shape[0] num_bands   = train['imagery'].shape[1] mean_bands = np.empty((num_classes, num_bands))  for class_val in np.unique(train['reference']):     mean_bands[class_val-1, :] = np.mean(train['imagery'][train['reference'] == class_val], axis=0)  spectral_lib = {'imagery': mean_bands, 'reference': np.unique(train['reference'])} <p>Let's visualise our spectral library</p> In\u00a0[\u00a0]: Copied! <pre>for cls in range(mean_bands.shape[0]):\n    plt.plot(mean_bands[cls, :], label=class_names[cls+1])\nplt.legend()\n</pre> for cls in range(mean_bands.shape[0]):     plt.plot(mean_bands[cls, :], label=class_names[cls+1]) plt.legend() In\u00a0[\u00a0]: Copied! <pre># Load raster\nraster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))\n\n# Flattern spatial dimension of the raster\nraster_shape = raster['imagery'].shape\nraster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],\n                                        raster_shape[2])\n</pre> # Load raster raster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))  # Flattern spatial dimension of the raster raster_shape = raster['imagery'].shape raster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],                                         raster_shape[2]) In\u00a0[\u00a0]: Copied! <pre>sam = SAM()\npredicted_flat = sam.classify(raster_flat[:, None], mean_bands, threshold=0.9)[:, 0]\n</pre> sam = SAM() predicted_flat = sam.classify(raster_flat[:, None], mean_bands, threshold=0.9)[:, 0] In\u00a0[\u00a0]: Copied! <pre>predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1])\n</pre> predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1]) <p>You can also visualise the result:</p> In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    predicted_raster, ds_name=ds_name)\n</pre> visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     predicted_raster, ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># save using joblib.dump(object, filename)\nmodel_path = join(model_save_folder, 'SAM.joblib')\ndump(sam, model_path)\n</pre> # save using joblib.dump(object, filename) model_path = join(model_save_folder, 'SAM.joblib') dump(sam, model_path) In\u00a0[\u00a0]: Copied! <pre># load using joblib.load(filename)\nsam = load(model_path)\n</pre> # load using joblib.load(filename) sam = load(model_path) In\u00a0[\u00a0]: Copied! <pre>inference_utils.export_result(out_path, predicted_raster, raster['geoinfo'])\n</pre> inference_utils.export_result(out_path, predicted_raster, raster['geoinfo']) In\u00a0[\u00a0]: Copied! <pre>sam = SAM()\nclassified_test = sam.classify(test['imagery'][:, None], mean_bands, threshold=0.9)[:, 0]\n</pre> sam = SAM() classified_test = sam.classify(test['imagery'][:, None], mean_bands, threshold=0.9)[:, 0] In\u00a0[\u00a0]: Copied! <pre>print(classification_report(test['reference'], classified_test,\n      target_names=class_names[1:]))\n</pre> print(classification_report(test['reference'], classified_test,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_confusion_matrix(test['reference'], classified_test,\n                                          ds_name=ds_name)\n</pre> visualisation_utils.show_confusion_matrix(test['reference'], classified_test,                                           ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Read test reference\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\ntest_flat = test_arr['reference'].reshape(\n    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\ntest_filtered = test_flat[test_flat &gt; 0]\n\n# Read sample result\nsample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\nsample_flat = sample_arr['reference'].reshape(\n    sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\nsample_filtered = sample_flat[test_flat &gt; 0]\n</pre> # Read test reference test_arr = image_preprocessing.read_gdal(imagery_path, test_path) test_flat = test_arr['reference'].reshape(     test_arr['reference'].shape[0]*test_arr['reference'].shape[1]) test_filtered = test_flat[test_flat &gt; 0]  # Read sample result sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path) sample_flat = sample_arr['reference'].reshape(     sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1]) sample_filtered = sample_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre># Visualise the sample result\nvisualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    sample_arr['reference'],\n                                    ds_name=ds_name)\n</pre> # Visualise the sample result visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     sample_arr['reference'],                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Print a classification report for the sample result\nprint(classification_report(test_filtered, sample_filtered,\n      target_names=class_names[1:]))\n</pre> # Print a classification report for the sample result print(classification_report(test_filtered, sample_filtered,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre># Show a Confusion matrix for the sample result\nvisualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n                                          ds_name=ds_name)\n</pre> # Show a Confusion matrix for the sample result visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,                                           ds_name=ds_name) <p>Return to exercises</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#spectral-angle-mapper-sam-hyperspectral-data-classification","title":"Spectral Angle Mapper (SAM) hyperspectral data classification\u00b6","text":"<p>In this notebook, you will train and apply a Spectral Angle Mapper classifier on hyperspectral data from B\u00edl\u00e1 Louka, Krkono\u0161e mountains, Czechia. Please start by reading about the dataset and area of interest here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#structure-of-this-exercise","title":"Structure of this exercise\u00b6","text":"<p>What are you going to encounter during this exercise.</p> <ol> <li>Load libraries, set paths</li> <li>Load and Preprocess training data</li> <li>Extract mean spectral curves from training data</li> <li>Apply Classifier</li> <li>Evaluate Result</li> <li>Sample Solution</li> </ol>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#0-load-external-libraries-and-set-paths","title":"0. Load external libraries and set paths\u00b6","text":"<p>First, we need to import external libraries:</p> <ul> <li><p>numpy - Arrays to hold our data</p> </li> <li><p>matplotlib.pyplot - Draw images</p> </li> <li><p>pysptools.classification - Define and use a SAM classifier</p> </li> <li><p>sklearn.metrics - Compute accuracy metrics using scikit-learn</p> </li> <li><p>sklearn.preprocessing - Normalizing input data using scikit-learn</p> </li> <li><p>time.perf_counter - Track how long individual functions take to run</p> </li> <li><p>os.path - Path manipulation</p> </li> <li><p>tqdm - show progress bars during training</p> </li> <li><p>joblib - Saving and loading trained classifiers</p> </li> <li><p>etrainee_m4_utils.image_preprocessing - Our library holding functions for image tiling, preprocessing, etc.</p> </li> <li><p>etrainee_m4_utils.inference_utils - Our library for correctly exporting classifed images</p> </li> <li><p>etrainee_m4_utils.visualisation_utils - Our library for visualising the data</p> </li> </ul> <p>Two external libraries are not imported directly in this notebook, but are used by functions in image_preprocessing and inference_utils:</p> <ul> <li>gdal - Manipulates spatial data</li> <li>scipy.io - Reads .mat files</li> </ul>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#1-load-and-preprocess-training-data","title":"1. Load and preprocess training data\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#11-data-loading-into-numpy","title":"1.1. Data loading into NumPy\u00b6","text":"<p>Let's start by reading an image into a numpy array, we do this in the background using GDAL.</p> <p>The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.</p> <p>For loading most raster datasets, we created a read_gdal() function in the image_preprocessing module. But loading .mat files for the Pavia City Centre requires a specific function (read_pavia_centre()). Both read_pavia_centre() and read_gdal() return a dictionary containing two numpy arrays with keys imagery and reference.</p> <p>If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile for neural networks, we crop the image to (1088, 1088, 102) here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#12-flatten-array","title":"1.2. Flatten array\u00b6","text":"<p>We will be classifying individual pixels, therefore we can transform the 3D image (height, width, spectral bands) to a 2D array (length, spectral bands). This transformation destroys spatial relationships within the image, however the classifiers can only use 1D features anyway and it simplifies the next step (filtering NoData).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#13-filter-out-nodata-pixels","title":"1.3. Filter out NoData pixels\u00b6","text":"<p>We can only train the classifier on pixels with a reference value, therefore we remove all pixels belonging to class 0 (NoData). This operation reduces our training dataset from ~1.18 milion to ~130 thousand pixels. We then visualise the spectral curves of individual pixels.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#14-splitting-data-for-trainingtesting","title":"1.4. Splitting data for training/testing\u00b6","text":"<p>Our reference dataset has to be split into three groups:</p> <p>training / validation / test</p> <p>In this step we divide the data into train+val and test groups. The variable train_fraction is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function train_test_split, which performs a stratified sampling from all classes.</p> <p>Please experiment with different fractions of training data.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#2-extract-mean-spectral-curves-from-training-data","title":"2. Extract mean spectral curves from training data\u00b6","text":"<p>The training data can then be used to extract mean spectral curves for each class, thus creating a spectral library. the spectral library is then used by the classifier.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#3-model-application-evaluation","title":"3. Model application &amp; evaluation\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#31-loading-and-preprocessing-the-data","title":"3.1. Loading and preprocessing the data\u00b6","text":"<p>Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.</p> <p>By default, the training raster (imagery_path) is used.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#32-defining-and-applying-the-classifier","title":"3.2. Defining and applying the classifier\u00b6","text":"<p>The following snippet applies the classifier to the loaded imagery, and then transforms the flattened array back into a raster.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#33-saveload-trained-sam-for-potential-future-use","title":"3.3. Save/load trained SAM for potential future use\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#34-export-resulting-raster","title":"3.4. Export resulting raster\u00b6","text":"<p>Export the resulting classified raster into out_path for distribution or further analysis (e.g. validation in GIS).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#4-evaluate-classification-result","title":"4. Evaluate Classification Result\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#41-apply-classifier-to-the-test-dastaset","title":"4.1. Apply classifier to the test dastaset\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#42-compute-accuracy-metrics","title":"4.2. Compute accuracy metrics\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#43-show-confusion-matrix","title":"4.3. Show Confusion Matrix\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#5-sample-solution","title":"5. Sample Solution\u00b6","text":"<p>Our classification has generated this result (please note that you may get a different result):</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sam.html#optional-classify-a-urban-scene","title":"Optional: Classify a urban scene\u00b6","text":"<p>Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.</p> <p>Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University. You will need to change the paths to input data and use the read_pavia_centre method to load the matlab matrices into numpy.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html","title":"04 time series specifics exercise sid","text":"<ul> <li><p>Prerequisities (This notebook can be run either online using Google Colab or on your local machine)</p> <ul> <li>A Google account for accessing Google Colab (link to notebook). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.</li> </ul> <p>or</p> <ul> <li><p>A Python environment with the necessary libraries (manual). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.</p> </li> <li><p>Downloaded data (module4.zip/theme4_exercise_machine_learning) The dataset consists of:</p> <ul> <li>Hyperspectral RPAS imagery of B\u00edl\u00e1 Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: BL_202008_imagery.tif</li> <li>a raster with reference data: BL_202008_reference.tif</li> <li>Pretrained models and corresponding classified rasters: /sample_results/*</li> </ul> </li> </ul> </li> <li><p>Tasks</p> <ul> <li>Preprocess hyperspectral imagery</li> <li>Classify the hyperspectral image using SID</li> <li>Observe how changing the training set size changes the resulting classification</li> <li>Evaluate your results and compare to our pretrained classifier</li> <li>Optional: Classify a urban scene</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom time import perf_counter\nfrom os.path import join\nimport scipy\n\nfrom pysptools.classification import SID\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report\n\nfrom tqdm import notebook as tqdm\nfrom joblib import dump, load\n\nfrom etrainee_m4_utils import image_preprocessing\nfrom etrainee_m4_utils import inference_utils\nfrom etrainee_m4_utils import visualisation_utils\n\n# GLOBAL SETTINGS\nplt.rcParams['figure.figsize'] = [5, 5]\nnp.set_printoptions(precision=2, suppress=True)  # Array print precision\n</pre> import numpy as np import matplotlib.pyplot as plt  from time import perf_counter from os.path import join import scipy  from pysptools.classification import SID  from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.model_selection import KFold, StratifiedKFold, train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import classification_report  from tqdm import notebook as tqdm from joblib import dump, load  from etrainee_m4_utils import image_preprocessing from etrainee_m4_utils import inference_utils from etrainee_m4_utils import visualisation_utils  # GLOBAL SETTINGS plt.rcParams['figure.figsize'] = [5, 5] np.set_printoptions(precision=2, suppress=True)  # Array print precision In\u00a0[\u00a0]: Copied! <pre># Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n# default: 'pavia_centre'\nds_name = 'bila_louka'\n\n# Get a list of class names\n_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)\n</pre> # Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre' # default: 'pavia_centre' ds_name = 'bila_louka'  # Get a list of class names _, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name) <p>Please fill correct paths to your training and reference rasters (just pointing the root_path variable to the project folder should do):</p> In\u00a0[\u00a0]: Copied! <pre>root_path = 'C:/folder/where/this/project/is/saved'\n\n# PATHS TO TRAINING DATA\n# Bila Louka\nimagery_path   = join(root_path, 'BL_202008_imagery.tif')\nreference_path = join(root_path, 'BL_202008_reference.tif')\n# Pavia\n#imagery_path   = join(root_path, 'Pavia.mat')\n#reference_path = join(root_path, 'Pavia_gt.mat')\n\n# PATH TO SAVE MODELS\nmodel_save_folder = join(root_path, 'models')\n\n# PATH TO SAVE CLASSIFIED IMAGE\nout_path  = join(root_path, 'results/Krkonose_SID.tif')\n\n# PATH TO THE SAMPLE RESULTS\nsample_result_path  = join(root_path, 'sample_results/SID_sample_result.tif')\n</pre> root_path = 'C:/folder/where/this/project/is/saved'  # PATHS TO TRAINING DATA # Bila Louka imagery_path   = join(root_path, 'BL_202008_imagery.tif') reference_path = join(root_path, 'BL_202008_reference.tif') # Pavia #imagery_path   = join(root_path, 'Pavia.mat') #reference_path = join(root_path, 'Pavia_gt.mat')  # PATH TO SAVE MODELS model_save_folder = join(root_path, 'models')  # PATH TO SAVE CLASSIFIED IMAGE out_path  = join(root_path, 'results/Krkonose_SID.tif')  # PATH TO THE SAMPLE RESULTS sample_result_path  = join(root_path, 'sample_results/SID_sample_result.tif') In\u00a0[\u00a0]: Copied! <pre>loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n#loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n    # reference_path, out_shape=(1088, 1088, 102))\n\nprint(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\nprint(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')\n</pre> loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path) #loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,     # reference_path, out_shape=(1088, 1088, 102))  print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}') print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}') In\u00a0[5]: Copied! <pre>visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],\n                                 loaded_raster[\"reference\"], ds_name=ds_name)\n</pre> visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],                                  loaded_raster[\"reference\"], ds_name=ds_name) <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> In\u00a0[\u00a0]: Copied! <pre>orig_shape = loaded_raster['imagery'].shape\n\nflat_arrs = {}\nflat_arrs['imagery'] = loaded_raster['imagery'].reshape(\n    orig_shape[0]*orig_shape[1], orig_shape[2])\nflat_arrs['reference'] = loaded_raster['reference'].reshape(\n    orig_shape[0]*orig_shape[1])\n\nprint(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}')\n</pre> orig_shape = loaded_raster['imagery'].shape  flat_arrs = {} flat_arrs['imagery'] = loaded_raster['imagery'].reshape(     orig_shape[0]*orig_shape[1], orig_shape[2]) flat_arrs['reference'] = loaded_raster['reference'].reshape(     orig_shape[0]*orig_shape[1])  print(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>filtered_arrs = {}\nfiltered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0]\nfiltered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]\n\nprint(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}')\n</pre> filtered_arrs = {} filtered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0] filtered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]  print(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>pixel_number = 1000\nvisualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,\n                                        ds_name=ds_name)\n</pre> pixel_number = 1000 visualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>train_fraction = 1/3\n\nX_train, X_test, y_train, y_test = train_test_split(filtered_arrs['imagery'], filtered_arrs['reference'], train_size=train_fraction)\n\n\ntrain = {'imagery': X_train, 'reference': y_train}\ntest  = {'imagery': X_test,  'reference': y_test}\n</pre> train_fraction = 1/3  X_train, X_test, y_train, y_test = train_test_split(filtered_arrs['imagery'], filtered_arrs['reference'], train_size=train_fraction)   train = {'imagery': X_train, 'reference': y_train} test  = {'imagery': X_test,  'reference': y_test} In\u00a0[\u00a0]: Copied! <pre># How many training samples do we have for each class?\nunique, counts = np.unique(train['reference'], return_counts=True)\nprint(f'The individual classes contain {counts} training pixels.')\n</pre> # How many training samples do we have for each class? unique, counts = np.unique(train['reference'], return_counts=True) print(f'The individual classes contain {counts} training pixels.') In\u00a0[\u00a0]: Copied! <pre>num_classes = np.unique(train['reference']).shape[0]\nnum_bands   = train['imagery'].shape[1]\nmean_bands = np.empty((num_classes, num_bands))\n\nfor class_val in np.unique(train['reference']):\n    mean_bands[class_val-1, :] = np.mean(train['imagery'][train['reference'] == class_val], axis=0)\n\nspectral_lib = {'imagery': mean_bands, 'reference': np.unique(train['reference'])}\n</pre> num_classes = np.unique(train['reference']).shape[0] num_bands   = train['imagery'].shape[1] mean_bands = np.empty((num_classes, num_bands))  for class_val in np.unique(train['reference']):     mean_bands[class_val-1, :] = np.mean(train['imagery'][train['reference'] == class_val], axis=0)  spectral_lib = {'imagery': mean_bands, 'reference': np.unique(train['reference'])} <p>Let's visualise our spectral library</p> In\u00a0[\u00a0]: Copied! <pre>for cls in range(mean_bands.shape[0]):\n    plt.plot(mean_bands[cls, :], label=class_names[cls+1])\nplt.legend()\n</pre> for cls in range(mean_bands.shape[0]):     plt.plot(mean_bands[cls, :], label=class_names[cls+1]) plt.legend() In\u00a0[\u00a0]: Copied! <pre># Load raster\nraster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))\n\n# Flattern spatial dimension of the raster\nraster_shape = raster['imagery'].shape\nraster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],\n                                        raster_shape[2])\n</pre> # Load raster raster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))  # Flattern spatial dimension of the raster raster_shape = raster['imagery'].shape raster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],                                         raster_shape[2]) In\u00a0[\u00a0]: Copied! <pre>sid = SID()\npredicted_flat = sid.classify(raster_flat[:, None], mean_bands, threshold=0.9)[:, 0]\n</pre> sid = SID() predicted_flat = sid.classify(raster_flat[:, None], mean_bands, threshold=0.9)[:, 0] In\u00a0[\u00a0]: Copied! <pre>predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1])\n</pre> predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1]) <p>You can also visualise the result:</p> In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    predicted_raster, ds_name=ds_name)\n</pre> visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     predicted_raster, ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># save using joblib.dump(object, filename)\nmodel_path = join(model_save_folder, 'SID.joblib')\ndump(sid, model_path)\n</pre> # save using joblib.dump(object, filename) model_path = join(model_save_folder, 'SID.joblib') dump(sid, model_path) In\u00a0[\u00a0]: Copied! <pre># load using joblib.load(filename)\nsid = load(model_path)\n</pre> # load using joblib.load(filename) sid = load(model_path) In\u00a0[\u00a0]: Copied! <pre>inference_utils.export_result(out_path, predicted_raster, raster['geoinfo'])\n</pre> inference_utils.export_result(out_path, predicted_raster, raster['geoinfo']) In\u00a0[\u00a0]: Copied! <pre>sid = SID()\nclassified_test = sid.classify(test['imagery'][:, None], mean_bands, threshold=0.9)[:, 0]\n</pre> sid = SID() classified_test = sid.classify(test['imagery'][:, None], mean_bands, threshold=0.9)[:, 0] In\u00a0[\u00a0]: Copied! <pre>print(classification_report(test['reference'], classified_test,\n      target_names=class_names[1:]))\n</pre> print(classification_report(test['reference'], classified_test,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_confusion_matrix(test['reference'], classified_test,\n                                          ds_name=ds_name)\n</pre> visualisation_utils.show_confusion_matrix(test['reference'], classified_test,                                           ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Read test reference\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\ntest_flat = test_arr['reference'].reshape(\n    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\ntest_filtered = test_flat[test_flat &gt; 0]\n\n# Read sample result\nsample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\nsample_flat = sample_arr['reference'].reshape(\n    sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\nsample_filtered = sample_flat[test_flat &gt; 0]\n</pre> # Read test reference test_arr = image_preprocessing.read_gdal(imagery_path, test_path) test_flat = test_arr['reference'].reshape(     test_arr['reference'].shape[0]*test_arr['reference'].shape[1]) test_filtered = test_flat[test_flat &gt; 0]  # Read sample result sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path) sample_flat = sample_arr['reference'].reshape(     sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1]) sample_filtered = sample_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre># Visualise the sample result\nvisualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    sample_arr['reference'],\n                                    ds_name=ds_name)\n</pre> # Visualise the sample result visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     sample_arr['reference'],                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Print a classification report for the sample result\nprint(classification_report(test_filtered, sample_filtered,\n      target_names=class_names[1:]))\n</pre> # Print a classification report for the sample result print(classification_report(test_filtered, sample_filtered,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre># Show a Confusion matrix for the sample result\nvisualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n                                          ds_name=ds_name)\n</pre> # Show a Confusion matrix for the sample result visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,                                           ds_name=ds_name) <p>Return to exercises</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#spectral-information-divergence-sid-hyperspectral-data-classification","title":"Spectral Information Divergence (SID) hyperspectral data classification\u00b6","text":"<p>In this notebook, you will train and apply a Spectral information divergence classifier on hyperspectral data from B\u00edl\u00e1 Louka, Krkono\u0161e mountains, Czechia. Please start by reading about the dataset and area of interest here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#structure-of-this-exercise","title":"Structure of this exercise\u00b6","text":"<p>What are you going to encounter during this exercise.</p> <ol> <li>Load libraries, set paths</li> <li>Load and Preprocess training data</li> <li>Extract mean spectral curves from training data</li> <li>Apply Classifier</li> <li>Evaluate Result</li> <li>Sample Solution</li> </ol>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#0-load-external-libraries-and-set-paths","title":"0. Load external libraries and set paths\u00b6","text":"<p>First, we need to import external libraries:</p> <ul> <li><p>numpy - Arrays to hold our data</p> </li> <li><p>matplotlib.pyplot - Draw images</p> </li> <li><p>pysptools.classification - Define and use a SID classifier</p> </li> <li><p>sklearn.metrics - Compute accuracy metrics using scikit-learn</p> </li> <li><p>sklearn.preprocessing - Normalizing input data using scikit-learn</p> </li> <li><p>time.perf_counter - Track how long individual functions take to run</p> </li> <li><p>os.path - Path manipulation</p> </li> <li><p>tqdm - show progress bars during training</p> </li> <li><p>joblib - Saving and loading trained classifiers</p> </li> <li><p>etrainee_m4_utils.image_preprocessing - Our library holding functions for image tiling, preprocessing, etc.</p> </li> <li><p>etrainee_m4_utils.inference_utils - Our library for correctly exporting classifed images</p> </li> <li><p>etrainee_m4_utils.visualisation_utils - Our library for visualising the data</p> </li> </ul> <p>Two external libraries are not imported directly in this notebook, but are used by functions in image_preprocessing and inference_utils:</p> <ul> <li>gdal - Manipulates spatial data</li> <li>scipy.io - Reads .mat files</li> </ul>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#1-load-and-preprocess-training-data","title":"1. Load and preprocess training data\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#11-data-loading-into-numpy","title":"1.1. Data loading into NumPy\u00b6","text":"<p>Let's start by reading an image into a numpy array, we do this in the background using GDAL.</p> <p>The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.</p> <p>For loading most raster datasets, we created a read_gdal() function in the image_preprocessing module. But loading .mat files for the Pavia City Centre requires a specific function (read_pavia_centre()). Both read_pavia_centre() and read_gdal() return a dictionary containing two numpy arrays with keys imagery and reference.</p> <p>If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile for neural networks, we crop the image to (1088, 1088, 102) here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#12-flatten-array","title":"1.2. Flatten array\u00b6","text":"<p>We will be classifying individual pixels, therefore we can transform the 3D image (height, width, spectral bands) to a 2D array (length, spectral bands). This transformation destroys spatial relationships within the image, however the classifiers can only use 1D features anyway and it simplifies the next step (filtering NoData).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#13-filter-out-nodata-pixels","title":"1.3. Filter out NoData pixels\u00b6","text":"<p>We can only train the classifier on pixels with a reference value, therefore we remove all pixels belonging to class 0 (NoData). This operation reduces our training dataset from ~1.18 milion to ~130 thousand pixels. We then visualise the spectral curves of individual pixels.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#14-splitting-data-for-trainingtesting","title":"1.4. Splitting data for training/testing\u00b6","text":"<p>Our reference dataset has to be split into three groups:</p> <p>training / validation / test</p> <p>In this step we divide the data into train+val and test groups. The variable train_fraction is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function train_test_split, which performs a stratified sampling from all classes.</p> <p>Please experiment with different fractions of training data.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#2-extract-mean-spectral-curves-from-training-data","title":"2. Extract mean spectral curves from training data\u00b6","text":"<p>The training data can then be used to extract mean spectral curves for each class, thus creating a spectral library. the spectral library is then used by the classifier.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#3-model-application-evaluation","title":"3. Model application &amp; evaluation\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#31-loading-and-preprocessing-the-data","title":"3.1. Loading and preprocessing the data\u00b6","text":"<p>Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.</p> <p>By default, the training raster (imagery_path) is used.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#32-defining-and-applying-the-classifier","title":"3.2. Defining and applying the classifier\u00b6","text":"<p>The following snippet applies the classifier to the loaded imagery, and then transforms the flattened array back into a raster.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#33-saveload-trained-sid-for-potential-future-use","title":"3.3. Save/load trained SID for potential future use\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#34-export-resulting-raster","title":"3.4. Export resulting raster\u00b6","text":"<p>Export the resulting classified raster into out_path for distribution or further analysis (e.g. validation in GIS).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#4-evaluate-classification-result","title":"4. Evaluate Classification Result\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#41-apply-classifier-to-the-test-dastaset","title":"4.1. Apply classifier to the test dastaset\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#42-compute-accuracy-metrics","title":"4.2. Compute accuracy metrics\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#43-show-confusion-matrix","title":"4.3. Show Confusion Matrix\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#5-sample-solution","title":"5. Sample Solution\u00b6","text":"<p>Our classification has generated this result (please note that you may get a different result):</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_sid.html#optional-classify-a-urban-scene","title":"Optional: Classify a urban scene\u00b6","text":"<p>Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.</p> <p>Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University. You will need to change the paths to input data and use the read_pavia_centre method to load the matlab matrices into numpy.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html","title":"Subpixel classification","text":"<p>The aim of this exercise is to explore the possibilities of subpixel classification, namely the different methods of endmember collection, regression-based spectral unmixing, and evaluation of the resulting species maps.</p> <ul> <li> <p>Prerequisites</p> <ul> <li>Installed EnMAP-Box plugin for QGIS (manual) By now you should have EnMAP-Box installed on your computer as a QGIS plugin and be familiar with the basic software operations. If not, please refer back to Exploration of hyperspectral data using EnMAP-Box.</li> <li>Downloaded data (module4/theme4_exercise_subpixel_classification)   The dataset consists of:<ul> <li>preprocessed (geometrically and radiometrically corrected) hyperspectral images: BL_hyperspectral_image.dat (data file) and BL_hyperspectral_image.hdr (header file). More information about the acquisition of the data can be found here.</li> <li>training polygons for all classes (species): BL_training_polygons.shp (shapefile)</li> <li>pixel purity index image: BL_ppi_image.dat (data file) and BL_ppi_image.hdr (header file):</li> <li>spectral library with semi-automatically pre-extracted endmembers: BL_spectral_library.gpkg (GeoPackage).</li> </ul> </li> </ul> </li> <li> <p>Tasks</p> <ul> <li>Manually extracting endmembers from hyperspectral images </li> <li>Comparing the acquired spectra with the provided spectral library </li> <li>Spectral unmixing \u2013 subpixel classification</li> <li>Results evaluation</li> </ul> </li> </ul>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html#0-data-preparation-and-exploration","title":"0. Data preparation and exploration","text":"<p>The area of interest is B\u00edl\u00e1 louka meadow, a patchwork of alpine grasslands located in the Krkono\u0161e mountains (50.729N, 15.683E). Matgrass (Nardus stricta), reedgrass (Calamagrostis villosa), and purple moor-grass (Molinia caerulea) form monodominant stands that are occasionally mixed with other grasses and herbs. Several isolated Norway spruce (Picea abies) trees can be found. Despite the grass stands low species diversity, they play an important role from an environmental point of view. Thus, the development of methods for reliable monitoring of vegetation change is crucial. Our exercise goal will be to experiment with subpixel species classification.</p> Species Abbreviation Calamagrostis villosa cv Molinia caerulea mol Nardus stricta nard B\u00edl\u00e1 louka species and their abbreviations <p>Start with visualizing the hyperspectral image <code>BL_hyperspectral_image.dat</code> in EnMAP-Box, mapping Band 24 to Red, Band 14 to Green, and Band 8 to Blue, and stretching the values to Min and Max using a Cumulative count cut should give you an image resembling true colors. It is also possible to use a different band combination for the RGB composite.</p> <p> </p> Band Rendering settings to achieve true color visualization <p>Overlay the image with the training polygons <code>BL_training_polygons.shp</code>. Using \"Layer Properties-&gt; Symbology\" and the Value stored in the field CLASS classify the polygons, so each species is visualized with a different color. Feel free to play around with the data. </p> <pre><code>Determine the spatial and spectral resolution of the image from metadata. What is the wavelength range?\n</code></pre>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html#1-endmember-extraction","title":"1. Endmember extraction","text":"<p>\u201cA hyperspectral endmember (also known as \u2018pure pixel\u2019) is an idealized, pure signature of a spectral class. The pure spectral signature signifies the complete reflectance of a pixel exclusively occupied by a single surface material.\u201d (Karbhari et al., 2015)  </p> <p>Endmembers can be acquired from an existing spectral library, by measuring spectra in the field or lab, or extracted from hyperspectral/multispectral images. The first task is to manually extract endmembers from the hyperspectral image. </p> <p>Add the Pixel Purity Index (PPI) image <code>BL_ppi.dat</code> as an available data source and visualize it. Using the knowledge from Exploration of hyperspectral data using EnMAP-Box create a labeled spectral library with at least one endmember for each species.</p> <p>The training polygons should give you an idea where the species are located, and the PPI image serves as a measure of pixel purity (the higher the value the purer). You can link multiple Map windows in \u201cView -&gt; Set Map Linking -&gt; Link Map Scale and Center\u201d to easily identify and select pure pixels located inside the training polygons. </p> <p> </p> Imported spectral library view BL_spectral_library.gpkg <pre><code>Compare the extracted endmember spectra with the semi-automatically pre-extracted endmembers located in the spectral library BL_spectral_library.gpgk  \nDo they differ significantly and if yes, then why?\n</code></pre> <p>To obtain the provided PPI image, first the Minimum Noise Fraction (MNF) transformation was applied on the hyperspectral image. Based on the eigenvalues, only the first 11 MNF bands were considered. To reduce spatial dimensions, the PPI was then calculated on the MNF transformed data using a threshold factor of 2.5 and 20 000 iterations.</p> <p> </p> First five bands after applying MNF transformation"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html#2-spectral-unmixing","title":"2. Spectral unmixing","text":"<p>Open the Regression-based unmixing application in EnMAP-Box. As the \"Endmember dataset\" choose either the provided spectral library <code>BL_spectral_library.gpkg</code> or the one you created during the previous steps. The library can be added using the \u201cCreate Classification dataset (from categorized spectral library)\u201d option. The other input is the \u201cRaster layer\u201d on which the spectral unmixing is going to be performed, in our case that is the hyperspectral image. </p> <p>Next, the Regressor algorithm needs to be defined. Both linear and nonlinear options are available. EnMAP-Box uses the Python scikit-learn library. More information on the different machine learning (ML) algorithms can be found in the documentation. Use an ML algorithm of your choice, but keep in mind that some algorithms are more costly than others, thus, to reduce running time, one of the following is advisable: LinearRegression, RandomForestRegressor, SupportVectorRegressor.  </p> <p>The process of creating the synthetically mixed spectra as training data from the endmember library is affected by the mixing parameters. \u201cNumber of mixtures per class\u201d defines the total number of mixtures per class to be created. \u201cMixing complexity probability\u201d consists of a list of probabilities for using 2, 3, 4, ... endmembers in the mixing models. Other options decide whether to include the original library spectra into the dataset or allow mixtures with profiles belonging to the same class. For the first run, you can leave all of these values at their defaults.</p> <p> </p> Regression based-unmixing workflow (Figure by Janz et al. 2023/ GPL-3.0) <p>Finally, the outputs need to be specified. Fill in the file destination for the \u201cOutput class fraction layer\u201d and the \u201cOutput classification layer\u201d, which is derived from the class fraction layer and corresponds to the maximum class fraction.  </p> <p>Now you are ready to run the application.  </p> <pre><code>Pick one or more of the mixing parameters and determine their changing effect on the results in terms of visual inspection of the output images, accuracy etc.\n</code></pre> <p> </p> Regression based-unmixing application settings <p> </p> Create Classification dataset (from categorized spectral library) settings"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html#3-results-evaluation","title":"3. Results evaluation","text":"<p>Visualize the classification layer and the class fraction layer. You can create an RGB image from the class fraction layer using Symbology or the EnMAP-Box Processing Toolbox, respectively the \u201cCreate RGB image from class probability/fraction layer\u201d. The RGB pixel color of a single pixel will then be given by the weighted mean of the given class colors. The weights are given by class fractions/probabilities (i.e. values between 0 and 1). For example, pure pixels with cover fractions of 1 appear in its pure class color. A mixed pixel with 50% fractions in two classes colored in red and green, appears in a dull yellow.</p> <p> </p> Create RGB image from class probability/fraction layer settings <p>A different way to assess the fraction layer is to use single band visualization. In \"Layer Properties -&gt; Symbology -&gt; Band Rendering\" choose Singleband gray as \u201cRender type\u201d. Each band corresponds to a class/species. The fraction of a given class/species in each pixel will then be defined by a grayscale value. </p> <p> </p> Single band visualization settings <p>Accuracy assessment of classification results should generally include computation of various accuracy metrics, such as overall accuracy, confusion matrix, user and producer accuracy, F1 score. However, visual comparison will suffice for the purpose of this exercise.</p> <p>Examine the original hyperspectral image, all the unmixing outputs, the reference map below obtained using pixel classification, and try to answer the following questions: </p> <pre><code>What does the class fraction layer represent?  \nDescribe the resulting RGB image, are there mixed/pure pixels?  \n\nThe northwest edges of compact mol areas seem to be mixed with cv, but not the other edges.  \nCan you determine what is causing this effect?  \n(hint: Carefully looking at the original hyperspectral image might help.)\n\nThe reference map contains the class desch which we did not classify.  \nHow are these areas manifested in the class fraction layer and the classification layer? \n\nCompare the classification layer and reference map.  \nDo both maps show a good agreement in terms of spatial patterns? Discuss all the possible effects on the outcome.\n\nWhat kind of data might be more suitable for subpixel classification and why? \n</code></pre> <p> </p> Reference map obtained using pixel classification"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html#references","title":"References","text":"<p>Karbhari, K., Solankar, M., Nalawade, D. (2020): Hyperspectral Endmember Extraction Techniques. 10.5772/intechopen.88910.  </p> <p>Janz, A., Jakimow, B., Thiel, F., Linden, S., Hostert, P. (2022): EnMAP-Box 3 Documentation. enmap-box.readthedocs.io.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html#exercise-solution","title":"Exercise solution","text":"<p>Proceed to example solution Subpixel classification - report</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_subpixel.html#back-to-theme","title":"Back to theme","text":"<p>Proceed by returning to Machine learning in imaging spectroscopy</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html","title":"04 time series specifics exercise svm","text":"<ul> <li><p>Prerequisities (This notebook can be run either online using Google Colab or on your local machine)</p> <ul> <li>A Google account for accessing Google Colab (link to notebook). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.</li> </ul> <p>or</p> <ul> <li><p>A Python environment with the necessary libraries (manual). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.</p> </li> <li><p>Downloaded data (module4.zip/theme4_exercise_machine_learning) The dataset consists of:</p> <ul> <li>Hyperspectral RPAS imagery of B\u00edl\u00e1 Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: BL_202008_imagery.tif</li> <li>a raster with reference data: BL_202008_reference.tif</li> <li>Pretrained models and corresponding classified rasters: /sample_results/*</li> </ul> </li> </ul> </li> <li><p>Tasks</p> <ul> <li>Preprocess imagery for Machine Learning</li> <li>Classify the hyperspectral image using SVM</li> <li>Observe how hyperparameter values alter classification results</li> <li>Evaluate your results and compare to our pretrained classifier</li> <li>Optional: Classify a urban scene</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom time import perf_counter\nfrom os.path import join\nimport scipy\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\n\nfrom tqdm import notebook as tqdm\nfrom joblib import dump, load\n\nfrom etrainee_m4_utils import image_preprocessing\nfrom etrainee_m4_utils import inference_utils\nfrom etrainee_m4_utils import visualisation_utils\n\n# GLOBAL SETTINGS\nplt.rcParams['figure.figsize'] = [5, 5]\nnp.set_printoptions(precision=2, suppress=True)  # Array print precision\n</pre> import numpy as np import matplotlib.pyplot as plt  from time import perf_counter from os.path import join import scipy  from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.model_selection import KFold, StratifiedKFold, train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report  from tqdm import notebook as tqdm from joblib import dump, load  from etrainee_m4_utils import image_preprocessing from etrainee_m4_utils import inference_utils from etrainee_m4_utils import visualisation_utils  # GLOBAL SETTINGS plt.rcParams['figure.figsize'] = [5, 5] np.set_printoptions(precision=2, suppress=True)  # Array print precision In\u00a0[\u00a0]: Copied! <pre># Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n# default: 'pavia_centre'\nds_name = 'bila_louka'\n\n# Get a list of class names\n_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)\n</pre> # Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre' # default: 'pavia_centre' ds_name = 'bila_louka'  # Get a list of class names _, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name) <p>Please fill correct paths to your training and reference rasters (just pointing the root_path variable to the project folder should do):</p> In\u00a0[\u00a0]: Copied! <pre>root_path = 'C:/folder/where/this/project/is/saved'\n\n# PATHS TO TRAINING DATA\n# Bila Louka\nimagery_path   = join(root_path, 'BL_202008_imagery.tif')\nreference_path = join(root_path, 'BL_202008_reference.tif')\n# Pavia\n#imagery_path   = join(root_path, 'Pavia.mat')\n#reference_path = join(root_path, 'Pavia_gt.mat')\n\n# PATH TO SAVE MODELS\nmodel_save_folder = join(root_path, 'models')\n\n# PATH TO SAVE CLASSIFIED IMAGE\nout_path  = join(root_path, 'results/Krkonose_SVM.tif')\n\n# PATH TO THE SAMPLE RESULTS\nsample_result_path  = join(root_path, 'sample_results/SVM_sample_result.tif')\n</pre> root_path = 'C:/folder/where/this/project/is/saved'  # PATHS TO TRAINING DATA # Bila Louka imagery_path   = join(root_path, 'BL_202008_imagery.tif') reference_path = join(root_path, 'BL_202008_reference.tif') # Pavia #imagery_path   = join(root_path, 'Pavia.mat') #reference_path = join(root_path, 'Pavia_gt.mat')  # PATH TO SAVE MODELS model_save_folder = join(root_path, 'models')  # PATH TO SAVE CLASSIFIED IMAGE out_path  = join(root_path, 'results/Krkonose_SVM.tif')  # PATH TO THE SAMPLE RESULTS sample_result_path  = join(root_path, 'sample_results/SVM_sample_result.tif') In\u00a0[\u00a0]: Copied! <pre>loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n#loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n    # reference_path, out_shape=(1088, 1088, 102))\n\nprint(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\nprint(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')\n</pre> loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path) #loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,     # reference_path, out_shape=(1088, 1088, 102))  print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}') print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}') In\u00a0[5]: Copied! <pre>visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],\n                                 loaded_raster[\"reference\"], ds_name=ds_name)\n</pre> visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],                                  loaded_raster[\"reference\"], ds_name=ds_name) <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> In\u00a0[\u00a0]: Copied! <pre>orig_shape = loaded_raster['imagery'].shape\n\nflat_arrs = {}\nflat_arrs['imagery'] = loaded_raster['imagery'].reshape(\n    orig_shape[0]*orig_shape[1], orig_shape[2])\nflat_arrs['reference'] = loaded_raster['reference'].reshape(\n    orig_shape[0]*orig_shape[1])\n\nprint(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}')\n</pre> orig_shape = loaded_raster['imagery'].shape  flat_arrs = {} flat_arrs['imagery'] = loaded_raster['imagery'].reshape(     orig_shape[0]*orig_shape[1], orig_shape[2]) flat_arrs['reference'] = loaded_raster['reference'].reshape(     orig_shape[0]*orig_shape[1])  print(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>filtered_arrs = {}\nfiltered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0]\nfiltered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]\n\nprint(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}')\n</pre> filtered_arrs = {} filtered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] &gt; 0] filtered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] &gt; 0]  print(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}') In\u00a0[\u00a0]: Copied! <pre>pixel_number = 1000\nvisualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,\n                                        ds_name=ds_name)\n</pre> pixel_number = 1000 visualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>scaler = StandardScaler()\nscaler.fit(flat_arrs['imagery'])\n\nscaled_arrs = {\n    'imagery': scaler.transform(filtered_arrs['imagery']),\n    'reference': filtered_arrs['reference']}\n</pre> scaler = StandardScaler() scaler.fit(flat_arrs['imagery'])  scaled_arrs = {     'imagery': scaler.transform(filtered_arrs['imagery']),     'reference': filtered_arrs['reference']} In\u00a0[\u00a0]: Copied! <pre>pixel_number = 5\nvisualisation_utils.show_spectral_curve(scaled_arrs, pixel_number,\n                                        ds_name=ds_name)\n</pre> pixel_number = 5 visualisation_utils.show_spectral_curve(scaled_arrs, pixel_number,                                         ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>train_fraction = 1/3\n\nX_train, X_test, y_train, y_test = train_test_split(scaled_arrs['imagery'], scaled_arrs['reference'], train_size=train_fraction)\n\n\ntrain = {'imagery': X_train, 'reference': y_train}\ntest  = {'imagery': X_test,  'reference': y_test}\n</pre> train_fraction = 1/3  X_train, X_test, y_train, y_test = train_test_split(scaled_arrs['imagery'], scaled_arrs['reference'], train_size=train_fraction)   train = {'imagery': X_train, 'reference': y_train} test  = {'imagery': X_test,  'reference': y_test} In\u00a0[\u00a0]: Copied! <pre># How many training samples do we have for each class?\nunique, counts = np.unique(train['reference'], return_counts=True)\nprint(f'The individual classes contain {counts} training pixels.')\n</pre> # How many training samples do we have for each class? unique, counts = np.unique(train['reference'], return_counts=True) print(f'The individual classes contain {counts} training pixels.') In\u00a0[\u00a0]: Copied! <pre># define potential hyperparameter values for the SVM (values to try)\nparameters_svm = {\n    'kernel': ['poly', 'rbf'],               # Define the kernel function\n    'degree': [2, 3],                        # Degree of polynomial used for the 'poly' kernel\n    'C': [0.1, 1, 10, 100, 1000, 10000],     # Define the penalty value\n    'gamma': [.00001, .0001, .001, .01, .1], # Kernel parameter\n}\n</pre> # define potential hyperparameter values for the SVM (values to try) parameters_svm = {     'kernel': ['poly', 'rbf'],               # Define the kernel function     'degree': [2, 3],                        # Degree of polynomial used for the 'poly' kernel     'C': [0.1, 1, 10, 100, 1000, 10000],     # Define the penalty value     'gamma': [.00001, .0001, .001, .01, .1], # Kernel parameter } In\u00a0[\u00a0]: Copied! <pre># Create the optimizer and run the optimization\nopt = RandomizedSearchCV(SVC(), parameters_svm, cv=5, scoring=\"jaccard_micro\", n_iter=8, refit=False, n_jobs=-2, verbose=4)\nopt.fit(X=train['imagery'], y=train['reference'])\nprint(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}')\n</pre> # Create the optimizer and run the optimization opt = RandomizedSearchCV(SVC(), parameters_svm, cv=5, scoring=\"jaccard_micro\", n_iter=8, refit=False, n_jobs=-2, verbose=4) opt.fit(X=train['imagery'], y=train['reference']) print(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}') In\u00a0[\u00a0]: Copied! <pre># Create the optimizer and run the GridSerach optimization\nopt = GridSearchCV(SVC(), parameters_svm, cv=5, scoring=\"jaccard_micro\", refit=False, n_jobs=-2, verbose=4)\nopt.fit(X=train['imagery'], y=train['reference'])\nprint(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}')\n</pre> # Create the optimizer and run the GridSerach optimization opt = GridSearchCV(SVC(), parameters_svm, cv=5, scoring=\"jaccard_micro\", refit=False, n_jobs=-2, verbose=4) opt.fit(X=train['imagery'], y=train['reference']) print(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}') In\u00a0[\u00a0]: Copied! <pre>svm = SVC(**opt.best_params_)\n\nsvm.fit(X=train['imagery'], y=train['reference'])\nsvm.score(train['imagery'], train['reference'])\n</pre> svm = SVC(**opt.best_params_)  svm.fit(X=train['imagery'], y=train['reference']) svm.score(train['imagery'], train['reference']) In\u00a0[\u00a0]: Copied! <pre># save using joblib.dump(object, filename)\nmodel_path = join(model_save_folder, 'SVM.joblib')\ndump(rf, model_path)\n</pre> # save using joblib.dump(object, filename) model_path = join(model_save_folder, 'SVM.joblib') dump(rf, model_path) In\u00a0[\u00a0]: Copied! <pre># load using joblib.load(filename)\nrf = load(model_path)\n</pre> # load using joblib.load(filename) rf = load(model_path) In\u00a0[\u00a0]: Copied! <pre># Load raster\nraster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))\n\n# Flattern spatial dimension of the raster\nraster_shape = raster['imagery'].shape\nraster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],\n                                        raster_shape[2])\n\n# Scale the input data\nscaler = StandardScaler()\nscaler.fit(raster_flat)\nraster_scaled = scaler.transform(raster_flat)\n</pre> # Load raster raster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))  # Flattern spatial dimension of the raster raster_shape = raster['imagery'].shape raster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],                                         raster_shape[2])  # Scale the input data scaler = StandardScaler() scaler.fit(raster_flat) raster_scaled = scaler.transform(raster_flat) In\u00a0[\u00a0]: Copied! <pre>predicted_flat = svm.predict(raster_scaled)\n</pre> predicted_flat = svm.predict(raster_scaled) In\u00a0[\u00a0]: Copied! <pre>predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1])\n</pre> predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1]) <p>You can also visualise the result:</p> In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    predicted_raster, ds_name=ds_name)\n</pre> visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     predicted_raster, ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre>inference_utils.export_result(out_path, predicted_raster, raster['geoinfo'])\n</pre> inference_utils.export_result(out_path, predicted_raster, raster['geoinfo']) In\u00a0[\u00a0]: Copied! <pre>test_predicted = svm.predict(test['imagery'])\n</pre> test_predicted = svm.predict(test['imagery']) In\u00a0[\u00a0]: Copied! <pre>print(classification_report(test['reference'], test_predicted,\n      target_names=class_names[1:]))\n</pre> print(classification_report(test['reference'], test_predicted,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre>visualisation_utils.show_confusion_matrix(test['reference'], test_predicted,\n                                          ds_name=ds_name)\n</pre> visualisation_utils.show_confusion_matrix(test['reference'], test_predicted,                                           ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Read test reference\ntest_arr = image_preprocessing.read_gdal(imagery_path, test_path)\ntest_flat = test_arr['reference'].reshape(\n    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\ntest_filtered = test_flat[test_flat &gt; 0]\n\n# Read sample result\nsample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\nsample_flat = sample_arr['reference'].reshape(\n    sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\nsample_filtered = sample_flat[test_flat &gt; 0]\n</pre> # Read test reference test_arr = image_preprocessing.read_gdal(imagery_path, test_path) test_flat = test_arr['reference'].reshape(     test_arr['reference'].shape[0]*test_arr['reference'].shape[1]) test_filtered = test_flat[test_flat &gt; 0]  # Read sample result sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path) sample_flat = sample_arr['reference'].reshape(     sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1]) sample_filtered = sample_flat[test_flat &gt; 0] In\u00a0[\u00a0]: Copied! <pre># Visualise the sample result\nvisualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n                                    loaded_raster['reference'],\n                                    sample_arr['reference'],\n                                    ds_name=ds_name)\n</pre> # Visualise the sample result visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],                                     loaded_raster['reference'],                                     sample_arr['reference'],                                     ds_name=ds_name) In\u00a0[\u00a0]: Copied! <pre># Print a classification report for the sample result\nprint(classification_report(test_filtered, sample_filtered,\n      target_names=class_names[1:]))\n</pre> # Print a classification report for the sample result print(classification_report(test_filtered, sample_filtered,       target_names=class_names[1:])) In\u00a0[\u00a0]: Copied! <pre># Show a Confusion matrix for the sample result\nvisualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n                                          ds_name=ds_name)\n</pre> # Show a Confusion matrix for the sample result visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,                                           ds_name=ds_name) <p>Return to exercises</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#support-vector-machine-svm-hyperspectral-data-classification","title":"Support Vector Machine (SVM) hyperspectral data classification\u00b6","text":"<p>In this notebook, you will train and apply a common Machine Learning classifier - Support Vector Machine for classification of hyperspectral data from B\u00edl\u00e1 Louka, Krkono\u0161e mountains, Czechia. Please start by reading about the dataset and area of interest here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#structure-of-this-exercise","title":"Structure of this exercise\u00b6","text":"<p>What are you going to encounter during this exercise.</p> <ol> <li>Load libraries, set paths</li> <li>Load and Preprocess training data</li> <li>Create/Fit Support Vector Machine</li> <li>Apply Classifier</li> <li>Evaluate Result</li> <li>Sample Solution</li> </ol>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#0-load-external-libraries-and-set-paths","title":"0. Load external libraries and set paths\u00b6","text":"<p>First, we need to import external libraries:</p> <ul> <li><p>numpy - Arrays to hold our data</p> </li> <li><p>matplotlib.pyplot - Draw images</p> </li> <li><p>sklearn.svm - Support Vector Machine classifier</p> </li> <li><p>sklearn.model_selection - Cross-validation and hyperparameter tuning implemented in scikit-learn</p> </li> <li><p>sklearn.metrics - Compute accuracy metrics using scikit-learn</p> </li> <li><p>sklearn.preprocessing - Normalizing input data using scikit-learn</p> </li> <li><p>time.perf_counter - Track how long individual functions take to run</p> </li> <li><p>os.path - Path manipulation</p> </li> <li><p>tqdm - show progress bars during training</p> </li> <li><p>joblib - Saving and loading trained classifiers</p> </li> <li><p>etrainee_m4_utils.image_preprocessing - Our library holding functions for image tiling, preprocessing, etc.</p> </li> <li><p>etrainee_m4_utils.inference_utils - Our library for correctly exporting classifed images</p> </li> <li><p>etrainee_m4_utils.visualisation_utils - Our library for visualising the data</p> </li> </ul> <p>Two external libraries are not imported directly in this notebook, but are used by functions in image_preprocessing and inference_utils:</p> <ul> <li>gdal - Manipulates spatial data</li> <li>scipy.io - Reads .mat files</li> </ul>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#1-load-and-preprocess-training-data","title":"1. Load and preprocess training data\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#11-data-loading-into-numpy","title":"1.1. Data loading into NumPy\u00b6","text":"<p>Let's start by reading an image into a numpy array, we do this in the background using GDAL.</p> <p>The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys imagery and reference. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.</p> <p>For loading most raster datasets, we created a read_gdal() function in the image_preprocessing module. But loading .mat files for the Pavia City Centre requires a specific function (read_pavia_centre()). Both read_pavia_centre() and read_gdal() return a dictionary containing two numpy arrays with keys imagery and reference.</p> <p>If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile for neural networks, we crop the image to (1088, 1088, 102) here.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#12-flatten-array","title":"1.2. Flatten array\u00b6","text":"<p>We will be classifying individual pixels, therefore we can transform the 3D image (height, width, spectral bands) to a 2D array (length, spectral bands). This transformation destroys spatial relationships within the image, however the classifiers can only use 1D features anyway and it simplifies the next step (filtering NoData).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#13-filter-out-nodata-pixels","title":"1.3. Filter out NoData pixels\u00b6","text":"<p>We can only train the classifier on pixels with a reference value, therefore we remove all pixels belonging to class 0 (NoData). This operation reduces our training dataset from ~1.18 milion to ~130 thousand pixels. We then visualise the spectral curves of individual pixels.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#14-data-scaling","title":"1.4. Data scaling\u00b6","text":"<p>After filtering the training data, we can move onto data scaling. In Machine Learning, it is common to scale all features before classification, because many classifiers assume that all features vary on comparable scales  and that each feature has values close to zero.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#15-splitting-data-for-trainingtesting","title":"1.5. Splitting data for training/testing\u00b6","text":"<p>Our reference dataset has to be split into three groups:</p> <p>training / validation / test</p> <p>In this step we divide the data into train+val and test groups. The variable train_fraction is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function train_test_split, which performs a stratified sampling from all classes.</p> <p>Please experiment with different fractions of training data.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#2-support-vector-machine-definition-and-training","title":"2. Support Vector Machine definition and training\u00b6","text":"<p>After preprocessing our data, we can move onto defining our machine learning models. You can either train your own classifiers or use ones we already trained for you (sample_results/SVM_sample_trained.joblib). In case you are using the pretrained SVM, skip ahead to section 2A.3.</p> <p>This training uses a support vector machine implementation from scikit-learn, a popular Machine Learning library for Python. The documentation is available at https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#21-find-most-suitable-parameters","title":"2.1. Find most suitable parameters\u00b6","text":"<p>To function proprely, SVM has to have suitable values for some hyperparameters. A common approach is to train classifiers with different hyperparameter values and select the most suitable ones.</p> <p>Scikit-Learn makes this easy using RandomizedSearch or GridSearch, these functions train the classifier multiple times using different hyperparameter values and determine the most suitable combination. Each combination of hyperparameter values is tried multiple times using cross-validation (out-of-sample testing).</p> <p>Run either the cell with RandomizedSearchCV or with GridSearchCV, while Grid Search may be able to find better hyperparameters, Randomized Search will likely also find good solutions in a much shorter amount of time.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#22-fit","title":"2.2. Fit\u00b6","text":"<p>The best hyperparameter values identified during cross-validation are then used for training the model on the whole training dataset.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#23-saveload-trained-rf-for-potential-future-use","title":"2.3. Save/load trained RF for potential future use\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#3-model-application-evaluation","title":"3. Model application &amp; evaluation\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#31-loading-and-preprocessing-the-data","title":"3.1. Loading and preprocessing the data\u00b6","text":"<p>Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.</p> <p>By default, the training raster (imagery_path) is used.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#32-applying-the-classifier","title":"3.2. Applying the classifier\u00b6","text":"<p>The following snippet applies the classifier to the loaded imagery, and then transforms the flattened array back into a raster.</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#33-export-resulting-raster","title":"3.3. Export resulting raster\u00b6","text":"<p>Export the resulting classified raster into out_path for distribution or further analysis (e.g. validation in GIS).</p>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#4-evaluate-classification-result","title":"4. Evaluate Classification Result\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#41-apply-classifier-to-the-test-dastaset","title":"4.1. Apply classifier to the test dastaset\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#42-compute-accuracy-metrics","title":"4.2. Compute accuracy metrics\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#43-show-confusion-matrix","title":"4.3. Show Confusion Matrix\u00b6","text":""},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#5-sample-solution","title":"5. Sample Solution\u00b6","text":"<p>We have generated this result using these training parameters (please note that just using the same training parameters will not yield the same result):</p> <ul> <li>Kernel: Polynomial</li> <li>Polynom degree: 2</li> <li>C: 1000</li> <li>Gamma: 0.01</li> </ul>"},{"location":"module4/04_time_series_specifics/04_time_series_specifics_exercise_svm.html#optional-classify-a-urban-scene","title":"Optional: Classify a urban scene\u00b6","text":"<p>Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.</p> <p>Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University. You will need to change the paths to input data and use the read_pavia_centre method to load the matlab matrices into numpy.</p>"},{"location":"module4/04_time_series_specifics/solution/04_time_series_specifics_exercise_subpixel_solution.html","title":"E-TRAINEE: Subpixel classification - report","text":""},{"location":"module4/04_time_series_specifics/solution/04_time_series_specifics_exercise_subpixel_solution.html#report","title":"Report","text":"<p>The hyperspectral image was loaded in the EnMAP-Box environment and visualized in true colors using the recommended bands. From the image metadata the following information was extracted: </p> <p>number of bands: 54 wavelength range: 397.663 to 1001.839 nanometers spatial resolution: 0.09 m</p> <p>Overlaying the image with the training polygons gives a clear overview of the location of the dominant grass stands. The mol species corresponds to the green polygons, the cv species to red, and the nard species is in violet. </p> <p> </p> Hyperspectral image in true colors overlaid with training polygons"},{"location":"module4/04_time_series_specifics/solution/04_time_series_specifics_exercise_subpixel_solution.html#endmember-extraction","title":"Endmember extraction","text":"<p>One endmember spectra for each species was extracted using the pixel purity index image. Pixels with the highest purity lying inside the training polygons were chosen. When compared to the provided semi-automatically pre-extracted endmembers, the reflectance values are generally lower in the longer wavelengths, however, the shape/trend of the curve remains the same. </p> <p> </p> Created spectral library <p> </p> Comparison of the extracted endmember spectra (yellow) with the semi-automatically pre-extracted endmembers (white)"},{"location":"module4/04_time_series_specifics/solution/04_time_series_specifics_exercise_subpixel_solution.html#spectral-unmixing","title":"Spectral unmixing","text":"<p>Spectral unmixing was carried out  on the hyperspectral image using the provided spectral library. The \u201cRegressor\u201d algorithm RandomForestRegressor was chosen to observe the effect of the \u201cNumber of mixtures per class\u201d parameter. Values 100, 500, and 1000 were tested sequentially. All the other parameters were left at their defaults. Changing the value had a significant effect on the class fraction layer. A low mixture per class value favored the cv species, while a higher value favored the mol species. Increasing the amount also resulted in a less noisy image and more compact areas.</p> <p> </p> Effect of changing parameter Number of mixtures per class (Left: 100; Center: 500; Right: 1000) <p>Setting the \u201cRegressor\u201d algorithm to LinearSVR and leaving all the other parameters at their default state gave the following results:</p> <p> </p> Class fraction layer and classification layer (Red: cv; Green: mol; Blue: nard) <p> </p> Single band visualization"},{"location":"module4/04_time_series_specifics/solution/04_time_series_specifics_exercise_subpixel_solution.html#qa","title":"Q&amp;A","text":"<ul> <li>What does the class fraction layer represent? Describe the resulting RGB image, are there mixed/pure pixels?  <ul> <li>the class fraction layer represents the fraction/probability of the classes in  pixels; each image band corresponds to one species and holds the value of the fraction one can distinguish compact areas with relatively pure pixels of cv (bright red), compact areas of mol mixed with cv (bright red and green), and mixed nard forming the \u201cbackground\u201d</li> </ul> </li> <li>The northwest edges of compact mol areas seem to be mixed with cv, but not the other edges. Can you determine what is causing this effect?<ul> <li>shadows in the image</li> </ul> </li> <li>The reference map contains the class desch which we did not classify. How are these areas manifested in the class fraction layer and the classification layer? <ul> <li>class fraction layer: mixed species areas where none of the species is dominant </li> <li>classification layer: nard species</li> </ul> </li> <li>Compare the classification layer and reference map. Do both maps show a good agreement in terms of spatial patterns? Discuss all the possible effects on the outcome.<ul> <li>spatial patterns yes - otherwise not so much</li> <li>effects: data acquisition, geometric and radiometric corrections, endmember extraction, chosen mixing parameters and regression algorithm </li> </ul> </li> <li>What kind of data might be more suitable for subpixel classification and why? <ul> <li>data with lower spatial resolution</li> <li>more separable classes (spectrally distinct)</li> </ul> </li> </ul>"},{"location":"module4/04_time_series_specifics/solution/04_time_series_specifics_exercise_subpixel_solution.html#back-to-theme","title":"Back to theme","text":"<p>Proceed by returning to Machine learning in imaging spectroscopy</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html","title":"Spatial vs. spectral resolution and temporal resolution","text":"<p>The main information about the spectral, spatial and temporal resolutions of remote sensing data were introduced in Module 2. In this lesson we will discuss consequences of the spatial, spectral and temporal resolutions, mainly on the examples from the Krkono\u0161e Mts. Tundra vegetation (particularly grasslands) showing classification results of the same vegetation species from imagery with different spatial, spectral and temporal resolution.</p> <p>In the mapping context, different scales play a role: the scale of time, and the scale of spectral and spatial data resolution have to be considered. Considering vegetation studies, an important aspect is the vegetation unit as the study object. It is selected depending on what the final map will be used for: do we want to focus on individual species in a specific place, or are we interested in a general but more spatial overview of vegetations communities? Different physiognomies of species in different growing stages can be captured by remote-sensing instruments, depending on the data resolution and date of acquisition. In this context, special attention should be paid to the spatial, spectral and temporal resolutions of acquired data; however, as is well known, they vary for different platforms \u2013 satellite, aerial or UAV. For monitoring practice, it is important to achieve a high accuracy of mapped unit classification. Hence, beside the aforementioned aspects, the methods of data processing and analysis play a crucial role. Refer to the methods of the data analysis to Modules 1 and 2.</p> <p>All above mentioned scales/aspects are interconnected and their evaluation should lead to a better understanding of how remote sensing can improve vegetation monitoring practice. The key aspects of vegetation monitoring using remote sensing that influence the final accuracy are shown in Figure 1. </p> <p> </p> <p>Figure 1: Key aspects of vegetation monitoring using remote sensing. Source: Kupkov\u00e1 et al. (2021)</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#classified-vegetation-unit","title":"Classified vegetation unit","text":"<p>The minimal mapping unit is usually set according to the goals of the vegetation monitoring and is dependent on the features of available remote-sensing data. Traditionally, vegetation units are delimited based on expert phytosociological knowledge. Since we are analyzing vegetation with the use of remote sensing data, we expect that different units may be \u2018visible\u2019 from the sensor. Based on common literature in which vegetation has been classified with remote-sensing data, we can distinguish single species or larger complexes that form communities or habitats, or, ultimately, we may be interested in a vegetation ecosystem as a whole (Figure 2). Selecting the appropriate unit that can be classified with specific data allows us to determine the optimal legend of the final map. </p> <p> </p> <p>Figure 2: Vegetation units that can be classified based on mainly spatial resolution of remote sensing data. Source:Kupkov\u00e1 et al. (2021).</p> <p>For example, classifications of grasslands in relict Arctic\u2013alpine tundra of the Krkono\u0161e Mts (Czechia) at the level of habitats and communities (closed alpine grasslands dominated by Nardus stricta, grasses except Nardus stricta and subalpine vaccinium vegetation) were performed by Such\u00e1 et al. (2016) and Kupkov\u00e1 et al. (2017) on satellite remote-sensing data (Landsat 8 and Sentinel-2) with the pixel size 10 \u2013 25 m/pixel using the legend on the level of communities (see Figure 3 \u2013 left part) with satisfactory classification accuracy. However, classification on the species level from this data provided very low reliability. The same studies brought rather good results on the level of selected individual dominant grass species classified from aerial hyperspectral (HS) and multispectral (MS) data with the pixel size around 4 m/pixel (the right part of Figure 3). For example, Nardus stricta stands were classified with 79% PA and 87% UA from aerial HS data. In the case of Deschampsia caespitosa, stands PA reached 88% and 89% UA using the same data. The difference in the legend and classification outputs are presented in Figure 3. As it can be seen, the final classification pattern is rather different \u2013 caused mainly by different pixel sizes.</p> <p> </p> <p>Figure 3: Classification outputs for different vegetation units from remote sensing data with different spatial resolution (left \u2013 Landsat 8, right \u2013 hyperspectral sensor APEX). Source: Such\u00e1 et al., 2016 and Kupkov\u00e1 et al. (2017).*</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#platform-height-of-data-acquisition","title":"Platform (height) of data acquisition","text":"<p>Scale of mapping and related spatial resolution are dependent on the platform used (height of the data acquisition) \u2013 see Table 1. Each platform has some advantages and weaknesses that are also mentioned.</p> <p> </p> <p>Table 1: Main features of remote-sensing platforms. Source: Marcinkowska-Ochytra and Kupkov\u00e1 (2021).</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#spatial-resolution","title":"Spatial resolution","text":"<p>Spatial resolution is the main parameter influencing classification detail (the number of legend categories that are distinguishable). In the imagery with different spatial resolution, different level of spatial detail can be distinguished \u2013 see Figure 4. </p> <p> </p> <p>Figure 4: The same area (B\u00edl\u00e1 louka meadow \u2013 the Krkono\u0161e Mts. captured in the images with different spatial resolutions. Source: Kupkov\u00e1 et al. (2021).</p> <p>To be able to determine the best scale of elaboration, studies that tested different spatial resolutions within the same area and used classification legends with different numbers of categories are important. When we compared data with different spatial resolutions classified for the same area in the Krkono\u0161e Mts, 8 categories (six grass species and two other vegetation categories) were distinguishable from UAV HS and MS data (see Figure 5 \u2013 results for UAV data) with the pixel size 3 cm/pixel. While the legend had to be generalized for satellite and aerial data with bigger pixels. It was not possible to distinguish some less abundant grass categories and individual trees/shrubs in their pixels.</p> <p> </p> <p>Figure 5: Data and classification results (research plot Zahr\u00e1dka; Kupkov\u00e1 et al. (2021)).</p> <p>The other aspect is classification accuracy in case of different number of classified categories. Figure 6 compares obtained classification accuracies in case of legends with 8, 5 and 3 categories. When pixel size reached centimeters (HS UAV data), and also meters (PlanetScope data), it was possible to classify dominant Calamagrostis villosa and Nardus stricta species with reasonably high accuracies. </p> <p> </p> <p>Figure 6: Best achieved results of PA (left) and UA (right) in % for selected grass categories.  AV \u2013 Avenella flexuosa; CV \u2013 Calamagrostis villosa; NS \u2013 Nardus stricta; PS 3C \u2013 PlanetScope data, legend with 3 categories; APEX 3C \u2013 APEX data, legend with 3 categories; OF 5C \u2013 UAV RGB orthophoto, legend with 5 categories; HS 5C \u2013 UAV HS data, legend with 5 categories; OF 8C \u2013 UAV RGB orthophoto, legend with 8 categories; HS 8C \u2013 UAV HS data, legend with 8 categories). Source: Kupkov\u00e1 et al. (2021).</p> <p>Moreover, the number of classified categories did not play a significant role in final accuracy (see the accuracy of Calamagrostis villosa and Nardus stricta for HS UAV data or ortophoto using legend with 5 and 8 categories). Meanwhile, for species with low coverage (Avenella fleuxosa), even data with extremely high spatial (and spectral) resolution need not provide sufficient accuracy (UA around 50% for HS UAV data with 9 cm pixel). And it seems that higher number of categories leads in this case to a lower classification accuracy (compare UAs and PAs of Avenella fleuxosa for orthophoto and HS UAV data using legend with 5 and 8 categories). Smaller pixel size does not always bring better accuracy, the species coverage/abundance and a number of classified categories also play a role. </p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#spectral-resolution","title":"Spectral resolution","text":"<p>The differentiation between particular species/habitat/communities in available spectral ranges allows them to be classified properly. Thus, we could anticipate in general that hyperspectral data with many narrow contiguous bands and a detailed measurement step (hence a continuous and detailed spectral curve) should yield better classification results than multispectral data with only a few measurements within the whole spectral range. However, according to our experience (see Kupkov\u00e1 et al. (2023)), if the multispectral sensor includes bands that are important for vegetation resolution (NIR, RedEdge), then excellent classification accuracy can be achieved using a small number of bands (multispectral data). </p> <p>In our study (Kupkov\u00e1 et. al, 2023).   we compared classification accuracy of multispectral data with 8 spectral bands (combination of 5 bands from MicaSense RedEdge M and 3 bands from RGB camera Sony A7 ILCE-7) and hyperspectral data with 54 spectral bands (data from hyperspectral camera Headwall NANO-Hyperspec camera \u2013 originally 269 bands, resampled to 54 bands and transformed using MNF transformation to 9 bands). The multispectral and hyperspectral data were obtained from UAV and had the same spatial resolution \u2013 3 cm/pixel. Surprisingly, the higher spectral resolution of hyperspectral data did not increase accuracies, and the results for the multispectral and hyperspectral data were comparable. As Figure 7 shows, considering best producer\u2019s and user\u2019s accuracies of the four dominant grass species (except of Avenella flexuosa and Carex bigelowi), there was only a slight difference in classification accuracy between the multispectral and hyperspectral data. Larger differences were found for species with low coverage - Avenella flexuosa and Carex bigelowi, classified with considerably lower accuracies of ~60% for AF and as low as 48.0% UA for CB. For the F1-score, the multispectral multi-temporal composites generally had better results, but the differences for the dominant species were not significant, compared with the hyperspectral multi-temporal composites.</p> <p> </p> <p>Figure 7: Best Producer\u2019s accuracy (PA), User\u2019s accuracy (UA) (a), and the F1-score (b) for UAV hyperspectral and MS multispectral for permanent research plot B\u00edl\u00e1 louka Meadow in the Krkono\u0161e Mts. Species abbreviations: NS - Nardus stricta, CV - Calamagrostis villosa, MC - Molinia caerulea, DC - Deschampsia caespitosa, AF - Avenella flexuosa, CB - Carex bigelowii. Source: Kupkov\u00e1 et al. (2023).</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#spectral-vs-spatial-resolution-which-is-more-important","title":"Spectral vs. spatial resolution \u2013 which is more important?","text":"<p>The significance of spectral resolution as compared to spatial resolution, and their synergy, is discussed in vegetation mapping studies. The question is which resolution may seem more important in vegetation research. In Such\u00e1 et al. (2016), orthoimages of 12.5-cm pixel and four bands performed better than WorldView-2 satellite data with better spectral resolution (eight bands) and lower spatial resolution (2 m), so spatial resolution proved to be more significant (in both cases, bands were registered in visible and near infrared [VNIR] range) \u2013 see Figure 8. </p> <p> </p> <p>Figure 8: Comparison of classification accuracies based on data with different spatial and spectral resolution. Source: Such\u00e1 et al. (2023).</p> <p>However, of course, this is not a rule and depends on the type of data and classified units. For example, UAV multispectral data with 1-cm pixel (orthophoto) were classified with significantly lower accuracy than UAV hyperspectral data with 9-cm pixel (OA differ by about 10 percentage points for eight and five legend categories ([Kupkov\u00e1 et al., 2021] (#references)). In this case the main problem of the orthophoto was most probably that it did not include near infrared and/or RedEdge bands. Another study analysed and compared data with similar spectral resolution but different spatial resolution. When vegetation of he Krkono\u0161e Mts. in Poland was classified on spectrally similar APEX (Airborne Prism Experiment) and EnMAP (Environmental Mapping and Analysis Program) data at very different spatial resolutions of 3.12 m and 30 m, respectively, the results for grasslands were both still highly satisfactory (for APEX about 98% UA and PA and for EnMAP 94% PA and 86% UA; Marcinkowska-Ochtyra et al., 2017 [Marcinkowska-Ochtyra et al., 2017] (#references)). However, due to the large EnMAP pixel, it was not possible to distinguish two classes (herbs and ruderal vegetation), so the final legend was developed just for this data. Comparison of these classification outputs is presented on Figure 9.</p> <p> </p> <p>Figure 9: Comparison of classification outputs from data with similar spectral and different spatial resolution. Source: Marcinkowka-Ochytra et al. (2017).</p> <p>As mentioned above, an important feature is the coverage of the vegetation unit in the polygon used for classifier training. In the research where 1-m HySpex data were used for species classification in the Jaworzno Meadows Natura 2000 area in Poland, homogeneous patches of Molinia caerulea species with rare coexistence with other species were determined with high accuracy (more than 80%), while for Calamagrostis epigejos, often co-occurring with Solidago spp., it was difficult (about 60% PA and UA; [Marcinkowska-Ochtyra et al., 2018a] (#references)). Empirical studies have revealed that less than 40% coverage of species results in lower accuracy.</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#temporal-resolution","title":"Temporal resolution","text":"<p>As the physiognomy of vegetation is dynamic in time (during one season) and affected by, for example, weather or management practice, the most important date during the growing season should be indicated for a given species or community/habitat. This specific timing is important because it allows vegetation to be distinguished from background based on knowledge of the specific phenological development of particular classes. For example, the best time for species identification was September for Calamagrostis epigejos, as this was the time of optimum fruit formation, and August for Molinia caerulea, when it was in flower (Marcinkowska-Ochytra et al., 2018a). However, that last date of data acquisition was the beginning of September and Molinia were not yet changing colour, as Schuster et al. (2015) recommend. So, the results could be better if data from near to the end of September were used. The temporal dimension and different phenological phases and thus different possibility to classify individual species is obvious from the Figure 10. </p> <p> </p> <p>Figure 10: Temporal dimension \u2013different species can be distinguished in individual dates during the season in UAV multispectral data. Source: (Kupkov\u00e1 et al. (2023))</p> <p>Multi-temporal classification, which takes into account datasets consisting of several terms (within one season) of data acquisition, can be more effective when spectral information from one date is insufficient to distinguish similar categories of vegetation. The significant improvements of classification accuracy using multi-temporal composites was demonstrated for UAV hyperspectral and multispectral data using various classification methods by (Kupkov\u00e1 et al. (2021)) \u2013 see Figure 11.</p> <p> </p> <p>Figure 11: Comparison of overall accuracies for UAV monotemporal (1-date \u2013 July) and multitemporal data (2-date and 3-data \u2013 July, August, September) using Maximum likelihood (MLC), Random forest (RF) and Object-based classification (OBIA). Source: (Kupkov\u00e1 et al. (2023))</p> <p>When analysing three grassland Natura 2000 habitats at the Ostoja Nidzia\u0144ska site in Poland (6210 \u2013 semi-natural dry grasslands and scrubland facies, 6410 \u2013 Molinia meadows and 6510 \u2013 lowland hay meadows) HySpex combined data from May, July and September also allowed higher accuracy than single-date data (Marcinkowska-Ochtyra et al., 2019). However, July and September datasets provided comparable results (the differences in PA and UA less than 2%) \u2013 refer to Figure 12. Figure 12: Differences of classification accuracies using monotemporal and multitemporal datasets for the classification of Nature 2000 habitats.</p> <p> </p> <p>Figure 12: Differences of classification accuracies using monotemporal and multitemporal datasets for the classification of Nature 2000 habitats. Source: Marcinkowska-Ochytra et al., 2019.</p> <p>In the case of some vegetation types \u2013 for example for invasive or expansive species classification the knowledge about the best time for data acquisition is crucial for making the decisions on the proper management of the areas of conservation value. In this example in Jaworzno Meadows in Poland, September connected with the fruiting phase turned out to be the best month to classify Calamagrostis epigejos with the highest accuracy \u2013 see Figure 13.</p> <p> </p> <p>Figure 13: Different phenological phases of Calamagrostis epigejos captured in HySpex data and comparison of classification accuracies according to the date of individual phenological phase (date of the data acquisitions). Source: Marcinkowska-Ochytra et al., 2018.</p> <p>It is worth emphasizing that there are not many studies that use multi-temporal hyperspectral data for vegetation studies. At the moment they are rather expensive, and the use of satellite data such as Sentinel-2 in this context is valuable. Wakuli\u0144ska and Marcinkowska-Ochtyra (2020) proved that combining the first three out of four analysed terms (31 May, 7 and 27 August, and 18 September) provided the best OA (about 80%; 70\u201372% for single-date) and, of the eight analysed vegetation classes, the greatest significance was for grasslands. The aspect of high temporal resolution allows denser time series to be used that can lead to even more detailed analysis.</p> <p>Conclusions</p> <ul> <li>Platform of data acquisition is essential for spatial resolution of images; even today\u2019s advanced satellite technologies cannot provide pixels of tens of cm. </li> <li>Each platform, or type of remote-sensing data, has its own advantages and drawbacks; these must therefore be taken into account when planning a vegetation mapping project (we should consider vegetation unit, detail of mapping, required accuracy, etc.). </li> <li>To achieve the best accuracy on the species level it is essential to combine data with very high spatial (cm) and good spectral (bands important for vegetation classification);</li> <li>Increased temporal resolution can improve classification accuracy \u2013 it is recommended to combine data from the main months of the vegetation season (at least two dates \u2013 July and August). A similar relationship is evident in the classification of habitats/communities and types of ecosystem, but a whole set is not always needed \u2013 it is important to choose the best terms. As the unit increases, high spatial resolution is no longer as important as high spectral resolution. </li> <li>When data of resolution below 1 m are not available, the dominant vegetation species can still be classified with rather high accuracy from aerial or satellite data with meter resolution, and a classification legend on the level of individual species can be used; however, species with low abundance will be classified with rather low accuracy (less than 60%). </li> <li>Testing of different legends (with different number of categories and different levels of generalization) is recommended to reach the best accuracy for different types of data and individual vegetation categories (with regards to the spatial resolution of the data). </li> <li>Collaboration between remote-sensing specialists and botanists is highly recommended for training and validation of data collection and legends elaboration. </li> <li>Additional variables can increase the accuracy of results, but it is important to optimize the classification process and decide which variables will be particularly important for the vegetation considered, depending on their physiognomy and their preferred conditions. </li> <li>As for the different classification methods, OBIA seems to provide better results for extremely high spatial resolution data. Different pixel-based classifiers could work with different levels of reliability for different data and different vegetation categories.  When planning a vegetation mapping project all mentioned types of resolution should be considered and the most suitable data and methods should be selected for the study goal.</li> </ul> <p>Remote sensing brings special features to vegetation monitoring and is a powerful tool in monitoring practice and nature preservation. However, remote-sensing specialists, organizations and companies, together with practitioners, will have to undertake further research to maximize the reliability of obtained products.</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#next-unit","title":"Next unit","text":"<p>Proceed with a case study on seasonal spectral separability of selected grass species of the Krkono\u0161e Mts. tundra ecosystem</p>"},{"location":"module4/05_specific_resolution_contribution/05_specific_resolution_contribution.html#references","title":"References","text":"<p>Ali I., Cawkwell F., Dwyer E., Barrett B. and Green S. (2016) Satellite remote sensing of grasslands: from observation to management. Journal of Plant Ecology 9, 649-671. </p> <p>Bock M., Xofis P., Mitchley J., Rossner G. and Wissen M. (2005) Object-oriented methods for habitat mapping at multiple scales \u2013 Case studies from Northern Germany and Wye Downs, UK. Journal for Nature Conservation 13, 75-89. </p> <p>Feilhauer H., Dahlke C., Doktor D. Lausch A., Schmidtlein S., Schulz G., Stenzel S. (2014) Mapping the local variability of Natura 2000 habitats with remote sensing. Applied Vegetation Science 17, 765-779.  Jensen R.H. (2005) Introductory Digital Image Processing. A Remote Sensing Perspective. Pearson Prentice Hall </p> <p>Kupkov\u00e1 L., \u010cerven\u00e1 L., Such\u00e1 R., Zagajewski B., B\u0159ezina S. and Albrechtov\u00e1 J. (2017) Classification of Tundra Vegetation in the Krkono\u0161e Mts. National Park Using APEX, AISA Dual and Sentinel-2A Data. European Journal of Remote Sensing 50, 1, 29-46. </p> <p>Kupkov\u00e1 L., \u010cerven\u00e1 L. and Lys\u00e1k J. (2021) Grassland mapping using remote sensing data acquired from different platforms. Charles University Prague. Unpublished manuscript. </p> <p>Kupkov\u00e1, L., \u010cerven\u00e1,L., Pot\u016f\u010dkov\u00e1, M., Lys\u00e1k, J., Roubalov\u00e1, M., Hr\u00e1zsk\u00fd,Z., B\u0159ezina, S., Epstein, H.E., M\u00fcllerov\u00e1, J. 2023. Towards reliable monitoring of grass species in nature conservation: Evaluation of the potential of UAV and PlanetScope multi-temporal data in the Central European tundra, Remote Sensing of Environment, Volume 294, 2023, 113645, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2023.113645.</p> <p>Marcinkowska-Ochtyra A., Gryguc K., Ochtyra A., Kope\u0107 D., Jaroci\u0144ska A. and S\u0142awik \u0141. (2019) Multitemporal Hyperspectral Data Fusion with Topographic Indices\u2014Improving Classification of Natura 2000 Grassland Habitats. Remote Sensing 11, 2264. Marcinkowska-Ochtyra A., Jaroci\u0144ska A., Bzd\u0119ga K. and Tokarska-Guzik B. (2018a) Classification of Expansive Grassland Species in Different Growth Stages Based on Hyperspectral and LiDAR Data. Remote Sensing 10(12), 2019. </p> <p>Marcinkowska-Ochtyra A., Zagajewski B., Raczko E., Ochtyra A. and Jaroci\u0144ska A. (2018b) Classification of high-mountain vegetation communities within a diverse giant mountains ecosystem using Airborne APEX Hyperspectral Imagery. Remote Sensing 10, 570. </p> <p>Marcinkowska-Ochtyra A., Zagajewski B., Ochtyra A., Jaroci\u0144ska A., Wojtu\u0144 B., Rogass Ch., Mielke Ch. and Lavender S. (2017) Subalpine and alpine vegetation classification based on hyperspectral APEX and simulated EnMAP images. International Journal of Remote Sensing 38(7), 1839-1864. </p> <p>Marcinkowska-Ochtyra A. and Kupkov\u00e1 L. (2021): Grassland vegetation monitoring: scale is important. Grassland Science in Europe, Vol. 26 \u2013 Sensing \u2013 New Insights into Grassland Science and Practice  </p> <p>Schuster C., Schmidt T., Conrad C., Kleinschmit B. and F\u00f6rster M. (2015) Grassland habitat mapping by intra-annual time series analysis -Comparison of RapidEye and TerraSAR-X satellite data. International Journal of Applied Earth Observation and Geoinformation 34, 25-34. </p> <p>Such\u00e1 R., Jake\u0161ov\u00e1 L., Kupkov\u00e1 L. and \u010cerven\u00e1 L. (2016): Classification of vegetation above the treeline in the Krkono\u0161e Mts. National Park using remote sensing multispectral data. AUC Geographica 51(1), 113-129. </p> <p>Wakuli\u0144ska M. and Marcinkowska-Ochtyra A. (2020) Multi-Temporal Sentinel-2 Data in Classification of Mountain Vegetation. Remote Sensing 12, 2696.</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html","title":"Case study: Discrimination of selected grass species from time series of RPAS hyperspectral imagery","text":""},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#introduction","title":"Introduction","text":"<p>This case study presents data, methods, and results elaborated in the Krkono\u0161e Mts. National Park by our TILSPEC team at Charles University Prague,  within the project \u201cDevelopment of methods for monitoring of the Krkono\u0161e Mts. tundra vegetation changes using multispectral, hyperspectral and LIDAR sensors from UAV\u201d.  Important results of the project have been published in Kupkov\u00e1 et al. (2023).  As we proved in this study, time series (in this case, a multitemporal UAV intra-seasonal dataset) can improve vegetation classification accuracy in comparison with one-date images.  Following our research, the classification accuracy of selected tundra grass species derived from mono- and multitemporal (within one season) hyperspectral RPAS imagery is compared based on reference botanical datasets collected in the Krkono\u0161e Mts. in 2020.  Also tested is the influence of the pre-processing step, comprising the minimum noise fraction (MNF), on classification accuracy. The RF classification is carried out in R. </p> <p> </p> <p>Figure 1. Image of study area near to Lu\u010dn\u00ed bouda hut in the Krkono\u0161e Mts. National Park \u2013 permanent research plot B\u00edl\u00e1 louka meadow (red).</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#objectives","title":"Objectives","text":"<ul> <li> <p>To classify grass vegetation in the Krkono\u0161e Mts. on a permanent research plot 100 x 100 m (Figure 1) using Random Forest classifier (script in R) from UAV hyperspectral data acquired with the Headwall NANO-Hyperspec pushbroom camera.</p> </li> <li> <p>To evaluate and quantify a potential improvement in classification accuracy of the multi-temporal time series compared to mono-temporal imagery.</p> </li> <li> <p>To evaluate the influence of MNF transformation on the final classification result. (optional)</p> </li> </ul>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#questions-to-be-answered","title":"Questions to be answered","text":"<ul> <li> <p>Is it possible to classify individual grass species from a mono-temporal UAS dataset with very high spatial resolution (9 cm) and spectral resolution (54 bands) with an overall accuracy higher than 85%?</p> </li> <li> <p>What is the classification accuracy of the dominant and sparse growth species?</p> </li> <li> <p>Can we reach higher accuracy using time series of intra-seasonal data? How significant are the differences?</p> </li> <li> <p>Optional: Can image data transformation that reduces noise and data dimensionality (MNF transformation) produce better results than the original hyperspectral dataset?</p> </li> </ul>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#data","title":"Data","text":"<p>We will use (module4/case_study_discrimination_grass_species):  </p> <ul> <li> <p>Hyperspectral image data acquired with the Headwall Nano-Hyperspec\u00ae camera fastened on the DJI Matrice 600 Pro drone on June 16 and August 11 2020 (Figure 2), with ground sampling distance of 9 cm and spectral resolution of 54 bands (resampled from 269 bands to reduce correlation in neighboring bands): <code>BL_202006.tif (data from June; 54 bands, for visualization in true colors use bands R-21/G-13/B-7)</code> <code>BL_202008.tif (data from August; 54 bands, for visualization in true colors use bands R-21/G-13/B-7)</code> <code>MNF_08_10.tif (MNF transformed image from August; first 10 bands)</code> </p> </li> <li> <p>Field reference dataset (Figure 3) collected by botanists (in 2019 and 2020) divided between training data (polygons) and validation data (points). For an explanation of how the reference dataset was collected and divided between training and validation data, see Kupkov\u00e1 et al. (2023): <code>train_polygons.zip (training data)</code> <code>valid_points.zip (validation data)</code></p> </li> </ul> <p> </p> <p>Figure 2. Hyperspectral data \u2013 imagery used for classification.</p> <p> </p> <p>Figure 3. Reference data \u2013 training polygons and an example of validation points.</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#classification-scheme","title":"Classification scheme","text":"<p>The classification scheme (Figure 4) includes four categories of dominant grass species: one originally common species (Nardus stricta, NS) and three currently expanding grasses Calamagrostis villosa (CV), Molinia caerulea (MC), and Deschampsia cespitosa. Also, species with sparse growth on the permanent research plot\u00a0were classified : Avenella flexuosa (AFS), Carex bigelowii (CB), and Picea abies (PAb).</p> <p> </p> <p>Figure 4. Classified grassland species.</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#methodology","title":"Methodology","text":""},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#1-random-forest-classification-in-r","title":"1. Random forest classification in R","text":"<p>For RF classification (Belgiu and Dr\u0103gut, 2016; Breiman, 2001) in R software, we will use the \u2018randomForest\u2019 package (Liaw and Wiener, 2002).  Random Forest represents one of the increasingly used machine learning methods.  This classifier creates a specified number of decision trees (Ntree parameter) from the training data to determine the class membership.  Each such tree is built for randomly selected training data, with decision rules formed by a random subset of features (feature variables) of a specified size (the Mtry parameter).  The resulting class of each pixel in the image is then determined by the voting result of each decision tree.</p> <p>This classifier has the advantage of reduced sensitivity to noise in the data as well as high accuracy when dealing with voluminous data (Belgiu and Dr\u0103gu\u0163, 2016).</p> <p>In our study, we used various combinations of two input parameters to test the RF classifier: the number of trees (<code>ntree</code>) and the number of input variables (features) randomly sampled at each split (<code>mtry</code>). The best results were achieved for the <code>ntree</code> value of 1000. The tests performed on <code>mtry</code> values showed that the default <code>mtry</code> value was sufficient.</p> <p>Besides the spectral features (retrieved from the original/transformed bands), we will also use standard textural features (mean texture, variance, homogeneity, contrast, dissimilarity, entropy, second moment) (Haralick et al., 1973) calculated by using the Gray-Level-Co-Occurrence Matrix (GLCM) in the \u2018glcm\u2019 R package (Zvoleff, 2020) with a window size of 3 x 3 pixels and default parameter settings.  These variables were selected based on the values of the Importance score.  The Importance score (see an example in Appendix 1) can be generated as one of the outputs from RF classifications and shows the importance of feature variables.  Features with high values for this score are generally regarded as more important.</p> <p>Use the provided code in R, which can be seen below, to classify the monotemporal hyperspectral datasets from June <code>BL_202006.tif</code> and August <code>BL_202008.tif</code> 2020, multitemporal composite, and MNF transformed image. The script is self-explanatory, with comments and instructions following <code>#</code>. Don't forget to set the working directory and adapt the input data paths according to your data structure.  Be aware that the computation is time-consuming. Thus, example result classification rasters are also be provided with the data. </p> <pre><code># =============================================================================\n# Import libraries\n# This loads required packages (and installs them if necessary)\nl_packages &lt;- c(\"raster\", \"rgdal\", \"randomForest\", \"glcm\", \"spacetime\")\nfor (package in l_packages){\n  if(! package %in% installed.packages()){\n    install.packages(package, dependencies = TRUE)\n  }\n  library(package, character.only = T)\n}\n\n# Set working directory - insert path to your data\nsetwd(\"c:/path/to/data\")\ngetwd()\n\n# =============================================================================\n# 1.\n# Monotemporal dataset - uncomment this part to perform classification on one of the monotemporal datasets\n# =============================================================================\n# Load input hyperspectral (HS) image\nimg &lt;- brick(\"BL_202006.tif\")\n# Assign band names to image\nnames(img) &lt;-  paste0(\"B\", c(1:54))\n\n# =============================================================================\n# 2.\n# Multitemporal composite - uncomment this part to composite the two datasets\n# =============================================================================\n#img_1 &lt;- brick(\"BL_202006.tif\")\n#img_2 &lt;- brick(\"BL_202008.tif\")\n#img   &lt;- stack(img_1, img_2)\n# Assign band names to image\n#names(img) &lt;-  paste0(\"B\", c(1:108))\n\n\n\n# =============================================================================\n# Load training data\n# =============================================================================\n# Load vector training data\ntraining &lt;- readOGR(dsn=getwd(), layer=\"train_polygons\") # dsn name of folder containing shp, with the name in layer\nView(training)\n\n# Vizualize of image and training data for control\nplot(img$B3)\nplot(training, bg=\"transparent\", add=TRUE)\n\n# =============================================================================\n# Feature extraction\n# =============================================================================\n# Calculate GLCM textures from each spectral band\n# TAKES LONG TIME!!!\nout &lt;- list()\nfor(band in 1:dim(img)[3]) {\n  out[band] &lt;- glcm::glcm(img[[band]], window = c(3, 3), na_opt = 'center', \n                       statistics = c(\"mean\", \"variance\", \"homogeneity\", \"contrast\",\n                                      \"dissimilarity\", \"entropy\", \"second_moment\"))\n  print(paste0('Finished computing textures for band #', band, '/', dim(img)[3]))\n}\ntextures &lt;- stack(out)\n\n# Stack image and textures\npredictors &lt;- stack(img, textures)\n\n# Preparation of training data\n# Extract raster values from training polygons\ndf_features &lt;- extract(predictors, training, df=TRUE) # TAKES LONG TIME!!!\n\n# Remove rows with nodata values\ntraining &lt;- na.omit(df_features)\n# Rename column ID to Classvalue\nnames(training)[names(training) == 'ID'] &lt;- 'Classvalue'\n\n# See number of features for each class\ntable(training$Classvalue)\n\n# =============================================================================\n# Train + Apply a RF model\n# =============================================================================\n# TAKES LONG TIME!!\nmodel &lt;- randomForest(as.factor(Classvalue) ~.,data=training,\n                      ntree=100, importance=TRUE, do.trace=50) ### original model had ntree=1000\n\n# Classify image using trained model\npredicted &lt;- predict(predictors, model)\n\n# Vizualize classification\nplot(predicted, main = 'Classification Random Forest')\n\n# Print information about the trained model\nmodel\n\n# =============================================================================\n# Export results\n# =============================================================================\n# Export feature importance table\nfeature_importance &lt;- model$importance\nwrite.table(feature_importance, \"BL_202006_RF_feature_importance.txt\")\n\n# Save classification as a raster\nwriteRaster(predicted, filename=\"BL_202006_RF_classified.tif\",overwrite=TRUE)\nSys.time()\n</code></pre> <p>The script outputs a classified raster <code>*tif</code>, the coordinate system/projection (S-JTSK/Krovak East North, EPSG: 5514) needs to be defined along with the color information and class names. This can be done in the QGIS software package or in R with the following code snippet: <code>crs(\"image_filename\") &lt;- \"+proj=krovak +ellps=bessel\"</code>. The feature importance output file <code>*txt</code>, column <code>MeanDecreaseAccuracy</code>, shows the importance of each band in the classification process.  </p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#2-accuracy-assessment","title":"2. Accuracy assessment","text":"<p>Assessment should be elaborated in the GIS environment (AcaTaMa plugin or Semi-Automatic Classification plugin for QGIS). Use the provided validation points, <code>valid_points.zip</code>, to compute the accuracy scores. Select the classified raster to be evaluated, and compare the classification output with class values at validation samples. Compute the confusion matrix and derive the overall accuracy, precision, recall, and F1-score.\u00a0</p> Class/Species Number afs 1 cv 2 cxbig 3 desch 4 mol 5 nard 6 smrk 7"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#tasks","title":"Tasks","text":"<ol> <li>Use the provided R script to classify:   <ul> <li>image from June 2020</li> <li>image from August 2020</li> <li>multitemporal composite from June and August 2020</li> <li>MNF transformed image.  </li> </ul> </li> <li>Perform accuracy assessment of all classification outputs.  </li> <li>Prepare a report \u2013 present shortly used methodology, overall accuracies, and F1-scores of all classification outputs, comment and compare the results for:<ul> <li>mono-temporal and multitemporal images.  </li> </ul> </li> <li>Discuss the results. </li> <li>Try to answer the questions from the introduction of the case study.  </li> </ol>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#acknowledgement","title":"Acknowledgement","text":"<p>We want to thank the project \u201cDevelopment of methods for monitoring of the Krkono\u0161e Mts. tundra vegetation changes using multispectral, hyperspectral and LIDAR sensors from UAV\u201d supported by the European fund of regional development and European fund for projects in environmental protection.</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#references","title":"References","text":"<p>Kupkov\u00e1, L., \u010cerven\u00e1,L., Pot\u016f\u010dkov\u00e1, M., Lys\u00e1k, J., Roubalov\u00e1, M., Hr\u00e1zsk\u00fd, Z., B\u0159ezina, S., Epstein, H.E., M\u00fcllerov\u00e1, J. 2023. Towards reliable monitoring of grass species in nature conservation: Evaluation of the potential of UAV and PlanetScope multi-temporal data in the Central European tundra, Remote Sensing of Environment, 294, 113645. ISSN 0034-4257. 10.1016/j.rse.2023.113645. </p> <p>Belgiu, M. and Dragut, L. 2016. Random Forest in Remote Sensing: A Review of Applications and Future Directions. ISPRS Journal of Photogrammetry and Remote Sensing, 114, 24-31. 10.1016/j.isprsjprs.2016.01.011.</p> <p>Breiman, L. 2001. Random Forests. Machine Learning, 45, 5-32. 10.1023/A:1010933404324.</p> <p>Haralick, R. M., Shanmugam, K., Dinstein, I. 1973. Textural Features for Image Classification. IEEE Transactions on Systems, Man, and Cybernetics, 3, 6, 610-621. 10.1109/TSMC.1973.4309314.</p> <p>Zvoleff, A. 2016. GLCM: calculate textures from grey-level co-occurrence matrices (GLCMs). github.com/azvoleff/glcm.</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#case-study-results","title":"Case study results","text":"<p>Proceed to case study example results</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/06_Krkonose_tundra_grasslands.html#next-unit","title":"Next unit","text":"<p>Proceed with a case study on seasonal dynamics of flood-plain forests</p>"},{"location":"module4/06_Krkonose_tundra_grasslands/solution/06_Krkonose_tundra_grasslands_solution.html","title":"E-TRAINEE: Discrimination of selected grass species from time series of RPAS hyperspectral imagery - report","text":""},{"location":"module4/06_Krkonose_tundra_grasslands/solution/06_Krkonose_tundra_grasslands_solution.html#example-results","title":"Example results","text":""},{"location":"module4/06_Krkonose_tundra_grasslands/solution/06_Krkonose_tundra_grasslands_solution.html#random-forest-classification-in-r","title":"Random forest classification in R","text":"Image Classification June August June + August August MNF Classification legend"},{"location":"module4/06_Krkonose_tundra_grasslands/solution/06_Krkonose_tundra_grasslands_solution.html#accuracy-assessment","title":"Accuracy assessment","text":"June OA=85.91% August OA=88.03% Class Recall Precision F1-score Recall Precision F1-score afs 29.13 48.05 36.27 29.13 56.92 38.54 cv 96.89 86.16 91.21 99.42 99.13 99.27 cxbig 0.00 0.00 0.00 0.00 0.00 0.00 desch 87.30 87.30 87.30 80.95 74.45 77.56 mol 88.12 95.10 91.48 96.62 94.07 95.33 nard 79.28 71.56 75.22 85.62 75.70 80.35 smrk 90.00 97.83 93.75 26.00 92.86 40.63 June + August OA=92.60% August MNF OA=91.89% Class Recall Precision F1-score Recall Precision F1-score afs 41.73 47.75 44.54 51.18 54.62 52.84 cv 99.22 98.08 98.65 99.42 99.22 99.32 cxbig 10.00 62.50 17.24 18.00 81.82 29.51 desch 90.00 86.04 87.98 90.63 84.72 87.58 mol 97.58 99.02 98.29 98.07 97.60 97.83 nard 93.23 86.81 89.91 84.57 82.82 83.69 smrk 92.00 97.87 94.84 72.00 100.00 83.72"},{"location":"module4/06_Krkonose_tundra_grasslands/solution/06_Krkonose_tundra_grasslands_solution.html#qa","title":"Q&amp;A","text":"<ul> <li>Is it possible to classify individual grass species from a mono-temporal UAS dataset with very high spatial resolution (9 cm) and spectral resolution (54 bands) with an overall accuracy higher than 85%?  <ul> <li>yes, both of the mono-temporal datasets (June and August) achieved an OA above 85%</li> <li>however, the precision and recall varied for different species</li> </ul> </li> <li>What is the classification accuracy of the dominant and sparse growth species?  <ul> <li>dominant grass species (cv, mol, nard, desch) reach excellent accuracy; sparse species not so much</li> <li>abundance/density/homogeneity of the species and its cover are essential for classification accuracy</li> </ul> </li> <li>Can we reach higher accuracy using time series of intra-seasonal data? How significant are the differences?  <ul> <li>accuracy increased with spatial resolution; multitemporal composite from June and August 2020 reached the highest OA of 92.6%</li> </ul> </li> <li>Optional: Can image data transformation that reduces noise and data dimensionality (MNF transformation) produce better results than the original hyperspectral dataset?  <ul> <li>yes, the MNF transformed image OA is 91.89%, which corresponds to a 3.96 % increase when compared to the classification of the original August dataset</li> </ul> </li> </ul>"},{"location":"module4/06_Krkonose_tundra_grasslands/solution/06_Krkonose_tundra_grasslands_solution.html#back-to-case-study","title":"Back to case study","text":"<p>Proceed by returning to Discrimination of selected grass species from time series of RPAS hyperspectral imagery</p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html","title":"Case study: Seasonal dynamics of flood-plain forests","text":"<p>The overall condition of a plant or an entire ecosystem is often assessed by its chlorophyll content.  To be able to obtain or validate absolute values of chlorophyll content from remote sensing data, reliable ground truth data sets are needed.  Non-destructive measurements, such as various portable chlorophyll meters, are often used as ground truth data in remote sensing studies. </p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html#objectives","title":"Objectives","text":"<p>The objectives of this case study are as follows:</p> <ul> <li> <p>To estimate the accuracy of chlorophyll content measured by the portable transmittance-based chlorophyll meter SPAD-502 by comparison with a laboratory reference. </p> </li> <li> <p>To compute  chlorophyll indices based on spectra measured by the spectroradiometer ASD FieldSpec4 Standard-Res coupled with an integrating sphere; to build and validate empirical models between indices and chlorophyll content.</p> </li> <li> <p>To compare the performance of empirical models using data from the entire vegetation season and data from one term. </p> </li> </ul> <p>The case study will be presented on a dataset from a floodplain forest in Czechia where all the measurements were conducted on the set of various deciduous trees during the vegetation periods of 2019 and 2020. </p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html#data","title":"Data","text":"<p>The dataset (module4/case_study_flood_plain_forests) was acquired in the Soutok floodplain forest district located between the rivers Morava and Dyje (48.68\u00b0 N, 16.94\u00b0 E) in 2019 (four field campaigns conducted in April, July, September, and October; 204 samples) and 2020 (three field campaigns in May, July, ans October; 193 samples). During each field campaign, sunlit and shaded branches were trimmed from eighteen deciduous trees of six species: Austrian oak, English oak, Narrow-leaved ash, European hornbeam, White poplar and Small-leaved linden (Figure 1).  Representative leaves of all types (small, large, green, colored, young, old) were measured using the chlorophyll meter SPAD-502 and the ASD FieldSpec4 Standard-Res spectroradiometer (350 \u2013 2,500 nm sampling spectral domain) coupled with the integrating sphere RTS-3ZC. The leaves were then taken to the laboratory for spectrophotometric determination of chlorophyll content from dimethylformamide extracts (Figure 2).  Data from 2019 will be used for training purposes, and data from 2020 will serve as validation of the results.</p> <p>Input data file <code>FloodplainForest_input_data.xlsx</code> consists of two sheets: <code>2019_training</code> and <code>2020_validation</code>. Data is also available in the form of text files separated by a tabulator for work in R (<code>FloodplainForest_2019_training.txt</code> and <code>FloodplainForest_2020_validation.txt</code>). All the files have the same columns: </p> <ul> <li><code>CampNo_year</code> \u2013 field campaign number (1 to 7) and year (19 for 2019, 20 for 2020)</li> <li><code>Month</code> \u2013 month on acquisition</li> <li><code>Species</code> \u2013 tree species</li> <li><code>TreeNo</code> \u2013 number of sampled tree </li> <li><code>ID_Biochem</code> \u2013 ID of leaf used for chlorophyll measurement</li> <li><code>SPAD_Cab</code> \u2013 SPAD value correlating with content of chlorophyll a+b</li> <li><code>Total_chloro_ug_cm2</code> \u2013 total chlorophyll content in \u03bcg/cm\u00b2 extracted in laboratory (further in text referred to as laboratory chlorophyll content)</li> <li><code>350 \u2013 2500</code> \u2013 reflectance at a given wavelength measured by the spectroradiometer</li> </ul> <p> </p> <p>Figure 1. Map of the study area showing the location of 18 sampled trees; aerial hyperspectral CASI data in the background.</p> <p> </p> <p>Figure 2. Examples of sampled leaves in October 2020 (up), field laboratory (down).</p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html#chlorophyll-meter-vs-laboratory-chlorophyll-content","title":"Chlorophyll meter vs. laboratory chlorophyll content","text":"<p>To begin, consider the relationship between both methods of determining chlorophyll content and seasonal changes in chlorophyll content.  By building simple regressions (table editor: <code>XY point graph / add trendline</code> or R: <code>lm(SPAD_Cab ~ Total_chloro_ug_cm\u00b2)</code>) for each month  (April 2019, May 2020, July 2019, July 2020, September 2019, October 2019, October 2020) and year (2019, 2020), we can see how the two values are correlated and how the chlorophyll content and the correlation values change over the seasons (Figure 3).  At the start of the season, data variability is so low that the correlation fails (April 2019, R\u00b2 = 0.04).  In May, the correlation becomes stronger, which is consistent with some studies that have looked for the most appropriate term to distinguish between different deciduous tree species.  Lisein et al., 2015; Grybas and Congalton, 2021 discovered that the end of May is ideal as on species' phenology is well synchronized but inter-species differences are still visible.  Our result (R\u00b2 = 0.8) is influenced by two outliers, but when they are removed, the correlation remains very strong R\u00b2 = 0.73 (not shown in Figure 3).  The correlations are weaker in July (July 2019 \u2013 R\u00b2 = 0.50, July 2020 \u2013 R\u00b2 = 0.70) as all the species are at their phenology peak.  A similar trend can be observed in September 2019 (R\u00b2 = 0.72, not shown in Figure 3).  When the leaves begin to change color in the autumn, the variability in chlorophyll content in different species and leaves increases, and correlations improve (October 2019 \u2013 R\u00b2 = 0.82, October 2020 \u2013 R\u00b2 = 0.87).  Autumn is another term that is frequently used to distinguish deciduous trees. The best results are obtained by incorporating all of the months (R\u00b2 for 2019 = 0.86 and for 2020 = 0.87), but the trend is no longer linear.  </p> <p> </p> <p>Figure 3. Correlation between SPAD values and laboratory chlorophyll content in different time periods.</p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html#empirical-modeling","title":"Empirical modeling","text":"<p>There are numerous indices for estimating chlorophyll content of vegetation. Most of the indices can be found in the remotes sensing Index DataBase or in various articles,  e.g. comparison of indices for estimating chlorophyll content were made by Main et al., 2011; Croft et al., 2014; le Maire et al., 2008, vegetation changes were evaluated using indices by Mi\u0161urec et al., 2016.  It is usually a good idea to compute as many indices as feasible to discover the optimal correlation. In this case, we created a script in R to compute thirty indices based on spectroradiometer data  and to construct simple linear regressions between indices and laboratory chlorophyll content, respectively SPAD values (<code>Code 1</code>).  To begin, run the script for data from the entire season 2019 (shown in <code>Code 1</code>, download) and then for data from individual months in 2019.  The results are shown in Figure 4. The majority of the models were statistically significant, with more non-significant models discovered only in April (with only 36 samples).  Based on the data from April, it can also be seen that the coefficients of determination are extremely low, particularly for regression of indices and laboratory chlorophyll content.  For SPAD values, slightly better results were achieved, which could be attributed to the method of measuring chlorophyll content by SPAD-502.  Transmittance is measured at two wavelengths (650 and 940 nm), which are similar to wavelengths used for computing the vegetation indices.  As the season progresses, the regression becomes stronger, because dataset variability increases and more samples are collected (46 in July, 56 in September, and 66 in October).  The October dataset and datasets containing data from the entire season yield the best results. </p> <p>Compute thirty indices and linear regressions for data from the entire season of 2019 with the following code in R (<code>Code 1</code>):</p> <pre><code>data=read.delim(\"FloodplainForest_2019_training.txt\") # load data\nfix(data)   # view data\nattach(data)    \n</code></pre> <pre><code># Chlorophyll content indices\n# Main et al. 2011\n\nMCARI=((X700-X670)-0.2*(X700-X550))*(X700/X670)     # (Main et al. 2011; Daughtry et al. 2000)\nMCARI2=((X750-X705)-0.2*(X750-X550))*(X750/X705)    # (Main et al. 2011; Wu et al. 2008)\nTCARI=3*((X700-X670)-0.2*(X700-X550)*(X700/X670))   # (Main et al. 2011; Haboudane et al. 2002)\nOSAVI=(1+0.16)*(X800-X670)/(X800+X670+0.16)     # (Main et al. 2011; Rondeaux et al 1996)\nOSAVI2=(1+0.16)*(X750-X705)/(X750+X705+0.16)    # (Main et al. 2011; Wu et al. 2008)\nTCARIOSAVI=TCARI/OSAVI                  # (Main et al. 2011; Haboudane et al. 2002)\nMCARIOSAVI=MCARI/OSAVI                  # (Main et al. 2011; Daughtry et al. 2000)\nMCARI2OSAVI2=MCARI2/OSAVI2          # (Main et al. 2011; Wu et al. 2008)\nTVI=0.5*(120*(X750-X550)-200*(X670-X550))       # (Main et al. 2011; Broge and Leblanc 2000)\nMTCI=(X754-X709)/(X709-X681)            # (Main et al. 2011; Dash and Curran 2004)\nmND705=(X750-X705)/(X750+X705-2*X445)     # (Main et al. 2011; Sims and Gamon 2002)\nGitelson2=(X750-X800/X695-X740)-1         # (Main et al. 2011; Gitelson et al. 2003)\nMaccioni=(X780-X710)/(X780-X680)          # (Main et al. 2011; Maccioni et al. 2001)\nVogelmann=X740/X720                       # (Main et al. 2011; Vogelmann et al. 1993)\nVogelmann2=(X734-X747)/(X715+X726)        # (Main et al. 2011; Vogelmann et al. 1993)\nDatt=(X850-X710)/(X850-X680)              # (Main et al. 2011; Datt 1999)\nDatt2=X850/X710                           # (Main et al. 2011; Datt 1999)\nSR1=X750/X710                             # (Main et al. 2011; Zarco-Tejada and Miller 1999)\nSR2=X440/X690               # (Main et al. 2011;Lichtenthaler et al. 1996)\nDD=(X749-X720)-(X701-X672)                # (Main et al. 2011; le Maire et al. 2004)\nCarter=X695/X420                          # (Main et al. 2011; Carter 1994)\nCarter2=X710/X760                         # (Main et al. 2011; Carter 1994)\nREP_LI=700+40*((X670+X780/2)/(X740-X700)) # (Main et al. 2011; Guyot and Baret 1988)\n\n# Misurec et al. 2016\n\nREP=700+40*((((X670+X780)/2)-X700)/(X740-X700))         # (Misurec 2016; Curran et al 1995)\nRDVI=(X800-X670)/sqrt(X800+X670)            # (Misurec 2016; Roujean and Breon 1995)\nMSR=((800-670)-1)/sqrt((X800/X670)+1)           # (Misurec 2016; Chen 1996 )\nMSAVI=0.5*((2*X800+1-sqrt((X800+1)^(2))-8*(X800-X670)))     # (Misurec 2016; Qi et al. 1994)\nN705=(X705-X675)/(X750-X670)                # (Misurec 2016; Campbell et al. 2004)\nN715=(X715-X675)/(X750-X670)                # (Misurec 2016; Campbell et al. 2004)\nN725=(X725-X675)/(X750-X670)                            # (Misurec 2016; Campbell et al. 2004)\n\n</code></pre> <pre><code># add indices to the original table\n\ndataVI=(cbind(data,MCARI,MCARI2,TCARI,OSAVI,OSAVI2,TCARIOSAVI,MCARIOSAVI,MCARI2OSAVI2,TVI,MTCI,mND705,Gitelson2,Maccioni,Vogelmann,Vogelmann2,Datt,Datt2,SR1,SR2,DD,Carter,Carter2,REP_LI,REP,RDVI,MSR,MSAVI,\nN705,N715,N725))\n\nwrite.table(dataVI,\"FloodplainForest_2019_training_indices.txt\") # save data and indices to the text file\n\nfix(dataVI) # view data + indices\n\n</code></pre> <pre><code># explore the dimensions of the original table and table with indices; next a for loop will be computed only for indices, i. e. from the number of columns of the original data +1 till the end\n\ndim(data)\ndim(dataVI)\n\n# create empty vectors\nresults_p &lt;- vector()\nresults_R &lt;- vector()\nresults_i &lt;- vector()\nresults_koef &lt;- vector()\n\n</code></pre> <pre><code># compute linear regression model between indices and laboratory chlorophyll content in a for loop and saving the p-values, coeficient of determination and equation coefficients to the text files\n\nfor (I in 2159:2188) # change the values of I based on the results of dim function\n{\nmodel&lt;-lm(dataVI$Total_chloro_ug_cm2~dataVI[,I]);\nsum=summary(model)\nout&lt;-capture.output(sum);\nresults_p &lt;- append(results_p,grep (\"p-value\", out, value = TRUE) );\nresults_R &lt;- append(results_R,grep (\"Multiple R-squared\", out, value = TRUE) );\nresults_i &lt;- append(results_i,grep (\"(Intercept)\", out, value = TRUE) );\nresults_koef &lt;- append(results_koef,grep (\"dataVI\", out, value = TRUE) );\n}\n\nwrite(results_p, \"2019_Tot_chl_results_p.txt\")\nwrite(results_R, \"2019_Tot_chl_results_R.txt\")\nwrite(results_i, \"2019_Tot_chl_results_i.txt\")\nwrite(results_koef, \"2019_Tot_chl_results_koef.txt\")\n\n</code></pre> <pre><code># the same loop for SPAD values\n\nresults_p &lt;- vector()\nresults_R &lt;- vector()\nresults_i &lt;- vector()\nresults_koef &lt;- vector()\n\nfor (I in 2159:2188) # change the values of I based on the results of dim function\n{\nmodel&lt;-lm(dataVI$SPAD_Cab~dataVI[,I]);\nsum=summary(model)\nout&lt;-capture.output(sum);\nresults_p &lt;- append(results_p,grep (\"p-value\", out, value = TRUE) );\nresults_R &lt;- append(results_R,grep (\"Multiple R-squared\", out, value = TRUE) );\nresults_i &lt;- append(results_i,grep (\"(Intercept)\", out, value = TRUE) );\nresults_koef &lt;- append(results_koef,grep (\"dataVI\", out, value = TRUE) );\n}\n\nwrite(results_p, \"2019_SPAD_Cab_results_p.txt\")\nwrite(results_R, \"2019_SPAD_Cab_results_R.txt\")\nwrite(results_i, \"2019_SPAD_Cab_results_i.txt\")\nwrite(results_koef, \"2019_SPAD_Cab_results_koef.txt\")\n\n</code></pre> <p> </p> <p>Figure 4. Coefficient of determination (R\u00b2) for linear regression between SPAD values (upper table) resp. laboratory chlorophyll content (lower table) and thirty vegetation indices based on datasets from April, July, September, October and the whole season of 2019. Red means the worst models, green the best.</p> <p>Because the dataset for the entire season includes all potential chlorophyll values, the regression equations should be able to estimate chlorophyll content based on the indices from different seasons.  Let\u2019s compute the indices for the 2020 validation dataset, and based on the regression equations from 2019, estimate the SPAD values and laboratory chlorophyll content for the best performing indices, i.e. for SPAD value \u2013 N705 (R\u00b2 = 0.8566, SPAD_value = -61.215 * N705 + 55.688) and for laboratory chlorophyll content \u2013 Vogelmann (R\u00b2 = 0.809, Lab_Chlorophyll = 88.635 * Vogelmann - 84.965).  The intercept can be found in script output text files ending with <code>_i</code> and slope values can be found in script output text files ending with <code>_koef</code>. For more details, see Figure 5.  Then, let\u2019s compare the actual and predicted values using graphs (Figure 6) and the computing root mean square error (RMSE), which measures the average difference between a statistical model\u2019s predicted and actual values (Formula 1).</p> <p> </p> <p>Figure 5. Results of regression model for laboratory chlorophyll content and Vogelmann index with highlighted intercept and slope values.</p> <p> </p> <p>Figure 6. Actual vs. predicted values of laboratory chlorophyll content (left) and SPAD values (right), 2020 dataset.</p> <p> </p> Formula 1. RMSE. <p>RMSE for predicted laboratory chlorophyll content was 6.6 \u03bcg/cm\u00b2 (average of 2020 validation dataset was 31.9 \u03bcg/cm\u00b2 and standard deviation 15.2 \u03bcg/cm2),  for predicted SPAD values 4.9 \u03bcg/cm\u00b2 (average of 2020 validation dataset was 31.9 and standard deviation 11.3).  It can be seen that computed regressions for both indices and parameters also perform well for other seasons. </p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html#conclusions","title":"Conclusions","text":"<p>In the first part of the case study, we explored the relation between two methods of chlorophyll content determination \u2013 destructive (total chlorophyll content extracted in laboratory) and non-destructive (measured by the portable transmittance-based chlorophyll meter SPAD-502).  We found out that if there is enough variability in data, the correlation between these two methods of chlorophyll content determination works well.  The time-demanding and destructive method of chlorophyll content extraction in the laboratory can be substituted by fast non-destructive measurements, and prediction equations can be used to obtain total chlorophyll content in \u03bcg/cm\u00b2.  But these equations are not transferable to different ecosystems with potentially different chlorophyll content values. Each ecosystem requires new calibration. </p> <p>In the second part of the case study, we explored the correlation of vegetation indices computed from spectra measured by the spectroradiometer coupled with the integrating sphere and the chlorophyll content values.  The best performing indices were Vogelmann for laboratory chlorophyll content and N705 for SPAD values.  Equations developed using data from the entire season of 2019 were successfully applied to the 2020 dataset (RMSE validation).  Slightly better performance was reached for predicting SPAD values from the indices, which can be caused by similar methods of chlorophyll content estimation (both optical measurements).  These findings confirm that spectroradiometer measurements of leaf reflectance are promising method for estimating chlorophyll content in leaves.</p> <p>Seasonal changes were also taken into account.  When working with the entire season's dataset, all possible chlorophyll content values for the specific ecosystem were considered, and the strongest models were produced. However, this required a significant amount of field work.  For a floodplain forest, October could be recommended as a suitable month to conduct a similar study. The variability in leaves is high, and the models perform accordingly. </p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html#references","title":"References","text":"<p>Croft, H., J.M. Chen, &amp; Y. Zhang. 2014. \u201eThe Applicability of Empirical Vegetation Indices for Determining Leaf Chlorophyll Content over Different Leaf and Canopy Structures\". Ecological Complexity 17 (March): 119\u201330. 10.1016/j.ecocom.2013.11.005.</p> <p>Grybas, H., &amp; Congalton, R. G. 2021. A comparison of multi-temporal rgb and multispectral uas imagery for tree species classification in heterogeneous new hampshire forests. Remote Sensing, 13 (13). 10.3390/rs13132631. </p> <p>Lisein, J., Michez, A., Claessens, H., &amp; Lejeune, P. 2015. Discrimination of deciduous tree species from time series of unmanned aerial system imagery. PLoS ONE, 10 (11). 10.1371/journal.pone.0141006. </p> <p>Main, Russell, Moses Azong Cho, Renaud Mathieu, Martha M. O\u2019Kennedy, Abel Ramoelo, &amp; Susan Koch. 2011. \u201eAn Investigation into Robust Spectral Indices for Leaf Chlorophyll Estimation\". ISPRS Journal of Photogrammetry and Remote Sensing 66 (6): 751\u201361. 10.1016/j.isprsjprs.2011.08.001.</p> <p>Maire, G. le, C. Fran\u00e7ois, &amp; E. Dufr\u00eane. 2004. \u201eTowards Universal Broad Leaf Chlorophyll Indices Using PROSPECT Simulated Database and Hyperspectral Reflectance Measurements\". Remote Sensing of Environment 89 (1): 1\u201328. 10.1016/j.rse.2003.09.004.</p> <p>Mi\u0161urec, Jan, Veronika Kopa\u010dkov\u00e1, Zuzana Lhot\u00e1kov\u00e1, Petya Campbell, &amp; Jana Albrechtov\u00e1. 2016. \u201eDetection of Spatio-Temporal Changes of Norway Spruce Forest Stands in Ore Mountains Using Landsat Time Series and Airborne Hyperspectral Imagery\". Remote Sensing 8 (2): 92. 10.3390/rs8020092.</p>"},{"location":"module4/07_flood_plain_forest/07_flood_plain_forest.html#back-to-the-start","title":"Back to the start","text":"<p>Proceed by returning to module overview</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html","title":"Case study: Seasonal spectral separability of selected grass species of the Krkono\u0161e Mts. tundra ecosystem","text":"<p>This case study focuses on the separability of different grass species in Krkono\u0161e Mts.  during the vegetation growing season. It is inspired by a study by \u010cerven\u00e1 et al. (2020),  where differences in optical properties of three grass species were investigated on three scale levels:  green leaf spectra measured by the spectroradiometer ASD FieldSpec4 Wide-Res coupled with a contact probe in laboratory conditions (leaf level),  canopy spectra measured by the same spectroradiometer using the fiber optic cable with a pistol grip in the field (canopy level), and hyperspectral image data acquired with the Nano-Hyperspec\u00ae fastened to the DJI Matrice 600 Pro drone (image level). </p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#objectives","title":"Objectives","text":"<p>The objectives of this case study are as follows:</p> <ul> <li> <p>test the separability of four grass species during the 2020 vegetation season at the image level using statistical tests for each wavelength,</p> </li> <li> <p>separability analysis (Jeffries-Matusita distance) in R. </p> </li> </ul> <p>To get familiar with the area of interest, see Tundra vegetation monitoring in Krkono\u0161e Mountains in the Use Cases and Data section of the course.</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#data","title":"Data","text":"<p>Hyperspectral images acquired with the Headwall Nano-Hyperspec\u00ae fastened to the DJI Matrice 600 Pro drone on June 16th, July 13th, and August 11th, 2020 (Figure 1) were resampled from 269 spectral bands to 54 bands (to eliminate noise and correlation in neighboring bands), and ground sampling distance was reduced from 3 to 9 cm. For every of the four dominant grass species (Nardus stricta (<code>nard</code>) and competitive grasses Calamagrostis villosa (<code>cv</code>), Molinia caerulea (<code>mol</code>), and Deschampsia cespitosa (<code>desch</code>)) 450 random pixels were selected (Figure 1). Spectral curves for these pixels were extracted based on all three hyperspectral images. The results are in a table, where the first column is called <code>classname</code> and contains class values <code>desch</code>, <code>cv</code>, <code>mol</code>, and <code>nard</code>, and the rest are columns with reflectance for each band in each month (e.g., <code>b1_2006</code> is band 1 of image acquired in June 2020, etc.).  Data (module4/case_study_spectral_separability_grass) is available in three text files, each for every month (<code>Rin_grasses_2020_month.txt</code>).</p> <p> </p> <p>Figure 1. Hyperspectral data acquired in June (upper left), July (upper right), and August (lower) 2020. In the August image, there are also 450 random pixels selected for each of the four studied species.</p> <p> </p> <p>Figure 2. The first ten rows of the table containing exported reflectance for 450 randomly selected pixels for each species.</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#methods","title":"Methods","text":""},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#separability-analysis-jeffries-matusita-distance","title":"Separability analysis \u2013 Jeffries-Matusita distance","text":"<p>Separability analysis is often performed on the training data to find out whether all the defined classes will be distinguishable from each other. It can also help to estimate the expected error in the classification for various feature (band) combinations.  Separability measures include, for example, Euclidean distance, Transformed divergence, Mahalanobis distance and its improved variants, Bhattacharyya distance and Jeffries-Matusita distance; for formulas, see Figure 3 (Schowengerdt, 2007).  We will use the Jeffries-Matusita distance (JM distance).  This analysis expects a normal distribution of the input data.  Be aware that there are two formulas (Figure 3) for computing this separability measure.  The most used formula in remote sensing is the one without the square root (Figure 3, variant 2).  This variant is also used in the ENVI software. It can take values in the range [0, 2], where values greater than 1.9 indicate good separability of the classes; in case of separability lower than 1, it is probably a good idea to combine classes.  However, originally the formula was defined with the square root (Figure 3, variant 1), so it means the values in the range [0, \u221a2].  This formula is used, for example, in package varSel in R (Dalponte et al., 2013).  As it is open source code, you can easily edit the function to the variant 2 used in ENVI and Richards (2013); see also Code 1.</p> <p> </p> <p>Figure 3. Formulas for Mahalanobis, Bhattacharyya, and Jeffries-Matusita distances. \u03bc are means and C are covariance matrices. (Richards, 2013; Schowengerdt, 2007).</p> <pre><code>JMdist2 &lt;- function(g,X){\n\n  X&lt;-as.matrix(X)\n\n  nfeat &lt;- ncol(X)\n  nclass &lt;- length(unique(g))\n\n  mu &lt;- by(X,g,colMeans)\n\n  Cov &lt;- by(X,g,stats::cov)\n\n  ncomb &lt;- t(utils::combn(unique(g),2))\n  Bhat &lt;- c()\n  jm &lt;- c()\n  for(j in 1:nrow(ncomb)){\n    mu.i &lt;- mu[[ncomb[j,1]]]\n    cov.i &lt;- Cov[[ncomb[j,1]]]\n    mu.j &lt;- mu[[ncomb[j,2]]]\n    cov.j &lt;- Cov[[ncomb[j,2]]]\n    if(nfeat==1){\n      Bhat[j]&lt;-(1/8)*t(mu.i-mu.j) %*% (solve((cov.i+cov.j)/2)) %*% (mu.i-mu.j) + 0.5*log((((cov.i+cov.j)/2))/(sqrt((cov.i))*sqrt((cov.j))),base=exp(1))\n    }else{\n      Bhat[j]&lt;-(1/8)*t(mu.i-mu.j) %*% (solve((cov.i+cov.j)/2)) %*% (mu.i-mu.j) + 0.5*log(det(((cov.i+cov.j)/2))/(sqrt(det(cov.i))*sqrt(det(cov.j))),base=exp(1))\n    }\n    jm[j] &lt;- (2*(1-exp(-Bhat[j])))\n  }\n\n  return(list(classComb=ncomb,jmdist=jm))\n</code></pre> <p>Code 1. Definition of the function JMdist2 in R: JM distance calculated based on Richards (2013) and how it is used in ENVI, code edited based on https://rdrr.io/cran/varSel/src/R/JMdist.R. Available as a file JMdist2_function.R.</p> <p>JM distance can be calculated for all available bands together, so you will get one separability number based on all used bands (like in the ENVI software).  But it can also be calculated for separate bands; in this case, it shows which bands are better for separating the classes.  With the function defined in Code 1, you can calculate both, but for separate bands, it is necessary to use the function in a loop (Code 2).</p> <pre><code>data=read.delim(\"Rin_grasses_2020_08.txt\");\n\nfix(data)\n\nd=dim(data)\nd\n\nattach(data)\n\n# JM distance for all bands together\nJMdist2(classname, data[,2:55])\n\n# JM distance for separate bands (August example, rename the output text file for other months)\nresults &lt;- matrix() # definition of the empty matrix \nJM_band1 &lt;- JMdist2(classname, data[,2]) # calculation of JM distance for the first band\nJM_band1 # JM distance for the first band, you can copy the combination of classes here, it remains the same also in the loop\nresults &lt;- JM_band1$jmdist # save the JM distance for first band to the empty matrix (it gives the structure to the matrix)\n\nfor (I in 3:55) # the loop for bands 2 - 54\n{\n    JM_band &lt;- JMdist2(classname, data[,I]) # calculate JM distance for band I \n    results &lt;- rbind(results, JM_band$jmdist); # save result for the I band as a new row to the matrix \"results\"\n}\n\nwrite.table(results, \"JM_distance2_bands_August.txt\") # save the results for all the bands to the text file\n</code></pre> <p>Code 2. Calculation of JM distance for all bands together and separate bands; example on August data. Available also as a file Script_JM_distance.R.</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#anova-welchs-t-test-wilcoxon-test","title":"ANOVA, Welch's t-test, Wilcoxon test","text":"<p>Another way to evaluate differences among the classes is to run one of the commonly used statistical tests to find out if the two population means are the same (Welch\u2019s t-test, in comparison to the well-known Student\u2019s t-test, is designed for unequal population variances). That means it has to be done in a loop for every band and for every combination of two classes. For all classes, you can compute the Analysis of variance (ANOVA). It can indicate whether at least one species' reflectance in a given band significantly differs from all others. However, it is unclear whether there are significant differences between all of the classes or whether only one class differs from the others. According to the null hypothesis, the mean reflectance values for the two compared species are the same at a given wavelength. The null hypothesis is rejected when the p-value is less than a pre-specified significance level, which is usually set to 0.05. In a loop, you can save p-values for every band and then visualize the results and interpret them. Both of the above-mentioned methods assume the normality of input data. If the data does not have a normal distribution, an alternative to Welch's t-test is the Wilcoxon rank test.  The example script for both tests is in the attachment <code>Script_ttest_wilcox.txt</code>.</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#results","title":"Results","text":"<p>First, let's examine the results of JM distance for all bands together (see Figure 4).  All of the grasses have good separability throughout the season. Calamagrostis villosa (<code>cv</code>) and Molinia caerulea (<code>mol</code>) are the most separable species in all months,  while Nardus stricta (<code>nard</code>) and Molinia caerulea (<code>mol</code>) are the least separable in June and July;  however, their separability is good in August.  Deschampsia cespitosa (<code>desch</code>) can be distinguished from Nardus stricta (<code>nard</code>) in June and July, but not in August.  Just for comparison, Figure 5 shows ENVI results, for example, for August data; the results are identical.</p> <p> </p> <p>Figure 4. JM distance calculated in R for all bands combined for all months and species combinations (greener = better separability).</p> <p> </p> <p>Figure 5. Separability report from the ENVI software showing JM distance and transformed divergence for the August dataset.</p> <p>Figure 6 shows the results calculated for separate bands (JM distance, T-test, and Wilcoxon test) and average reflectance spectra.  When only one band is used, JM distance values are generally lower (maximal values around 1.7) than when all bands are used (minimal value is 1.7).  JM distance reaches low values for all species combinations in all months in the red edge area (around 720 nm).  When we look at average spectra, there is a crossing of spectral curves.  Similarly, lower JM distances at shorter wavelengths (blue and green) in July can be explained by the similarity of all of the spectral curves.  On the contrary, the largest spectral curve differences between <code>desch</code> and <code>mol</code> in the red band in June and between <code>desch</code> and <code>cv</code> in the near infrared band in August result in the largest JM distances.  Both statistical tests are less sensitive than JM distance, but when one of the tests has a very high p-value (i.e., the null hypothesis of equal mean reflectance values for two compared species is rejected at significance level 0.05), JM distance is also very low.</p> <p> </p> <p>Figure 6. Average spectra (reflectance * 10000), JM distance, and p-values of Welch\u2019s t-test and Wilcoxon rank test for all months and species calculated for separate bands in R.</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#conclusions","title":"Conclusions","text":"<p>In this case study, we proved that all four grass species are separable based on the given hyperspectral dataset in all compared months in a season (June, July, and August).  However, separability based on JM distance is lower for Nardus stricta (<code>nard</code>) and Molinia caerulea (<code>mol</code>) in June and July and for Deschampsia cespitosa (<code>desch</code>) and Nardus stricta (<code>nard</code>) in August.  The best separability in all months is reached for Molinia caerulea (<code>mol</code>) and Calamagrostis villosa (<code>cv</code>).  Separability measured by JM distance based on only one band was generally lower.  The species were usually inseparable at bands around 720 nm, where all the spectral curves meet.  We showed that JM distance is a better indicator of class separability than the commonly used statistical tests such as Welch\u2019s t-test or Wilcoxon rank test.</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#references","title":"References","text":"<p>\u010cerven\u00e1, L., Kupkov\u00e1, L., Pot\u016f\u010dkov\u00e1, M., Lys\u00e1k, J. (2020). SEASONAL SPECTRAL SEPARABILITY OF SELECTED GRASSES: CASE STUDY FROM THE KRKONO\u0160E MTS. TUNDRA ECOSYSTEM. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. XLIII-B3-2020, 371\u2013376. 10.5194/isprs-archives-XLIII-B3-2020-371-2020.</p> <p>Dalponte, M., Orka, H.O., Gobakken, T., Gianelle, D., Naesset, E. (2013). Tree Species Classification in Boreal Forests With Hyperspectral Data. IEEE Trans. Geosci. Remote Sens. 51, 2632\u20132645. 10.1109/TGRS.2012.2216272.</p> <p>Richards, J.A. (2013). Remote sensing digital image analysis: an introduction. Fifth edition. ed. Springer, Berlin.</p> <p>Schowengerdt, R.A. (2007). Remote sensing, models, and methods for image processing. 3rd ed. ed. Academic Press, Burlington, MA.</p>"},{"location":"module4/08_spectral_separability_grass/08_spectral_separability_grass.html#next-unit","title":"Next unit","text":"<p>Proceed with a case study on discrimination of selected grass species from time series of RPAS hyperspectral imagery</p>"},{"location":"software/software_cloudcompare.html","title":"CloudCompare","text":"<p>CloudCompare is an open-source 3D point cloud processing software:  http://www.cloudcompare.org/</p>"},{"location":"software/software_cloudcompare.html#download-and-installation","title":"Download and installation","text":"<p>Download and install CloudCompare from the official website: http://www.cloudcompare.org/</p> <p>We recommend to use the latest stable release.</p>"},{"location":"software/software_cloudcompare.html#getting-started","title":"Getting started","text":"<p>Help for using CloudCompare can be found in the documentation: https://www.cloudcompare.org/doc/wiki/index.php/Main_Page</p> <p>If you are new to CloudCompare, check the tutorials on the website.</p>"},{"location":"software/software_enmap_box.html","title":"EnMAP-Box","text":"<p>EnMAP-box is a QGIS plugin for working with imaging spectroscopy remote sensing data: https://www.enmap.org/data_tools/enmapbox/</p> <p>It is a free and open source software released under the GNU General Public License.</p>"},{"location":"software/software_enmap_box.html#download-and-installation","title":"Download and installation","text":"<p>Follow the instructions for installing the EnMAP-box QGIS plugin Manual for installing the plugin </p> <p>You need to install QGIS and required Python packages first.</p>"},{"location":"software/software_enmap_box.html#getting-started-external-material","title":"Getting started / external material","text":"<p>Introduction to the EnMAP-box and how to handle spectral Iibraries can be found on the EnMAP-box workshop tutorial website. Description of the specific software functions can be found in the User Manual.</p>"},{"location":"software/software_gee.html","title":"Google Earth Engine code editor","text":"<p>Google Earth Engine is a cloud-based geospatial processing service. It allows for large scale processing of large datasets. </p>"},{"location":"software/software_gee.html#registration","title":"Registration","text":"<p>You can register to use Google Earth Engine on this website: https://code.earthengine.google.com/register. You need a Google account.</p> <p>Create the personal cloud project to access Code Editor.</p> <p>If prompted follow the steps as described below:</p> <p>Register a Noncommercial or Commercial Cloud project &gt; Unpaid usage / Project type: Academia &amp; Research or No affiliation &gt; Create a new Google Cloud Project for this course / Set Project-ID and optionally Project Name &gt; Continue to summary.</p>"},{"location":"software/software_gee.html#getting-started","title":"Getting started","text":"<p>We recommend the following introductory material to familiarize yourself with GEE.</p> <p>Mandatory tutorials (in this order):</p> <ol> <li>Get Started with Earth Engine</li> <li>Introduction to JavaScript for Earth Engine part 1</li> <li>Earth Engine Code Editor</li> </ol> <p>Take also Feature Tour to get to know Code Editor environment.</p> <p>Additional materials:  </p> <ul> <li>GEE API docs</li> <li>Introduction to JavaScript for Earth Engine part 2</li> <li>Introduction to JavaScript for Earth Engine part 3</li> <li>StackExchange GEE tagged questions</li> <li>GEE google group</li> </ul> <p>Consider going through this presentation and this tutorial.</p> <p>More resources and tutorials:  </p> <ul> <li>END-TO-END GOOGLE EARTH ENGINE by Ujaval Gandhi</li> </ul>"},{"location":"software/software_python.html","title":"Python","text":"<p>Python is a powerful scripting/programming language widely used in scientific processing and analysis: https://www.python.org/</p>"},{"location":"software/software_python.html#download-and-installation","title":"Download and installation","text":"<p>There are many ways to install Python. We recommend using the fast cross-platform package manager Mamba from the Miniforge distribution, which provides a safe, flexible and fast way for installing Python with different packages and all their dependencies. Instructions for installation on various platforms are available in the Mamba installation guide. Using <code>mamba</code>, a Python environment containing all required packages (modules) can be created, see suggested procedure below.  <code>mamba</code> is a reimplementation of the <code>conda</code> package manager in C++ and is generally faster and better at respolving dependencies. It is a drop-in replacement and uses the same commands and configuration options as <code>conda</code>.</p>"},{"location":"software/software_python.html#required-packages-modules","title":"Required packages (modules)","text":"<p>Different Python packages (modules) are required for in different modules/themes of this course. Find the list of required packages in the <code>.yml</code> files. You can download the yaml files for the respective modules here: Module 1: etrainee_m1.yml  Module 3: etrainee_m3.yml  Module 4: etrainee_m4.yml </p>"},{"location":"software/software_python.html#creating-the-etrainee-python-environment-with-mamba","title":"Creating the etrainee Python environment with Mamba","text":"<p>A recommended practice to set up Python for this course, is to create a Mamba environment with all required modules (packages). If you are using Conda/Mamba, you may use the following procedure to create an <code>etrainee_mX</code> Python environment for each module:</p> <pre><code>mamba env create -f etrainee_m1.yml\nmamba env create -f etrainee_m2.yml\nmamba env create -f etrainee_m3.yml\n</code></pre>"},{"location":"software/software_python.html#getting-started","title":"Getting started","text":"<p>Basic knowledge of programming and Python are required in this course. If you are new to Python, check the resources given in the pre-module. If you are already familiar with some Python basics but not with Conda/Mamba and environments, with Jupyter Notebooks, and with geodata processing in Python then we recommend to go through the Toolbox intro at the beginning of Module 1.</p>"},{"location":"software/software_qgis.html","title":"QGIS","text":"<p>QGIS or Quantum GIS is a popular open source Geospatial Information System (GIS) package. Besides its intuitive user interface, a large portion of its popularity comes from modularity, which enables it to be extended by third party plugins and other GIS packages. Using QGIS is by no means required, though we recommend it as all corresponding exercises in this course use it.</p>"},{"location":"software/software_qgis.html#download-and-installation","title":"Download and installation","text":"<p>Installation instructions for QGIS can be found in the download section of the project website. Opening the download section of the QGIS site might feel overwhelming at first due to the choice of different installer packages. For Windows, we recommend installing QGIS via the \"OSGeo4W Network Installer (64-bit)\", which installs other GIS tools (SAGA GIS, GRASS GIS, etc.) and makes them available via the QGIS user interface. When using the OSGeo4W installer, choose \"Desktop Express Install\" during the installation process. Installation procedure for other operating systems is also well documented on the website.</p>"},{"location":"software/software_qgis.html#getting-started-external-material","title":"Getting started / external material","text":"<p>When starting out with QGIS or GIS in general, a natural starting point might be the well-prepared QGIS training manual with extensive exercises using preprepared data. The training manual covers many advanced topics, however this course only requires knowledge from the first couple of training modules (e.g., modules 1-8).</p> <p>If online video tutorials are more up your alley, we recommend checking out introductory videos by GeoDelta Labs:</p> <p>or Klas Karlsson:</p> <p>Another solid option for an introduction to QGIS is the free \"Getting started with QGIS\" Crash Course by Udemy.</p> <p>Overall we believe that GIS users coming to QGIS from other platforms are not going to struggle, and their learning curve will be mostly centered around looking through the user interface for tools they already know.</p>"},{"location":"software/software_r_language.html","title":"R language","text":"<p>R is a scripting/programming language suitable for statistical computing. It offers a wide range of tools for statistical data analysis, including graphics:</p> <p>https://www.r-project.org/</p> <p>It is a free software developed released under the GNU General Public License.</p>"},{"location":"software/software_r_language.html#download-and-installation","title":"Download and installation","text":"<p>R can be extended by RStudio, an integrated development environment (IDE) which makes working in R significantly easier.</p> <p>Firstly, install the core R package from the Comprehensive R Archive Network (CRAN) website:</p> <p>https://cran.r-project.org</p> <p>The installation and administration guidelines are available on the R-project homepage:  </p> <p>https://cran.r-project.org/doc/manuals/r-release/R-admin.html</p> <p>After installing R, RStudio can be downloaded and installed from:</p> <p>https://posit.co/download/rstudio-desktop/</p> <p>We recommend installing everything with default settings.</p>"},{"location":"software/software_r_language.html#module-2-environment","title":"Module 2 environment","text":"<p>Note: If you encounter any errors during each of installation steps you may need to temporarily disable your firewall or anti-virus software.</p> <p>Module 2 contents use R version 4.3.1 \"Beagle Scouts\". Choose files appropriate for your operating system here https://cran.r-project.org/. For Windows version use this link to download installer: https://cran.r-project.org/bin/windows/base/R-4.3.1-win.exe.</p> <p>If you use R on Windows some packages may need RTools software. Download and install version 4.3 of RTools from this website: https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html</p> <p>A recommended way to setup working environment in RStudio is to use renv (short for reproducible environment) package. Project created using these instructions will contain appropriate versions of packages used to develop and run scripts in Module 2.</p> <p>Instructions</p> <p>Step 1. Before creating R environment make sure you have downloaded Module 2 data packages from Zenodo.  </p> <p>Step 2. Create a new catalogue and name it module2. Move the downloaded data packages to this folder. Unzip the data inside this catalogue and remove module2_ from the folder names. This is what your module2 catalogue should look like after completing Step 2.</p> <p> <p></p> <p>Step 3. Launch RStudio Step 4. Check out you R version. After you launch RStudio the first line in the console should read</p> <pre><code>R version 4.3.1 (2023-06-16 ucrt) -- \"Beagle Scouts\"\n</code></pre> <p>If you have different R versions installed make sure to change version to R 4.3.1 as shown below</p> <p>Click on the GIF to open full size version in a new tab</p> <p> </p> <p>Step 5. Create a New Project. Connect it with existing directory. Change the path to module2 data catalog, in which you unzipped packages with data. </p> <p>Click on the GIF to open full size version in a new tab</p> <p> </p> <p>Step 6. Install <code>renv</code> using <code>install.packages(\"renv\")</code> command in the console  Step 7. Load <code>renv</code> package executing <code>library(renv)</code> in the console  Step 8. Use <code>renv::init()</code> to initialize <code>renv</code> within a project created in Step 5. Step 9. Replace the default <code>renv.lock</code> file inside the project folder with the downloaded one: download renv.lock Step 10. Use <code>renv::restore()</code> to install specific package versions recorded in the lockfile. When prompted type <code>y</code> in the console to install packages recorded in <code>renv.lock</code> file  </p> <p>By following these steps you ensure that you will work on the most up-to-date environment that the contents of Module 2 use. To work in a project just open <code>.Rproj</code> file. This action will open RStudio with you environment setup.</p> <p>If you followed the recommended steps this is the content of module2 folder you should be seeing. Note that after unzipping you can safely remove <code>.zip</code> packages from this folder.</p> <p> <p></p> <p>Note: To ensure compatibility and consistent results across exercises, please adhere to the following specified versions of R and packages. Running exercises on different versions may lead to unexpected results and potential inconsistencies and errors.</p>"},{"location":"software/software_r_language.html#getting-started-external-material","title":"Getting started / external material","text":"<p>If you are new to R / programming, consider going through one or more of these tutorials first:</p> <p>Introduction to the R language and statistical analysis: https://cran.r-project.org/doc/manuals/r-release/R-intro.html</p> <p>Instructions on how to import and export data using R and respective packages: https://cran.r-project.org/doc/manuals/r-release/R-data.html</p> <p>Friendly introduction to R programming and using R for data analysis can be found in these books:</p> <p>'Hands-On Programming with R' by Garrett Grolemund</p> <p>'R for Data Science (2e)' by Hadley Wickham, Mine \u00c7etinkaya-Rundel, and Garrett Grolemund</p> <p>'Geocomputation with R' by Robin Lovelace, Jakub Nowosad and Jannes Muenchow</p>"}]}