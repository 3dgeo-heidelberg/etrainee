{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deluxe-luxembourg",
   "metadata": {},
   "source": [
    "# Random Forest (RF) hyperspectral data classification\n",
    "\n",
    "In this notebook, you will train and apply a common Machine Learning classifier - Random Forest for classification of hyperspectral data from Luční Hora, Krkonoše mountains, Czechia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5928ce5d-8cf8-438b-a544-9769100281cc",
   "metadata": {},
   "source": [
    "- *Prerequisities* (This notebook can be run either online using Google Colab or on your local machine)\n",
    "\t- A Google account for accessing Google Colab BROKEN LINK([link to notebook](https://colab.research.google.com/drive/17LVAThXbv9sOkGL7JCwRjzSAxumnigrs)). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.\n",
    "\t\n",
    "\tor\n",
    "\t- A Python environment with the necessary libraries ([manual](../../software/software_python.md)). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.\n",
    "\n",
    "    - Downloaded data ([module4/theme4_exercise_ml_cnn]())\n",
    "    The dataset consists of:\n",
    "        + Hyperspectral RPAS imagery of Luční Hora, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: *LH_202008_imagery.tif*\n",
    "        + a raster with reference data: *LH_202008_reference.tif*\n",
    "        + Pretrained models and corresponding classified rasters: _/sample_results/*_\n",
    "         \n",
    "- *Tasks*\n",
    "    - Preprocess imagery for Machine Learning\n",
    "    - Classify the hyperspectral image using RF\n",
    "\t- Observe how hyperparameter values alter classification results\n",
    "\t- Evaluate your results and compare to our pretrained classifiers\n",
    "    - *Optional:* Classify a urban scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-california",
   "metadata": {},
   "source": [
    "## Structure of this exercise\n",
    "\n",
    "What are you going to encounter during this exercise.\n",
    "\n",
    "0. Load libraries, set paths\n",
    "1. Load and Preprocess training data\n",
    "2. Create/Fit Random Forest\n",
    "3. Apply Classifier\n",
    "4. Evaluate Result\n",
    "5. Sample Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-anger",
   "metadata": {},
   "source": [
    "## 0. Load external libraries and set paths\n",
    "\n",
    "First, we need to import external libraries:\n",
    "\n",
    "- __numpy__ - Arrays to hold our data\n",
    "- __matplotlib.pyplot__ - Draw images\n",
    "\n",
    "- __sklearn.svm / sklearn.ensemble__ - Machine Learning Classifiers\n",
    "- __sklearn.model_selection__ - Cross-validation and hyperparameter tuning implemented in scikit-learn\n",
    "- __sklearn.metrics__ - Compute accuracy metrics using scikit-learn\n",
    "- __sklearn.preprocessing__ - Normalizing input data using scikit-learn\n",
    "\n",
    "- __time.perf_counter__ - Track how long individual functions take to run\n",
    "- __os.path__ - Path manipulation\n",
    "- __tqdm__ - show progress bars during training\n",
    "- __joblib__ - Saving and loading trained classifiers\n",
    "\n",
    "- __image_preprocessing__ - Our library holding functions for image tiling, preprocessing, etc.\n",
    "- __inference_utils__ - Our library for correctly exporting classifed images\n",
    "- __visualisation_utils__ - Our library for visualising the data\n",
    "\n",
    "Two external libraries are not imported directly in this notebook, but are used by functions in _image_preprocessing_ and _inference_utils_:\n",
    "\n",
    "- __gdal__ - Manipulates spatial data\n",
    "- __scipy.io__ - Reads .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import perf_counter\n",
    "from os.path import join\n",
    "import scipy\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "\n",
    "# from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tqdm import notebook as tqdm\n",
    "from joblib import dump, load\n",
    "\n",
    "import image_preprocessing\n",
    "import inference_utils\n",
    "import visualisation_utils\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset name (used by visualisation functions) - 'krknonose' or 'pavia_centre'\n",
    "# default: 'pavia_centre'\n",
    "ds_name = 'krkonose'\n",
    "\n",
    "# Get a list of class names\n",
    "_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e681f8da",
   "metadata": {},
   "source": [
    "Please fill correct paths to your training and reference rasters (just pointing the _root_path_ variable to the project folder should do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'f:/datasets/etrainee'\n",
    "# root_path = 'C:/folder/where/this/project/is/saved'\n",
    "\n",
    "# PATHS TO TRAINING DATA\n",
    "# Krkonose\n",
    "imagery_path   = join(root_path, 'LH_202008_imagery.tif')\n",
    "reference_path = join(root_path, 'LH_202008_reference.tif')\n",
    "# Pavia\n",
    "#imagery_path = join(root_path, 'Pavia.mat')\n",
    "#reference_path   = join(root_path, 'Pavia_gt.mat')\n",
    "\n",
    "# PATH TO SAVE MODELS\n",
    "model_save_folder = join(root_path, 'models')\n",
    "\n",
    "# PATH TO SAVE CLASSIFIED IMAGE\n",
    "out_path  = join(root_path, 'results/Krkonose_RF.tif')\n",
    "\n",
    "# PATH TO THE SAMPLE RESULTS\n",
    "sample_result_path  = join(root_path, 'sample_results/RF_sample_result.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-velvet",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-tongue",
   "metadata": {},
   "source": [
    "### 1.1. Data loading into NumPy\n",
    "Let's start by reading an image into a numpy array, we do this in the background using GDAL.\n",
    "\n",
    "The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys _imagery_ and _reference_. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.\n",
    "\n",
    "For loading most raster datasets, we created a _read_gdal()_ function in the _image_preprocessing_ module. But loading .mat files for the Pavia City Centre requires a specific function (_read_pavia_centre()_). Both _read_pavia_centre()_ and _read_gdal()_ return a dictionary containing two numpy arrays with keys _imagery_ and _reference_.\n",
    "\n",
    "If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile for neural networks, we crop the image to (1088, 1088, 102) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n",
    "#loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n",
    "    # train_path, out_shape=(1088, 1088, 102))\n",
    "\n",
    "print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\n",
    "print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_utils.show_img_ref(loaded_raster[\"imagery\"][:, :, [25, 15, 5]],\n",
    "                                 loaded_raster[\"reference\"], ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-harris",
   "metadata": {},
   "source": [
    "### 1.2. Flatten array\n",
    "We will be using SVM and RF to classify individual pixels, therefore we can transform the 3D image (height, width, spectral bands) to a 2D array (length, spectral bands). This transformation destroys spatial relationships within the image, however the classifiers can only use 1D features anyway and it simplifies the next step (filtering NoData)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_shape = loaded_raster['imagery'].shape\n",
    "\n",
    "flat_arrs = {}\n",
    "flat_arrs['imagery'] = loaded_raster['imagery'].reshape(\n",
    "    orig_shape[0]*orig_shape[1], orig_shape[2])\n",
    "flat_arrs['reference'] = loaded_raster['reference'].reshape(\n",
    "    orig_shape[0]*orig_shape[1])\n",
    "\n",
    "print(f'The flat imagery array has shape {flat_arrs[\"imagery\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-federal",
   "metadata": {},
   "source": [
    "### 1.3. Filter out NoData pixels\n",
    "\n",
    "We can only train the classifier on pixels with a reference value, therefore we remove all pixels belonging to class 0 (NoData). This operation reduces our training dataset from ~1.18 milion to ~150 thousand pixels. We then visualise the spectral curves of individual pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_arrs = {}\n",
    "filtered_arrs['imagery'] = flat_arrs['imagery'][flat_arrs['reference'] > 0]\n",
    "filtered_arrs['reference'] = flat_arrs['reference'][flat_arrs['reference'] > 0]\n",
    "\n",
    "print(f'The filtered array has shape {filtered_arrs[\"imagery\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_number = 1000\n",
    "visualisation_utils.show_spectral_curve(filtered_arrs, pixel_number,\n",
    "                                        ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-simpson",
   "metadata": {},
   "source": [
    "### 1.4. Data scaling\n",
    "After filtering the training data, we can move onto data scaling. In Machine Learning, it is common to scale all features before classification, because many classifiers assume that all features vary on comparable scales  and that each feature has values close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(flat_arrs['imagery'])\n",
    "\n",
    "scaled_arrs = {\n",
    "    'imagery': scaler.transform(filtered_arrs['imagery']),\n",
    "    'reference': filtered_arrs['reference']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-provincial",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pixel_number = 5\n",
    "visualisation_utils.show_spectral_curve(scaled_arrs, pixel_number,\n",
    "                                        ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557eff42-b59b-4a65-a735-14db1ae77464",
   "metadata": {},
   "source": [
    "### 1.5. Splitting data for training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55004d6d-eff7-4e06-afc3-7346871ba4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 1/3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_arrs['imagery'], scaled_arrs['reference'], train_size=train_fraction)\n",
    "\n",
    "\n",
    "training = {'imagery': X_train, 'reference': y_train}\n",
    "test     = {'imagery': X_test,  'reference': y_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-christopher",
   "metadata": {},
   "source": [
    "### 1.5. Subsetting the training data\n",
    "With the whole training dataset, the training procedure may take relatively long. Instead we randomly select a number of samples from each class for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many training samples do we have for each class?\n",
    "unique, counts = np.unique(filtered_arrs['reference'], return_counts=True)\n",
    "print(f'The individual classes contain {counts} training pixels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples per class do we pick?\n",
    "n_class_samples = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array for random indices\n",
    "indices_random = np.zeros((n_class_samples*len(unique)), dtype=np.uint32)\n",
    "sorted_reference = np.argsort(filtered_arrs['reference'])\n",
    "\n",
    "# Randomly select the same number of indices from each class\n",
    "start = 0\n",
    "for idx in range(len(counts)):\n",
    "    indices_random[(idx*n_class_samples):((idx+1)*n_class_samples)] = np.random.choice(\n",
    "        sorted_reference[start:(start+counts[idx])], size=n_class_samples, replace=False)\n",
    "    start += counts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the original arrays using the random indices\n",
    "subset_arrs = {}\n",
    "subset_arrs['imagery'] = filtered_arrs['imagery'][indices_random, :]\n",
    "subset_arrs['reference'] = filtered_arrs['reference'][indices_random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-parameter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check how many pixels are in each class now\n",
    "unique, counts = np.unique(subset_arrs['reference'], return_counts=True)\n",
    "print(f'Now, the individual classes contain {counts} training pixels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6d07b",
   "metadata": {},
   "source": [
    "Let's visualise some of the pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-manitoba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pixel_number = 300\n",
    "visualisation_utils.show_spectral_curve(subset_arrs, pixel_number,\n",
    "                                        ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-fence",
   "metadata": {},
   "source": [
    "## 2. Random Forest definition and training\n",
    "\n",
    "After preprocessing our data, we can move onto defining a machine learning model. You can either train your own classifiers or use ones we already trained for you (_sample_results/RF_sample_trained.joblib_). In case you are using the pretrained RF, skip ahead to section 2.3.\n",
    "\n",
    "This training uses a Random Forest implementation from scikit-learn, a popular Machine Learning library for Python. The documentation is available at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-percentage",
   "metadata": {},
   "source": [
    "### 2.1. Find most suitable parameters\n",
    "\n",
    "To function proprely, RF has to have suitable values for some hyperparameters. A common approach is to train classifiers with different hyperparameter values and select the most suitable ones.\n",
    "\n",
    "Scikit-Learn makes this easy using RandomizedSearch or GridSearch, these functions train the classifier multiple times using different hyperparameter values and determine the most suitable combination. Each combination of hyperparameter values is tried multiple times using cross-validation (out-of-sample testing).\n",
    "\n",
    "Run either the cell with RandomizedSearchCV or with GridSearchCV, while Grid Search may be able to find better hyperparameters, Randomized Search will likely also find good solutions in a much shorter amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define potential parameter values for the SVM\n",
    "parameters_rf = {\n",
    "    'n_estimators': [50, 100, 250, 500, 750], # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 10, 20, 50],          # Maximum depth of a tree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-starter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the optimizer and run the optimization\n",
    "opt = RandomizedSearchCV(RandomForestClassifier(), parameters_rf, cv=5, scoring=\"jaccard_macro\", n_iter=8, refit=False, n_jobs=-2, verbose=4)\n",
    "opt.fit(X=training['imagery'], y=training['reference'])\n",
    "print(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer and run the GridSerach optimization\n",
    "opt = GridSearchCV(RandomForestClassifier(), parameters_rf, cv=5, scoring=\"jaccard_micro\", refit=False, n_jobs=-2, verbose=4)\n",
    "opt.fit(X=training['imagery'], y=training['reference'])\n",
    "print(f'The optimisation process identified these parameters as the most suitable: {opt.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-disaster",
   "metadata": {},
   "source": [
    "### 2.2. Fit\n",
    "The best hyperparameter values identified during cross-validation are then used for training the model on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(**opt.best_params_)\n",
    "\n",
    "rf.fit(X=training['imagery'], y=training['reference'])\n",
    "rf.score(training['imagery'], training['reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-terry",
   "metadata": {},
   "source": [
    "### 2.3. Save/load trained RF for potential future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-uncle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save using joblib.dump(object, filename)\n",
    "model_path = join(model_save_folder, 'RF.joblib')\n",
    "dump(rf, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load using joblib.load(filename)\n",
    "rf = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-communications",
   "metadata": {},
   "source": [
    "## 3. Model application & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-conditions",
   "metadata": {},
   "source": [
    "### 3.1. Loading and preprocessing the data\n",
    "\n",
    "Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.\n",
    "\n",
    "By default, the training raster (_imagery_path_) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raster\n",
    "raster = image_preprocessing.read_gdal_with_geoinfo(imagery_path, (0,0))\n",
    "\n",
    "# Flattern spatial dimension of the raster\n",
    "raster_shape = raster['imagery'].shape\n",
    "raster_flat = raster['imagery'].reshape(raster_shape[0]*raster_shape[1],\n",
    "                                        raster_shape[2])\n",
    "\n",
    "# Preprocess(scale) the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(raster_flat)\n",
    "raster_scaled = scaler.transform(raster_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d63b9",
   "metadata": {},
   "source": [
    "### 3.2. Applying the classifier\n",
    "\n",
    "The following snippet applies the classifier to the loaded imagery, and then transforms the flattened array back into a raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_flat = rf.predict(raster_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_raster = predicted_flat.reshape(raster_shape[0], raster_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda8415",
   "metadata": {},
   "source": [
    "You can also visualise the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed94cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n",
    "                                    loaded_raster['reference'],\n",
    "                                    predicted_raster, ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-alert",
   "metadata": {},
   "source": [
    "### 3.3. Export resulting raster\n",
    "\n",
    "Export the resulting classified raster into _out_path_ for distribution or further analysis (e.g. validation in GIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_utils.export_result(out_path, predicted_raster, raster['geoinfo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40337fd0",
   "metadata": {},
   "source": [
    "## 4. Evaluate Classification Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-bunch",
   "metadata": {},
   "source": [
    "### 4.1. Apply classifier to the test dastaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = rf.predict(test['imagery'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-picking",
   "metadata": {},
   "source": [
    "### 4.2. Compute accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b5f75-b652-4817-9f97-0d5622412c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test['reference'], test_predicted,\n",
    "      target_names=class_names[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3352cd4",
   "metadata": {},
   "source": [
    "### 4.3. Show Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_utils.show_confusion_matrix(test['reference'], test_predicted,\n",
    "                                          ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd86483",
   "metadata": {},
   "source": [
    "## 5. Sample Solution\n",
    "\n",
    "We have generated this result using these training parameters (please note that just using the same training parameters will not yield the same result):\n",
    "\n",
    "- Number of trees: 750\n",
    "- Maximium tree depth: 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575fd4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read test reference\n",
    "test_arr = image_preprocessing.read_gdal(imagery_path, test_path)\n",
    "test_flat = test_arr['reference'].reshape(\n",
    "    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\n",
    "test_filtered = test_flat[test_flat > 0]\n",
    "\n",
    "# Read sample result\n",
    "sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\n",
    "sample_flat = sample_arr['reference'].reshape(\n",
    "    sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\n",
    "sample_filtered = sample_flat[test_flat > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the sample result\n",
    "visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n",
    "                                    loaded_raster['reference'],\n",
    "                                    sample_arr['reference'],\n",
    "                                    ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a classification report for the sample result\n",
    "print(classification_report(test_filtered, sample_filtered,\n",
    "      target_names=class_names[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a Confusion matrix for the sample result\n",
    "visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n",
    "                                          ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34bacaf-94d8-4a08-adf9-8f5a01b7a43a",
   "metadata": {},
   "source": [
    "## *Optional:* Classify a urban scene\n",
    "\n",
    "Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.\n",
    "\n",
    "Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University). You will need to change the paths to input data and use the *read_pavia_centre* method to load the matlab matrices into numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59076e22-9107-45e1-8851-81dfc7bcc054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
