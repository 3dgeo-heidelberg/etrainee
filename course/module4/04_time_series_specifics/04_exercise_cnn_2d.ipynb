{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a 2D Convolutional Neural Network for hyperspectral data classification\n",
    "\n",
    "In this notebook, you will train and apply a two-dimensional convolutional neural network for classification of hyperspectral data from Bílá Louka, Krkonoše mountains, Czechia. Please start by reading about the dataset and area of interest [here](../../data_usecases/usecase_grasses_krkonose.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Prerequisities* (This notebook can be run either online using Google Colab or on your local machine)\n",
    "\t- A Google account for accessing Google Colab ([link to notebook](https://drive.google.com/file/d/1IHSOpCqnSd6e1frbd5OjYqC52Iem3-Y1/view?usp=sharing)). If running the exercise in Google Colab, please copy the notebook to your own Google Drive and follow the exercise, you don't need to download the dataset.\n",
    "\t\n",
    "\tor\n",
    "\t- A Python environment with the necessary libraries ([manual](../../software/software_python.md)). Download this notebook (click the download button in the top right corner of the page) and follow the exercise.\n",
    "\n",
    "    - Downloaded data ([module4.zip/theme4_exercise_machine_learning](https://doi.org/10.5281/zenodo.10003574))\n",
    "    The dataset consists of:\n",
    "        + Hyperspectral RPAS imagery of Bílá Louka, Czechia (50.728N, 15.682E) acquired in August of 2020 and resampled to 54 spectral bands with ground sampling distance of 9 cm: *BL_202008_imagery.tif*\n",
    "        + a raster with reference data: *BL_202008_reference.tif*\n",
    "        + Pretrained models and corresponding classified rasters: _/sample_results/*_\n",
    "         \n",
    "- *Tasks*\n",
    "    - Preprocess imagery for Deep Learning\n",
    "    - Classify the hyperspectral image using 2D CNN\n",
    "\t- Observe how hyperparameter values alter classification results\n",
    "\t- Evaluate your results and compare to our pretrained classifier\n",
    "    - *Optional:* Classify a urban scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of this exercise\n",
    "\n",
    "What are you going to encounter during this exercise.\n",
    "\n",
    "0. Load libraries, set paths\n",
    "1. Load and Preprocess training data\n",
    "2. Neural Network Definition / Training\n",
    "3. Apply Network\n",
    "4. Evaluate Result\n",
    "5. Sample Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load external libraries and set paths\n",
    "\n",
    "First, we need to import external libraries:\n",
    "\n",
    "- __torch, torch.nn, torch.optim, torchnet__ - Pytorch related libraries for deep learning\n",
    "- __numpy__ - Arrays to hold our data\n",
    "- __matplotlib.pyplot__ - Draw images\n",
    "- __sklearn.model_selection__ - Cross-validation implemented in scikit-learn\n",
    "- __sklearn.metrics__ - Compute accuracy metrics using scikit-learn\n",
    "- __time.perf_counter__ - Track how long individual functions take to run\n",
    "- __os.path__ - Path manipulation\n",
    "- __tqdm__ - show progress bars during training\n",
    "\n",
    "- __etrainee_m4_utils.image_preprocessing__ - Our library holding functions for image tiling, preprocessing, etc.\n",
    "- __etrainee_m4_utils.inference_utils__ - Our library for correctly exporting classifed images\n",
    "- __etrainee_m4_utils.visualisation_utils__ - Our library for visualising the data\n",
    "\n",
    "Two external libraries are not imported directly in this notebook, but are used by functions in _image_preprocessing_ and _inference_utils_:\n",
    "\n",
    "- __gdal__ - Manipulates spatial data\n",
    "- __scipy.io__ - Reads .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchnet as tnt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from time import perf_counter\n",
    "from tqdm import notebook as tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "\n",
    "from etrainee_m4_utils import image_preprocessing\n",
    "from etrainee_m4_utils import inference_utils\n",
    "from etrainee_m4_utils import visualisation_utils\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset name (used by visualisation functions) - 'bila_louka' or 'pavia_centre'\n",
    "# default: 'pavia_centre'\n",
    "ds_name = 'bila_louka'\n",
    "\n",
    "# Get a list of class names\n",
    "_, class_names = visualisation_utils._create_colorlist_classnames(ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill correct paths to your training and reference rasters (just pointing the _root_path_ variable to the project folder should do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'C:/folder/where/this/project/is/saved'\n",
    "\n",
    "# PATHS TO TRAINING DATA\n",
    "# Krkonose\n",
    "imagery_path   = join(root_path, 'BL_202008_imagery.tif')\n",
    "reference_path = join(root_path, 'BL_202008_reference.tif')\n",
    "# Pavia\n",
    "#imagery_path   = join(root_path, 'Pavia.mat')\n",
    "#reference_path = join(root_path, 'Pavia_gt.mat')\n",
    "\n",
    "# PATH TO SAVE MODELS\n",
    "model_save_folder = join(root_path, 'models')\n",
    "\n",
    "# PATH TO SAVE CLASSIFIED IMAGE\n",
    "out_path = join(root_path, 'results/Krkonose_2D_CNN.tif')\n",
    "\n",
    "# PATH TO THE SAMPLE RESULTS\n",
    "sample_result_path = join(root_path, 'sample_results/2D_CNN_sample_result.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and preprocessing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data loading into NumPy\n",
    "Let's start by reading an image into a numpy array, we do this in the background using either scipy or GDAL.\n",
    "\n",
    "The result of our function is a dictionary named loaded_raster, which contains two numpy arrays under keys _imagery_ and _reference_. As we can see, the loaded hyperspectral dataset has 1088 by 1088 pixels with 54 spectral bands. The raster containing our reference data has the same dimensions in height and width.\n",
    "\n",
    "For loading most raster datasets, we created a _read_gdal()_ function in the _image_preprocessing_ module. But loading .mat files for the Pavia City Centre requires a specific function (_read_pavia_centre()_). Both _read_pavia_centre()_ and _read_gdal()_ return a dictionary containing two numpy arrays with keys _imagery_ and _reference_.\n",
    "\n",
    "If using the Pavia City Centre dataset, you may notice that the original image has a shape of (1096, 1096, 102), but to make the data easier to tile, we crop the image to (1088, 1088, 102) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_raster = image_preprocessing.read_gdal(imagery_path, reference_path)\n",
    "# loaded_raster = image_preprocessing.read_pavia_centre(imagery_path,\n",
    "    # reference_path, out_shape=(1088, 1088, 102))\n",
    "\n",
    "print(f'Tiled imagery shape {loaded_raster[\"imagery\"].shape}')\n",
    "print(f'Tiled reference shape {loaded_raster[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_utils.show_img_ref(loaded_raster['imagery'][:, :, [25, 15, 5]],\n",
    "                                 loaded_raster['reference'],\n",
    "                                 ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Image tiling\n",
    "We have our data loaded into a numpy array, the next step is to divide the image into individual tiles, which will be the input for our neural network.\n",
    "\n",
    "As we want to perform convolution only in the spatial dimensions, we need to divide the hyperspectral image into tiles of a given shape. Standard tile sizes are based on multiples of two, for example 2^8 = 256.\n",
    "\n",
    "_Overlap_ is important as there could be inconsistencies (\"jagged\" edges) on boundaries of classified tiles, we therefore combine results of overlapping tiles to avoid the inconsistencies.\n",
    "\n",
    "As you can see, tiling procedure transformed the original 1088x1088 image into 256 tiles of 128x128 pixels, with the original number of spectral bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_shape = (128, 128)\n",
    "overlap = 64\n",
    "\n",
    "dataset_tiles = image_preprocessing.tile_training(loaded_raster,\n",
    "                                                  tile_shape,\n",
    "                                                  overlap)\n",
    "print(f'Tiled imagery shape {dataset_tiles[\"imagery\"].shape}')\n",
    "print(f'Tiled reference shape {dataset_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tile filtration\n",
    "\n",
    "However, some of the created tiles do not contain training data, we therefore need to filter them and only keep the tiles with a field-collected reference.\n",
    "\n",
    "This process reduces the size of our dataset from 256 to 226 tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_tiles = image_preprocessing.filter_useful_tiles(dataset_tiles,\n",
    "                                                         nodata_vals=[0],\n",
    "                                                         is_training=True)\n",
    "print(f'Filtered imagery shape {filtered_tiles[\"imagery\"].shape}')\n",
    "print(f'Filtered reference shape {filtered_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Data normalization\n",
    "After filtering the tiles to only include training data, we can move onto a final part of the preprocessing - data normalization. In Machine Learning, it is common to normalize all data before classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tiles, unique, counts = image_preprocessing.normalize_tiles(filtered_tiles, nodata_vals=[0], is_training=True)\n",
    "print(f'Preprocessed imagery shape {normalized_tiles[\"imagery\"].shape}')\n",
    "print(f'Preprocessed reference shape {normalized_tiles[\"reference\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Splitting data for training/testing\n",
    "Our reference dataset has to be split into three groups:\n",
    "\n",
    "*training / validation / test*\n",
    "\n",
    "In this step we divide the data into train+val and test groups. The variable _train_fraction_ is used to establish how big of a fraction of the reference dataset is used for training and validation. By default, a third of the referece data is useed for training and validation, while the other two thirds are used for testing model preformance. The splitting is done using a scikit learn function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split), which performs a stratified sampling from all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 1/3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_tiles['imagery'], normalized_tiles['reference'], train_size=train_fraction)\n",
    "\n",
    "training = {'imagery': X_train, 'reference': y_train}\n",
    "test  = {'imagery': X_test,  'reference': y_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Conversion to Pytorch Tensors\n",
    "The resulting preprocessed tiles are subsequently transformed from numpy arrays into pytorch tensors for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tnt.dataset.TensorDataset([training['imagery'],\n",
    "                                     training['reference']])\n",
    "\n",
    "print(f'Class labels: \\n{unique}\\n')\n",
    "print(f'Number of pixels in a class: \\n{counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network definition\n",
    "\n",
    "After preprocessing our data, we can move onto defining our neural network and functions for training. You can either train your own neural network or use the one we already trained for you (_sample_results/2D_CNN_sample_trained.pt_). In case you are using the pretrained network, please run only the following code snippet (2.1.) and skip ahead to section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Network structure\n",
    "\n",
    "Our network is named SpatialNet, and it's based on the popular U-Net / SegNet architectures. The Structure of our network is defined in the SpatialNet class, which has three methods:\n",
    "- **__init__** - This method runs automatically when defining an instance of the class, it defines individual layers of the networks (2D convolutions, transposed convolutions, maxpooling and also a dropout layer).\n",
    "- **init_weights** - Randomly initialising network weights based on a normal distribution.\n",
    "- **forward** - Defining how data should flow through the network during a forward pass (network structure definition). The PyTorch library automatically creates a method for backward passes based on this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialNet(nn.Module):\n",
    "    \"\"\"U-Net for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        Initialize the U-Net model.\n",
    "\n",
    "        n_channels, int, number of input channel\n",
    "        size_e, int list, size of the feature maps of convs for the encoder\n",
    "        size_d, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        super(SpatialNet, self).__init__(\n",
    "        )  # necessary for all classes extending the module class\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, 2, return_indices=False)\n",
    "        self.dropout = nn.Dropout2d(p=0.5, inplace=True)\n",
    "\n",
    "        self.n_channels = args['n_channel']\n",
    "        self.size_e = args['size_e']\n",
    "        self.size_d = args['size_d']\n",
    "        self.n_class = args['n_class']\n",
    "        \n",
    "        def conv_layer_2d(in_ch, out_ch, k_size=3, conv_bias=False):\n",
    "            \"\"\"Create default conv layer.\"\"\"\n",
    "            return nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=k_size,\n",
    "                                           padding=1, padding_mode='reflect',\n",
    "                                           bias=conv_bias),\n",
    "                                 nn.BatchNorm2d(out_ch), nn.ReLU())\n",
    "\n",
    "        # Encoder layer definitions\n",
    "        self.c1 = conv_layer_2d(self.n_channels, self.size_e[0])\n",
    "        self.c2 = conv_layer_2d(self.size_e[0], self.size_e[1])\n",
    "        self.c3 = conv_layer_2d(self.size_e[1], self.size_e[2])\n",
    "        self.c4 = conv_layer_2d(self.size_e[2], self.size_e[3])\n",
    "        self.c5 = conv_layer_2d(self.size_e[3], self.size_e[4])\n",
    "        self.c6 = conv_layer_2d(self.size_e[4], self.size_e[5])\n",
    "        \n",
    "        # Decoder layer definitions\n",
    "        self.trans1 = nn.ConvTranspose2d(self.size_e[5], self.size_d[0],\n",
    "                                         kernel_size=2, stride=2)\n",
    "        self.c7 = conv_layer_2d(self.size_d[0], self.size_d[1])\n",
    "        self.c8 = conv_layer_2d(self.size_d[1], self.size_d[2])\n",
    "        self.trans2 = nn.ConvTranspose2d(self.size_d[2], self.size_d[3],\n",
    "                                         kernel_size=2, stride=2)\n",
    "        self.c9 = conv_layer_2d(self.size_d[3], self.size_d[4])\n",
    "        self.c10 = conv_layer_2d(self.size_d[4], self.size_d[5])\n",
    "\n",
    "        # Final classifying layer\n",
    "        self.classifier = nn.Conv2d(self.size_d[5], self.n_class,\n",
    "                                    1, padding=0)\n",
    "\n",
    "        # Weight initialization\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "\n",
    "        # Put the model on GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def init_weights(self, layer):  # gaussian init for the conv layers\n",
    "        \"\"\"Initialise layer weights.\"\"\"\n",
    "        nn.init.kaiming_normal_(\n",
    "            layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Define model structure.\"\"\"\n",
    "        # Encoder\n",
    "        # level 1\n",
    "        x1 = self.c2(self.c1(input_data))\n",
    "        x2 = self.maxpool(x1.clone())\n",
    "        # level 2\n",
    "        x3 = self.c4(self.c3(x2.clone()))\n",
    "        x4 = self.maxpool(x3.clone())\n",
    "        # level 3\n",
    "        x5 = self.c6(self.c5(x4.clone()))\n",
    "\n",
    "        # Decoder\n",
    "        # level 2\n",
    "        y4 = self.trans1(x5.clone())\n",
    "        y3 = self.c8(self.c7(y4.clone()))\n",
    "        # level 1\n",
    "        y2 = self.trans2(y3.clone())\n",
    "        y1 = self.c10(self.c9(y2.clone()))\n",
    "        # Output\n",
    "        out = self.classifier(self.dropout(y1.clone()))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Functions for network training\n",
    "\n",
    "Training the network is handled by four functions:\n",
    "- __augment__ - Augments the training data by adding random noise.\n",
    "- __train__ - Trains the network for one epoch.\n",
    "- __eval__ - Evaluates the results on a validation set.\n",
    "- __train_full__ - Performs the full training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__augment__ takes in the training tile and the corresponding reference labels. It then adds a random value (taken from a normal distribution) at each wavelength and thus slightly modifies the training data. Change _tile_number_ to see the augmentation effect for different tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(obs, g_t):\n",
    "    \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"\n",
    "    sigma, clip= 0.005, 0.02\n",
    "\n",
    "    # Random noise\n",
    "    rand = torch.clamp(torch.mul(sigma, torch.randn([1, 54, tile_shape[0],tile_shape[1]])), -clip, clip)\n",
    "    obs = torch.add(obs, rand)\n",
    "\n",
    "    # Random rotation 0 90 180 270 degree\n",
    "    n_turn = np.random.randint(4) #number of 90 degree turns, random int between 0 and 3\n",
    "    obs = torch.rot90(obs, n_turn, dims=(2,3))\n",
    "    g_t = torch.rot90(g_t, n_turn, dims=(1,2))\n",
    "\n",
    "    return obs, g_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tile_number = 60\n",
    "visualisation_utils.show_augment_spatial(training, tile_number,\n",
    "                                         augment, ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__train__ trains the network for one epoch. This function contains a for loop, which loads the training data in individual batches. Each batch of training data goes through the network, after which we compute the loss function (cross-entropy). Last step of training is performing an optimiser step, which changes the networks heights.\n",
    "\n",
    "__eval__ evaluates the results on a validation set, should be done periodically during training to check for overfitting.\n",
    "\n",
    "__train_full__ performs the full training loop. It first initialises the model and optimiser. Then the train function is called in a loop, with periodic evaluation on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "    \"\"\"train for one epoch\"\"\"\n",
    "    model.train() #switch the model in training mode\n",
    "  \n",
    "    #the loader function will take care of the batching\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args['batch_size'], sampler=args['train_subsampler'])\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    #will keep track of the loss\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "        optimizer.zero_grad() #put gradient to zero\n",
    "\n",
    "        tiles, gt = augment(tiles, gt)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            pred = model(tiles.cuda()) #compute the prediction\n",
    "        else:\n",
    "            pred = model(tiles)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=args['class_weights'])\n",
    "        loss.backward() #compute gradients\n",
    "\n",
    "        for p in model.parameters(): #we clip the gradient at norm 1\n",
    "            p.grad.data.clamp_(-1, 1) #this helps learning faster\n",
    "\n",
    "        optimizer.step() #one SGD step\n",
    "        loss_meter.add(loss.item())\n",
    "        \n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "def eval(model, sampler):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "  \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=sampler)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                pred = model(tiles.cuda()) #compute the prediction\n",
    "            else:\n",
    "                pred = model(tiles)\n",
    "\n",
    "            loss = nn.functional.cross_entropy(pred.cpu(), gt)\n",
    "            loss_meter.add(loss.item())\n",
    "\n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "    \"\"\"The full training loop\"\"\"\n",
    "\n",
    "    #initialize the model\n",
    "    model = SpatialNet(args)\n",
    "\n",
    "    print(f'Total number of parameters: {sum([p.numel() for p in model.parameters()])}')\n",
    "  \n",
    "    #define the Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=args['scheduler_milestones'],\n",
    "                                               gamma=args['scheduler_gamma'])\n",
    "  \n",
    "    train_loss = np.empty(args['n_epoch'])\n",
    "    test_epochs = []\n",
    "    test_loss = []\n",
    "\n",
    "    for i_epoch in range(args['n_epoch']):\n",
    "        #train one epoch\n",
    "        print(f'Epoch #{str(i_epoch+1)}')\n",
    "        train_loss[i_epoch] = train(model, optimizer, args)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Periodic testing on the validation set\n",
    "        if (i_epoch == args['n_epoch'] - 1) or ((i_epoch + 1) % args['n_epoch_test'] == 0):\n",
    "            print('Evaluation')\n",
    "            loss_test = eval(model, args['test_subsampler'])\n",
    "            test_epochs.append(i_epoch + 1)\n",
    "            test_loss.append(loss_test)\n",
    "            \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1,1,1,ylim=(0,5), xlabel='Epoch #', ylabel='Loss')\n",
    "    plt.plot([i+1 for i in range(args['n_epoch'])], train_loss, label='Training loss')\n",
    "    plt.plot(test_epochs, test_loss, label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(train_loss)\n",
    "    print(test_loss)\n",
    "    args['loss_test'] = test_loss[-1]\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Hyperparameter definition\n",
    "Training networks requires first setting several hyperparameters, please feel free to play around with them and try different values for the number of training epochs, learning rate or batch size.\n",
    "\n",
    "- __n_channel__ - number of channels, set to 1 for our task\n",
    "- __n_class__ - number of classification classes\n",
    "- __size_e__ - number of filters in each NN layer of the encoder\n",
    "- __size_d__ - number of filters in each NN layer of the decoder\n",
    "- __crossval_nfolds__ - Number of folds for crossvalidation\n",
    "- __n_epoch_test__ - after how many training epochs to evaluate on the validation set\n",
    "- __scheduler_milestones__ - after how many epochs do we reduce the training rate\n",
    "- __scheduler_gamma__ - by what factor do we reduce the training rate\n",
    "- __class_weights__ - training weights for individual classes, used to offset imbalanced class distribution\n",
    "\n",
    "- __n_epoch__ - how many epochs are performed during training\n",
    "- __lr__ - how fast can individual network parameters change during one training epoch\n",
    "- __batch_size__ - how many tiles should be included in each gradient descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = { # Dict to store all model parameters\n",
    "    'n_channel': 54,\n",
    "    'n_class': len(unique),\n",
    "    'size_e': [64,64,128,128,256,256],\n",
    "    'size_d': [256,128,128,128,64,64],\n",
    "    \n",
    "    'crossval_nfolds': 3,\n",
    "    'n_epoch_test': 2,\n",
    "    'scheduler_milestones': [50,75,90],\n",
    "    'scheduler_gamma': 0.3,\n",
    "    #'class_weights': torch.tensor([0.0, 0.2, 0.34, 0.033, 0.16, 0.14, 0.03, 0.014, 0.023, 0.06]),\n",
    "    'class_weights': torch.tensor([.0, .15, .05, .3, .05, .05, .1, .3]),\n",
    "    \n",
    "    'n_epoch': 50,\n",
    "    'lr': 1e-6,\n",
    "    'batch_size': 4,\n",
    "}\n",
    "\n",
    "print(f'''Number of models to be trained:\n",
    "    {args['crossval_nfolds']}\n",
    "Number of spectral channels:\n",
    "    {args['n_channel']}\n",
    "Initial learning rate:\n",
    "    {args['lr']}\n",
    "Batch size:\n",
    "    {args['batch_size']}\n",
    "Number of training epochs:\n",
    "    {args['n_epoch']}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Network training\n",
    "\n",
    "Run the training procedure using set hyperparameters, evaluate the training/validation loss graphs produced during training to adjust hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a 2D network\n",
    "kfold = KFold(n_splits = args['crossval_nfolds'], shuffle=True)\n",
    "trained_models = []\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'Training starts for model number {str(fold+1)}')\n",
    "    \n",
    "    a = perf_counter()\n",
    "    args['train_subsampler'] = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    args['test_subsampler'] = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    trained_models.append((train_full(args), args['loss_test']))\n",
    "    \n",
    "    state_dict_path = join(model_save_folder, f'2D_CNN_fold_{str(fold)}.pt')\n",
    "    torch.save(trained_models[fold][0].state_dict(), state_dict_path)\n",
    "    print(f'Model saved to: {state_dict_path}')\n",
    "    print(f'Training finished in {str(perf_counter()-a)}s')\n",
    "    print('\\n\\n')\n",
    "\n",
    "print(f'Resulting loss for individual folds: \\n{[i for _, i in trained_models]}')\n",
    "print(f'Mean loss across all folds: \\n{np.mean([i for _, i in trained_models])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model definition\n",
    "args = {\n",
    "    'n_class': 8,\n",
    "    'n_channel': 54,\n",
    "    'size_e': [64,64,128,128,256,256],\n",
    "    'size_d': [256,128,128,128,64,64]\n",
    "}\n",
    "\n",
    "# Select which model to load by using a different filename index\n",
    "state_dict_path = join(model_save_folder, '2D_CNN_fold_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = SpatialNet(args)\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loading and preprocessing the data\n",
    "\n",
    "Load a raster to classify. This can be the one that we used for training, but it can also be a different raster with the same number of bands.\n",
    "\n",
    "By default, the training raster (_imagery_path_) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_shape = (128, 128)\n",
    "overlap = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "# Load raster\n",
    "raster_orig = image_preprocessing.read_gdal_with_geoinfo(imagery_path)\n",
    "#raster_orig = image_preprocessing.read_pavia_centre(imagery_path, out_shape=(1088, 1088, 102))\n",
    "\n",
    "# Split raster into tiles\n",
    "dataset_full_tiles = image_preprocessing.run_tiling_dims(raster_orig['imagery'],\n",
    "                         out_shape=tile_shape, out_overlap=overlap)\n",
    "# Normalize tiles\n",
    "dataset_full = image_preprocessing.normalize_tiles(dataset_full_tiles,\n",
    "                                                   nodata_vals=[0])\n",
    "# Convert to Pytorch TensorDataset\n",
    "dataset = tnt.dataset.TensorDataset(dataset_full['imagery'])\n",
    "\n",
    "print('')\n",
    "print(f'Loading and preprocessing the imagery took {visualisation_utils.sec_to_hms(perf_counter() - start)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Applying the CNN\n",
    "The following snippet applies the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timer_start = perf_counter()\n",
    "arr_class = inference_utils.combine_tiles_2d(model, dataset,\n",
    "                                             tile_shape, overlap,\n",
    "                                             dataset_full_tiles['dimensions'])\n",
    "print(f'Model applied to all tiles in {visualisation_utils.sec_to_hms(perf_counter() - timer_start)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualise the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualisation_utils.show_classified(raster_orig['imagery'][:, :, [25, 15, 5]],\n",
    "                                    loaded_raster['reference'], arr_class,\n",
    "                                    ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Export Resulting Raster\n",
    "\n",
    "Export the resulting classified raster into _out_path_ for distribution or further analysis (e.g. validation in GIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The classified result gets saved to {out_path}')\n",
    "inference_utils.export_result(out_path, arr_class, raster_orig['geoinfo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Classification Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Apply classifier to the test dataset\n",
    "\n",
    "The classified raster is comapred to the test reference data. The reference data is saved in the _test_ dictionary with keys _imagery_ and _reference_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_arr = inference_utils.classify_tiles(model, test['imagery'], mode='2D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping both rasters to 1D\n",
    "test_flat = test['reference'].reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])\n",
    "pred_flat = predicted_arr.reshape(test['reference'].shape[0] * test['reference'].shape[1] * test['reference'].shape[2])\n",
    "\n",
    "# Filtering to only include pixels with reference data\n",
    "pred_filtered = pred_flat[test_flat > 0]\n",
    "test_filtered = test_flat[test_flat > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Compute Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_filtered, pred_filtered, zero_division=0,\n",
    "                            target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Show Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualisation_utils.show_confusion_matrix(test_filtered, pred_filtered,\n",
    "                                          ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Solution\n",
    "\n",
    "We have generated this result using these training parameters (please note that just using the same training parameters will not yield the same result, as the network is randomly initailised):\n",
    "\n",
    "- Number of epochs: 100\n",
    "- Batch Size: 4\n",
    "- Learning Rate: 1e-6\n",
    "- Learning Rate reduced after Epochs #: [50,75,90]\n",
    "- Learning Rate reduced by (scheduler_gamma): 0.3\n",
    "- Class Weights:\n",
    "        [0, 0.197, 0.343, 0.032,\n",
    "         0.166, 0.141, 0.029,\n",
    "         0.013, 0.022, 0.057]\n",
    "- Number of Convolutional filters in the Encoder (_size_e_):  \n",
    "        [256,256,512,512,1024,1024]\n",
    "- Number of Convolutional filters in the Decoder (_size_d_):  \n",
    "        [1024,512,512,512,256,256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test reference\n",
    "test_arr = image_preprocessing.read_gdal(imagery_path, test_path)\n",
    "test_flat = test_arr['reference'].reshape(\n",
    "    test_arr['reference'].shape[0]*test_arr['reference'].shape[1])\n",
    "test_filtered = test_flat[test_flat > 0]\n",
    "\n",
    "# Read sample result\n",
    "sample_arr = image_preprocessing.read_gdal(imagery_path, sample_result_path)\n",
    "sample_flat = sample_arr['reference'].reshape(sample_arr['reference'].shape[0] * sample_arr['reference'].shape[1])\n",
    "sample_filtered = sample_flat[test_flat > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the sample result\n",
    "visualisation_utils.show_classified(loaded_raster['imagery'][:, :, [25, 15, 5]],\n",
    "                                    loaded_raster['reference'],\n",
    "                                    sample_arr['reference'],\n",
    "                                    ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a classification report for the sample result\n",
    "print(classification_report(test_filtered, sample_filtered,\n",
    "                            target_names=class_names[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a Confusion matrix for the sample result\n",
    "visualisation_utils.show_confusion_matrix(test_filtered, sample_filtered,\n",
    "                                          ds_name=ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional:* Classify a urban scene\n",
    "\n",
    "Try using this notebook to classify a urban scene (Pavia City Centre). Reflect on how the different landscape structure and clearer class definitions influence the classification result.\n",
    "\n",
    "Pavia city centre is a common benchmark for hyperspectral data classification and can be obtained from [http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University). You will need to change the paths to input data and use the *read_pavia_centre* method to load the matlab matrices into numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [exercises](04_time_series_specifics_exercise.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
